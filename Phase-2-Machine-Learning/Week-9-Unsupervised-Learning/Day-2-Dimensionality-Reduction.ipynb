{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ðŸ“ Day 2: Dimensionality Reduction\n",
    "\n",
    "**ðŸŽ¯ Goal:** Master techniques to visualize and compress high-dimensional data\n",
    "\n",
    "**â±ï¸ Time:** 60-75 minutes\n",
    "\n",
    "**ðŸŒŸ Why This Matters for AI:**\n",
    "- OpenAI embeddings are 1536-dimensional â†’ you need to visualize them!\n",
    "- RAG systems: reduce embedding dimensions for faster search\n",
    "- Transformers produce high-dimensional representations â†’ understand them visually\n",
    "- Used by OpenAI, Anthropic, Google to visualize model behaviors\n",
    "- Essential for multimodal AI: reduce image, text, and audio embeddings\n",
    "- Powers data exploration, feature engineering, and model interpretability\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ðŸ¤” What is Dimensionality Reduction?\n",
    "\n",
    "**The curse of dimensionality:**\n",
    "- OpenAI's `text-embedding-3-small`: **1536 dimensions**\n",
    "- Google's `text-embedding-gecko`: **768 dimensions**\n",
    "- CLIP image embeddings: **512-1024 dimensions**\n",
    "\n",
    "**Problem:** Humans can only visualize 2-3 dimensions!\n",
    "\n",
    "**Solution:** Dimensionality reduction!\n",
    "- Compress 1536D â†’ 2D or 3D for visualization\n",
    "- Reduce 1536D â†’ 256D for faster computation\n",
    "- Keep the most important information, discard noise\n",
    "\n",
    "**Real-world analogy:**\n",
    "- Imagine a 3D object (like a statue)\n",
    "- Take a photo â†’ you reduced 3D to 2D\n",
    "- You lost depth info, but kept the essential shape!\n",
    "\n",
    "**We'll learn 3 powerful techniques:**\n",
    "1. **PCA** (Principal Component Analysis) - Linear, fast, interpretable\n",
    "2. **t-SNE** - Non-linear, preserves local structure, great for visualization\n",
    "3. **UMAP** - Modern, faster than t-SNE, preserves global + local structure\n",
    "\n",
    "Let's dive in! ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_digits, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# UMAP (install with: pip install umap-learn)\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    UMAP_AVAILABLE = False\n",
    "    print(\"âš ï¸  UMAP not installed. Run: pip install umap-learn\")\n",
    "\n",
    "# Set random seed and style\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"ðŸ“Š Ready to reduce dimensions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Technique 1: PCA (Principal Component Analysis)\n",
    "\n",
    "**PCA** = Find the directions (principal components) that capture the most variance in your data.\n",
    "\n",
    "### How it works:\n",
    "1. **Standardize** the data (mean=0, std=1)\n",
    "2. **Compute covariance** matrix (how features vary together)\n",
    "3. **Find eigenvectors** (directions of maximum variance)\n",
    "4. **Project data** onto top K eigenvectors\n",
    "\n",
    "**Simple analogy:**\n",
    "- Imagine a long, thin cloud of 3D points (like a cigar)\n",
    "- PCA finds the \"long axis\" (PC1 - most variance)\n",
    "- Then finds the \"medium axis\" (PC2)\n",
    "- Then the \"short axis\" (PC3)\n",
    "- You keep PC1 & PC2, discard PC3 â†’ 3D to 2D!\n",
    "\n",
    "### Strengths:\n",
    "- âœ… Fast and scalable (works on millions of points)\n",
    "- âœ… Interpretable (components have meaning)\n",
    "- âœ… Reversible (can reconstruct original data)\n",
    "- âœ… Deterministic (same result every time)\n",
    "\n",
    "### Weaknesses:\n",
    "- âŒ Linear only (assumes linear relationships)\n",
    "- âŒ Sensitive to scale (must standardize)\n",
    "- âŒ May not preserve local structure well\n",
    "\n",
    "### Real AI Use (2024-2025):\n",
    "- **RAG optimization:** Reduce embedding dimensions (1536D â†’ 256D) for faster search\n",
    "- **Feature engineering:** Create compact representations\n",
    "- **Noise reduction:** Remove noisy dimensions\n",
    "- **Data compression:** Store embeddings more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load handwritten digits dataset (64 dimensions = 8x8 pixel images)\n",
    "digits = load_digits()\n",
    "X_digits = digits.data  # Shape: (1797, 64) - 1797 images, 64 pixels each\n",
    "y_digits = digits.target  # Labels: 0-9\n",
    "\n",
    "print(\"ðŸ“Š Original Data:\")\n",
    "print(f\"   Shape: {X_digits.shape}\")\n",
    "print(f\"   Dimensions: {X_digits.shape[1]} (one per pixel!)\")\n",
    "print(f\"   Samples: {X_digits.shape[0]} handwritten digits\")\n",
    "print(f\"   Classes: {len(np.unique(y_digits))} (digits 0-9)\")\n",
    "\n",
    "# Visualize some original digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_digits[i].reshape(8, 8), cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_digits[i]}\", fontsize=12)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Original Digits (8x8 = 64 dimensions)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Challenge: Can we visualize these 64D points in 2D and still tell digits apart?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA: 64D â†’ 2D\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization\n",
    "X_pca = pca.fit_transform(X_digits)\n",
    "\n",
    "print(\"ðŸŽ¯ PCA Results:\")\n",
    "print(f\"   Original dimensions: {X_digits.shape[1]}\")\n",
    "print(f\"   Reduced dimensions: {X_pca.shape[1]}\")\n",
    "print(f\"   Variance explained by PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"   Variance explained by PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "print(f\"   Total variance retained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Visualize PCA projection\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                     c=y_digits, cmap='tab10', \n",
    "                     s=50, alpha=0.7, edgecolors='black', linewidths=0.5)\n",
    "plt.colorbar(scatter, label='Digit', ticks=range(10))\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "plt.title('PCA: 64D â†’ 2D Projection of Handwritten Digits', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Insights:\")\n",
    "print(\"   - Similar digits cluster together (e.g., all 1s in one region)\")\n",
    "print(\"   - Some overlap (e.g., 3 and 8 can look similar)\")\n",
    "print(\"   - We kept ~30% of variance with just 2 dimensions!\")\n",
    "print(\"   - This helps us 'see' patterns in 64-dimensional space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### ðŸ” Choosing Number of Components: Explained Variance\n",
    "\n",
    "**Question:** How many components should we keep?\n",
    "\n",
    "**Answer:** Look at **explained variance**!\n",
    "- Each component captures some % of variance\n",
    "- Plot cumulative variance\n",
    "- Choose K where you hit 90-95% variance\n",
    "\n",
    "**Rule of thumb:**\n",
    "- Visualization: 2-3 components\n",
    "- Compression: 80-95% variance\n",
    "- Noise reduction: 95-99% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with all components to see explained variance\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_digits)\n",
    "\n",
    "# Calculate cumulative variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Individual variance per component\n",
    "axes[0].bar(range(1, 21), pca_full.explained_variance_ratio_[:20], alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[0].set_ylabel('Variance Explained', fontsize=12)\n",
    "axes[0].set_title('Variance Explained by Each Component (First 20)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \n",
    "            marker='o', linewidth=2, markersize=4, color='darkgreen')\n",
    "axes[1].axhline(y=0.90, color='red', linestyle='--', label='90% variance', linewidth=2)\n",
    "axes[1].axhline(y=0.95, color='orange', linestyle='--', label='95% variance', linewidth=2)\n",
    "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Variance Explained', fontsize=12)\n",
    "axes[1].set_title('Cumulative Variance: How Many Components?', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find components needed for 90% and 95%\n",
    "n_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "n_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(\"ðŸ“Š Component Selection Guide:\")\n",
    "print(f\"   Components for 90% variance: {n_90} (reduced from 64!)\")\n",
    "print(f\"   Components for 95% variance: {n_95}\")\n",
    "print(f\"   Compression ratio (90%): {64/n_90:.1f}x smaller!\")\n",
    "print(\"\\nðŸ’¡ For RAG embeddings (1536D):\")\n",
    "print(\"   If similar pattern: 1536D â†’ ~200D for 90% variance\")\n",
    "print(\"   That's 7.5x faster similarity search! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## ðŸŒ€ Technique 2: t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "**t-SNE** = Non-linear method that preserves **local structure** (nearby points stay nearby).\n",
    "\n",
    "### How it works (simplified):\n",
    "1. **High-dimensional space:** Calculate probability that points are neighbors\n",
    "2. **Low-dimensional space:** Initialize random positions\n",
    "3. **Optimize:** Move points to match high-dim neighborhood probabilities\n",
    "4. **Result:** Points that were close in 64D stay close in 2D!\n",
    "\n",
    "### Key parameter: Perplexity\n",
    "- **Perplexity** â‰ˆ \"How many neighbors to focus on?\"\n",
    "- Range: 5-50 (typical: 30)\n",
    "- Lower: Focus on very local structure\n",
    "- Higher: Consider broader structure\n",
    "\n",
    "### Strengths:\n",
    "- âœ… Excellent for visualization (creates beautiful clusters)\n",
    "- âœ… Preserves local structure (similar items stay together)\n",
    "- âœ… Non-linear (captures complex relationships)\n",
    "- âœ… Reveals hidden patterns\n",
    "\n",
    "### Weaknesses:\n",
    "- âŒ Slow (doesn't scale to millions of points)\n",
    "- âŒ Non-deterministic (different results each run)\n",
    "- âŒ Can't transform new data (no .transform() method)\n",
    "- âŒ Distances not meaningful (only clusters matter)\n",
    "- âŒ Global structure not preserved\n",
    "\n",
    "### Real AI Use:\n",
    "- **Visualizing embeddings:** OpenAI/Anthropic use t-SNE to visualize model behavior\n",
    "- **Exploratory analysis:** Understand high-dimensional data\n",
    "- **Debugging models:** See what the model learned\n",
    "- **Research papers:** Almost every AI paper has a t-SNE plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE: 64D â†’ 2D\n",
    "# Note: t-SNE is slower, so we'll use a subset for demo\n",
    "subset_size = 1000\n",
    "X_subset = X_digits[:subset_size]\n",
    "y_subset = y_digits[:subset_size]\n",
    "\n",
    "print(\"â³ Running t-SNE (this may take 10-30 seconds)...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_subset)\n",
    "print(\"âœ… t-SNE complete!\")\n",
    "\n",
    "# Visualize t-SNE\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                     c=y_subset, cmap='tab10',\n",
    "                     s=50, alpha=0.7, edgecolors='black', linewidths=0.5)\n",
    "plt.colorbar(scatter, label='Digit', ticks=range(10))\n",
    "plt.xlabel('t-SNE Component 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Component 2', fontsize=12)\n",
    "plt.title('t-SNE: 64D â†’ 2D (Better Clusters than PCA!)', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Compare with PCA:\")\n",
    "print(\"   - t-SNE creates tighter, more separated clusters\")\n",
    "print(\"   - Each digit forms a clear 'island'\")\n",
    "print(\"   - Some overlap where digits are ambiguous (e.g., 4 and 9)\")\n",
    "print(\"   - Better for visualization, worse for speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### ðŸŽ¨ Effect of Perplexity\n",
    "\n",
    "Let's see how perplexity affects t-SNE results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different perplexity values\n",
    "perplexities = [5, 30, 50]\n",
    "X_small = X_digits[:300]  # Smaller subset for speed\n",
    "y_small = y_digits[:300]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, perp in enumerate(perplexities):\n",
    "    print(f\"â³ Running t-SNE with perplexity={perp}...\")\n",
    "    tsne_temp = TSNE(n_components=2, perplexity=perp, random_state=42, n_iter=1000)\n",
    "    X_tsne_temp = tsne_temp.fit_transform(X_small)\n",
    "    \n",
    "    scatter = axes[i].scatter(X_tsne_temp[:, 0], X_tsne_temp[:, 1],\n",
    "                            c=y_small, cmap='tab10', s=50, alpha=0.7,\n",
    "                            edgecolors='black', linewidths=0.5)\n",
    "    axes[i].set_title(f'Perplexity = {perp}', fontsize=14, fontweight='bold')\n",
    "    axes[i].set_xlabel('t-SNE 1', fontsize=11)\n",
    "    axes[i].set_ylabel('t-SNE 2', fontsize=11)\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.colorbar(scatter, ax=axes, label='Digit')\n",
    "plt.suptitle('Effect of Perplexity on t-SNE', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"âœ… Done!\")\n",
    "\n",
    "print(\"\\nðŸ“Š Perplexity Guide:\")\n",
    "print(\"   - Low (5-10): Very tight local clusters, may split natural groups\")\n",
    "print(\"   - Medium (20-50): Balanced, typical choice\")\n",
    "print(\"   - High (50+): Broader structure, may merge clusters\")\n",
    "print(\"   - Rule of thumb: Start with 30, adjust based on data size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## ðŸš€ Technique 3: UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "**UMAP** = Modern alternative to t-SNE (2018). Best of both worlds!\n",
    "\n",
    "### Why UMAP?\n",
    "- **Faster** than t-SNE (10-100x speedup)\n",
    "- **Preserves global structure** better than t-SNE\n",
    "- **Preserves local structure** like t-SNE\n",
    "- **Can transform new data** (unlike t-SNE)\n",
    "- **Scales to millions** of points\n",
    "\n",
    "### Key parameters:\n",
    "- **n_neighbors:** Like perplexity (5-50, default=15)\n",
    "- **min_dist:** Minimum distance between points (0.0-0.99, default=0.1)\n",
    "  - Lower = tighter clusters\n",
    "  - Higher = more spread out\n",
    "\n",
    "### Strengths:\n",
    "- âœ… Fast and scalable\n",
    "- âœ… Preserves both local AND global structure\n",
    "- âœ… Can transform new data\n",
    "- âœ… Beautiful visualizations\n",
    "- âœ… Works well for clustering\n",
    "\n",
    "### Weaknesses:\n",
    "- âŒ Newer, less understood theoretically\n",
    "- âŒ More parameters to tune\n",
    "\n",
    "### Real AI Use (2024-2025):\n",
    "- **OpenAI/Anthropic:** Visualize embeddings from GPT-4, Claude\n",
    "- **Vector databases:** Create 2D maps of knowledge bases\n",
    "- **Multimodal AI:** Visualize image+text embeddings together\n",
    "- **Research:** Standard tool for embedding visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if UMAP_AVAILABLE:\n",
    "    # Apply UMAP: 64D â†’ 2D\n",
    "    print(\"â³ Running UMAP (much faster than t-SNE!)...\")\n",
    "    umap_model = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "    X_umap = umap_model.fit_transform(X_digits)\n",
    "    print(\"âœ… UMAP complete!\")\n",
    "    \n",
    "    # Visualize UMAP\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1],\n",
    "                         c=y_digits, cmap='tab10',\n",
    "                         s=50, alpha=0.7, edgecolors='black', linewidths=0.5)\n",
    "    plt.colorbar(scatter, label='Digit', ticks=range(10))\n",
    "    plt.xlabel('UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Component 2', fontsize=12)\n",
    "    plt.title('UMAP: 64D â†’ 2D (Fast + Preserves Structure!)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ’¡ UMAP Advantages:\")\n",
    "    print(\"   - Clearer clusters than PCA\")\n",
    "    print(\"   - Faster than t-SNE (especially on large datasets)\")\n",
    "    print(\"   - Preserves global structure better (relative positions meaningful)\")\n",
    "    print(\"   - Can transform new data (useful for production systems)\")\n",
    "else:\n",
    "    print(\"âš ï¸  UMAP not available. Install with: pip install umap-learn\")\n",
    "    print(\"   Skipping UMAP demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### ðŸ“Š PCA vs t-SNE vs UMAP: Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three methods side-by-side\n",
    "subset_size = 1000\n",
    "X_compare = X_digits[:subset_size]\n",
    "y_compare = y_digits[:subset_size]\n",
    "\n",
    "# We already have PCA and t-SNE results, let's create them fresh for fair comparison\n",
    "print(\"â³ Computing all three methods for comparison...\")\n",
    "\n",
    "# PCA\n",
    "pca_comp = PCA(n_components=2)\n",
    "X_pca_comp = pca_comp.fit_transform(X_compare)\n",
    "\n",
    "# t-SNE\n",
    "tsne_comp = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne_comp = tsne_comp.fit_transform(X_compare)\n",
    "\n",
    "# UMAP\n",
    "if UMAP_AVAILABLE:\n",
    "    umap_comp = umap.UMAP(n_components=2, n_neighbors=15, random_state=42)\n",
    "    X_umap_comp = umap_comp.fit_transform(X_compare)\n",
    "\n",
    "print(\"âœ… Done!\\n\")\n",
    "\n",
    "# Plot comparison\n",
    "n_methods = 3 if UMAP_AVAILABLE else 2\n",
    "fig, axes = plt.subplots(1, n_methods, figsize=(6*n_methods, 5))\n",
    "\n",
    "# PCA\n",
    "scatter = axes[0].scatter(X_pca_comp[:, 0], X_pca_comp[:, 1],\n",
    "                         c=y_compare, cmap='tab10', s=30, alpha=0.6)\n",
    "axes[0].set_title('PCA\\n(Fast, Linear)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('PC1', fontsize=11)\n",
    "axes[0].set_ylabel('PC2', fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# t-SNE\n",
    "axes[1].scatter(X_tsne_comp[:, 0], X_tsne_comp[:, 1],\n",
    "               c=y_compare, cmap='tab10', s=30, alpha=0.6)\n",
    "axes[1].set_title('t-SNE\\n(Slow, Local Structure)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('t-SNE 1', fontsize=11)\n",
    "axes[1].set_ylabel('t-SNE 2', fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# UMAP\n",
    "if UMAP_AVAILABLE:\n",
    "    axes[2].scatter(X_umap_comp[:, 0], X_umap_comp[:, 1],\n",
    "                   c=y_compare, cmap='tab10', s=30, alpha=0.6)\n",
    "    axes[2].set_title('UMAP\\n(Fast, Local+Global)', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('UMAP 1', fontsize=11)\n",
    "    axes[2].set_ylabel('UMAP 2', fontsize=11)\n",
    "    axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.colorbar(scatter, ax=axes, label='Digit', ticks=range(10))\n",
    "plt.suptitle('Dimensionality Reduction: Method Comparison', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Visual Comparison Summary:\")\n",
    "print(\"\\nðŸ”µ PCA:\")\n",
    "print(\"   âœ… Fast, deterministic, interpretable\")\n",
    "print(\"   âŒ Clusters overlap (linear limitation)\")\n",
    "print(\"   ðŸ’¡ Use for: Quick exploration, compression, noise reduction\")\n",
    "\n",
    "print(\"\\nðŸŸ¢ t-SNE:\")\n",
    "print(\"   âœ… Beautiful, tight clusters\")\n",
    "print(\"   âŒ Slow, non-deterministic\")\n",
    "print(\"   ðŸ’¡ Use for: Final visualizations, presentations, papers\")\n",
    "\n",
    "if UMAP_AVAILABLE:\n",
    "    print(\"\\nðŸŸ£ UMAP:\")\n",
    "    print(\"   âœ… Fast, good clusters, preserves structure\")\n",
    "    print(\"   âœ… Can transform new data\")\n",
    "    print(\"   ðŸ’¡ Use for: Production, large datasets, general purpose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## ðŸ¤– Real AI Example: Visualizing Transformer Embeddings\n",
    "\n",
    "**Scenario:** You're building a RAG system using OpenAI embeddings.\n",
    "- Embedded 500 documents using `text-embedding-3-small` (1536 dimensions)\n",
    "- Goal: Visualize document relationships and clusters\n",
    "\n",
    "Let's simulate this! (In practice, you'd use actual OpenAI API embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate document embeddings (in reality: from OpenAI API)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create 5 topic clusters in high-dimensional space\n",
    "n_dims = 128  # Simulating high-dimensional embeddings (real: 1536)\n",
    "topics = {\n",
    "    'Machine Learning': {'n': 100, 'center': np.random.randn(n_dims) * 2 + 5},\n",
    "    'Web Development': {'n': 100, 'center': np.random.randn(n_dims) * 2 - 5},\n",
    "    'Data Science': {'n': 100, 'center': np.random.randn(n_dims) * 2 + np.array([3]*n_dims)},\n",
    "    'DevOps': {'n': 100, 'center': np.random.randn(n_dims) * 2 - np.array([3]*n_dims)},\n",
    "    'AI Research': {'n': 100, 'center': np.random.randn(n_dims) * 2}\n",
    "}\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for topic_name, topic_info in topics.items():\n",
    "    # Create docs around topic center\n",
    "    topic_embeddings = np.random.randn(topic_info['n'], n_dims) * 1.5 + topic_info['center']\n",
    "    embeddings.append(topic_embeddings)\n",
    "    labels.extend([topic_name] * topic_info['n'])\n",
    "\n",
    "X_docs = np.vstack(embeddings)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Create numeric labels for coloring\n",
    "label_map = {topic: i for i, topic in enumerate(topics.keys())}\n",
    "y_docs = np.array([label_map[label] for label in labels])\n",
    "\n",
    "print(\"ðŸ“š Simulated RAG Document Embeddings\")\n",
    "print(f\"   Documents: {X_docs.shape[0]}\")\n",
    "print(f\"   Embedding dimensions: {X_docs.shape[1]} (real OpenAI: 1536)\")\n",
    "print(f\"   Topics: {list(topics.keys())}\")\n",
    "print(f\"\\nâ³ Reducing dimensions for visualization...\")\n",
    "\n",
    "# Apply dimensionality reduction\n",
    "# Step 1: PCA for initial compression (128D â†’ 50D)\n",
    "pca_compress = PCA(n_components=50)\n",
    "X_compressed = pca_compress.fit_transform(X_docs)\n",
    "print(f\"   Step 1: PCA {X_docs.shape[1]}D â†’ 50D (kept {pca_compress.explained_variance_ratio_.sum():.1%} variance)\")\n",
    "\n",
    "# Step 2: UMAP for visualization (50D â†’ 2D)\n",
    "if UMAP_AVAILABLE:\n",
    "    umap_viz = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "    X_viz = umap_viz.fit_transform(X_compressed)\n",
    "    method_name = 'UMAP'\n",
    "else:\n",
    "    # Fallback to t-SNE if UMAP not available\n",
    "    tsne_viz = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    X_viz = tsne_viz.fit_transform(X_compressed)\n",
    "    method_name = 't-SNE'\n",
    "\n",
    "print(f\"   Step 2: {method_name} 50D â†’ 2D\")\n",
    "print(\"âœ… Visualization ready!\\n\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 10))\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(topics)))\n",
    "\n",
    "for i, (topic_name, color) in enumerate(zip(topics.keys(), colors)):\n",
    "    mask = (y_docs == i)\n",
    "    plt.scatter(X_viz[mask, 0], X_viz[mask, 1],\n",
    "               c=[color], label=topic_name, s=60, alpha=0.7,\n",
    "               edgecolors='black', linewidths=0.5)\n",
    "\n",
    "plt.xlabel(f'{method_name} Component 1', fontsize=13)\n",
    "plt.ylabel(f'{method_name} Component 2', fontsize=13)\n",
    "plt.title('RAG System: Document Embedding Visualization\\n(Simulating OpenAI text-embedding-3-small)', \n",
    "         fontsize=16, fontweight='bold')\n",
    "plt.legend(title='Document Topic', fontsize=11, title_fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸŽ¯ RAG INSIGHTS FROM VISUALIZATION:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n1ï¸âƒ£  Topic Clustering:\")\n",
    "print(\"   âœ… Clear separation between topics\")\n",
    "print(\"   âœ… 'Machine Learning' and 'AI Research' are closer (related topics!)\")\n",
    "print(\"   âœ… 'Web Development' and 'DevOps' are closer (related topics!)\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£  Quality Assessment:\")\n",
    "print(\"   âœ… Tight clusters = Consistent topic coverage\")\n",
    "print(\"   âš ï¸  Scattered points = Diverse or outlier documents\")\n",
    "print(\"   âš ï¸  Overlapping clusters = May need better documentation or categories\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£  RAG Optimization Opportunities:\")\n",
    "print(\"   ðŸ’¡ Use cluster info to pre-filter searches\")\n",
    "print(\"   ðŸ’¡ Sample diverse results from different clusters\")\n",
    "print(\"   ðŸ’¡ Identify gaps in documentation (sparse regions)\")\n",
    "print(\"   ðŸ’¡ Find redundant/duplicate docs (very close points)\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£  Production Benefits:\")\n",
    "print(\"   ðŸš€ Visual debugging: See what your RAG is 'seeing'\")\n",
    "print(\"   ðŸš€ Content gaps: Find missing topics\")\n",
    "print(\"   ðŸš€ Quality control: Spot outliers and errors\")\n",
    "print(\"   ðŸš€ User interface: Create interactive doc maps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ YOUR TURN: Compress RAG Embeddings Challenge\n",
    "\n",
    "**Real-world scenario:**\n",
    "Your RAG system has 10,000 documents with 1536-dimensional OpenAI embeddings.\n",
    "Similarity search is too slow!\n",
    "\n",
    "**Your task:**\n",
    "1. Simulate high-dimensional embeddings\n",
    "2. Use PCA to compress them while retaining 95% variance\n",
    "3. Measure the compression ratio\n",
    "4. Estimate speed improvement\n",
    "5. Visualize with UMAP or t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Complete this exercise!\n",
    "\n",
    "# Step 1: Simulate OpenAI embeddings (1536 dimensions)\n",
    "n_docs = 1000  # 1000 documents for faster computation\n",
    "embedding_dim = 1536  # OpenAI text-embedding-3-small dimension\n",
    "\n",
    "# Create synthetic embeddings with some structure\n",
    "# YOUR CODE - create random embeddings with make_blobs or random data\n",
    "X_embeddings = # YOUR CODE\n",
    "\n",
    "print(f\"ðŸ“Š Original embeddings: {X_embeddings.shape}\")\n",
    "\n",
    "# Step 2: Apply PCA to find 95% variance threshold\n",
    "pca_compress = PCA(n_components=# YOUR CODE)  # What should this be?\n",
    "pca_compress.fit(X_embeddings)\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "cumsum_variance = # YOUR CODE\n",
    "n_components_95 = # YOUR CODE - find where cumsum reaches 0.95\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Compression Analysis:\")\n",
    "print(f\"   Components for 95% variance: {n_components_95}\")\n",
    "print(f\"   Compression ratio: {embedding_dim / n_components_95:.2f}x\")\n",
    "\n",
    "# Step 3: Compress embeddings\n",
    "pca_final = PCA(n_components=n_components_95)\n",
    "X_compressed = # YOUR CODE\n",
    "\n",
    "print(f\"   Original size: {X_embeddings.shape}\")\n",
    "print(f\"   Compressed size: {X_compressed.shape}\")\n",
    "\n",
    "# Step 4: Estimate speed improvement\n",
    "# Similarity search complexity is O(d) where d = dimensions\n",
    "speed_improvement = # YOUR CODE - embedding_dim / n_components_95\n",
    "print(f\"\\nâš¡ Estimated speed improvement: {speed_improvement:.2f}x faster!\")\n",
    "\n",
    "# Step 5: Visualize with UMAP or t-SNE\n",
    "# YOUR CODE - apply UMAP or t-SNE to compressed embeddings\n",
    "# Then plot the results\n",
    "\n",
    "print(\"\\nðŸ’¡ Would you use this compression in production? Why or why not?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### âœ… Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: RAG Embedding Compression\n",
    "\n",
    "# Step 1: Simulate OpenAI embeddings\n",
    "np.random.seed(42)\n",
    "n_docs = 1000\n",
    "embedding_dim = 1536\n",
    "\n",
    "# Create synthetic embeddings with structure (5 topic clusters)\n",
    "X_embeddings, y_topics = make_blobs(\n",
    "    n_samples=n_docs,\n",
    "    n_features=embedding_dim,\n",
    "    centers=5,\n",
    "    cluster_std=10.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š SIMULATED RAG EMBEDDINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Documents: {n_docs}\")\n",
    "print(f\"Embedding dimensions: {embedding_dim} (OpenAI text-embedding-3-small)\")\n",
    "print(f\"Shape: {X_embeddings.shape}\")\n",
    "\n",
    "# Step 2: Find components for 95% variance\n",
    "pca_analyze = PCA()\n",
    "pca_analyze.fit(X_embeddings)\n",
    "\n",
    "cumsum_variance = np.cumsum(pca_analyze.explained_variance_ratio_)\n",
    "n_components_95 = np.argmax(cumsum_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nðŸŽ¯ COMPRESSION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Components for 95% variance: {n_components_95}\")\n",
    "print(f\"Compression ratio: {embedding_dim / n_components_95:.2f}x smaller\")\n",
    "print(f\"Variance retained: {cumsum_variance[n_components_95-1]:.2%}\")\n",
    "\n",
    "# Plot variance curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, min(500, len(cumsum_variance)) + 1), \n",
    "         cumsum_variance[:min(500, len(cumsum_variance))],\n",
    "         linewidth=2, color='darkblue')\n",
    "plt.axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='95% variance')\n",
    "plt.axvline(x=n_components_95, color='green', linestyle='--', linewidth=2, \n",
    "           label=f'Optimal: {n_components_95} components')\n",
    "plt.xlabel('Number of Components', fontsize=12)\n",
    "plt.ylabel('Cumulative Variance Explained', fontsize=12)\n",
    "plt.title('PCA: Finding Optimal Compression for RAG Embeddings', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim(0, min(500, len(cumsum_variance)))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Compress embeddings\n",
    "pca_final = PCA(n_components=n_components_95)\n",
    "X_compressed = pca_final.fit_transform(X_embeddings)\n",
    "\n",
    "print(f\"\\nðŸ“¦ COMPRESSION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original: {X_embeddings.shape} (1000 docs Ã— 1536 dims)\")\n",
    "print(f\"Compressed: {X_compressed.shape} (1000 docs Ã— {n_components_95} dims)\")\n",
    "print(f\"Memory savings: {(1 - n_components_95/embedding_dim):.1%}\")\n",
    "\n",
    "# Step 4: Speed analysis\n",
    "speed_improvement = embedding_dim / n_components_95\n",
    "\n",
    "print(f\"\\nâš¡ PERFORMANCE IMPROVEMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Similarity search speedup: ~{speed_improvement:.2f}x faster\")\n",
    "print(f\"\\nExample: 1000 doc search\")\n",
    "print(f\"   Original (1536D): ~{1000*1536:.0f} operations\")\n",
    "print(f\"   Compressed ({n_components_95}D): ~{1000*n_components_95:.0f} operations\")\n",
    "print(f\"   Speedup: {speed_improvement:.2f}x\")\n",
    "\n",
    "# Step 5: Visualize\n",
    "print(f\"\\nâ³ Creating visualization...\")\n",
    "\n",
    "# Further reduce to 2D for visualization\n",
    "if UMAP_AVAILABLE:\n",
    "    reducer = umap.UMAP(n_components=2, n_neighbors=15, random_state=42)\n",
    "    X_viz = reducer.fit_transform(X_compressed)\n",
    "    method = 'UMAP'\n",
    "else:\n",
    "    reducer = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    X_viz = reducer.fit_transform(X_compressed[:500])  # Subset for t-SNE speed\n",
    "    y_topics = y_topics[:500]\n",
    "    method = 't-SNE'\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_viz[:, 0], X_viz[:, 1],\n",
    "                     c=y_topics, cmap='Set2', s=50, alpha=0.7,\n",
    "                     edgecolors='black', linewidths=0.5)\n",
    "plt.colorbar(scatter, label='Topic Cluster')\n",
    "plt.xlabel(f'{method} Component 1', fontsize=12)\n",
    "plt.ylabel(f'{method} Component 2', fontsize=12)\n",
    "plt.title(f'Compressed RAG Embeddings Visualization\\n({embedding_dim}D â†’ {n_components_95}D â†’ 2D)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… COMPRESSION SUCCESSFUL!\\n\")\n",
    "\n",
    "print(\"ðŸ’¡ PRODUCTION DECISION:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nâœ… YES, use this compression IF:\")\n",
    "print(\"   - You need faster search (latency-critical)\")\n",
    "print(\"   - You have millions of documents\")\n",
    "print(\"   - Storage/memory is a concern\")\n",
    "print(\"   - 95% variance is acceptable for your use case\")\n",
    "\n",
    "print(\"\\nâš ï¸  BE CAREFUL IF:\")\n",
    "print(\"   - You need maximum accuracy (use full embeddings)\")\n",
    "print(\"   - Your data has subtle semantic differences (info may be in that 5%)\")\n",
    "print(\"   - You're doing fine-grained similarity (e.g., duplicate detection)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ BEST PRACTICE:\")\n",
    "print(\"   1. Test retrieval quality: full vs. compressed\")\n",
    "print(\"   2. Measure actual speed gains in your system\")\n",
    "print(\"   3. Consider hybrid: compress for initial filter, full for re-ranking\")\n",
    "print(\"   4. Monitor retrieval metrics in production\")\n",
    "\n",
    "print(f\"\\nðŸ’° TRADE-OFF SUMMARY:\")\n",
    "print(f\"   Gain: {speed_improvement:.1f}x faster, {(1-n_components_95/embedding_dim):.0%} less storage\")\n",
    "print(f\"   Cost: {(1-cumsum_variance[n_components_95-1]):.1%} information loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Method Selection Guide\n",
    "\n",
    "| Use Case | Best Method | Why? |\n",
    "|----------|-------------|------|\n",
    "| Quick exploration | PCA | Fast, deterministic |\n",
    "| Publication visualization | t-SNE | Beautiful clusters |\n",
    "| Production system | UMAP | Fast + can transform new data |\n",
    "| Compression/speedup | PCA | Reversible, interpretable |\n",
    "| Large datasets (>100K) | UMAP or PCA | t-SNE too slow |\n",
    "| Need to transform new data | PCA or UMAP | t-SNE can't |\n",
    "| Preserve global structure | PCA or UMAP | t-SNE doesn't |\n",
    "| Best local structure | t-SNE or UMAP | Both excellent |\n",
    "\n",
    "**Decision tree:**\n",
    "1. **Need to transform new data?** â†’ PCA or UMAP\n",
    "2. **Data > 10K points?** â†’ PCA or UMAP (not t-SNE)\n",
    "3. **Need perfect reproducibility?** â†’ PCA (deterministic)\n",
    "4. **Need beautiful clusters for paper?** â†’ t-SNE or UMAP\n",
    "5. **Production RAG system?** â†’ PCA (compression) + UMAP (viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "**You just mastered:**\n",
    "- âœ… PCA for fast, linear dimensionality reduction\n",
    "- âœ… t-SNE for beautiful non-linear visualizations\n",
    "- âœ… UMAP for modern, scalable embedding visualization\n",
    "- âœ… Explained variance and component selection\n",
    "- âœ… Real AI applications: visualizing transformer embeddings, RAG optimization\n",
    "- âœ… When to use which technique\n",
    "\n",
    "**ðŸŽ¯ Key Takeaways:**\n",
    "1. **PCA** = Fast, linear, reversible, use for compression and quick exploration\n",
    "2. **t-SNE** = Beautiful clusters, preserves local structure, slow, use for final viz\n",
    "3. **UMAP** = Best of both worlds, fast + good clusters, use in production\n",
    "4. **Variance** = Use explained variance to choose number of components\n",
    "5. **Real AI** = Essential for understanding high-dimensional embeddings from GPT, Claude, etc.\n",
    "\n",
    "**ðŸš€ Practice Challenge (Before Day 3!):**\n",
    "\n",
    "Build a complete embedding visualization pipeline:\n",
    "1. Get real embeddings (use OpenAI API or sentence-transformers)\n",
    "2. Apply all three methods (PCA, t-SNE, UMAP)\n",
    "3. Compare the results\n",
    "4. Create an interactive visualization (bonus: use plotly!)\n",
    "5. Measure compression ratios and speed\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ“š Next Lesson:** Day 3 - Anomaly Detection (Find outliers and unusual patterns in your data!)\n",
    "\n",
    "**ðŸ’¬ Questions?** Try these techniques on your own embeddings - you'll be amazed what you discover!\n",
    "\n",
    "---\n",
    "\n",
    "*\"The best way to understand high-dimensional data is to see it. Dimensionality reduction makes the invisible visible!\"* ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
