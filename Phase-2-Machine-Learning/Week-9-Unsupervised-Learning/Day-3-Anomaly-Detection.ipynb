{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üîç Day 3: Anomaly Detection\n",
    "\n",
    "**üéØ Goal:** Master techniques to find outliers, anomalies, and unusual patterns in data\n",
    "\n",
    "**‚è±Ô∏è Time:** 60-75 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Fraud detection: catch unusual transactions before they cause damage\n",
    "- AI safety: detect unusual or harmful AI agent behavior\n",
    "- RAG systems: identify low-quality or irrelevant documents\n",
    "- Used by OpenAI, Anthropic to monitor model outputs for safety\n",
    "- Critical for multimodal AI: detect adversarial inputs (jailbreaks)\n",
    "- Powers cybersecurity, quality control, and system monitoring\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ü§î What is Anomaly Detection?\n",
    "\n",
    "**Anomaly (Outlier)** = A data point that differs significantly from the majority.\n",
    "\n",
    "**Why it matters:**\n",
    "- üí≥ Fraud detection: Normal spending = $50-200, Anomaly = $5,000 purchase in foreign country\n",
    "- üè• Medical diagnosis: Detect unusual vital signs before emergency\n",
    "- ü§ñ AI safety: Detect when AI agent behaves unusually\n",
    "- üîí Cybersecurity: Identify intrusions and attacks\n",
    "- üè≠ Manufacturing: Catch defective products\n",
    "\n",
    "**The challenge:**\n",
    "- Anomalies are rare (1% or less of data)\n",
    "- Usually unlabeled (you don't know what's anomalous in advance)\n",
    "- Can be subtle or obvious\n",
    "- Context-dependent (unusual in one context, normal in another)\n",
    "\n",
    "**We'll learn 3 powerful techniques:**\n",
    "1. **Isolation Forest** - Fast, tree-based, great for high dimensions\n",
    "2. **One-Class SVM** - Boundary-based, works well for complex shapes\n",
    "3. **Autoencoders** - Deep learning approach, learns normal patterns\n",
    "\n",
    "Let's catch some anomalies! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Deep learning (for autoencoders)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  TensorFlow not installed. Autoencoder section will be skipped.\")\n",
    "    print(\"   Install with: pip install tensorflow\")\n",
    "\n",
    "# Set random seed and style\n",
    "np.random.seed(42)\n",
    "if TF_AVAILABLE:\n",
    "    tf.random.set_seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üîç Ready to detect anomalies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## üéØ Technique 1: Isolation Forest\n",
    "\n",
    "**Isolation Forest** = Isolate anomalies using random decision trees!\n",
    "\n",
    "### Core idea:\n",
    "**Anomalies are few and different** ‚Üí easier to isolate!\n",
    "\n",
    "**Analogy:** Finding a purple cow in a field of brown cows:\n",
    "- Ask: \"Is it purple?\" ‚Üí Immediately isolates the anomaly! (1 question)\n",
    "- Finding a specific brown cow ‚Üí Need many questions to isolate it\n",
    "\n",
    "### How it works:\n",
    "1. **Build trees:** Randomly select features and split points\n",
    "2. **Measure path length:** How many splits to isolate each point?\n",
    "3. **Anomaly score:** Short path = easy to isolate = anomaly!\n",
    "\n",
    "### Strengths:\n",
    "- ‚úÖ Fast and scalable (works on millions of points)\n",
    "- ‚úÖ Works well in high dimensions\n",
    "- ‚úÖ No need to define \"normal\" (unsupervised)\n",
    "- ‚úÖ Robust to noise\n",
    "- ‚úÖ Few hyperparameters\n",
    "\n",
    "### Weaknesses:\n",
    "- ‚ùå May struggle with local anomalies in dense regions\n",
    "- ‚ùå Random (slightly different results each run)\n",
    "\n",
    "### Real AI Use (2024-2025):\n",
    "- **Fraud detection:** Catch unusual credit card transactions\n",
    "- **RAG quality:** Identify low-quality or irrelevant documents\n",
    "- **AI monitoring:** Detect unusual model outputs\n",
    "- **Cybersecurity:** Identify network intrusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data: normal points + anomalies\n",
    "np.random.seed(42)\n",
    "\n",
    "# Normal data: two clusters\n",
    "X_normal, _ = make_blobs(n_samples=300, n_features=2, centers=2, \n",
    "                         cluster_std=0.5, random_state=42)\n",
    "\n",
    "# Anomalies: scattered outliers\n",
    "X_anomalies = np.random.uniform(low=-6, high=6, size=(20, 2))\n",
    "\n",
    "# Combine\n",
    "X_combined = np.vstack([X_normal, X_anomalies])\n",
    "y_true = np.array([0]*300 + [1]*20)  # 0=normal, 1=anomaly\n",
    "\n",
    "print(\"üìä Dataset:\")\n",
    "print(f\"   Normal points: {len(X_normal)}\")\n",
    "print(f\"   Anomalies: {len(X_anomalies)}\")\n",
    "print(f\"   Anomaly rate: {len(X_anomalies)/len(X_combined):.1%}\")\n",
    "\n",
    "# Visualize original data\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_normal[:, 0], X_normal[:, 1], \n",
    "           c='blue', s=50, alpha=0.6, label='Normal', edgecolors='black', linewidths=0.5)\n",
    "plt.scatter(X_anomalies[:, 0], X_anomalies[:, 1], \n",
    "           c='red', s=100, alpha=0.8, label='Anomalies', \n",
    "           marker='X', edgecolors='black', linewidths=1)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Dataset: Normal Points + Anomalies', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Goal: Detect the red anomalies automatically (unsupervised!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isolation Forest\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.1,  # Expected proportion of anomalies (10%)\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "y_pred = iso_forest.fit_predict(X_combined)\n",
    "# Note: IsolationForest returns 1 for normal, -1 for anomaly\n",
    "# Convert to 0/1 for consistency\n",
    "y_pred_binary = (y_pred == -1).astype(int)\n",
    "\n",
    "# Get anomaly scores (lower = more anomalous)\n",
    "anomaly_scores = iso_forest.score_samples(X_combined)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Predicted anomalies\n",
    "normal_mask = (y_pred == 1)\n",
    "anomaly_mask = (y_pred == -1)\n",
    "\n",
    "axes[0].scatter(X_combined[normal_mask, 0], X_combined[normal_mask, 1],\n",
    "               c='blue', s=50, alpha=0.6, label='Normal', edgecolors='black', linewidths=0.5)\n",
    "axes[0].scatter(X_combined[anomaly_mask, 0], X_combined[anomaly_mask, 1],\n",
    "               c='red', s=100, alpha=0.8, label='Detected Anomalies',\n",
    "               marker='X', edgecolors='black', linewidths=1)\n",
    "axes[0].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=11)\n",
    "axes[0].set_title('Isolation Forest: Detected Anomalies', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Anomaly scores (heatmap)\n",
    "scatter = axes[1].scatter(X_combined[:, 0], X_combined[:, 1],\n",
    "                         c=anomaly_scores, cmap='RdYlBu_r', s=50,\n",
    "                         edgecolors='black', linewidths=0.5)\n",
    "axes[1].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=11)\n",
    "axes[1].set_title('Anomaly Scores (Red = More Anomalous)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=axes[1], label='Anomaly Score')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "precision = precision_score(y_true, y_pred_binary)\n",
    "recall = recall_score(y_true, y_pred_binary)\n",
    "f1 = f1_score(y_true, y_pred_binary)\n",
    "\n",
    "print(\"üéØ ISOLATION FOREST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {accuracy:.2%} - Overall correctness\")\n",
    "print(f\"Precision: {precision:.2%} - Of detected anomalies, how many were real?\")\n",
    "print(f\"Recall:    {recall:.2%} - Of real anomalies, how many did we catch?\")\n",
    "print(f\"F1-Score:  {f1:.2%} - Balanced metric\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "if recall > 0.8:\n",
    "    print(\"   ‚úÖ Excellent! Caught most anomalies\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Missed some anomalies - consider adjusting contamination parameter\")\n",
    "\n",
    "if precision > 0.8:\n",
    "    print(\"   ‚úÖ High precision! Few false alarms\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Some false positives - flagged normal points as anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### üîç Tuning the Contamination Parameter\n",
    "\n",
    "**contamination** = Expected proportion of anomalies in your data\n",
    "\n",
    "**How to choose:**\n",
    "- If you know: Use domain knowledge (e.g., \"fraud is 0.1% of transactions\")\n",
    "- If unknown: Start with 0.05-0.10, then adjust based on results\n",
    "- Too low: Miss anomalies (low recall)\n",
    "- Too high: Flag normal points (low precision)\n",
    "\n",
    "Let's see the effect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different contamination values\n",
    "contaminations = [0.05, 0.1, 0.2]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, cont in enumerate(contaminations):\n",
    "    iso = IsolationForest(contamination=cont, random_state=42)\n",
    "    y_pred_temp = iso.fit_predict(X_combined)\n",
    "    \n",
    "    normal_mask = (y_pred_temp == 1)\n",
    "    anomaly_mask = (y_pred_temp == -1)\n",
    "    \n",
    "    axes[i].scatter(X_combined[normal_mask, 0], X_combined[normal_mask, 1],\n",
    "                   c='blue', s=40, alpha=0.6, label='Normal')\n",
    "    axes[i].scatter(X_combined[anomaly_mask, 0], X_combined[anomaly_mask, 1],\n",
    "                   c='red', s=80, alpha=0.8, label='Anomaly', marker='X')\n",
    "    \n",
    "    n_detected = np.sum(anomaly_mask)\n",
    "    axes[i].set_title(f'Contamination={cont}\\n({n_detected} anomalies detected)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Feature 1')\n",
    "    axes[i].set_ylabel('Feature 2')\n",
    "    axes[i].legend(fontsize=9)\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Effect of Contamination Parameter', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Contamination Guide:\")\n",
    "print(\"   0.05 (5%):  Conservative - fewer false alarms, may miss some\")\n",
    "print(\"   0.10 (10%): Balanced - good starting point\")\n",
    "print(\"   0.20 (20%): Aggressive - catch more, but more false alarms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## üéØ Technique 2: One-Class SVM\n",
    "\n",
    "**One-Class SVM** = Learn a boundary around normal data!\n",
    "\n",
    "### Core idea:\n",
    "- Train on **normal data only**\n",
    "- Learn a tight boundary around it\n",
    "- Anything outside the boundary = anomaly!\n",
    "\n",
    "**Analogy:** Drawing a fence around your house:\n",
    "- Everything inside fence = normal (your property)\n",
    "- Everything outside = anomaly (trespasser!)\n",
    "\n",
    "### How it works:\n",
    "1. **Map to high dimension:** Use kernel trick (like SVM)\n",
    "2. **Find hyperplane:** Separate normal data from origin\n",
    "3. **Maximize margin:** Create tight boundary\n",
    "4. **Classify:** Inside boundary = normal, outside = anomaly\n",
    "\n",
    "### Key parameter: nu (ŒΩ)\n",
    "- **nu** = Upper bound on fraction of outliers (like contamination)\n",
    "- Range: 0 to 1 (typical: 0.01 to 0.1)\n",
    "- Lower = tighter boundary, fewer anomalies\n",
    "- Higher = looser boundary, more anomalies\n",
    "\n",
    "### Strengths:\n",
    "- ‚úÖ Works well with complex, non-linear boundaries (kernel trick!)\n",
    "- ‚úÖ Theoretical foundation (strong math)\n",
    "- ‚úÖ Effective for small to medium datasets\n",
    "\n",
    "### Weaknesses:\n",
    "- ‚ùå Slow on large datasets (doesn't scale well)\n",
    "- ‚ùå Sensitive to kernel choice\n",
    "- ‚ùå More parameters to tune\n",
    "\n",
    "### Real AI Use:\n",
    "- **Medical diagnosis:** Learn normal vital signs, flag abnormal\n",
    "- **Manufacturing:** Detect defective products\n",
    "- **Security:** Identify unusual network behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more complex data (non-linear)\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# Add anomalies\n",
    "X_outliers = np.random.uniform(low=-2, high=3, size=(20, 2))\n",
    "X_complex = np.vstack([X_moons, X_outliers])\n",
    "y_true_complex = np.array([0]*300 + [1]*20)\n",
    "\n",
    "# Apply One-Class SVM\n",
    "ocsvm = OneClassSVM(\n",
    "    kernel='rbf',  # Radial Basis Function (can learn complex boundaries)\n",
    "    gamma='auto',  # Kernel coefficient\n",
    "    nu=0.1         # Expected outlier fraction\n",
    ")\n",
    "\n",
    "y_pred_svm = ocsvm.fit_predict(X_complex)\n",
    "y_pred_svm_binary = (y_pred_svm == -1).astype(int)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Results\n",
    "normal_mask_svm = (y_pred_svm == 1)\n",
    "anomaly_mask_svm = (y_pred_svm == -1)\n",
    "\n",
    "axes[0].scatter(X_complex[normal_mask_svm, 0], X_complex[normal_mask_svm, 1],\n",
    "               c='blue', s=50, alpha=0.6, label='Normal')\n",
    "axes[0].scatter(X_complex[anomaly_mask_svm, 0], X_complex[anomaly_mask_svm, 1],\n",
    "               c='red', s=100, alpha=0.8, label='Detected Anomalies', marker='X')\n",
    "axes[0].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=11)\n",
    "axes[0].set_title('One-Class SVM: Detected Anomalies', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "# Create mesh grid\n",
    "xx, yy = np.meshgrid(np.linspace(-2.5, 3.5, 500), np.linspace(-1.5, 2, 500))\n",
    "Z = ocsvm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "axes[1].contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap='Blues_r', alpha=0.6)\n",
    "axes[1].contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkblue', \n",
    "               linestyles='solid', label='Decision Boundary')\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c='blue', s=30, alpha=0.6, label='Normal')\n",
    "axes[1].scatter(X_outliers[:, 0], X_outliers[:, 1], c='red', s=80, alpha=0.8, \n",
    "               marker='X', label='Anomalies')\n",
    "axes[1].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=11)\n",
    "axes[1].set_title('One-Class SVM: Decision Boundary', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate\n",
    "accuracy_svm = accuracy_score(y_true_complex, y_pred_svm_binary)\n",
    "precision_svm = precision_score(y_true_complex, y_pred_svm_binary, zero_division=0)\n",
    "recall_svm = recall_score(y_true_complex, y_pred_svm_binary)\n",
    "f1_svm = f1_score(y_true_complex, y_pred_svm_binary, zero_division=0)\n",
    "\n",
    "print(\"üéØ ONE-CLASS SVM RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {accuracy_svm:.2%}\")\n",
    "print(f\"Precision: {precision_svm:.2%}\")\n",
    "print(f\"Recall:    {recall_svm:.2%}\")\n",
    "print(f\"F1-Score:  {f1_svm:.2%}\")\n",
    "\n",
    "print(\"\\nüí° Key Feature:\")\n",
    "print(\"   - One-Class SVM learned a complex, non-linear boundary!\")\n",
    "print(\"   - Blue shaded area = 'normal' region\")\n",
    "print(\"   - Dark blue line = decision boundary\")\n",
    "print(\"   - Points outside = anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## üß† Technique 3: Autoencoders for Anomaly Detection\n",
    "\n",
    "**Autoencoder** = Neural network that learns to compress and reconstruct data!\n",
    "\n",
    "### Core idea:\n",
    "- Train on **normal data only**\n",
    "- Network learns to compress ‚Üí reconstruct normal patterns\n",
    "- Normal data ‚Üí reconstructs well (low error)\n",
    "- Anomalies ‚Üí reconstructs poorly (high error)!\n",
    "\n",
    "**Analogy:** Learning to draw faces:\n",
    "- Train on 1000 normal human faces\n",
    "- You get good at drawing faces\n",
    "- Show you a normal face ‚Üí you can redraw it well (low error)\n",
    "- Show you an alien face ‚Üí you can't redraw it well (high error = anomaly!)\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Input (64D) ‚Üí Encoder ‚Üí Bottleneck (8D) ‚Üí Decoder ‚Üí Output (64D)\n",
    "```\n",
    "\n",
    "- **Encoder:** Compress input to lower dimension\n",
    "- **Bottleneck:** Compact representation (latent space)\n",
    "- **Decoder:** Reconstruct from bottleneck\n",
    "\n",
    "### How it works:\n",
    "1. **Train:** Minimize reconstruction error on normal data\n",
    "2. **Threshold:** Calculate reconstruction errors, set threshold (e.g., 95th percentile)\n",
    "3. **Detect:** High reconstruction error ‚Üí anomaly!\n",
    "\n",
    "### Strengths:\n",
    "- ‚úÖ Learns complex patterns (deep learning power!)\n",
    "- ‚úÖ Works well in very high dimensions\n",
    "- ‚úÖ Can handle images, text, any data type\n",
    "- ‚úÖ Interpretable (can visualize reconstruction errors)\n",
    "\n",
    "### Weaknesses:\n",
    "- ‚ùå Requires more data to train\n",
    "- ‚ùå Slower to train\n",
    "- ‚ùå More hyperparameters (architecture, learning rate, etc.)\n",
    "- ‚ùå Requires deep learning knowledge\n",
    "\n",
    "### Real AI Use (2024-2025):\n",
    "- **AI safety:** Detect unusual/harmful model outputs\n",
    "- **Content moderation:** Flag unusual images/text\n",
    "- **Fraud detection:** Detect unusual transaction patterns\n",
    "- **Manufacturing:** Detect defects in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Load MNIST digits for demonstration\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    \n",
    "    (X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    X_train_full = X_train_full.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    # Flatten images: 28x28 ‚Üí 784\n",
    "    X_train_full = X_train_full.reshape(-1, 784)\n",
    "    X_test = X_test.reshape(-1, 784)\n",
    "    \n",
    "    # Train on digits 0-8 only (treat 9 as anomaly!)\n",
    "    X_train = X_train_full[y_train_full < 9]\n",
    "    y_train = y_train_full[y_train_full < 9]\n",
    "    \n",
    "    print(\"üìä Autoencoder Anomaly Detection Setup:\")\n",
    "    print(f\"   Training on: Digits 0-8 (normal)\")\n",
    "    print(f\"   Treating as anomaly: Digit 9\")\n",
    "    print(f\"   Training samples: {len(X_train)}\")\n",
    "    print(f\"   Test samples: {len(X_test)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  TensorFlow not available. Skipping autoencoder section.\")\n",
    "    print(\"   Install with: pip install tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Build autoencoder\n",
    "    input_dim = 784  # 28x28 pixels\n",
    "    encoding_dim = 32  # Bottleneck size\n",
    "    \n",
    "    # Encoder\n",
    "    encoder = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(encoding_dim, activation='relu')  # Bottleneck\n",
    "    ], name='encoder')\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = keras.Sequential([\n",
    "        layers.Input(shape=(encoding_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(input_dim, activation='sigmoid')  # Reconstruct\n",
    "    ], name='decoder')\n",
    "    \n",
    "    # Full autoencoder\n",
    "    autoencoder = keras.Sequential([encoder, decoder], name='autoencoder')\n",
    "    \n",
    "    # Compile\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    print(\"üß† Autoencoder Architecture:\")\n",
    "    print(\"=\" * 60)\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    print(\"\\n‚è≥ Training autoencoder (this may take 1-2 minutes)...\")\n",
    "    history = autoencoder.fit(\n",
    "        X_train, X_train,  # Train to reconstruct itself!\n",
    "        epochs=10,\n",
    "        batch_size=256,\n",
    "        validation_split=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    print(\"‚úÖ Training complete!\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Reconstruction Error (MSE)', fontsize=12)\n",
    "    plt.title('Autoencoder Training: Learning Normal Patterns', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  TensorFlow not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Reconstruct test data\n",
    "    X_test_reconstructed = autoencoder.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Calculate reconstruction errors\n",
    "    reconstruction_errors = np.mean(np.square(X_test - X_test_reconstructed), axis=1)\n",
    "    \n",
    "    # Set threshold (95th percentile of normal data)\n",
    "    normal_errors = reconstruction_errors[y_test < 9]\n",
    "    threshold = np.percentile(normal_errors, 95)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    y_pred_ae = (reconstruction_errors > threshold).astype(int)\n",
    "    y_true_ae = (y_test == 9).astype(int)  # 1 if digit is 9, else 0\n",
    "    \n",
    "    # Visualize reconstruction errors\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Histogram of errors\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(reconstruction_errors[y_test < 9], bins=50, alpha=0.7, \n",
    "            label='Normal (0-8)', color='blue', edgecolor='black')\n",
    "    plt.hist(reconstruction_errors[y_test == 9], bins=50, alpha=0.7, \n",
    "            label='Anomaly (9)', color='red', edgecolor='black')\n",
    "    plt.axvline(threshold, color='green', linestyle='--', linewidth=2, \n",
    "               label=f'Threshold ({threshold:.4f})')\n",
    "    plt.xlabel('Reconstruction Error', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title('Reconstruction Error Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    data_to_plot = [reconstruction_errors[y_test < 9], reconstruction_errors[y_test == 9]]\n",
    "    box = plt.boxplot(data_to_plot, labels=['Normal (0-8)', 'Anomaly (9)'],\n",
    "                     patch_artist=True, widths=0.6)\n",
    "    box['boxes'][0].set_facecolor('lightblue')\n",
    "    box['boxes'][1].set_facecolor('lightcoral')\n",
    "    plt.axhline(threshold, color='green', linestyle='--', linewidth=2, label='Threshold')\n",
    "    plt.ylabel('Reconstruction Error', fontsize=12)\n",
    "    plt.title('Error Distribution by Class', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_ae = accuracy_score(y_true_ae, y_pred_ae)\n",
    "    precision_ae = precision_score(y_true_ae, y_pred_ae, zero_division=0)\n",
    "    recall_ae = recall_score(y_true_ae, y_pred_ae)\n",
    "    f1_ae = f1_score(y_true_ae, y_pred_ae, zero_division=0)\n",
    "    \n",
    "    print(\"üß† AUTOENCODER ANOMALY DETECTION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Accuracy:  {accuracy_ae:.2%}\")\n",
    "    print(f\"Precision: {precision_ae:.2%} - Of detected 9s, how many were actually 9s?\")\n",
    "    print(f\"Recall:    {recall_ae:.2%} - Of all actual 9s, how many did we detect?\")\n",
    "    print(f\"F1-Score:  {f1_ae:.2%}\")\n",
    "    \n",
    "    print(\"\\nüí° Insights:\")\n",
    "    print(f\"   - Threshold: {threshold:.4f}\")\n",
    "    print(f\"   - Normal digits (0-8): Low reconstruction error (trained on these!)\")\n",
    "    print(f\"   - Anomaly digit (9): High reconstruction error (never seen during training!)\")\n",
    "    print(\"   - The autoencoder 'knows' what normal looks like, so it spots the unusual!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  TensorFlow not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Visualize some reconstructions\n",
    "    n_examples = 10\n",
    "    \n",
    "    # Get some normal and anomalous examples\n",
    "    normal_idx = np.where(y_test < 9)[0][:5]\n",
    "    anomaly_idx = np.where(y_test == 9)[0][:5]\n",
    "    indices = np.concatenate([normal_idx, anomaly_idx])\n",
    "    \n",
    "    fig, axes = plt.subplots(3, n_examples, figsize=(15, 5))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Original\n",
    "        axes[0, i].imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_ylabel('Original', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Reconstructed\n",
    "        axes[1, i].imshow(X_test_reconstructed[idx].reshape(28, 28), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_ylabel('Reconstructed', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Difference (error)\n",
    "        diff = np.abs(X_test[idx] - X_test_reconstructed[idx])\n",
    "        axes[2, i].imshow(diff.reshape(28, 28), cmap='hot')\n",
    "        axes[2, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[2, i].set_ylabel('Error', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Title\n",
    "        error = reconstruction_errors[idx]\n",
    "        is_anomaly = \"ANOMALY\" if error > threshold else \"Normal\"\n",
    "        color = 'red' if error > threshold else 'blue'\n",
    "        axes[0, i].set_title(f\"Digit {y_test[idx]}\\n{is_anomaly}\\nError: {error:.4f}\",\n",
    "                            fontsize=9, color=color, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Autoencoder Reconstructions: Normal vs. Anomaly', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Visual Analysis:\")\n",
    "    print(\"   Left 5 columns: Normal digits (0-8)\")\n",
    "    print(\"      ‚Üí Low error (dark in error map)\")\n",
    "    print(\"      ‚Üí Good reconstruction\")\n",
    "    print(\"\\n   Right 5 columns: Anomaly digit (9)\")\n",
    "    print(\"      ‚Üí High error (bright in error map)\")\n",
    "    print(\"      ‚Üí Poor reconstruction (autoencoder struggles!)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  TensorFlow not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## ü§ñ Real AI Example: Detecting Unusual AI Agent Behavior\n",
    "\n",
    "**Scenario:** You're building an agentic AI system (like AutoGPT or BabyAGI).\n",
    "- Agent performs actions: search, write_file, send_email, etc.\n",
    "- Goal: Detect unusual/potentially harmful behavior patterns\n",
    "\n",
    "**Why this matters:**\n",
    "- AI safety: Catch agent going rogue\n",
    "- Security: Detect jailbreak attempts\n",
    "- Quality: Flag unusual patterns for human review\n",
    "\n",
    "Let's simulate this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate AI agent behavior logs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Features: [actions_per_minute, API_calls, file_writes, unique_tools_used, \n",
    "#            avg_response_time, error_rate]\n",
    "\n",
    "# Normal behavior patterns\n",
    "n_normal = 500\n",
    "X_normal_behavior = np.column_stack([\n",
    "    np.random.poisson(5, n_normal),      # 5 actions/min average\n",
    "    np.random.poisson(10, n_normal),     # 10 API calls average\n",
    "    np.random.poisson(2, n_normal),      # 2 file writes average\n",
    "    np.random.randint(1, 6, n_normal),   # 1-5 unique tools\n",
    "    np.random.normal(1.5, 0.3, n_normal), # 1.5s response time\n",
    "    np.random.beta(2, 20, n_normal)      # Low error rate (~10%)\n",
    "])\n",
    "\n",
    "# Anomalous behavior patterns (unusual/potentially harmful)\n",
    "n_anomalies = 30\n",
    "anomaly_types = [\n",
    "    # Type 1: Excessive API spam\n",
    "    np.column_stack([\n",
    "        np.random.poisson(50, 10),       # 50 actions/min (10x normal!)\n",
    "        np.random.poisson(100, 10),      # 100 API calls\n",
    "        np.random.poisson(2, 10),\n",
    "        np.random.randint(1, 6, 10),\n",
    "        np.random.normal(1.5, 0.3, 10),\n",
    "        np.random.beta(2, 20, 10)\n",
    "    ]),\n",
    "    \n",
    "    # Type 2: Excessive file operations (data exfiltration?)\n",
    "    np.column_stack([\n",
    "        np.random.poisson(8, 10),\n",
    "        np.random.poisson(12, 10),\n",
    "        np.random.poisson(50, 10),       # 50 file writes (25x normal!)\n",
    "        np.random.randint(1, 6, 10),\n",
    "        np.random.normal(1.5, 0.3, 10),\n",
    "        np.random.beta(2, 20, 10)\n",
    "    ]),\n",
    "    \n",
    "    # Type 3: Very slow responses + high errors (stuck/broken)\n",
    "    np.column_stack([\n",
    "        np.random.poisson(2, 10),\n",
    "        np.random.poisson(5, 10),\n",
    "        np.random.poisson(1, 10),\n",
    "        np.random.randint(1, 3, 10),\n",
    "        np.random.normal(10, 2, 10),     # 10s response (7x normal!)\n",
    "        np.random.beta(8, 2, 10)         # 80% error rate!\n",
    "    ])\n",
    "]\n",
    "\n",
    "X_anomalies_behavior = np.vstack(anomaly_types)\n",
    "\n",
    "# Combine\n",
    "X_agent = np.vstack([X_normal_behavior, X_anomalies_behavior])\n",
    "y_true_agent = np.array([0]*n_normal + [1]*n_anomalies)\n",
    "\n",
    "# Standardize (important for anomaly detection!)\n",
    "scaler = StandardScaler()\n",
    "X_agent_scaled = scaler.fit_transform(X_agent)\n",
    "\n",
    "print(\"ü§ñ AI AGENT BEHAVIOR MONITORING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total behavior logs: {len(X_agent)}\")\n",
    "print(f\"Normal behaviors: {n_normal}\")\n",
    "print(f\"Anomalous behaviors: {n_anomalies}\")\n",
    "print(f\"Features tracked: {X_agent.shape[1]}\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"   1. Actions per minute\")\n",
    "print(\"   2. API calls\")\n",
    "print(\"   3. File writes\")\n",
    "print(\"   4. Unique tools used\")\n",
    "print(\"   5. Average response time (seconds)\")\n",
    "print(\"   6. Error rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isolation Forest\n",
    "iso_agent = IsolationForest(contamination=0.08, random_state=42, n_estimators=100)\n",
    "y_pred_agent = iso_agent.fit_predict(X_agent_scaled)\n",
    "y_pred_agent_binary = (y_pred_agent == -1).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_agent = accuracy_score(y_true_agent, y_pred_agent_binary)\n",
    "precision_agent = precision_score(y_true_agent, y_pred_agent_binary)\n",
    "recall_agent = recall_score(y_true_agent, y_pred_agent_binary)\n",
    "f1_agent = f1_score(y_true_agent, y_pred_agent_binary)\n",
    "\n",
    "print(\"üéØ ANOMALY DETECTION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {accuracy_agent:.2%}\")\n",
    "print(f\"Precision: {precision_agent:.2%} - Of flagged behaviors, how many were actually anomalous?\")\n",
    "print(f\"Recall:    {recall_agent:.2%} - Of all anomalies, how many did we catch?\")\n",
    "print(f\"F1-Score:  {f1_agent:.2%}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_agent, y_pred_agent_binary)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['Normal', 'Anomaly'],\n",
    "           yticklabels=['Normal', 'Anomaly'])\n",
    "plt.title('AI Agent Behavior: Anomaly Detection', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze detected anomalies\n",
    "anomaly_indices = np.where(y_pred_agent_binary == 1)[0]\n",
    "actual_anomaly_indices = np.where(y_true_agent == 1)[0]\n",
    "\n",
    "print(\"\\nüîç DETECTED ANOMALY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total anomalies detected: {len(anomaly_indices)}\")\n",
    "print(f\"True anomalies caught: {np.sum((y_pred_agent_binary == 1) & (y_true_agent == 1))}\")\n",
    "print(f\"False alarms: {np.sum((y_pred_agent_binary == 1) & (y_true_agent == 0))}\")\n",
    "\n",
    "# Show examples of detected anomalies\n",
    "feature_names = ['Actions/min', 'API calls', 'File writes', \n",
    "                'Unique tools', 'Response time', 'Error rate']\n",
    "\n",
    "print(\"\\nüìä Example Detected Anomalies:\")\n",
    "for i, idx in enumerate(anomaly_indices[:5]):\n",
    "    print(f\"\\n   Anomaly {i+1} (Index {idx}):\")\n",
    "    for j, feature in enumerate(feature_names):\n",
    "        value = X_agent[idx, j]\n",
    "        avg_normal = X_agent[:n_normal, j].mean()\n",
    "        diff = ((value - avg_normal) / avg_normal) * 100\n",
    "        print(f\"      {feature}: {value:.2f} (normal avg: {avg_normal:.2f}, {diff:+.0f}%)\")\n",
    "    \n",
    "    actual = \"ACTUAL ANOMALY\" if y_true_agent[idx] == 1 else \"False alarm\"\n",
    "    print(f\"      Status: {actual}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüí° PRODUCTION USE CASE:\")\n",
    "print(\"\\n1Ô∏è‚É£  Real-time Monitoring:\")\n",
    "print(\"   - Run anomaly detection on agent behavior every minute\")\n",
    "print(\"   - Alert when anomalies detected\")\n",
    "print(\"   - Optionally pause agent for human review\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  Safety Measures:\")\n",
    "print(\"   - High API calls ‚Üí Rate limiting\")\n",
    "print(\"   - Excessive file writes ‚Üí Block file operations\")\n",
    "print(\"   - High error rate ‚Üí Restart agent\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  Investigation:\")\n",
    "print(\"   - Log all anomalies for analysis\")\n",
    "print(\"   - Review patterns to update safety rules\")\n",
    "print(\"   - Fine-tune contamination parameter based on false alarm rate\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  Continuous Improvement:\")\n",
    "print(\"   - Retrain on new normal behaviors\")\n",
    "print(\"   - Adjust thresholds based on production data\")\n",
    "print(\"   - Add new features (e.g., sentiment of outputs, tool combinations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Credit Card Fraud Detection Challenge\n",
    "\n",
    "**Scenario:** You work for a bank's fraud detection team.\n",
    "- Dataset: Credit card transactions\n",
    "- Features: amount, merchant_category, time_of_day, distance_from_home\n",
    "- Goal: Build an anomaly detector to catch fraud\n",
    "\n",
    "**Your task:**\n",
    "1. Generate synthetic transaction data\n",
    "2. Apply Isolation Forest\n",
    "3. Evaluate performance\n",
    "4. Analyze false positives and false negatives\n",
    "5. Recommend production deployment strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Complete this exercise!\n",
    "\n",
    "# Step 1: Generate transaction data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Normal transactions\n",
    "n_normal_trans = 1000\n",
    "X_normal_trans = np.column_stack([\n",
    "    # YOUR CODE - create normal transaction features\n",
    "    # amount: typical range $10-500\n",
    "    # merchant_category: 1-10\n",
    "    # time_of_day: 0-23 hours\n",
    "    # distance_from_home: 0-50 miles\n",
    "])\n",
    "\n",
    "# Fraudulent transactions\n",
    "n_fraud = 50\n",
    "X_fraud_trans = np.column_stack([\n",
    "    # YOUR CODE - create fraudulent patterns\n",
    "    # Large amounts, unusual times, far from home, etc.\n",
    "])\n",
    "\n",
    "# Step 2: Combine and scale\n",
    "X_transactions = # YOUR CODE\n",
    "y_true_trans = # YOUR CODE - labels\n",
    "\n",
    "# Standardize\n",
    "scaler_trans = StandardScaler()\n",
    "X_transactions_scaled = # YOUR CODE\n",
    "\n",
    "# Step 3: Apply Isolation Forest\n",
    "iso_fraud = IsolationForest(\n",
    "    contamination=# YOUR CODE - what's the fraud rate?,\n",
    "    random_state=42\n",
    ")\n",
    "y_pred_trans = # YOUR CODE\n",
    "y_pred_trans_binary = # YOUR CODE\n",
    "\n",
    "# Step 4: Evaluate\n",
    "# YOUR CODE - calculate metrics\n",
    "accuracy_trans = \n",
    "precision_trans = \n",
    "recall_trans = \n",
    "f1_trans = \n",
    "\n",
    "print(\"üè¶ FRAUD DETECTION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {accuracy_trans:.2%}\")\n",
    "print(f\"Precision: {precision_trans:.2%}\")\n",
    "print(f\"Recall:    {recall_trans:.2%}\")\n",
    "print(f\"F1-Score:  {f1_trans:.2%}\")\n",
    "\n",
    "# Step 5: Analyze errors\n",
    "# YOUR CODE - find false positives and false negatives\n",
    "# What do they have in common?\n",
    "\n",
    "# Step 6: Production strategy\n",
    "print(\"\\nüí° YOUR RECOMMENDATION:\")\n",
    "print(\"   Would you deploy this to production?\")\n",
    "print(\"   What recall/precision trade-off makes sense for fraud?\")\n",
    "print(\"   How would you handle false positives (blocking legitimate purchases)?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Credit Card Fraud Detection\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Normal transactions\n",
    "n_normal_trans = 1000\n",
    "X_normal_trans = np.column_stack([\n",
    "    np.random.lognormal(4, 0.8, n_normal_trans),    # Amount: $10-500, skewed\n",
    "    np.random.randint(1, 11, n_normal_trans),       # Merchant category: 1-10\n",
    "    np.random.normal(14, 4, n_normal_trans),        # Time: ~2pm ¬± 4hrs\n",
    "    np.random.exponential(10, n_normal_trans)       # Distance: mostly close, some far\n",
    "])\n",
    "\n",
    "# Clip values to reasonable ranges\n",
    "X_normal_trans[:, 0] = np.clip(X_normal_trans[:, 0], 10, 500)\n",
    "X_normal_trans[:, 2] = np.clip(X_normal_trans[:, 2], 0, 23)\n",
    "X_normal_trans[:, 3] = np.clip(X_normal_trans[:, 3], 0, 50)\n",
    "\n",
    "# Fraudulent transactions (3 patterns)\n",
    "n_fraud = 50\n",
    "\n",
    "# Pattern 1: Large amounts\n",
    "fraud_pattern1 = np.column_stack([\n",
    "    np.random.uniform(2000, 5000, 20),  # $2000-5000!\n",
    "    np.random.randint(1, 11, 20),\n",
    "    np.random.randint(0, 24, 20),\n",
    "    np.random.uniform(0, 50, 20)\n",
    "])\n",
    "\n",
    "# Pattern 2: Unusual times + far from home\n",
    "fraud_pattern2 = np.column_stack([\n",
    "    np.random.uniform(100, 800, 15),\n",
    "    np.random.randint(1, 11, 15),\n",
    "    np.random.choice([2, 3, 4, 22, 23], 15),  # Late night/early morning\n",
    "    np.random.uniform(100, 500, 15)  # 100-500 miles away!\n",
    "])\n",
    "\n",
    "# Pattern 3: Rapid sequence (same minute, different merchants)\n",
    "fraud_pattern3 = np.column_stack([\n",
    "    np.random.uniform(50, 300, 15),\n",
    "    np.random.randint(1, 11, 15),\n",
    "    np.full(15, 14.5),  # All within same hour\n",
    "    np.random.uniform(20, 200, 15)  # Geographically dispersed\n",
    "])\n",
    "\n",
    "X_fraud_trans = np.vstack([fraud_pattern1, fraud_pattern2, fraud_pattern3])\n",
    "\n",
    "# Combine\n",
    "X_transactions = np.vstack([X_normal_trans, X_fraud_trans])\n",
    "y_true_trans = np.array([0]*n_normal_trans + [1]*n_fraud)\n",
    "\n",
    "# Standardize\n",
    "scaler_trans = StandardScaler()\n",
    "X_transactions_scaled = scaler_trans.fit_transform(X_transactions)\n",
    "\n",
    "print(\"üè¶ CREDIT CARD FRAUD DETECTION DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total transactions: {len(X_transactions)}\")\n",
    "print(f\"Normal: {n_normal_trans} ({n_normal_trans/len(X_transactions):.1%})\")\n",
    "print(f\"Fraudulent: {n_fraud} ({n_fraud/len(X_transactions):.1%})\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"   1. Amount ($)\")\n",
    "print(\"   2. Merchant Category (1-10)\")\n",
    "print(\"   3. Time of Day (0-23)\")\n",
    "print(\"   4. Distance from Home (miles)\")\n",
    "print(\"\\nFraud Patterns:\")\n",
    "print(\"   - Pattern 1: Large amounts ($2000-5000)\")\n",
    "print(\"   - Pattern 2: Late night + far from home\")\n",
    "print(\"   - Pattern 3: Rapid sequence in different locations\")\n",
    "\n",
    "# Apply Isolation Forest\n",
    "fraud_rate = n_fraud / len(X_transactions)\n",
    "iso_fraud = IsolationForest(\n",
    "    contamination=fraud_rate,  # Use actual fraud rate\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "y_pred_trans = iso_fraud.fit_predict(X_transactions_scaled)\n",
    "y_pred_trans_binary = (y_pred_trans == -1).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_trans = accuracy_score(y_true_trans, y_pred_trans_binary)\n",
    "precision_trans = precision_score(y_true_trans, y_pred_trans_binary)\n",
    "recall_trans = recall_score(y_true_trans, y_pred_trans_binary)\n",
    "f1_trans = f1_score(y_true_trans, y_pred_trans_binary)\n",
    "\n",
    "print(\"\\nüéØ FRAUD DETECTION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {accuracy_trans:.2%}\")\n",
    "print(f\"Precision: {precision_trans:.2%} - Of flagged transactions, how many were fraud?\")\n",
    "print(f\"Recall:    {recall_trans:.2%} - Of all fraud, how many did we catch?\")\n",
    "print(f\"F1-Score:  {f1_trans:.2%}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_fraud = confusion_matrix(y_true_trans, y_pred_trans_binary)\n",
    "tn, fp, fn, tp = cm_fraud.ravel()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_fraud, annot=True, fmt='d', cmap='Reds',\n",
    "           xticklabels=['Legitimate', 'Fraud'],\n",
    "           yticklabels=['Legitimate', 'Fraud'])\n",
    "plt.title('Credit Card Fraud Detection: Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä DETAILED BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"True Positives (TP): {tp} - Correctly caught fraud üéØ\")\n",
    "print(f\"True Negatives (TN): {tn} - Correctly approved legitimate üí≥\")\n",
    "print(f\"False Positives (FP): {fp} - Blocked legitimate (customer frustration!) ‚ö†Ô∏è\")\n",
    "print(f\"False Negatives (FN): {fn} - Missed fraud (money lost!) ‚ùå\")\n",
    "\n",
    "# Cost analysis\n",
    "avg_fraud_amount = X_fraud_trans[:, 0].mean()\n",
    "cost_of_fraud = fn * avg_fraud_amount\n",
    "cost_of_false_alarm = fp * 5  # $5 customer service cost per false alarm\n",
    "\n",
    "print(\"\\nüí∞ COST ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average fraud amount: ${avg_fraud_amount:.2f}\")\n",
    "print(f\"Cost of missed fraud (FN): ${cost_of_fraud:.2f} ({fn} √ó ${avg_fraud_amount:.2f})\")\n",
    "print(f\"Cost of false alarms (FP): ${cost_of_false_alarm:.2f} ({fp} √ó $5)\")\n",
    "print(f\"Total cost: ${cost_of_fraud + cost_of_false_alarm:.2f}\")\n",
    "\n",
    "# Analyze errors\n",
    "fp_indices = np.where((y_pred_trans_binary == 1) & (y_true_trans == 0))[0]\n",
    "fn_indices = np.where((y_pred_trans_binary == 0) & (y_true_trans == 1))[0]\n",
    "\n",
    "if len(fp_indices) > 0:\n",
    "    print(\"\\n‚ö†Ô∏è  FALSE POSITIVES (Legitimate flagged as fraud):\")\n",
    "    print(\"   These customers will be frustrated!\")\n",
    "    for i, idx in enumerate(fp_indices[:3]):\n",
    "        amount, category, time, distance = X_transactions[idx]\n",
    "        print(f\"   FP {i+1}: ${amount:.0f}, Cat {category:.0f}, {time:.0f}:00, {distance:.0f} miles\")\n",
    "\n",
    "if len(fn_indices) > 0:\n",
    "    print(\"\\n‚ùå FALSE NEGATIVES (Fraud that slipped through):\")\n",
    "    print(\"   These are costly! Money lost!\")\n",
    "    for i, idx in enumerate(fn_indices[:3]):\n",
    "        amount, category, time, distance = X_transactions[idx]\n",
    "        print(f\"   FN {i+1}: ${amount:.0f}, Cat {category:.0f}, {time:.0f}:00, {distance:.0f} miles\")\n",
    "\n",
    "# Production recommendation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüéØ PRODUCTION DEPLOYMENT RECOMMENDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ DEPLOY WITH THESE SAFEGUARDS:\")\n",
    "print(\"\\n1Ô∏è‚É£  Two-Tier System:\")\n",
    "print(\"   - High confidence fraud (score < threshold) ‚Üí Block transaction\")\n",
    "print(\"   - Medium confidence ‚Üí Require 2FA/verification\")\n",
    "print(\"   - Low confidence ‚Üí Allow but monitor\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  Recall Priority (Catch More Fraud):\")\n",
    "print(f\"   - Current recall: {recall_trans:.1%}\")\n",
    "print(\"   - Target: 95%+ (miss at most 5% of fraud)\")\n",
    "print(\"   - Adjust contamination parameter higher if needed\")\n",
    "print(\"   - Accept more false positives (better safe than sorry!)\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  False Positive Mitigation:\")\n",
    "print(\"   - Don't auto-block, send SMS verification first\")\n",
    "print(\"   - Learn from customer feedback ('Was this you?')\")\n",
    "print(\"   - Whitelist trusted merchants\")\n",
    "print(\"   - Allow quick appeal process\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  Continuous Monitoring:\")\n",
    "print(\"   - Track false positive rate daily\")\n",
    "print(\"   - Retrain monthly on new fraud patterns\")\n",
    "print(\"   - A/B test different contamination values\")\n",
    "print(\"   - Monitor customer satisfaction (blocked tx = angry customers)\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£  Hybrid Approach:\")\n",
    "print(\"   - Combine with rule-based filters (e.g., amount > $10k ‚Üí always flag)\")\n",
    "print(\"   - Use anomaly detection + supervised ML (if you have labeled data)\")\n",
    "print(\"   - Add behavioral features (typing speed, mouse movements)\")\n",
    "\n",
    "print(\"\\nüí° FINAL VERDICT:\")\n",
    "if recall_trans >= 0.8 and precision_trans >= 0.5:\n",
    "    print(\"   ‚úÖ READY FOR PRODUCTION with 2FA fallback for flagged transactions!\")\n",
    "elif recall_trans >= 0.8:\n",
    "    print(\"   ‚ö†Ô∏è  Good recall, but too many false alarms. Add 2FA tier.\")\n",
    "else:\n",
    "    print(\"   ‚ùå NOT READY. Too much fraud slipping through. Tune parameters.\")\n",
    "\n",
    "print(\"\\n   Remember: In fraud detection, FALSE NEGATIVES are costlier than FALSE POSITIVES!\")\n",
    "print(\"   Missing fraud = direct money loss\")\n",
    "print(\"   False alarm = minor inconvenience (SMS verification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## üìã Algorithm Comparison Cheat Sheet\n",
    "\n",
    "| Feature | Isolation Forest | One-Class SVM | Autoencoder |\n",
    "|---------|-----------------|---------------|-------------|\n",
    "| **Speed** | üü¢ Fast | üü° Medium | üî¥ Slow (training) |\n",
    "| **Scalability** | üü¢ Excellent (millions) | üî¥ Poor (thousands) | üü° Good |\n",
    "| **High dimensions** | üü¢ Excellent | üü° Fair | üü¢ Excellent |\n",
    "| **Interpretability** | üü° Medium | üî¥ Low | üü¢ High (see errors) |\n",
    "| **Hyperparameters** | üü¢ Few | üü° Several | üî¥ Many |\n",
    "| **Training data needed** | üü¢ Small | üü° Medium | üî¥ Large |\n",
    "| **Complex patterns** | üü° Good | üü¢ Excellent | üü¢ Excellent |\n",
    "\n",
    "**Decision tree:**\n",
    "1. **Large dataset (>100K)?** ‚Üí Isolation Forest\n",
    "2. **Small dataset (<10K)?** ‚Üí One-Class SVM\n",
    "3. **Images/complex data?** ‚Üí Autoencoder\n",
    "4. **Need speed?** ‚Üí Isolation Forest\n",
    "5. **Need interpretability?** ‚Üí Autoencoder (visualize errors)\n",
    "6. **Default choice?** ‚Üí Isolation Forest (fast, scalable, works well)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just mastered:**\n",
    "- ‚úÖ Isolation Forest for fast, scalable anomaly detection\n",
    "- ‚úÖ One-Class SVM for complex boundary-based detection\n",
    "- ‚úÖ Autoencoders for deep learning-based anomaly detection\n",
    "- ‚úÖ Evaluation metrics for anomaly detection (precision, recall trade-offs)\n",
    "- ‚úÖ Real AI applications: fraud detection, AI safety monitoring\n",
    "- ‚úÖ Production deployment strategies\n",
    "\n",
    "**üéØ Key Takeaways:**\n",
    "1. **Isolation Forest** = Fast, scalable, works great out-of-the-box\n",
    "2. **One-Class SVM** = Complex boundaries, smaller datasets\n",
    "3. **Autoencoders** = Best for images/complex data, interpretable\n",
    "4. **Recall matters** = In fraud/safety, missing anomalies is costly!\n",
    "5. **Real AI** = Critical for AI safety, fraud detection, quality control\n",
    "\n",
    "**üöÄ Practice Challenge:**\n",
    "\n",
    "Build a complete anomaly detection system:\n",
    "1. Choose a real dataset (credit card, network traffic, sensor data)\n",
    "2. Try all three methods\n",
    "3. Compare results and choose the best\n",
    "4. Implement a two-tier flagging system (high/medium/low confidence)\n",
    "5. Calculate cost analysis (false positives vs. false negatives)\n",
    "6. Write a deployment recommendation\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Week 9 Complete!** You've mastered:\n",
    "- Day 1: Clustering (K-Means, Hierarchical, DBSCAN)\n",
    "- Day 2: Dimensionality Reduction (PCA, t-SNE, UMAP)\n",
    "- Day 3: Anomaly Detection (Isolation Forest, One-Class SVM, Autoencoders)\n",
    "\n",
    "**Next:** Week 10 - Neural Networks and Deep Learning!\n",
    "\n",
    "**üí¨ Questions?** Apply these techniques to your own data - that's when the magic happens!\n",
    "\n",
    "---\n",
    "\n",
    "*\"Anomalies are where the interesting stuff happens. In data, as in life, it's often the outliers that matter most!\"* üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
