{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ğŸ¯ Day 1: Clustering Algorithms\n",
    "\n",
    "**ğŸ¯ Goal:** Master clustering techniques to find hidden patterns in unlabeled data\n",
    "\n",
    "**â±ï¸ Time:** 60-75 minutes\n",
    "\n",
    "**ğŸŒŸ Why This Matters for AI:**\n",
    "- Essential for RAG systems: cluster similar documents for efficient retrieval\n",
    "- Customer segmentation: group users by behavior without predefined categories\n",
    "- Used by OpenAI to organize knowledge bases, by Meta for content grouping\n",
    "- Critical for multimodal AI: cluster images, text, and embeddings together\n",
    "- Powers recommendation systems, anomaly detection, and data exploration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ğŸ¤” What is Clustering?\n",
    "\n",
    "**Clustering** = Finding groups (clusters) in data where:\n",
    "- Items in the same cluster are **similar** to each other\n",
    "- Items in different clusters are **different** from each other\n",
    "\n",
    "**Key difference from classification:**\n",
    "- Classification (supervised): You have labels â†’ \"This is a cat, this is a dog\"\n",
    "- Clustering (unsupervised): No labels â†’ \"Find natural groups in this data\"\n",
    "\n",
    "**Real-world example:**\n",
    "Imagine you have 10,000 customer purchase histories but no categories. Clustering can discover:\n",
    "- Cluster 1: Budget shoppers (frequent small purchases)\n",
    "- Cluster 2: Premium buyers (rare expensive purchases)\n",
    "- Cluster 3: Seasonal shoppers (holiday spikes)\n",
    "\n",
    "Let's master the 3 most important clustering algorithms! ğŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Set random seed and style\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"ğŸ“Š Ready to explore clustering algorithms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## ğŸ¯ Algorithm 1: K-Means Clustering\n",
    "\n",
    "**K-Means** = The most popular clustering algorithm!\n",
    "\n",
    "### How it works:\n",
    "1. **Choose K** (number of clusters you want)\n",
    "2. **Initialize**: Randomly place K cluster centers (centroids)\n",
    "3. **Assign**: Assign each point to the nearest centroid\n",
    "4. **Update**: Move centroids to the center of their assigned points\n",
    "5. **Repeat** steps 3-4 until centroids stop moving\n",
    "\n",
    "### Strengths:\n",
    "- âœ… Fast and scalable (works on millions of points)\n",
    "- âœ… Simple to understand and implement\n",
    "- âœ… Guaranteed to converge\n",
    "\n",
    "### Weaknesses:\n",
    "- âŒ You must specify K in advance\n",
    "- âŒ Sensitive to initial centroid placement\n",
    "- âŒ Assumes spherical clusters (equal variance)\n",
    "- âŒ Struggles with non-circular shapes\n",
    "\n",
    "### Real AI Use (2024-2025):\n",
    "- **RAG systems:** Cluster document embeddings for faster retrieval\n",
    "- **Vector databases:** Organize embeddings (Pinecone, Weaviate use variants)\n",
    "- **Image compression:** Reduce colors by clustering pixel values\n",
    "- **Customer segmentation:** Group users by behavior patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data: customer shopping patterns\n",
    "# Features: [average_purchase_amount, purchase_frequency_per_month]\n",
    "X_customers, true_labels = make_blobs(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    centers=4,\n",
    "    cluster_std=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Apply K-Means with K=4 clusters\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_customers)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Before clustering (raw data)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_customers[:, 0], X_customers[:, 1], c='gray', alpha=0.6, s=50)\n",
    "plt.title('Before Clustering: Raw Customer Data', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Average Purchase Amount ($)', fontsize=11)\n",
    "plt.ylabel('Purchase Frequency (per month)', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: After K-Means clustering\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter = plt.scatter(X_customers[:, 0], X_customers[:, 1], \n",
    "                     c=cluster_labels, cmap='viridis', alpha=0.6, s=50)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], \n",
    "           c='red', marker='X', s=300, edgecolors='black', linewidths=2,\n",
    "           label='Centroids')\n",
    "plt.title('After K-Means: Discovered Customer Segments', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Average Purchase Amount ($)', fontsize=11)\n",
    "plt.ylabel('Purchase Frequency (per month)', fontsize=11)\n",
    "plt.legend()\n",
    "plt.colorbar(scatter, label='Cluster ID')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ¯ K-Means Results:\")\n",
    "print(f\"   - Found {kmeans.n_clusters} customer segments\")\n",
    "print(f\"   - Converged in {kmeans.n_iter_} iterations\")\n",
    "print(f\"   - Inertia (sum of squared distances): {kmeans.inertia_:.2f}\")\n",
    "print(\"\\nğŸ’¡ Interpretation:\")\n",
    "print(\"   Red X marks = Cluster centers (typical customer profile for each segment)\")\n",
    "print(\"   Each color = Different customer segment discovered automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### ğŸ” Finding the Optimal K: The Elbow Method\n",
    "\n",
    "**Problem:** How do we choose K?\n",
    "\n",
    "**Solution:** The Elbow Method!\n",
    "- Try different values of K (e.g., 1 to 10)\n",
    "- Plot the \"inertia\" (sum of squared distances to nearest centroid)\n",
    "- Look for the \"elbow\" where adding more clusters doesn't help much\n",
    "\n",
    "**Inertia:**\n",
    "- Lower = Better (points are closer to their centroids)\n",
    "- But it always decreases as K increases (K=N gives inertia=0)\n",
    "- We want the \"sweet spot\" where improvement slows down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try K values from 1 to 10\n",
    "k_values = range(1, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(X_customers)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    \n",
    "    # Silhouette score (only for k > 1)\n",
    "    if k > 1:\n",
    "        score = silhouette_score(X_customers, kmeans_temp.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "    else:\n",
    "        silhouette_scores.append(0)\n",
    "\n",
    "# Plot both metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Elbow method\n",
    "axes[0].plot(k_values, inertias, marker='o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia', fontsize=12)\n",
    "axes[0].set_title('Elbow Method: Finding Optimal K', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='Elbow at K=4')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Silhouette scores\n",
    "axes[1].plot(k_values, silhouette_scores, marker='s', color='green', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score: Cluster Quality', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Good threshold (0.5)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š How to Read These Charts:\")\n",
    "print(\"\\nğŸ”µ Elbow Method (Left):\")\n",
    "print(\"   - Look for the 'elbow' where the curve bends\")\n",
    "print(\"   - After the elbow, adding clusters gives diminishing returns\")\n",
    "print(\"   - Here: K=4 seems optimal (elbow point)\")\n",
    "print(\"\\nğŸŸ¢ Silhouette Score (Right):\")\n",
    "print(\"   - Range: -1 to 1 (higher is better)\")\n",
    "print(\"   - > 0.5 = Good clustering\")\n",
    "print(\"   - > 0.7 = Excellent clustering\")\n",
    "print(f\"   - Best score: K={k_values[np.argmax(silhouette_scores)]} (score: {max(silhouette_scores):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## ğŸŒ³ Algorithm 2: Hierarchical Clustering\n",
    "\n",
    "**Hierarchical Clustering** = Build a tree (dendrogram) of clusters!\n",
    "\n",
    "### Two approaches:\n",
    "1. **Agglomerative (Bottom-up):** Start with each point as a cluster, merge closest pairs\n",
    "2. **Divisive (Top-down):** Start with one cluster, split recursively\n",
    "\n",
    "We'll use **Agglomerative** (most common).\n",
    "\n",
    "### How it works:\n",
    "1. Start: Each point is its own cluster (N clusters for N points)\n",
    "2. Find the two closest clusters and merge them\n",
    "3. Repeat until you have K clusters (or just 1)\n",
    "\n",
    "### Linkage methods (how to measure \"distance\" between clusters):\n",
    "- **Single:** Closest points between clusters\n",
    "- **Complete:** Farthest points between clusters\n",
    "- **Average:** Average distance of all pairs\n",
    "- **Ward:** Minimize variance when merging (most popular)\n",
    "\n",
    "### Strengths:\n",
    "- âœ… Don't need to specify K in advance\n",
    "- âœ… Produces a dendrogram (visual hierarchy)\n",
    "- âœ… Deterministic (same result every time)\n",
    "- âœ… Can handle any distance metric\n",
    "\n",
    "### Weaknesses:\n",
    "- âŒ Slow for large datasets (O(NÂ²) or O(NÂ³))\n",
    "- âŒ Can't undo a merge (greedy algorithm)\n",
    "\n",
    "### Real AI Use:\n",
    "- **Document clustering:** Organize papers/articles into topic hierarchies\n",
    "- **Genomics:** Cluster genes by expression patterns\n",
    "- **Taxonomy:** Create hierarchical categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create smaller dataset for visualization (hierarchical is slower)\n",
    "X_small, _ = make_blobs(n_samples=50, n_features=2, centers=3, \n",
    "                        cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Apply Agglomerative Clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "hierarchical_labels = hierarchical.fit_predict(X_small)\n",
    "\n",
    "# Create linkage matrix for dendrogram\n",
    "linkage_matrix = linkage(X_small, method='ward')\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Dendrogram (tree structure)\n",
    "dendrogram(linkage_matrix, ax=axes[0], color_threshold=10)\n",
    "axes[0].set_title('Dendrogram: Hierarchical Cluster Tree', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sample Index', fontsize=11)\n",
    "axes[0].set_ylabel('Distance (Ward Linkage)', fontsize=11)\n",
    "axes[0].axhline(y=10, color='red', linestyle='--', label='Cut at 3 clusters')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Resulting clusters\n",
    "scatter = axes[1].scatter(X_small[:, 0], X_small[:, 1], \n",
    "                         c=hierarchical_labels, cmap='viridis', \n",
    "                         s=100, alpha=0.7, edgecolors='black')\n",
    "axes[1].set_title('Hierarchical Clustering Result', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=11)\n",
    "plt.colorbar(scatter, ax=axes[1], label='Cluster ID')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸŒ³ Hierarchical Clustering Results:\")\n",
    "print(f\"   - Created {hierarchical.n_clusters_} clusters\")\n",
    "print(\"\\nğŸ“Š How to Read the Dendrogram (Left):\")\n",
    "print(\"   - Bottom: Individual points\")\n",
    "print(\"   - Vertical lines: Merges between clusters\")\n",
    "print(\"   - Height: Distance when clusters merged\")\n",
    "print(\"   - Red horizontal line: Where we 'cut' the tree to get 3 clusters\")\n",
    "print(\"\\nğŸ’¡ Advantage: You can see the FULL hierarchy, then choose K afterward!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## ğŸ¯ Algorithm 3: DBSCAN (Density-Based Clustering)\n",
    "\n",
    "**DBSCAN** = Density-Based Spatial Clustering of Applications with Noise\n",
    "\n",
    "**Core idea:** Clusters are dense regions separated by sparse regions.\n",
    "\n",
    "### Key parameters:\n",
    "- **eps (Îµ):** Maximum distance between two points to be neighbors\n",
    "- **min_samples:** Minimum points in a neighborhood to form a dense region\n",
    "\n",
    "### Point types:\n",
    "1. **Core point:** Has â‰¥ min_samples points within eps distance\n",
    "2. **Border point:** Within eps of a core point, but not core itself\n",
    "3. **Noise point:** Neither core nor border (outlier!)\n",
    "\n",
    "### How it works:\n",
    "1. Pick a random point\n",
    "2. Find all points within eps distance\n",
    "3. If â‰¥ min_samples neighbors â†’ core point â†’ start a cluster\n",
    "4. Expand cluster by adding all reachable points\n",
    "5. Repeat for all unvisited points\n",
    "\n",
    "### Strengths:\n",
    "- âœ… Automatically finds number of clusters (no K needed!)\n",
    "- âœ… Can find arbitrarily-shaped clusters (not just circles)\n",
    "- âœ… Identifies outliers as noise\n",
    "- âœ… Robust to noise\n",
    "\n",
    "### Weaknesses:\n",
    "- âŒ Struggles with varying densities\n",
    "- âŒ Sensitive to eps and min_samples parameters\n",
    "- âŒ Not great for high-dimensional data\n",
    "\n",
    "### Real AI Use (2024-2025):\n",
    "- **Anomaly detection:** Noise points = anomalies!\n",
    "- **Geospatial clustering:** Group locations (rides, deliveries)\n",
    "- **RAG document clustering:** Find dense topic clusters, ignore outliers\n",
    "- **Image segmentation:** Group pixels by density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complex-shaped data (K-Means would fail here!)\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# Add some noise points (outliers)\n",
    "noise_points = np.random.uniform(low=-2, high=3, size=(20, 2))\n",
    "X_complex = np.vstack([X_moons, noise_points])\n",
    "\n",
    "# Try K-Means (will fail on this data!)\n",
    "kmeans_complex = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans_complex.fit_predict(X_complex)\n",
    "\n",
    "# Try DBSCAN (will succeed!)\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_complex)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: K-Means (struggles with non-circular shapes)\n",
    "axes[0].scatter(X_complex[:, 0], X_complex[:, 1], \n",
    "               c=kmeans_labels, cmap='viridis', s=50, alpha=0.7)\n",
    "axes[0].scatter(kmeans_complex.cluster_centers_[:, 0], \n",
    "               kmeans_complex.cluster_centers_[:, 1],\n",
    "               c='red', marker='X', s=300, edgecolors='black', linewidths=2)\n",
    "axes[0].set_title('âŒ K-Means Fails on Complex Shapes', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=11)\n",
    "\n",
    "# Plot 2: DBSCAN (handles complex shapes!)\n",
    "# Noise points are labeled as -1\n",
    "unique_labels = set(dbscan_labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    if label == -1:\n",
    "        # Noise points in black\n",
    "        col = 'black'\n",
    "        marker = 'x'\n",
    "        label_name = 'Noise'\n",
    "    else:\n",
    "        col = color\n",
    "        marker = 'o'\n",
    "        label_name = f'Cluster {label}'\n",
    "    \n",
    "    class_member_mask = (dbscan_labels == label)\n",
    "    xy = X_complex[class_member_mask]\n",
    "    axes[1].scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker,\n",
    "                   s=50, alpha=0.7, label=label_name, edgecolors='black', linewidths=0.5)\n",
    "\n",
    "axes[1].set_title('âœ… DBSCAN Succeeds!', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=11)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count clusters and noise\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(\"ğŸ¯ DBSCAN Results:\")\n",
    "print(f\"   - Found {n_clusters} clusters automatically (no K needed!)\")\n",
    "print(f\"   - Identified {n_noise} noise points (outliers)\")\n",
    "print(f\"   - Parameters: eps={dbscan.eps}, min_samples={dbscan.min_samples}\")\n",
    "print(\"\\nğŸ’¡ Key Insight:\")\n",
    "print(\"   - K-Means assumes circular clusters â†’ fails on crescent shapes\")\n",
    "print(\"   - DBSCAN uses density â†’ perfectly separates crescents!\")\n",
    "print(\"   - Bonus: DBSCAN found outliers (black X marks) automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## ğŸ¤– Real AI Example: RAG Document Clustering\n",
    "\n",
    "**Scenario:** You're building a RAG system for a company's knowledge base.\n",
    "- 1000 documents embedded using OpenAI's `text-embedding-3-small` (1536 dimensions)\n",
    "- Goal: Cluster similar documents to improve retrieval speed\n",
    "\n",
    "**Why cluster for RAG?**\n",
    "1. **Faster retrieval:** Search within relevant cluster first\n",
    "2. **Better organization:** Group docs by topic\n",
    "3. **Diverse results:** Sample from different clusters\n",
    "4. **Anomaly detection:** Find unique/outlier documents\n",
    "\n",
    "Let's simulate this with simplified embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate document embeddings (in reality: 1536-dim vectors from OpenAI)\n",
    "# We'll use 2D for visualization, but same logic applies to 1536D!\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create document embeddings representing different topics\n",
    "# Cluster 1: HR documents (benefits, policies)\n",
    "hr_docs = np.random.randn(100, 2) * 0.5 + np.array([2, 2])\n",
    "\n",
    "# Cluster 2: Engineering docs (APIs, architecture)\n",
    "eng_docs = np.random.randn(100, 2) * 0.5 + np.array([-2, 2])\n",
    "\n",
    "# Cluster 3: Sales docs (pricing, contracts)\n",
    "sales_docs = np.random.randn(100, 2) * 0.5 + np.array([0, -2])\n",
    "\n",
    "# Cluster 4: Marketing docs (campaigns, analytics)\n",
    "marketing_docs = np.random.randn(100, 2) * 0.5 + np.array([3, -1])\n",
    "\n",
    "# Some outlier docs (miscellaneous)\n",
    "outliers = np.random.randn(20, 2) * 0.3 + np.array([-3, -2])\n",
    "\n",
    "# Combine all\n",
    "X_docs = np.vstack([hr_docs, eng_docs, sales_docs, marketing_docs, outliers])\n",
    "doc_names = (['HR']*100 + ['Engineering']*100 + ['Sales']*100 + \n",
    "            ['Marketing']*100 + ['Other']*20)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans_rag = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "cluster_ids = kmeans_rag.fit_predict(X_docs)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_docs[:, 0], X_docs[:, 1], \n",
    "                     c=cluster_ids, cmap='tab10', s=50, alpha=0.6)\n",
    "plt.scatter(kmeans_rag.cluster_centers_[:, 0], \n",
    "           kmeans_rag.cluster_centers_[:, 1],\n",
    "           c='red', marker='â˜…', s=500, edgecolors='black', linewidths=2,\n",
    "           label='Cluster Centers', zorder=5)\n",
    "plt.title('RAG Document Clustering: Knowledge Base Organization', \n",
    "         fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Embedding Dimension 1', fontsize=12)\n",
    "plt.ylabel('Embedding Dimension 2', fontsize=12)\n",
    "plt.colorbar(scatter, label='Cluster ID')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"ğŸ“š RAG DOCUMENT CLUSTERING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "for cluster_id in range(5):\n",
    "    cluster_mask = (cluster_ids == cluster_id)\n",
    "    cluster_size = np.sum(cluster_mask)\n",
    "    print(f\"\\nğŸ—‚ï¸  Cluster {cluster_id}: {cluster_size} documents\")\n",
    "    \n",
    "    # Show distribution of doc types in this cluster\n",
    "    cluster_docs = np.array(doc_names)[cluster_mask]\n",
    "    unique, counts = np.unique(cluster_docs, return_counts=True)\n",
    "    for doc_type, count in zip(unique, counts):\n",
    "        percentage = (count / cluster_size) * 100\n",
    "        print(f\"   - {doc_type}: {count} docs ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nğŸ¯ How This Improves RAG Performance:\")\n",
    "print(\"\\n1ï¸âƒ£  Faster Retrieval:\")\n",
    "print(\"   - Query: 'What are our vacation policies?'\")\n",
    "print(\"   - Instead of searching 420 docs â†’ search HR cluster only (~100 docs)\")\n",
    "print(\"   - 4x speed improvement!\")\n",
    "print(\"\\n2ï¸âƒ£  Diverse Results:\")\n",
    "print(\"   - Sample top-k from different clusters\")\n",
    "print(\"   - Avoid retrieving 5 very similar documents\")\n",
    "print(\"\\n3ï¸âƒ£  Better Organization:\")\n",
    "print(\"   - Create topic-based indices in vector database\")\n",
    "print(\"   - Route queries to relevant clusters first\")\n",
    "print(\"\\n4ï¸âƒ£  Anomaly Detection:\")\n",
    "print(\"   - Small clusters or noise points = unique/outlier docs\")\n",
    "print(\"   - May need special handling or better categorization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## ğŸ“Š Clustering Evaluation Metrics\n",
    "\n",
    "**Challenge:** How do we measure clustering quality without labels?\n",
    "\n",
    "### Internal Metrics (no labels needed):\n",
    "\n",
    "**1. Silhouette Score**\n",
    "- Measures how similar a point is to its own cluster vs. other clusters\n",
    "- Range: -1 to 1\n",
    "  - 1 = Perfect (far from other clusters)\n",
    "  - 0 = On the boundary\n",
    "  - -1 = Wrong cluster\n",
    "- Formula: `(b - a) / max(a, b)` where:\n",
    "  - a = average distance to points in same cluster\n",
    "  - b = average distance to points in nearest other cluster\n",
    "\n",
    "**2. Davies-Bouldin Index**\n",
    "- Measures average similarity between clusters\n",
    "- Lower is better (0 = perfect)\n",
    "- Compares within-cluster scatter to between-cluster separation\n",
    "\n",
    "**3. Inertia (K-Means specific)**\n",
    "- Sum of squared distances to nearest centroid\n",
    "- Lower is better, but always decreases with more clusters\n",
    "- Use elbow method to find sweet spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our customer segmentation clustering\n",
    "silhouette_avg = silhouette_score(X_customers, cluster_labels)\n",
    "davies_bouldin = davies_bouldin_score(X_customers, cluster_labels)\n",
    "\n",
    "print(\"ğŸ“Š CLUSTERING QUALITY METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nâœ… Silhouette Score: {silhouette_avg:.3f}\")\n",
    "print(\"   Interpretation:\")\n",
    "if silhouette_avg > 0.7:\n",
    "    print(\"   ğŸŒŸ Excellent! Strong, well-separated clusters\")\n",
    "elif silhouette_avg > 0.5:\n",
    "    print(\"   âœ… Good! Reasonable cluster structure\")\n",
    "elif silhouette_avg > 0.25:\n",
    "    print(\"   âš ï¸  Fair. Clusters exist but may overlap\")\n",
    "else:\n",
    "    print(\"   âŒ Poor. Clusters are weak or artificial\")\n",
    "\n",
    "print(f\"\\nğŸ“‰ Davies-Bouldin Index: {davies_bouldin:.3f}\")\n",
    "print(\"   Interpretation:\")\n",
    "if davies_bouldin < 0.5:\n",
    "    print(\"   ğŸŒŸ Excellent! Tight, well-separated clusters\")\n",
    "elif davies_bouldin < 1.0:\n",
    "    print(\"   âœ… Good cluster separation\")\n",
    "else:\n",
    "    print(\"   âš ï¸  Clusters may be overlapping\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Inertia: {kmeans.inertia_:.2f}\")\n",
    "print(\"   (Lower is better, but use elbow method for comparison)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nğŸ’¡ Key Insight:\")\n",
    "print(\"   These metrics help you compare different clustering algorithms\")\n",
    "print(\"   or different values of K, even without ground truth labels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## ğŸ¯ YOUR TURN: Customer Segmentation Challenge\n",
    "\n",
    "You work for an e-commerce company. You have data on customer behavior:\n",
    "- Feature 1: Average order value (dollars)\n",
    "- Feature 2: Number of purchases per month\n",
    "- Feature 3: Days since last purchase\n",
    "\n",
    "**Your task:**\n",
    "1. Generate sample customer data\n",
    "2. Use K-Means to find customer segments\n",
    "3. Use the elbow method to find optimal K\n",
    "4. Evaluate clustering quality\n",
    "5. Interpret the segments for marketing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Complete this exercise!\n",
    "\n",
    "# Step 1: Generate customer data (3 features)\n",
    "X_ecommerce, _ = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=3,\n",
    "    centers=# YOUR CODE - try different values!,\n",
    "    cluster_std=# YOUR CODE - control how spread out clusters are,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Apply K-Means (start with K=3)\n",
    "kmeans_ecom = KMeans(\n",
    "    n_clusters=# YOUR CODE,\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "labels_ecom = # YOUR CODE - fit and predict\n",
    "\n",
    "# Step 3: Find optimal K using elbow method\n",
    "inertias_ecom = []\n",
    "k_range = range(1, 10)\n",
    "\n",
    "for k in k_range:\n",
    "    # YOUR CODE - fit K-Means with different K values\n",
    "    # Append inertia to list\n",
    "    pass\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "# YOUR CODE - create elbow plot\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Evaluate best clustering\n",
    "best_k = # YOUR CODE - choose based on elbow\n",
    "kmeans_best = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "labels_best = kmeans_best.fit_predict(X_ecommerce)\n",
    "\n",
    "silhouette = # YOUR CODE\n",
    "davies_bouldin = # YOUR CODE\n",
    "\n",
    "print(f\"Best K: {best_k}\")\n",
    "print(f\"Silhouette Score: {silhouette:.3f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.3f}\")\n",
    "\n",
    "# Step 5: Interpret segments\n",
    "print(\"\\nğŸ¯ CUSTOMER SEGMENTS:\")\n",
    "for i in range(best_k):\n",
    "    cluster_data = X_ecommerce[labels_best == i]\n",
    "    print(f\"\\nSegment {i}: {len(cluster_data)} customers\")\n",
    "    print(f\"  Avg Order Value: ${cluster_data[:, 0].mean():.2f}\")\n",
    "    print(f\"  Purchases/Month: {cluster_data[:, 1].mean():.1f}\")\n",
    "    print(f\"  Days Since Last: {cluster_data[:, 2].mean():.1f}\")\n",
    "    # YOUR CODE - add interpretation (e.g., \"VIP customers\", \"At-risk\", etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### âœ… Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Customer Segmentation\n",
    "\n",
    "# Step 1: Generate customer data\n",
    "X_ecommerce, _ = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=3,\n",
    "    centers=4,  # 4 natural segments\n",
    "    cluster_std=1.0,  # Moderate spread\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Make data more realistic (positive values, different scales)\n",
    "X_ecommerce[:, 0] = X_ecommerce[:, 0] * 20 + 100  # Avg order: $80-120\n",
    "X_ecommerce[:, 1] = np.abs(X_ecommerce[:, 1] * 2 + 5)  # Purchases: 1-10/month\n",
    "X_ecommerce[:, 2] = np.abs(X_ecommerce[:, 2] * 5 + 15)  # Days since: 5-30\n",
    "\n",
    "# Step 2: Find optimal K\n",
    "inertias_ecom = []\n",
    "silhouette_scores_ecom = []\n",
    "k_range = range(2, 10)\n",
    "\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_ecommerce)\n",
    "    inertias_ecom.append(km.inertia_)\n",
    "    silhouette_scores_ecom.append(silhouette_score(X_ecommerce, km.labels_))\n",
    "\n",
    "# Plot elbow\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(k_range, inertias_ecom, marker='o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia', fontsize=12)\n",
    "axes[0].set_title('Elbow Method: E-commerce Customer Segments', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='Optimal K=4')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(k_range, silhouette_scores_ecom, marker='s', color='green', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score by K', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Good threshold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Fit best model\n",
    "best_k = 4\n",
    "kmeans_best = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "labels_best = kmeans_best.fit_predict(X_ecommerce)\n",
    "\n",
    "silhouette = silhouette_score(X_ecommerce, labels_best)\n",
    "davies_bouldin = davies_bouldin_score(X_ecommerce, labels_best)\n",
    "\n",
    "print(\"ğŸ“Š CLUSTERING EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best K: {best_k}\")\n",
    "print(f\"Silhouette Score: {silhouette:.3f} - {'âœ… Good!' if silhouette > 0.5 else 'âš ï¸ Fair'}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.3f} - {'âœ… Good!' if davies_bouldin < 1.0 else 'âš ï¸ Fair'}\")\n",
    "\n",
    "# Step 4: Interpret segments\n",
    "segment_names = []\n",
    "print(\"\\nğŸ¯ CUSTOMER SEGMENTS & MARKETING STRATEGY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(best_k):\n",
    "    cluster_data = X_ecommerce[labels_best == i]\n",
    "    avg_order = cluster_data[:, 0].mean()\n",
    "    purchases_month = cluster_data[:, 1].mean()\n",
    "    days_since = cluster_data[:, 2].mean()\n",
    "    \n",
    "    # Classify segment based on behavior\n",
    "    if avg_order > 110 and purchases_month > 6:\n",
    "        name = \"ğŸ’ VIP Customers\"\n",
    "        strategy = \"Loyalty rewards, early access to sales, personalized service\"\n",
    "    elif purchases_month > 6:\n",
    "        name = \"ğŸ”¥ Frequent Buyers\"\n",
    "        strategy = \"Upsell premium products, subscription offers\"\n",
    "    elif avg_order > 110:\n",
    "        name = \"ğŸ‘‘ High-Value Buyers\"\n",
    "        strategy = \"Encourage repeat purchases with targeted campaigns\"\n",
    "    elif days_since > 20:\n",
    "        name = \"âš ï¸  At-Risk/Dormant\"\n",
    "        strategy = \"Re-engagement campaigns, discount codes, 'we miss you' emails\"\n",
    "    else:\n",
    "        name = \"ğŸ†• Casual Shoppers\"\n",
    "        strategy = \"Nurture with content, build brand loyalty, first-time buyer discounts\"\n",
    "    \n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  ğŸ“Š Size: {len(cluster_data)} customers ({len(cluster_data)/len(X_ecommerce)*100:.1f}%)\")\n",
    "    print(f\"  ğŸ’° Avg Order Value: ${avg_order:.2f}\")\n",
    "    print(f\"  ğŸ“¦ Purchases/Month: {purchases_month:.1f}\")\n",
    "    print(f\"  ğŸ“… Days Since Last: {days_since:.1f}\")\n",
    "    print(f\"  ğŸ¯ Strategy: {strategy}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nğŸ’¡ Business Value:\")\n",
    "print(\"   âœ… Personalized marketing campaigns for each segment\")\n",
    "print(\"   âœ… Efficient resource allocation (focus on high-value)\")\n",
    "print(\"   âœ… Identify at-risk customers for retention efforts\")\n",
    "print(\"   âœ… Optimize product recommendations per segment\")\n",
    "print(\"   âœ… Predict lifetime value and churn risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Algorithm Comparison Cheat Sheet\n",
    "\n",
    "| Feature | K-Means | Hierarchical | DBSCAN |\n",
    "|---------|---------|--------------|--------|\n",
    "| **Need to specify K?** | âœ… Yes | âœ… Yes | âŒ No |\n",
    "| **Cluster shape** | Spherical only | Any | Any |\n",
    "| **Handles noise/outliers** | âŒ No | âŒ No | âœ… Yes |\n",
    "| **Scalability** | ğŸŸ¢ Fast (millions) | ğŸ”´ Slow (thousands) | ğŸŸ¡ Medium |\n",
    "| **Deterministic** | âŒ No (random init) | âœ… Yes | âœ… Yes |\n",
    "| **Memory usage** | ğŸŸ¢ Low | ğŸ”´ High | ğŸŸ¡ Medium |\n",
    "| **Best for** | Large data, spherical | Small data, hierarchy | Complex shapes, noise |\n",
    "\n",
    "**Decision tree:**\n",
    "1. **Data > 100K points?** â†’ K-Means\n",
    "2. **Need hierarchy?** â†’ Hierarchical\n",
    "3. **Complex shapes or noise?** â†’ DBSCAN\n",
    "4. **Unknown K, simple shapes?** â†’ K-Means + Elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "**You just mastered:**\n",
    "- âœ… K-Means clustering and elbow method\n",
    "- âœ… Hierarchical clustering and dendrograms\n",
    "- âœ… DBSCAN for density-based clustering and outlier detection\n",
    "- âœ… Clustering evaluation metrics (Silhouette, Davies-Bouldin)\n",
    "- âœ… Real AI applications: RAG document clustering, customer segmentation\n",
    "- âœ… When to use which algorithm\n",
    "\n",
    "**ğŸ¯ Key Takeaways:**\n",
    "1. **K-Means** = Fast, simple, needs K, assumes spherical clusters\n",
    "2. **Hierarchical** = Visual tree, no K needed initially, slow on large data\n",
    "3. **DBSCAN** = Finds any shape, detects outliers, no K needed\n",
    "4. **Evaluation** = Use Silhouette & Davies-Bouldin for quality measurement\n",
    "5. **Real AI** = RAG systems use clustering for faster retrieval!\n",
    "\n",
    "**ğŸš€ Practice Challenge (Before Day 2!):**\n",
    "\n",
    "Apply clustering to a real dataset:\n",
    "1. Load a dataset (e.g., Iris, customer data, or text embeddings)\n",
    "2. Try all three algorithms (K-Means, Hierarchical, DBSCAN)\n",
    "3. Compare results using evaluation metrics\n",
    "4. Visualize and interpret the clusters\n",
    "5. Which algorithm works best? Why?\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“š Next Lesson:** Day 2 - Dimensionality Reduction (PCA, t-SNE, UMAP for visualizing high-dimensional embeddings!)\n",
    "\n",
    "**ğŸ’¬ Questions?** Try clustering different shapes of data - you'll see when each algorithm shines!\n",
    "\n",
    "---\n",
    "\n",
    "*\"In unsupervised learning, the patterns you find are often more interesting than the patterns you expected!\"* ğŸ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
