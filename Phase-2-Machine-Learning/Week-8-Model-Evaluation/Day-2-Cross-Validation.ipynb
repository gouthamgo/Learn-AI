{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Day 2: Cross-Validation\n",
    "\n",
    "**üéØ Goal:** Learn how to get reliable, trustworthy model performance estimates\n",
    "\n",
    "**‚è±Ô∏è Time:** 45-60 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Single train-test split can be misleading (lucky or unlucky split)\n",
    "- Cross-validation gives you confidence your model will work in production\n",
    "- Standard practice at OpenAI, Google, Meta for model development\n",
    "- Critical for research papers, Kaggle competitions, and real deployments\n",
    "- Prevents overfitting and gives realistic performance estimates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì The Problem: Is Your Model Really That Good?\n",
    "\n",
    "**Scenario:** You built an AI model and tested it:\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {score:.2%}\")  # 95%! Amazing!\n",
    "```\n",
    "\n",
    "**But wait...**\n",
    "- What if your test set happened to be easy? ü§î\n",
    "- What if you got lucky with the random split?\n",
    "- Will it still be 95% with different data?\n",
    "\n",
    "**Solution: Cross-Validation!** Test your model on multiple different splits. üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    LeaveOneOut,\n",
    "    cross_val_score,\n",
    "    cross_validate\n",
    ")\n",
    "from sklearn.datasets import make_classification, load_iris, load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Set style and random seed\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ Single Split vs Cross-Validation\n",
    "\n",
    "Let's see the difference with a real example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a real dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Method 1: Single split (traditional way)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "single_score = model.score(X_test, y_test)\n",
    "\n",
    "print(\"üé≤ SINGLE TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy: {single_score:.2%}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Problem: This is based on only ONE random split!\")\n",
    "print(f\"   What if we got lucky (or unlucky)?\\n\")\n",
    "\n",
    "# Method 2: Multiple splits (cross-validation)\n",
    "print(\"üîÑ CROSS-VALIDATION (5 different splits)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Try 5 different random splits\n",
    "scores = []\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "    print(f\"Split {i+1}: {score:.2%}\")\n",
    "\n",
    "print(f\"\\nüìä Average: {np.mean(scores):.2%} (+/- {np.std(scores):.2%})\")\n",
    "print(f\"\\n‚úÖ Better! Now we have confidence in our estimate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö K-Fold Cross-Validation\n",
    "\n",
    "**How it works:**\n",
    "1. Split your data into K equal parts (folds)\n",
    "2. For each fold:\n",
    "   - Use it as test set\n",
    "   - Use other K-1 folds as training set\n",
    "   - Train model and evaluate\n",
    "3. Average the K scores\n",
    "\n",
    "**Visual:**\n",
    "```\n",
    "5-Fold Cross-Validation:\n",
    "Fold 1: [TEST] [TRAIN] [TRAIN] [TRAIN] [TRAIN]\n",
    "Fold 2: [TRAIN] [TEST] [TRAIN] [TRAIN] [TRAIN]\n",
    "Fold 3: [TRAIN] [TRAIN] [TEST] [TRAIN] [TRAIN]\n",
    "Fold 4: [TRAIN] [TRAIN] [TRAIN] [TEST] [TRAIN]\n",
    "Fold 5: [TRAIN] [TRAIN] [TRAIN] [TRAIN] [TEST]\n",
    "```\n",
    "\n",
    "**Common K values:**\n",
    "- K=5: Fast, good for large datasets\n",
    "- K=10: Standard choice, good balance\n",
    "- K=number of samples: Leave-One-Out (expensive!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize K-Fold Cross-Validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create sample data\n",
    "X_sample = np.arange(20).reshape(-1, 1)\n",
    "y_sample = np.arange(20)\n",
    "\n",
    "# Create 5-fold CV\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Visualize the splits\n",
    "fig, axes = plt.subplots(5, 1, figsize=(12, 8))\n",
    "fig.suptitle('5-Fold Cross-Validation Visualization', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(kfold.split(X_sample)):\n",
    "    # Create array to color\n",
    "    colors = np.array(['blue'] * len(X_sample))\n",
    "    colors[test_idx] = 'red'\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].scatter(range(len(X_sample)), [1]*len(X_sample), \n",
    "                     c=colors, s=200, alpha=0.6)\n",
    "    axes[idx].set_xlim(-1, 20)\n",
    "    axes[idx].set_ylim(0.5, 1.5)\n",
    "    axes[idx].set_yticks([])\n",
    "    axes[idx].set_title(f'Fold {idx+1}: {len(test_idx)} test samples (red), '\n",
    "                       f'{len(train_idx)} training samples (blue)', \n",
    "                       fontsize=11)\n",
    "    axes[idx].set_xlabel('Sample Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üé® Legend:\")\n",
    "print(\"  üî¥ Red = Test set for this fold\")\n",
    "print(\"  üîµ Blue = Training set for this fold\")\n",
    "print(\"\\nüí° Every sample gets to be in the test set exactly once!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Implementing K-Fold Cross-Validation\n",
    "\n",
    "Let's use it properly with `cross_val_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a real dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "print(f\"üìä Dataset: {len(X)} samples, {X.shape[1]} features\")\n",
    "print(f\"   Classes: {np.bincount(y)} (0=malignant, 1=benign)\\n\")\n",
    "\n",
    "# Create model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"üîÑ 10-Fold Cross-Validation Results\")\n",
    "print(\"=\" * 50)\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"Fold {i:2d}: {score:.4f} ({score*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"üìä Mean Accuracy: {cv_scores.mean():.4f} ({cv_scores.mean()*100:.2f}%)\")\n",
    "print(f\"üìä Std Deviation: {cv_scores.std():.4f} ({cv_scores.std()*100:.2f}%)\")\n",
    "print(f\"üìä 95% Confidence Interval: {cv_scores.mean():.2%} +/- {1.96 * cv_scores.std():.2%}\")\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), cv_scores, 'bo-', linewidth=2, markersize=8, label='Fold Scores')\n",
    "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', linewidth=2, label=f'Mean: {cv_scores.mean():.2%}')\n",
    "plt.fill_between(range(1, 11), \n",
    "                 cv_scores.mean() - cv_scores.std(), \n",
    "                 cv_scores.mean() + cv_scores.std(), \n",
    "                 alpha=0.2, color='red', label=f'¬±1 Std Dev')\n",
    "plt.xlabel('Fold Number', fontsize=12)\n",
    "plt.ylabel('Accuracy Score', fontsize=12)\n",
    "plt.title('10-Fold Cross-Validation Performance', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.ylim([0.9, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(f\"   - Average performance: {cv_scores.mean():.2%}\")\n",
    "print(f\"   - Performance varies by {cv_scores.std():.2%} between folds\")\n",
    "print(f\"   - Low variance = Model is stable! ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Stratified K-Fold: For Imbalanced Data\n",
    "\n",
    "**Problem with regular K-Fold:**\n",
    "- Random splits might create imbalanced folds\n",
    "- One fold might have mostly class 0, another mostly class 1\n",
    "\n",
    "**Solution: Stratified K-Fold**\n",
    "- Preserves class distribution in each fold\n",
    "- Each fold has the same percentage of each class as the original dataset\n",
    "\n",
    "**When to use:**\n",
    "- ‚úÖ Imbalanced datasets (fraud, rare disease, etc.)\n",
    "- ‚úÖ Classification problems (always a good choice!)\n",
    "- ‚ùå Regression (use regular K-Fold)\n",
    "\n",
    "**Real AI Use:**\n",
    "- Standard for NLP classification tasks\n",
    "- Medical diagnosis with rare diseases\n",
    "- Fraud detection, anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imbalanced dataset\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    weights=[0.9, 0.1],  # 90% class 0, 10% class 1 (imbalanced!)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"üìä Imbalanced Dataset:\")\n",
    "print(f\"   Total samples: {len(y_imb)}\")\n",
    "print(f\"   Class 0: {np.sum(y_imb == 0)} ({np.sum(y_imb == 0)/len(y_imb)*100:.1f}%)\")\n",
    "print(f\"   Class 1: {np.sum(y_imb == 1)} ({np.sum(y_imb == 1)/len(y_imb)*100:.1f}%)\")\n",
    "print(f\"   ‚ö†Ô∏è  Very imbalanced! (90/10 split)\\n\")\n",
    "\n",
    "# Compare regular K-Fold vs Stratified K-Fold\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Regular K-Fold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "regular_scores = cross_val_score(model, X_imb, y_imb, cv=kfold, scoring='f1')\n",
    "\n",
    "# Stratified K-Fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "stratified_scores = cross_val_score(model, X_imb, y_imb, cv=stratified_kfold, scoring='f1')\n",
    "\n",
    "# Compare results\n",
    "print(\"üîÑ Regular K-Fold:\")\n",
    "print(f\"   F1 Scores: {regular_scores}\")\n",
    "print(f\"   Mean: {regular_scores.mean():.4f}, Std: {regular_scores.std():.4f}\\n\")\n",
    "\n",
    "print(\"‚úÖ Stratified K-Fold (BETTER for imbalanced data):\")\n",
    "print(f\"   F1 Scores: {stratified_scores}\")\n",
    "print(f\"   Mean: {stratified_scores.mean():.4f}, Std: {stratified_scores.std():.4f}\\n\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(5)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, regular_scores, width, label='Regular K-Fold', alpha=0.7, color='orange')\n",
    "bars2 = ax.bar(x + width/2, stratified_scores, width, label='Stratified K-Fold', alpha=0.7, color='green')\n",
    "\n",
    "ax.axhline(y=regular_scores.mean(), color='orange', linestyle='--', alpha=0.5, \n",
    "          label=f'Regular Mean: {regular_scores.mean():.3f}')\n",
    "ax.axhline(y=stratified_scores.mean(), color='green', linestyle='--', alpha=0.5,\n",
    "          label=f'Stratified Mean: {stratified_scores.mean():.3f}')\n",
    "\n",
    "ax.set_xlabel('Fold Number', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.set_title('Regular vs Stratified K-Fold on Imbalanced Data', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Fold {i+1}' for i in range(5)])\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   Stratified K-Fold ensures each fold has the same 90/10 class distribution!\")\n",
    "print(\"   This gives more reliable and consistent scores. ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Leave-One-Out Cross-Validation (LOOCV)\n",
    "\n",
    "**Extreme case:** K = number of samples\n",
    "\n",
    "**How it works:**\n",
    "- For each sample:\n",
    "  - Use it as test set (1 sample)\n",
    "  - Use all others as training set (N-1 samples)\n",
    "  - Train and evaluate\n",
    "- Average all N scores\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Maximum training data (N-1 samples)\n",
    "- ‚úÖ No randomness (deterministic)\n",
    "- ‚úÖ Good for very small datasets\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Very slow (train N models!)\n",
    "- ‚ùå High variance in estimates\n",
    "- ‚ùå Impractical for large datasets\n",
    "\n",
    "**When to use:**\n",
    "- Only for small datasets (< 1000 samples)\n",
    "- When training is fast\n",
    "- Research purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a small dataset for LOOCV\n",
    "iris = load_iris()\n",
    "X_small = iris.data[:50]  # Use only 50 samples\n",
    "y_small = iris.target[:50]\n",
    "\n",
    "print(f\"üìä Small Dataset: {len(X_small)} samples\\n\")\n",
    "\n",
    "# Create a fast model (Logistic Regression)\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Compare different CV strategies\n",
    "print(\"‚è±Ô∏è  Comparing Cross-Validation Methods:\\n\")\n",
    "\n",
    "import time\n",
    "\n",
    "# 5-Fold\n",
    "start = time.time()\n",
    "scores_5fold = cross_val_score(model, X_small, y_small, cv=5)\n",
    "time_5fold = time.time() - start\n",
    "print(f\"5-Fold CV:\")\n",
    "print(f\"   Mean Score: {scores_5fold.mean():.4f}\")\n",
    "print(f\"   Time: {time_5fold:.4f} seconds\")\n",
    "print(f\"   Models trained: 5\\n\")\n",
    "\n",
    "# 10-Fold\n",
    "start = time.time()\n",
    "scores_10fold = cross_val_score(model, X_small, y_small, cv=10)\n",
    "time_10fold = time.time() - start\n",
    "print(f\"10-Fold CV:\")\n",
    "print(f\"   Mean Score: {scores_10fold.mean():.4f}\")\n",
    "print(f\"   Time: {time_10fold:.4f} seconds\")\n",
    "print(f\"   Models trained: 10\\n\")\n",
    "\n",
    "# Leave-One-Out\n",
    "loo = LeaveOneOut()\n",
    "start = time.time()\n",
    "scores_loo = cross_val_score(model, X_small, y_small, cv=loo)\n",
    "time_loo = time.time() - start\n",
    "print(f\"Leave-One-Out CV:\")\n",
    "print(f\"   Mean Score: {scores_loo.mean():.4f}\")\n",
    "print(f\"   Time: {time_loo:.4f} seconds\")\n",
    "print(f\"   Models trained: {len(X_small)} (one per sample!)\\n\")\n",
    "\n",
    "# Visualize\n",
    "methods = ['5-Fold', '10-Fold', 'LOOCV']\n",
    "means = [scores_5fold.mean(), scores_10fold.mean(), scores_loo.mean()]\n",
    "times = [time_5fold, time_10fold, time_loo]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy\n",
    "ax1.bar(methods, means, color=['blue', 'green', 'orange'], alpha=0.7)\n",
    "ax1.set_ylabel('Mean Accuracy', fontsize=12)\n",
    "ax1.set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0.9, 1.0])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(means):\n",
    "    ax1.text(i, v + 0.005, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: Time\n",
    "ax2.bar(methods, times, color=['blue', 'green', 'orange'], alpha=0.7)\n",
    "ax2.set_ylabel('Time (seconds)', fontsize=12)\n",
    "ax2.set_title('Computation Time', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(times):\n",
    "    ax2.text(i, v + 0.001, f'{v:.3f}s', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Takeaway:\")\n",
    "print(f\"   LOOCV is {time_loo/time_5fold:.1f}x slower than 5-Fold!\")\n",
    "print(f\"   But gives similar accuracy estimate.\")\n",
    "print(f\"   ‚úÖ For most cases, 5-Fold or 10-Fold is best!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Multiple Metrics with Cross-Validation\n",
    "\n",
    "Don't just evaluate accuracy! Get precision, recall, F1 all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define multiple scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'roc_auc': 'roc_auc'\n",
    "}\n",
    "\n",
    "# Perform cross-validation with multiple metrics\n",
    "cv_results = cross_validate(model, X, y, cv=5, scoring=scoring, return_train_score=True)\n",
    "\n",
    "# Display results\n",
    "print(\"üìä COMPREHENSIVE CROSS-VALIDATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<15} {'Test Mean':<12} {'Test Std':<12} {'Train Mean':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "    test_key = f'test_{metric}'\n",
    "    train_key = f'train_{metric}'\n",
    "    test_mean = cv_results[test_key].mean()\n",
    "    test_std = cv_results[test_key].std()\n",
    "    train_mean = cv_results[train_key].mean()\n",
    "    \n",
    "    print(f\"{metric.upper():<15} {test_mean:>6.4f}       {test_std:>6.4f}       {train_mean:>6.4f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a detailed DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    'Test Score': [\n",
    "        cv_results['test_accuracy'].mean(),\n",
    "        cv_results['test_precision'].mean(),\n",
    "        cv_results['test_recall'].mean(),\n",
    "        cv_results['test_f1'].mean(),\n",
    "        cv_results['test_roc_auc'].mean()\n",
    "    ],\n",
    "    'Train Score': [\n",
    "        cv_results['train_accuracy'].mean(),\n",
    "        cv_results['train_precision'].mean(),\n",
    "        cv_results['train_recall'].mean(),\n",
    "        cv_results['train_f1'].mean(),\n",
    "        cv_results['train_roc_auc'].mean()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, results_df['Train Score'], width, \n",
    "              label='Training Score', alpha=0.7, color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, results_df['Test Score'], width, \n",
    "              label='Test Score (CV)', alpha=0.7, color='orange')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Cross-Validation: All Metrics (Train vs Test)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Metric'])\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0.9, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "gap = results_df['Train Score'].mean() - results_df['Test Score'].mean()\n",
    "print(f\"   Average Train-Test Gap: {gap:.4f}\")\n",
    "if gap < 0.05:\n",
    "    print(\"   ‚úÖ Great! Model generalizes well (low overfitting)\")\n",
    "elif gap < 0.10:\n",
    "    print(\"   ‚ö†Ô∏è  Some overfitting detected\")\n",
    "else:\n",
    "    print(\"   ‚ùå High overfitting! Model memorizing training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Real AI Example: Robust Model Evaluation for Production\n",
    "\n",
    "**Scenario:** You're deploying an AI content moderation system for a social media platform.\n",
    "\n",
    "**Requirements:**\n",
    "- Must be reliable across different types of content\n",
    "- Performance must be consistent\n",
    "- Need confidence intervals for stakeholders\n",
    "\n",
    "Let's use cross-validation properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate content moderation dataset\n",
    "# 1 = toxic content, 0 = safe content\n",
    "X_content, y_content = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=50,  # Text embeddings from BERT/GPT\n",
    "    n_informative=40,\n",
    "    n_redundant=10,\n",
    "    n_classes=2,\n",
    "    weights=[0.85, 0.15],  # 15% toxic content\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"üåê CONTENT MODERATION AI - PRODUCTION EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: {len(y_content)} posts\")\n",
    "print(f\"Safe: {np.sum(y_content == 0)} ({np.sum(y_content == 0)/len(y_content)*100:.1f}%)\")\n",
    "print(f\"Toxic: {np.sum(y_content == 1)} ({np.sum(y_content == 1)/len(y_content)*100:.1f}%)\\n\")\n",
    "\n",
    "# Test multiple models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "}\n",
    "\n",
    "# Stratified 10-Fold CV (best for imbalanced classification)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_content, y_content, cv=cv, scoring='f1')\n",
    "    results[name] = {\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'scores': scores,\n",
    "        'ci_lower': scores.mean() - 1.96 * scores.std(),\n",
    "        'ci_upper': scores.mean() + 1.96 * scores.std()\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"üìä Model Comparison (F1-Score):\\n\")\n",
    "print(f\"{'Model':<25} {'Mean':<10} {'Std':<10} {'95% CI'}\")\n",
    "print(\"-\" * 60)\n",
    "for name, res in results.items():\n",
    "    ci = f\"[{res['ci_lower']:.3f}, {res['ci_upper']:.3f}]\"\n",
    "    print(f\"{name:<25} {res['mean']:.4f}     {res['std']:.4f}     {ci}\")\n",
    "\n",
    "# Visualize with confidence intervals\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Mean scores with error bars\n",
    "model_names = list(results.keys())\n",
    "means = [results[m]['mean'] for m in model_names]\n",
    "stds = [results[m]['std'] for m in model_names]\n",
    "\n",
    "ax1.bar(model_names, means, yerr=stds, capsize=10, alpha=0.7, \n",
    "       color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "ax1.set_ylabel('F1-Score', fontsize=12)\n",
    "ax1.set_title('Model Performance with Standard Deviation', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, (m, s) in enumerate(zip(means, stds)):\n",
    "    ax1.text(i, m + s + 0.02, f'{m:.3f}\\n¬±{s:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: Distribution of scores across folds\n",
    "positions = [1, 2, 3]\n",
    "bp = ax2.boxplot([results[m]['scores'] for m in model_names],\n",
    "                 labels=model_names,\n",
    "                 patch_artist=True,\n",
    "                 notch=True,\n",
    "                 showmeans=True)\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax2.set_ylabel('F1-Score', fontsize=12)\n",
    "ax2.set_title('Score Distribution Across 10 Folds', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommendation\n",
    "best_model = max(results.keys(), key=lambda k: results[k]['mean'])\n",
    "print(f\"\\nüèÜ RECOMMENDATION FOR PRODUCTION:\")\n",
    "print(f\"   Best Model: {best_model}\")\n",
    "print(f\"   Expected F1-Score: {results[best_model]['mean']:.2%}\")\n",
    "print(f\"   95% Confidence: {results[best_model]['ci_lower']:.2%} - {results[best_model]['ci_upper']:.2%}\")\n",
    "print(f\"   Consistency (low std): {results[best_model]['std']:.4f} ‚úÖ\")\n",
    "print(f\"\\nüí° This means in production, you can expect F1-Score between \")\n",
    "print(f\"   {results[best_model]['ci_lower']:.1%} and {results[best_model]['ci_upper']:.1%} with 95% confidence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Evaluate Your Own Model\n",
    "\n",
    "**Challenge:** You're building a spam email detector.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the dataset (provided below)\n",
    "2. Use Stratified 5-Fold CV (data is imbalanced)\n",
    "3. Evaluate with multiple metrics\n",
    "4. Compare 3 different models\n",
    "5. Choose the best one for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spam detection dataset\n",
    "X_spam, y_spam = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=30,\n",
    "    n_informative=25,\n",
    "    n_classes=2,\n",
    "    weights=[0.7, 0.3],  # 30% spam\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"üìß Spam Detection Dataset\")\n",
    "print(f\"Total emails: {len(y_spam)}\")\n",
    "print(f\"Ham (normal): {np.sum(y_spam == 0)}\")\n",
    "print(f\"Spam: {np.sum(y_spam == 1)}\")\n",
    "print(\"\\nüéØ YOUR TASK: Complete the evaluation below!\\n\")\n",
    "\n",
    "# YOUR CODE HERE!\n",
    "# 1. Create 3 different models\n",
    "model1 = # YOUR CODE\n",
    "model2 = # YOUR CODE\n",
    "model3 = # YOUR CODE\n",
    "\n",
    "# 2. Create Stratified K-Fold CV\n",
    "cv = # YOUR CODE (hint: use StratifiedKFold with 5 splits)\n",
    "\n",
    "# 3. Evaluate each model\n",
    "# Hint: use cross_val_score with scoring='f1'\n",
    "scores1 = # YOUR CODE\n",
    "scores2 = # YOUR CODE\n",
    "scores3 = # YOUR CODE\n",
    "\n",
    "# 4. Print results\n",
    "print(\"Model 1:\", scores1.mean())\n",
    "print(\"Model 2:\", scores2.mean())\n",
    "print(\"Model 3:\", scores3.mean())\n",
    "\n",
    "# 5. Which model would you choose? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution (Run after trying!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "print(\"üìß SPAM DETECTION - COMPLETE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Create models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "}\n",
    "\n",
    "# 2. Stratified K-Fold (important for imbalanced data!)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 3. Evaluate with multiple metrics\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "print(\"\\nüìä Evaluation Results:\\n\")\n",
    "all_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    model_scores = {}\n",
    "    for metric in scoring:\n",
    "        scores = cross_val_score(model, X_spam, y_spam, cv=cv, scoring=metric)\n",
    "        model_scores[metric] = scores\n",
    "        print(f\"{metric.upper():<12}: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "    \n",
    "    all_results[name] = model_scores\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Spam Detection: Model Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, metric in enumerate(scoring):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    means = [all_results[m][metric].mean() for m in models.keys()]\n",
    "    stds = [all_results[m][metric].std() for m in models.keys()]\n",
    "    \n",
    "    bars = ax.bar(models.keys(), means, yerr=stds, capsize=5, \n",
    "                 alpha=0.7, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "    ax.set_ylabel(metric.upper(), fontsize=11)\n",
    "    ax.set_title(f'{metric.upper()} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim([0.7, 1.0])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, (m, s) in enumerate(zip(means, stds)):\n",
    "        ax.text(i, m + s + 0.01, f'{m:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final recommendation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üèÜ FINAL RECOMMENDATION:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Choose based on F1-score (balance of precision and recall)\n",
    "f1_scores = {name: all_results[name]['f1'].mean() for name in models.keys()}\n",
    "best_model = max(f1_scores.keys(), key=lambda k: f1_scores[k])\n",
    "\n",
    "print(f\"\\n‚úÖ Best Model: {best_model}\")\n",
    "print(f\"\\nüìä Performance:\")\n",
    "for metric in scoring:\n",
    "    mean = all_results[best_model][metric].mean()\n",
    "    std = all_results[best_model][metric].std()\n",
    "    print(f\"   {metric.upper():<12}: {mean:.2%} (+/- {std:.2%})\")\n",
    "\n",
    "print(f\"\\nüí° Why {best_model}?\")\n",
    "print(\"   - Highest F1-score (best balance of precision/recall)\")\n",
    "print(\"   - Low variance (consistent across folds)\")\n",
    "print(\"   - Good for spam detection where both metrics matter\")\n",
    "print(\"\\nüöÄ Ready for production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Cross-Validation Best Practices\n",
    "\n",
    "**1. Choose the Right K:**\n",
    "- K=5: Fast, good for large datasets (>10,000 samples)\n",
    "- K=10: Standard choice, good balance\n",
    "- K=20+: Overkill for most cases\n",
    "- LOOCV: Only for tiny datasets (<1000 samples)\n",
    "\n",
    "**2. Always Use Stratified for Classification:**\n",
    "- Preserves class distribution\n",
    "- More reliable estimates\n",
    "- Standard in Scikit-learn's `cross_val_score`\n",
    "\n",
    "**3. Report Mean AND Standard Deviation:**\n",
    "- Mean alone can be misleading\n",
    "- High std = model is unstable\n",
    "- Always report: \"95.2% (+/- 2.1%)\"\n",
    "\n",
    "**4. Use Multiple Metrics:**\n",
    "- Accuracy + Precision + Recall + F1\n",
    "- Different metrics tell different stories\n",
    "- Use `cross_validate` for multiple metrics\n",
    "\n",
    "**5. Watch for Overfitting:**\n",
    "- Compare train vs test scores\n",
    "- Large gap = overfitting\n",
    "- Use `return_train_score=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just mastered:**\n",
    "- ‚úÖ Why single train-test split is unreliable\n",
    "- ‚úÖ K-Fold Cross-Validation\n",
    "- ‚úÖ Stratified K-Fold for imbalanced data\n",
    "- ‚úÖ Leave-One-Out CV and when to use it\n",
    "- ‚úÖ Evaluating multiple metrics at once\n",
    "- ‚úÖ Getting confidence intervals for production\n",
    "- ‚úÖ Real AI application: robust model evaluation\n",
    "\n",
    "**üéØ Key Takeaways:**\n",
    "1. **Always use cross-validation** for reliable estimates\n",
    "2. **Stratified K-Fold** is best for classification\n",
    "3. **Report mean ¬± std** for transparency\n",
    "4. **K=5 or K=10** works for most cases\n",
    "5. **Multiple metrics** give complete picture\n",
    "\n",
    "**üöÄ Practice Exercise (Do before Day 3!):**\n",
    "\n",
    "Load the Iris dataset and:\n",
    "1. Compare Regular K-Fold vs Stratified K-Fold\n",
    "2. Use K=3, 5, 10 - how do results differ?\n",
    "3. Evaluate with accuracy, precision, recall, F1\n",
    "4. Which model would you deploy to production?\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Next Lesson:** Day 3 - Hyperparameter Tuning (Find the BEST model settings!)\n",
    "\n",
    "**üí¨ Questions?** Try different K values and see how estimates change!\n",
    "\n",
    "---\n",
    "\n",
    "*\"Cross-validation: Because one test set is never enough!\"* üîÑ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
