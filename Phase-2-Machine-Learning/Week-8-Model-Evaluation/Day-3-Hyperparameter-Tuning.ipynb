{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Day 3: Hyperparameter Tuning\n",
    "\n",
    "**üéØ Goal:** Find the best settings for your AI models to maximize performance\n",
    "\n",
    "**‚è±Ô∏è Time:** 45-60 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Default parameters are rarely optimal for your specific problem\n",
    "- Proper tuning can improve accuracy by 5-20%!\n",
    "- Essential for Kaggle competitions, research papers, production systems\n",
    "- Used to optimize GPT, BERT, transformers, and all modern AI\n",
    "- Difference between good model and winning model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î What Are Hyperparameters?\n",
    "\n",
    "**Parameters:** Learned from data during training\n",
    "- Example: Weights in neural networks\n",
    "- Model learns these automatically\n",
    "\n",
    "**Hyperparameters:** Set BEFORE training\n",
    "- Example: Number of trees in Random Forest\n",
    "- YOU must choose these\n",
    "- Huge impact on performance!\n",
    "\n",
    "**Examples:**\n",
    "- Random Forest: `n_estimators`, `max_depth`, `min_samples_split`\n",
    "- Neural Networks: `learning_rate`, `batch_size`, `num_layers`\n",
    "- GPT/Transformers: `num_heads`, `hidden_size`, `dropout_rate`\n",
    "\n",
    "**The Challenge:** How do you find the BEST combination? üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Set style and random seed\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùå The Naive Approach: Manual Trial & Error\n",
    "\n",
    "Let's see why manual tuning is painful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"üî¨ MANUAL HYPERPARAMETER TUNING (The Hard Way)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Try different combinations manually\n",
    "configs = [\n",
    "    {'n_estimators': 10, 'max_depth': 5},\n",
    "    {'n_estimators': 50, 'max_depth': 10},\n",
    "    {'n_estimators': 100, 'max_depth': 15},\n",
    "    {'n_estimators': 200, 'max_depth': 20},\n",
    "]\n",
    "\n",
    "results = []\n",
    "for config in configs:\n",
    "    model = RandomForestClassifier(**config, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    results.append(score)\n",
    "    print(f\"n_estimators={config['n_estimators']:3d}, max_depth={config['max_depth']:2d} ‚Üí Accuracy: {score:.4f}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Problems with Manual Tuning:\")\n",
    "print(\"   1. Time-consuming (tried only 4 combinations!)\")\n",
    "print(\"   2. May miss the best combination\")\n",
    "print(\"   3. No systematic search\")\n",
    "print(\"   4. Hard to explore many parameters\")\n",
    "print(\"\\nüí° Solution: Automated Hyperparameter Tuning! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Grid Search: Exhaustive Search\n",
    "\n",
    "**How it works:**\n",
    "1. Define a grid of hyperparameter values\n",
    "2. Try EVERY combination\n",
    "3. Use cross-validation for each\n",
    "4. Return the best combination\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],      # 3 values\n",
    "    'max_depth': [5, 10, 15, 20]        # 4 values\n",
    "}\n",
    "# Total combinations: 3 √ó 4 = 12\n",
    "# With 5-fold CV: 12 √ó 5 = 60 model trainings!\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Guarantees finding the best combination in the grid\n",
    "- ‚úÖ Simple and straightforward\n",
    "- ‚úÖ Good for few parameters\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Exponentially slow (curse of dimensionality)\n",
    "- ‚ùå Wastes time on bad regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"üîç GRID SEARCH\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Parameter grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "print(f\"\\nTotal combinations: {total_combinations}\")\n",
    "print(f\"With 5-fold CV: {total_combinations * 5} model trainings!\\n\")\n",
    "\n",
    "# Perform Grid Search\n",
    "print(\"‚è≥ Running Grid Search (this may take a minute)...\\n\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Grid Search completed in {grid_time:.2f} seconds\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üèÜ BEST HYPERPARAMETERS FOUND:\")\n",
    "print(\"=\" * 60)\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"üìä Test Score: {grid_search.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualizing Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display top 10 configurations\n",
    "print(\"üéØ TOP 10 CONFIGURATIONS:\\n\")\n",
    "top_10 = results_df.nsmallest(10, 'rank_test_score')[[\n",
    "    'param_n_estimators', 'param_max_depth', 'param_min_samples_split',\n",
    "    'param_min_samples_leaf', 'mean_test_score', 'std_test_score'\n",
    "]]\n",
    "\n",
    "print(top_10.to_string(index=False))\n",
    "\n",
    "# Visualize: n_estimators vs max_depth\n",
    "pivot_table = results_df.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_max_depth',\n",
    "    columns='param_n_estimators',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlGnBu', cbar_kws={'label': 'Accuracy'})\n",
    "plt.title('Grid Search: n_estimators vs max_depth (averaged over other params)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.xlabel('n_estimators', fontsize=12)\n",
    "plt.ylabel('max_depth', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insights from Heatmap:\")\n",
    "print(\"   - Darker blue = better performance\")\n",
    "print(\"   - Can see which parameter ranges work best\")\n",
    "print(\"   - Helps understand parameter interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ Random Search: Smarter Exploration\n",
    "\n",
    "**Problem with Grid Search:**\n",
    "- If you have 5 parameters with 10 values each: 10^5 = 100,000 combinations!\n",
    "- Completely impractical\n",
    "\n",
    "**Random Search Solution:**\n",
    "- Instead of trying ALL combinations, sample randomly\n",
    "- You decide how many combinations to try (e.g., 50)\n",
    "- Often finds good solutions faster!\n",
    "\n",
    "**Research Finding (Bergstra & Bengio, 2012):**\n",
    "- Random Search often outperforms Grid Search\n",
    "- Better explores the hyperparameter space\n",
    "- More efficient for high-dimensional spaces\n",
    "\n",
    "**Real AI Use:**\n",
    "- Standard for deep learning (too many hyperparameters)\n",
    "- BERT, GPT training uses variants of random search\n",
    "- Recommended by Google, OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions (not fixed values!)\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),           # Random integers between 50 and 300\n",
    "    'max_depth': randint(5, 30),                # Random integers between 5 and 30\n",
    "    'min_samples_split': randint(2, 20),        # Random integers between 2 and 20\n",
    "    'min_samples_leaf': randint(1, 10),         # Random integers between 1 and 10\n",
    "    'max_features': uniform(0.1, 0.9)           # Random float between 0.1 and 1.0\n",
    "}\n",
    "\n",
    "print(\"üé≤ RANDOM SEARCH\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Parameter distributions:\")\n",
    "for param, dist in param_distributions.items():\n",
    "    print(f\"  {param}: {dist}\")\n",
    "\n",
    "n_iterations = 50\n",
    "print(f\"\\nTrying {n_iterations} random combinations\")\n",
    "print(f\"With 5-fold CV: {n_iterations * 5} model trainings\\n\")\n",
    "\n",
    "# Perform Random Search\n",
    "print(\"‚è≥ Running Random Search...\\n\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_distributions,\n",
    "    n_iter=n_iterations,  # Number of random combinations to try\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "random_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Random Search completed in {random_time:.2f} seconds\")\n",
    "print(f\"‚ö° Grid Search took {grid_time:.2f} seconds\")\n",
    "print(f\"üöÄ Random Search was {grid_time/random_time:.2f}x faster!\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üèÜ BEST HYPERPARAMETERS FOUND:\")\n",
    "print(\"=\" * 60)\n",
    "for param, value in random_search.best_params_.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Best CV Score: {random_search.best_score_:.4f}\")\n",
    "print(f\"üìä Test Score: {random_search.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Compare with Grid Search\n",
    "print(\"\\n‚öñÔ∏è  GRID SEARCH vs RANDOM SEARCH:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Method':<20} {'Best CV Score':<15} {'Time (s)':<10} {'Evaluations'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Grid Search':<20} {grid_search.best_score_:<15.4f} {grid_time:<10.2f} {len(grid_search.cv_results_['params'])}\")\n",
    "print(f\"{'Random Search':<20} {random_search.best_score_:<15.4f} {random_time:<10.2f} {n_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualize Random Search Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze random search results\n",
    "random_results_df = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "# Plot score distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Random Search: Parameter Impact Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: n_estimators vs score\n",
    "axes[0, 0].scatter(random_results_df['param_n_estimators'], \n",
    "                   random_results_df['mean_test_score'],\n",
    "                   alpha=0.6, c=random_results_df['mean_test_score'], \n",
    "                   cmap='viridis', s=100)\n",
    "axes[0, 0].set_xlabel('n_estimators', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Mean CV Score', fontsize=11)\n",
    "axes[0, 0].set_title('n_estimators Impact', fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: max_depth vs score\n",
    "axes[0, 1].scatter(random_results_df['param_max_depth'], \n",
    "                   random_results_df['mean_test_score'],\n",
    "                   alpha=0.6, c=random_results_df['mean_test_score'], \n",
    "                   cmap='viridis', s=100)\n",
    "axes[0, 1].set_xlabel('max_depth', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Mean CV Score', fontsize=11)\n",
    "axes[0, 1].set_title('max_depth Impact', fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: min_samples_split vs score\n",
    "axes[1, 0].scatter(random_results_df['param_min_samples_split'], \n",
    "                   random_results_df['mean_test_score'],\n",
    "                   alpha=0.6, c=random_results_df['mean_test_score'], \n",
    "                   cmap='viridis', s=100)\n",
    "axes[1, 0].set_xlabel('min_samples_split', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Mean CV Score', fontsize=11)\n",
    "axes[1, 0].set_title('min_samples_split Impact', fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Score distribution over iterations\n",
    "sorted_scores = sorted(random_results_df['mean_test_score'], reverse=True)\n",
    "axes[1, 1].plot(range(len(sorted_scores)), sorted_scores, 'b-', linewidth=2)\n",
    "axes[1, 1].axhline(y=random_search.best_score_, color='r', linestyle='--', \n",
    "                   linewidth=2, label=f'Best: {random_search.best_score_:.4f}')\n",
    "axes[1, 1].fill_between(range(len(sorted_scores)), sorted_scores, \n",
    "                        alpha=0.3)\n",
    "axes[1, 1].set_xlabel('Iteration (sorted)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Mean CV Score', fontsize=11)\n",
    "axes[1, 1].set_title('Score Distribution', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Insights:\")\n",
    "print(f\"   - Top 10% of configurations: {sorted_scores[0]:.4f} - {sorted_scores[4]:.4f}\")\n",
    "print(f\"   - Worst configuration: {sorted_scores[-1]:.4f}\")\n",
    "print(f\"   - Improvement range: {(sorted_scores[0] - sorted_scores[-1]):.4f}\")\n",
    "print(f\"   ‚Üí Hyperparameter tuning made a {(sorted_scores[0] - sorted_scores[-1])*100:.2f}% difference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Real AI Example: Optimizing a Multimodal Classifier\n",
    "\n",
    "**Scenario:** You're building a classifier for a multimodal AI system (combining text and image features).\n",
    "\n",
    "**Goal:** Find the best Gradient Boosting model for this task.\n",
    "\n",
    "**Real-world application:**\n",
    "- Image + caption classification (Instagram, Pinterest)\n",
    "- Product categorization (Amazon, eBay)\n",
    "- Content moderation with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multimodal feature data\n",
    "print(\"üñºÔ∏è + üìù MULTIMODAL AI CLASSIFIER OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a challenging dataset\n",
    "X_multi, y_multi = make_classification(\n",
    "    n_samples=3000,\n",
    "    n_features=100,  # 50 image features + 50 text features\n",
    "    n_informative=80,\n",
    "    n_redundant=20,\n",
    "    n_classes=3,  # 3 categories\n",
    "    n_clusters_per_class=2,\n",
    "    weights=[0.5, 0.3, 0.2],  # Imbalanced\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(X_multi)} samples, {X_multi.shape[1]} features (multimodal)\")\n",
    "print(f\"Classes: {np.bincount(y_multi)}\")\n",
    "print(f\"Training set: {len(X_train_m)} samples\")\n",
    "print(f\"Test set: {len(X_test_m)} samples\\n\")\n",
    "\n",
    "# Gradient Boosting hyperparameters to tune\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'subsample': uniform(0.6, 0.4),  # 0.6 to 1.0\n",
    "    'max_features': uniform(0.5, 0.5)  # 0.5 to 1.0\n",
    "}\n",
    "\n",
    "print(\"üéõÔ∏è  Hyperparameters to optimize:\")\n",
    "for param in param_dist.keys():\n",
    "    print(f\"   - {param}\")\n",
    "\n",
    "# Random search with more iterations\n",
    "print(\"\\n‚è≥ Running comprehensive Random Search (100 iterations)...\\n\")\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_dist,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',  # Macro F1 for multiclass\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "random_search_gb.fit(X_train_m, y_train_m)\n",
    "tuning_time = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úÖ Tuning completed in {tuning_time:.2f} seconds ({tuning_time/60:.2f} minutes)\\n\")\n",
    "\n",
    "# Best model\n",
    "best_model = random_search_gb.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test_m)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üèÜ BEST HYPERPARAMETERS:\")\n",
    "print(\"=\" * 60)\n",
    "for param, value in random_search_gb.best_params_.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param:<20}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {param:<20}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä PERFORMANCE COMPARISON:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare default vs tuned\n",
    "default_model = GradientBoostingClassifier(random_state=42)\n",
    "default_model.fit(X_train_m, y_train_m)\n",
    "default_score = default_model.score(X_test_m, y_test_m)\n",
    "tuned_score = best_model.score(X_test_m, y_test_m)\n",
    "\n",
    "print(f\"\\nDefault parameters:  {default_score:.4f}\")\n",
    "print(f\"Tuned parameters:    {tuned_score:.4f}\")\n",
    "print(f\"Improvement:         {(tuned_score - default_score):.4f} ({(tuned_score - default_score)*100:.2f}%)\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã DETAILED CLASSIFICATION REPORT (Tuned Model):\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test_m, y_pred, target_names=['Class 0', 'Class 1', 'Class 2']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_m, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "           yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.title('Confusion Matrix: Tuned Multimodal Classifier', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Production Readiness:\")\n",
    "if tuned_score > 0.85:\n",
    "    print(\"   ‚úÖ Model performance is excellent!\")\n",
    "    print(\"   ‚úÖ Ready for A/B testing in production\")\n",
    "elif tuned_score > 0.75:\n",
    "    print(\"   ‚ö†Ô∏è  Good performance, but could be better\")\n",
    "    print(\"   ‚Üí Consider feature engineering or more data\")\n",
    "else:\n",
    "    print(\"   ‚ùå Needs improvement before production\")\n",
    "    print(\"   ‚Üí Try different models or collect more data\")\n",
    "\n",
    "print(f\"\\nüéØ ROI of Hyperparameter Tuning:\")\n",
    "print(f\"   Time invested: {tuning_time/60:.1f} minutes\")\n",
    "print(f\"   Performance gain: {(tuned_score - default_score)*100:.2f}%\")\n",
    "print(f\"   ‚Üí Worth it! ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Best Practices for Hyperparameter Tuning\n",
    "\n",
    "**1. Start with Random Search**\n",
    "- Faster exploration\n",
    "- Good for initial search\n",
    "- Then refine with Grid Search if needed\n",
    "\n",
    "**2. Use Cross-Validation**\n",
    "- ALWAYS use CV, never just train-test split\n",
    "- 5-fold is usually sufficient\n",
    "- Stratified for classification\n",
    "\n",
    "**3. Choose the Right Metric**\n",
    "- Match your business goal\n",
    "- Imbalanced data: F1, not accuracy\n",
    "- Multi-class: macro-F1 or weighted-F1\n",
    "\n",
    "**4. Search Space Design**\n",
    "- Start wide, then narrow down\n",
    "- Use log-scale for learning rates: [0.001, 0.01, 0.1, 1.0]\n",
    "- Know which parameters matter most\n",
    "\n",
    "**5. Computational Budget**\n",
    "- Grid Search: Good for ‚â§3 parameters\n",
    "- Random Search: Good for any number\n",
    "- Consider time vs improvement trade-off\n",
    "\n",
    "**6. Avoid Overfitting**\n",
    "- Hold out a final test set\n",
    "- Don't tune on test set!\n",
    "- Use nested CV for unbiased estimates\n",
    "\n",
    "**7. Parameter Importance**\n",
    "\n",
    "**Random Forest:**\n",
    "- Most important: `n_estimators`, `max_depth`, `min_samples_split`\n",
    "- Less important: `min_samples_leaf`, `max_features`\n",
    "\n",
    "**Gradient Boosting:**\n",
    "- Most important: `learning_rate`, `n_estimators`, `max_depth`\n",
    "- Less important: `subsample`, `min_samples_split`\n",
    "\n",
    "**Neural Networks:**\n",
    "- Most important: `learning_rate`, `batch_size`, `architecture`\n",
    "- Less important: `optimizer choice`, `weight initialization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Tune a Support Vector Machine\n",
    "\n",
    "**Challenge:** Optimize an SVM classifier for the breast cancer dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Define parameter distributions for SVM\n",
    "2. Use RandomizedSearchCV with 50 iterations\n",
    "3. Compare default vs tuned performance\n",
    "4. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"üéØ YOUR CHALLENGE: Tune an SVM Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# YOUR CODE HERE!\n",
    "\n",
    "# 1. Define parameter distributions for SVM\n",
    "# Hint: Important SVM parameters are C, gamma, kernel\n",
    "param_dist = {\n",
    "    'C': # YOUR CODE (try uniform(0.1, 10))\n",
    "    'gamma': # YOUR CODE (try uniform(0.001, 0.1))\n",
    "    'kernel': # YOUR CODE (try ['rbf', 'linear'])\n",
    "}\n",
    "\n",
    "# 2. Create RandomizedSearchCV\n",
    "random_search_svm = RandomizedSearchCV(\n",
    "    # YOUR CODE\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Fit and evaluate\n",
    "# YOUR CODE\n",
    "\n",
    "# 4. Compare with default\n",
    "# YOUR CODE\n",
    "\n",
    "# 5. Print results\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution (Run after trying!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"üéØ SOLUTION: SVM Hyperparameter Tuning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Important: Scale features for SVM!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 1. Define parameter distributions\n",
    "param_dist_svm = {\n",
    "    'C': uniform(0.1, 10),           # Regularization\n",
    "    'gamma': uniform(0.001, 0.1),    # Kernel coefficient\n",
    "    'kernel': ['rbf', 'linear']      # Kernel type\n",
    "}\n",
    "\n",
    "print(\"\\nüîß Parameter Space:\")\n",
    "for param, dist in param_dist_svm.items():\n",
    "    print(f\"   {param}: {dist}\")\n",
    "\n",
    "# 2. Random Search\n",
    "print(\"\\n‚è≥ Running Random Search (50 iterations)...\\n\")\n",
    "\n",
    "random_search_svm = RandomizedSearchCV(\n",
    "    SVC(random_state=42),\n",
    "    param_dist_svm,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "random_search_svm.fit(X_train_scaled, y_train)\n",
    "svm_time = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úÖ Tuning completed in {svm_time:.2f} seconds\\n\")\n",
    "\n",
    "# 3. Best parameters\n",
    "print(\"=\" * 60)\n",
    "print(\"üèÜ BEST HYPERPARAMETERS:\")\n",
    "print(\"=\" * 60)\n",
    "for param, value in random_search_svm.best_params_.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param:<10}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {param:<10}: {value}\")\n",
    "\n",
    "# 4. Compare default vs tuned\n",
    "default_svm = SVC(random_state=42)\n",
    "default_svm.fit(X_train_scaled, y_train)\n",
    "default_score = default_svm.score(X_test_scaled, y_test)\n",
    "\n",
    "tuned_score = random_search_svm.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä PERFORMANCE COMPARISON:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDefault SVM:  {default_score:.4f}\")\n",
    "print(f\"Tuned SVM:    {tuned_score:.4f}\")\n",
    "print(f\"Improvement:  {(tuned_score - default_score):.4f} ({(tuned_score - default_score)*100:.2f}%)\")\n",
    "\n",
    "# 5. Visualize results\n",
    "results_svm = pd.DataFrame(random_search_svm.cv_results_)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: C vs Score (for RBF kernel)\n",
    "rbf_results = results_svm[results_svm['param_kernel'] == 'rbf']\n",
    "ax1.scatter(rbf_results['param_C'], rbf_results['mean_test_score'], \n",
    "           c=rbf_results['param_gamma'], cmap='viridis', s=100, alpha=0.6)\n",
    "ax1.set_xlabel('C (Regularization)', fontsize=12)\n",
    "ax1.set_ylabel('Mean CV Accuracy', fontsize=12)\n",
    "ax1.set_title('SVM: C vs Accuracy (RBF kernel)', fontsize=13, fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "cbar1 = plt.colorbar(ax1.collections[0], ax=ax1)\n",
    "cbar1.set_label('gamma', fontsize=10)\n",
    "\n",
    "# Plot 2: Kernel comparison\n",
    "kernel_scores = results_svm.groupby('param_kernel')['mean_test_score'].agg(['mean', 'std'])\n",
    "kernel_scores.plot(kind='bar', y='mean', yerr='std', ax=ax2, \n",
    "                  color=['skyblue', 'orange'], alpha=0.7, capsize=5)\n",
    "ax2.set_xlabel('Kernel Type', fontsize=12)\n",
    "ax2.set_ylabel('Mean CV Accuracy', fontsize=12)\n",
    "ax2.set_title('Kernel Comparison', fontsize=13, fontweight='bold')\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.legend(['Mean Score'], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Findings:\")\n",
    "print(f\"   - Best kernel: {random_search_svm.best_params_['kernel']}\")\n",
    "print(f\"   - Optimal C: {random_search_svm.best_params_['C']:.4f}\")\n",
    "if 'gamma' in random_search_svm.best_params_:\n",
    "    print(f\"   - Optimal gamma: {random_search_svm.best_params_['gamma']:.6f}\")\n",
    "print(f\"   - Performance gain: {(tuned_score - default_score)*100:.2f}%\")\n",
    "print(\"\\nüéØ Hyperparameter tuning made a significant difference! ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Advanced: Comparing Multiple Models\n",
    "\n",
    "**Real-world workflow:** Try multiple models, tune each, then compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèÅ FINAL SHOWDOWN: Multiple Tuned Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define models and their parameter spaces\n",
    "models_to_tune = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': randint(50, 200),\n",
    "            'max_depth': randint(5, 20),\n",
    "            'min_samples_split': randint(2, 10)\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': randint(50, 200),\n",
    "            'learning_rate': uniform(0.01, 0.2),\n",
    "            'max_depth': randint(3, 10)\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'params': {\n",
    "            'C': uniform(0.1, 10),\n",
    "            'gamma': uniform(0.001, 0.1),\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune each model\n",
    "results_comparison = {}\n",
    "\n",
    "for name, config in models_to_tune.items():\n",
    "    print(f\"\\n‚öôÔ∏è  Tuning {name}...\")\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        config['model'],\n",
    "        config['params'],\n",
    "        n_iter=30,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Use scaled data for SVM\n",
    "    if name == 'SVM':\n",
    "        search.fit(X_train_scaled, y_train)\n",
    "        test_score = search.score(X_test_scaled, y_test)\n",
    "    else:\n",
    "        search.fit(X_train, y_train)\n",
    "        test_score = search.score(X_test, y_test)\n",
    "    \n",
    "    results_comparison[name] = {\n",
    "        'best_params': search.best_params_,\n",
    "        'cv_score': search.best_score_,\n",
    "        'test_score': test_score,\n",
    "        'cv_std': search.cv_results_['std_test_score'][search.best_index_]\n",
    "    }\n",
    "    \n",
    "    print(f\"   CV Score: {search.best_score_:.4f}\")\n",
    "    print(f\"   Test Score: {test_score:.4f}\")\n",
    "\n",
    "# Display final comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üèÜ FINAL MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Model':<20} {'CV Score':<12} {'Test Score':<12} {'Std'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, res in results_comparison.items():\n",
    "    print(f\"{name:<20} {res['cv_score']:.4f}       {res['test_score']:.4f}       {res['cv_std']:.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results_comparison.keys(), key=lambda k: results_comparison[k]['test_score'])\n",
    "best_result = results_comparison[best_model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"ü•á WINNER: {best_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Accuracy: {best_result['test_score']:.4f}\")\n",
    "print(f\"Best Parameters:\")\n",
    "for param, value in best_result['best_params'].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = list(results_comparison.keys())\n",
    "cv_scores = [results_comparison[m]['cv_score'] for m in models]\n",
    "test_scores = [results_comparison[m]['test_score'] for m in models]\n",
    "stds = [results_comparison[m]['cv_std'] for m in models]\n",
    "\n",
    "# Plot 1: CV vs Test scores\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, cv_scores, width, label='CV Score', alpha=0.7, color='skyblue')\n",
    "ax1.bar(x + width/2, test_scores, width, label='Test Score', alpha=0.7, color='orange')\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Model Comparison: CV vs Test', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Test scores with error bars\n",
    "ax2.bar(models, test_scores, yerr=stds, capsize=10, alpha=0.7, \n",
    "       color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "ax2.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax2.set_title('Test Performance with Std Dev', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (score, std) in enumerate(zip(test_scores, stds)):\n",
    "    ax2.text(i, score + std + 0.005, f'{score:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Production Recommendation:\")\n",
    "print(f\"   Deploy: {best_model_name}\")\n",
    "print(f\"   Expected accuracy: {best_result['test_score']:.2%}\")\n",
    "print(f\"   Confidence: High ‚úÖ (based on CV consistency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just mastered:**\n",
    "- ‚úÖ What hyperparameters are and why they matter\n",
    "- ‚úÖ Grid Search: exhaustive but slow\n",
    "- ‚úÖ Random Search: smart and efficient\n",
    "- ‚úÖ How to use GridSearchCV and RandomizedSearchCV\n",
    "- ‚úÖ Best practices for hyperparameter tuning\n",
    "- ‚úÖ Real AI application: multimodal classifier optimization\n",
    "- ‚úÖ Comparing multiple tuned models\n",
    "\n",
    "**üéØ Key Takeaways:**\n",
    "1. **Default parameters are rarely optimal** - always tune!\n",
    "2. **Random Search > Grid Search** for most cases\n",
    "3. **Use cross-validation** during tuning\n",
    "4. **Start wide, then narrow** your search space\n",
    "5. **Compare multiple models** after tuning each\n",
    "6. **Know which parameters matter** for each model type\n",
    "\n",
    "**üìä Week 8 Complete! You Now Know:**\n",
    "\n",
    "**Day 1:** Evaluation Metrics\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- Confusion Matrix, ROC, AUC\n",
    "- When to use which metric\n",
    "\n",
    "**Day 2:** Cross-Validation\n",
    "- K-Fold, Stratified K-Fold, LOOCV\n",
    "- Reliable performance estimates\n",
    "- Avoiding lucky/unlucky splits\n",
    "\n",
    "**Day 3:** Hyperparameter Tuning\n",
    "- Grid Search vs Random Search\n",
    "- Systematic optimization\n",
    "- Production-ready model selection\n",
    "\n",
    "**üöÄ Final Practice Project:**\n",
    "\n",
    "Build a complete ML pipeline:\n",
    "1. Load a dataset of your choice\n",
    "2. Try 3 different models\n",
    "3. Tune each with RandomizedSearchCV\n",
    "4. Evaluate with proper metrics (precision, recall, F1)\n",
    "5. Use Stratified 5-Fold CV\n",
    "6. Compare and choose the best\n",
    "7. Create a final evaluation report\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Next Week:** Week 9 - Feature Engineering & Selection\n",
    "\n",
    "**üí¨ Questions?** Experiment with different search spaces and see how results change!\n",
    "\n",
    "---\n",
    "\n",
    "*\"The difference between a good model and a great model is often just... proper hyperparameter tuning!\"* üéõÔ∏è\n",
    "\n",
    "**üåü You're now ready to build production-quality ML models! Congratulations! üåü**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
