{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Day 1: Evaluation Metrics\n",
    "\n",
    "**üéØ Goal:** Master the metrics that tell you how good your AI model really is\n",
    "\n",
    "**‚è±Ô∏è Time:** 45-60 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Accuracy alone can be misleading - learn when to use precision, recall, and F1\n",
    "- Essential for evaluating RAG systems, chatbots, image classifiers, and more\n",
    "- Used by OpenAI, Google, Meta to evaluate GPT-4, Gemini, and LLaMA models\n",
    "- Critical for medical AI (missing a disease = bad!), fraud detection, content moderation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ The Problem: Is 95% Accuracy Good?\n",
    "\n",
    "Imagine you built an AI model to detect credit card fraud:\n",
    "- Dataset: 10,000 transactions\n",
    "- Fraudulent: 100 (1%)\n",
    "- Legitimate: 9,900 (99%)\n",
    "\n",
    "**Your model:** \"All transactions are legitimate!\"\n",
    "- **Accuracy:** 99% ‚úÖ (Looks amazing!)\n",
    "- **Problem:** It catches ZERO fraud cases! ‚ùå\n",
    "\n",
    "**This is why we need better metrics!** Let's learn them all. üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import our tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç The Confusion Matrix: Foundation of All Metrics\n",
    "\n",
    "Before understanding metrics, you need to understand the **confusion matrix**.\n",
    "\n",
    "### Real Example: Email Spam Detection\n",
    "\n",
    "When your model makes predictions, there are 4 possible outcomes:\n",
    "\n",
    "| | Actually Spam | Actually Not Spam |\n",
    "|---|---|---|\n",
    "| **Predicted Spam** | ‚úÖ True Positive (TP)<br>*Correctly caught spam* | ‚ùå False Positive (FP)<br>*Wrongly marked as spam* |\n",
    "| **Predicted Not Spam** | ‚ùå False Negative (FN)<br>*Missed spam* | ‚úÖ True Negative (TN)<br>*Correctly identified normal* |\n",
    "\n",
    "**Let's see this in action:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated spam detection results\n",
    "# Let's say we tested 100 emails\n",
    "\n",
    "y_true = [1, 1, 0, 1, 0, 0, 0, 1, 1, 0,  # Actual labels (1=spam, 0=not spam)\n",
    "          0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
    "          0, 1, 0, 0, 0, 1, 1, 0, 0, 1]\n",
    "\n",
    "y_pred = [1, 1, 0, 1, 0, 0, 1, 1, 0, 0,  # Model's predictions\n",
    "          0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
    "          0, 1, 0, 0, 0, 0, 1, 0, 0, 1]\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Visualize it beautifully\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Spam', 'Spam'],\n",
    "            yticklabels=['Not Spam', 'Spam'])\n",
    "plt.title('Confusion Matrix: Email Spam Detection', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Extract values\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"üìä Confusion Matrix Breakdown:\")\n",
    "print(f\"\\n‚úÖ True Positives (TP): {tp} - Correctly identified spam\")\n",
    "print(f\"‚úÖ True Negatives (TN): {tn} - Correctly identified normal emails\")\n",
    "print(f\"‚ùå False Positives (FP): {fp} - Normal emails wrongly marked as spam\")\n",
    "print(f\"‚ùå False Negatives (FN): {fn} - Spam that got through\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìè Metric 1: Accuracy\n",
    "\n",
    "**Definition:** What percentage of predictions were correct?\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- ‚úÖ Balanced datasets (equal classes)\n",
    "- ‚ùå Imbalanced datasets (like fraud detection)\n",
    "\n",
    "**Real AI Use:**\n",
    "- MNIST digit classification (balanced: 10 digits)\n",
    "- Multimodal image classification with equal categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Manual calculation to understand\n",
    "manual_accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "print(f\"üéØ Accuracy: {accuracy:.2%}\")\n",
    "print(f\"üìù Manual calculation: ({tp} + {tn}) / ({tp} + {tn} + {fp} + {fn}) = {manual_accuracy:.2%}\")\n",
    "print(f\"\\nüí° This means {accuracy:.0%} of our predictions were correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Metric 2: Precision\n",
    "\n",
    "**Definition:** Of all the emails we flagged as spam, how many were actually spam?\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Precision = TP / (TP + FP)\n",
    "```\n",
    "\n",
    "**Question it answers:** \"When my model says YES, how often is it right?\"\n",
    "\n",
    "**When to use:**\n",
    "- ‚úÖ False Positives are costly (e.g., medical diagnosis for expensive treatment)\n",
    "- ‚úÖ Content recommendation (don't recommend bad content)\n",
    "- ‚úÖ RAG systems (don't retrieve irrelevant documents)\n",
    "\n",
    "**Real AI Use (2024-2025):**\n",
    "- **RAG Systems:** High precision = retrieved documents are actually relevant\n",
    "- **Agentic AI:** High precision = agent actions are correct when taken\n",
    "- **Video content moderation:** Don't falsely flag safe content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "\n",
    "# Manual calculation\n",
    "manual_precision = tp / (tp + fp)\n",
    "\n",
    "print(f\"üéØ Precision: {precision:.2%}\")\n",
    "print(f\"üìù Manual calculation: {tp} / ({tp} + {fp}) = {manual_precision:.2%}\")\n",
    "print(f\"\\nüí° When we flag an email as spam, we're correct {precision:.0%} of the time\")\n",
    "print(f\"‚ö†Ô∏è  {fp} normal emails were wrongly marked as spam (False Positives)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Metric 3: Recall (Sensitivity)\n",
    "\n",
    "**Definition:** Of all the actual spam emails, how many did we catch?\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Recall = TP / (TP + FN)\n",
    "```\n",
    "\n",
    "**Question it answers:** \"Of all the actual positives, how many did I find?\"\n",
    "\n",
    "**When to use:**\n",
    "- ‚úÖ False Negatives are costly (e.g., cancer detection - can't miss cases!)\n",
    "- ‚úÖ Fraud detection (catch all fraud)\n",
    "- ‚úÖ Search engines (find all relevant results)\n",
    "\n",
    "**Real AI Use (2024-2025):**\n",
    "- **Medical AI:** High recall = catch all disease cases\n",
    "- **Security systems:** High recall = detect all threats\n",
    "- **RAG retrieval:** High recall = find all relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "# Manual calculation\n",
    "manual_recall = tp / (tp + fn)\n",
    "\n",
    "print(f\"üîç Recall: {recall:.2%}\")\n",
    "print(f\"üìù Manual calculation: {tp} / ({tp} + {fn}) = {manual_recall:.2%}\")\n",
    "print(f\"\\nüí° We caught {recall:.0%} of all spam emails\")\n",
    "print(f\"‚ö†Ô∏è  {fn} spam emails slipped through (False Negatives)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Metric 4: F1-Score (The Balance)\n",
    "\n",
    "**Definition:** The harmonic mean of precision and recall\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- ‚úÖ You need a balance between precision and recall\n",
    "- ‚úÖ Imbalanced datasets\n",
    "- ‚úÖ You want a single metric that considers both FP and FN\n",
    "\n",
    "**Real AI Use:**\n",
    "- Standard metric for NLP tasks (text classification, named entity recognition)\n",
    "- Hugging Face model evaluation\n",
    "- Most Kaggle competitions use F1 or macro-F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1-score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Manual calculation\n",
    "manual_f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"‚öñÔ∏è  F1-Score: {f1:.2%}\")\n",
    "print(f\"üìù Manual calculation: 2 √ó ({precision:.2f} √ó {recall:.2f}) / ({precision:.2f} + {recall:.2f}) = {manual_f1:.2%}\")\n",
    "print(f\"\\nüí° F1 balances precision ({precision:.0%}) and recall ({recall:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã All Metrics Together\n",
    "\n",
    "Let's see a comprehensive report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report shows everything!\n",
    "print(\"üìä COMPLETE CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_true, y_pred, \n",
    "                          target_names=['Not Spam', 'Spam']))\n",
    "\n",
    "print(\"\\nüí° How to read this:\")\n",
    "print(\"- Precision: When model says 'Spam', how often is it right?\")\n",
    "print(\"- Recall: Of all actual spam, how much did we catch?\")\n",
    "print(\"- F1-score: Balanced metric combining both\")\n",
    "print(\"- Support: Number of actual occurrences in each class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà ROC Curve & AUC\n",
    "\n",
    "**ROC (Receiver Operating Characteristic) Curve:**\n",
    "- Shows the trade-off between True Positive Rate (Recall) and False Positive Rate\n",
    "- Helps you choose the best threshold for your model\n",
    "\n",
    "**AUC (Area Under the Curve):**\n",
    "- Single number to measure ROC curve quality\n",
    "- Range: 0.0 to 1.0 (higher is better)\n",
    "- 0.5 = random guessing, 1.0 = perfect model\n",
    "\n",
    "**Real AI Use:**\n",
    "- Standard for binary classification evaluation\n",
    "- Used in medical diagnostics, fraud detection\n",
    "- Threshold tuning for production systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ROC curve, we need probability scores\n",
    "# Let's create a simple classifier\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, \n",
    "                          n_informative=15, n_redundant=5,\n",
    "                          random_state=42, weights=[0.7, 0.3])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get probability predictions\n",
    "y_proba = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('ROC Curve: Model Performance', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üéØ AUC Score: {auc_score:.3f}\")\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "if auc_score >= 0.9:\n",
    "    print(\"   Excellent model! üåü\")\n",
    "elif auc_score >= 0.8:\n",
    "    print(\"   Good model! ‚úÖ\")\n",
    "elif auc_score >= 0.7:\n",
    "    print(\"   Fair model, room for improvement üìà\")\n",
    "else:\n",
    "    print(\"   Poor model, needs work üîß\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Real AI Example: Evaluating a RAG Retrieval System\n",
    "\n",
    "**Scenario:** You built a RAG (Retrieval-Augmented Generation) system for a customer support chatbot.\n",
    "\n",
    "**Question:** How do you evaluate if it retrieves the RIGHT documents?\n",
    "\n",
    "Let's simulate this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated RAG evaluation\n",
    "# Let's say we tested 50 queries\n",
    "\n",
    "# Ground truth: which documents are actually relevant (human-labeled)\n",
    "actually_relevant = np.array([1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
    "                             0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
    "                             0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
    "                             1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
    "                             0, 1, 0, 1, 0, 0, 1, 1, 0, 1])\n",
    "\n",
    "# What your RAG system retrieved\n",
    "retrieved = np.array([1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
    "                     0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
    "                     0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
    "                     1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
    "                     0, 1, 0, 1, 0, 1, 1, 1, 0, 1])\n",
    "\n",
    "# Calculate metrics\n",
    "rag_precision = precision_score(actually_relevant, retrieved)\n",
    "rag_recall = recall_score(actually_relevant, retrieved)\n",
    "rag_f1 = f1_score(actually_relevant, retrieved)\n",
    "\n",
    "print(\"ü§ñ RAG SYSTEM EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüìä Precision: {rag_precision:.2%}\")\n",
    "print(\"   ‚Üí Of documents retrieved, how many were actually relevant?\")\n",
    "print(\"   ‚Üí High precision = Low noise, users see relevant docs\")\n",
    "\n",
    "print(f\"\\nüîç Recall: {rag_recall:.2%}\")\n",
    "print(\"   ‚Üí Of all relevant documents, how many did we retrieve?\")\n",
    "print(\"   ‚Üí High recall = Comprehensive, didn't miss important info\")\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  F1-Score: {rag_f1:.2%}\")\n",
    "print(\"   ‚Üí Balanced metric for overall retrieval quality\")\n",
    "\n",
    "# Confusion matrix for RAG\n",
    "cm_rag = confusion_matrix(actually_relevant, retrieved)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_rag, annot=True, fmt='d', cmap='Greens',\n",
    "           xticklabels=['Not Retrieved', 'Retrieved'],\n",
    "           yticklabels=['Not Relevant', 'Relevant'])\n",
    "plt.title('RAG System: Retrieval Performance', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actually Relevant?', fontsize=12)\n",
    "plt.xlabel('Retrieved by RAG?', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ What This Means for Your RAG System:\")\n",
    "print(f\"‚úÖ Retrieved {rag_precision:.0%} relevant documents (precision)\")\n",
    "print(f\"‚úÖ Found {rag_recall:.0%} of all relevant documents (recall)\")\n",
    "print(\"\\nüí° Optimization Strategy:\")\n",
    "if rag_precision < 0.8:\n",
    "    print(\"   - Improve embedding quality (better model or fine-tuning)\")\n",
    "    print(\"   - Add re-ranking stage\")\n",
    "if rag_recall < 0.8:\n",
    "    print(\"   - Retrieve more candidates (increase top-k)\")\n",
    "    print(\"   - Improve query expansion\")\n",
    "    print(\"   - Check document chunking strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Medical Diagnosis System\n",
    "\n",
    "You're building an AI system to detect a rare disease from medical images.\n",
    "\n",
    "**Dataset:**\n",
    "- 1000 patients tested\n",
    "- 50 actually have the disease (5%)\n",
    "- 950 are healthy (95%)\n",
    "\n",
    "**Your model's predictions:**\n",
    "- Correctly identified: 45 sick patients (TP)\n",
    "- Missed: 5 sick patients (FN)\n",
    "- False alarms: 30 healthy patients wrongly flagged (FP)\n",
    "- Correctly identified: 920 healthy patients (TN)\n",
    "\n",
    "**Calculate:**\n",
    "1. Accuracy\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. F1-Score\n",
    "5. Is this a good model for medical diagnosis? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given values\n",
    "tp = 45   # Correctly identified sick patients\n",
    "fn = 5    # Missed sick patients (FALSE NEGATIVE - VERY BAD!)\n",
    "fp = 30   # Healthy wrongly flagged (False Positive - inconvenient but safer)\n",
    "tn = 920  # Correctly identified healthy\n",
    "\n",
    "# YOUR CODE HERE - Calculate the metrics\n",
    "# Hint: Use the formulas we learned!\n",
    "\n",
    "accuracy = # YOUR CODE\n",
    "precision = # YOUR CODE  \n",
    "recall = # YOUR CODE\n",
    "f1 = # YOUR CODE\n",
    "\n",
    "print(\"üè• MEDICAL DIAGNOSIS AI EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {accuracy:.2%}\")\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"Recall:    {recall:.2%}\")\n",
    "print(f\"F1-Score:  {f1:.2%}\")\n",
    "print(\"\\nü§î Analysis:\")\n",
    "print(f\"   - We missed {fn} sick patients (False Negatives)\")\n",
    "print(f\"   - We wrongly flagged {fp} healthy patients (False Positives)\")\n",
    "print(\"\\n‚ùì Which is worse for medical diagnosis? Think about it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution (Run this cell after trying!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "tp, fn, fp, tn = 45, 5, 30, 920\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"üè• SOLUTION: Medical Diagnosis AI Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüìä Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy:.2%} - {(tp+tn)}/{(tp+tn+fp+fn)} correct\")\n",
    "print(f\"   Precision: {precision:.2%} - {tp}/{tp+fp} flagged patients actually sick\")\n",
    "print(f\"   Recall:    {recall:.2%} - {tp}/{tp+fn} sick patients detected\")\n",
    "print(f\"   F1-Score:  {f1:.2%} - Balanced metric\")\n",
    "\n",
    "print(\"\\nüéØ ANALYSIS:\")\n",
    "print(f\"\\n‚úÖ GOOD: {recall:.0%} recall means we caught most sick patients\")\n",
    "print(f\"‚ö†Ô∏è  CONCERN: We missed {fn} sick patients (False Negatives)\")\n",
    "print(f\"‚ö†Ô∏è  CONCERN: {fp} healthy people will undergo unnecessary tests (False Positives)\")\n",
    "\n",
    "print(\"\\nüí° VERDICT:\")\n",
    "print(\"   For medical diagnosis:\")\n",
    "print(\"   - High RECALL is CRITICAL (can't miss sick patients!)\")\n",
    "print(\"   - {:.0%} recall is good, but not perfect\".format(recall))\n",
    "print(\"   - Consider: Better to have false alarms than miss disease\")\n",
    "print(\"   - Recommendation: Aim for 95%+ recall, accept lower precision\")\n",
    "\n",
    "print(\"\\nüîß How to improve:\")\n",
    "print(\"   1. Lower the decision threshold ‚Üí Higher recall (catch more cases)\")\n",
    "print(\"   2. Collect more training data for rare disease cases\")\n",
    "print(\"   3. Use ensemble models for better reliability\")\n",
    "print(\"   4. Always have human doctor review flagged cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Metric Selection Guide\n",
    "\n",
    "**Quick Reference: Which Metric to Use?**\n",
    "\n",
    "| Use Case | Metric | Why? |\n",
    "|----------|--------|------|\n",
    "| Balanced classes (MNIST) | Accuracy | Equal importance for all classes |\n",
    "| Cancer detection | Recall | Can't miss positive cases! |\n",
    "| Spam filter | Precision | Don't flag important emails |\n",
    "| RAG retrieval | F1 | Balance relevance & coverage |\n",
    "| Fraud detection | Recall | Catch all fraud |\n",
    "| YouTube recommendations | Precision | Don't recommend bad videos |\n",
    "| Search engines | Recall | Find all relevant results |\n",
    "| Imbalanced data | F1 or AUC | Single metric that considers both |\n",
    "\n",
    "**2024-2025 AI Applications:**\n",
    "- **GPT-4 evaluation:** Precision & Recall for factual accuracy\n",
    "- **RAG systems:** Precision@k, Recall@k, MRR (Mean Reciprocal Rank)\n",
    "- **Multimodal AI:** Per-modality F1 scores\n",
    "- **Agentic AI:** Task completion rate + precision of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ BONUS CHALLENGE: Build a Metric Dashboard\n",
    "\n",
    "Create a function that takes predictions and returns a beautiful evaluation dashboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, class_names=['Negative', 'Positive']):\n",
    "    \"\"\"\n",
    "    Complete evaluation dashboard for binary classification\n",
    "    \"\"\"\n",
    "    # Calculate all metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "               xticklabels=class_names, yticklabels=class_names)\n",
    "    axes[0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Actual', fontsize=12)\n",
    "    axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "    \n",
    "    # Plot 2: Metrics Bar Chart\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    values = [acc, prec, rec, f1]\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "    \n",
    "    bars = axes[1].bar(metrics, values, color=colors, alpha=0.7)\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    axes[1].set_ylabel('Score', fontsize=12)\n",
    "    axes[1].set_title('Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{value:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed report\n",
    "    print(\"\\nüìä COMPREHENSIVE EVALUATION REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n{'Metric':<15} {'Score':<10} {'Interpretation'}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Accuracy':<15} {acc:>6.2%}    Overall correctness\")\n",
    "    print(f\"{'Precision':<15} {prec:>6.2%}    Positive prediction accuracy\")\n",
    "    print(f\"{'Recall':<15} {rec:>6.2%}    True positive detection rate\")\n",
    "    print(f\"{'F1-Score':<15} {f1:>6.2%}    Precision-Recall balance\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}\n",
    "\n",
    "# Test it!\n",
    "test_true = np.array([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1])\n",
    "test_pred = np.array([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1])\n",
    "\n",
    "results = evaluate_model(test_true, test_pred, class_names=['Normal', 'Anomaly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just mastered:**\n",
    "- ‚úÖ Confusion matrix (TP, TN, FP, FN)\n",
    "- ‚úÖ Accuracy, Precision, Recall, F1-Score\n",
    "- ‚úÖ ROC curves and AUC\n",
    "- ‚úÖ When to use which metric\n",
    "- ‚úÖ Real AI applications (RAG systems, medical AI)\n",
    "- ‚úÖ How to evaluate models properly\n",
    "\n",
    "**üéØ Key Takeaways:**\n",
    "1. **Accuracy is not enough** - especially for imbalanced data\n",
    "2. **Precision** = \"When I predict positive, am I right?\"\n",
    "3. **Recall** = \"Of all positives, how many did I find?\"\n",
    "4. **F1** = Balance between precision and recall\n",
    "5. **Choose metrics based on your use case** (medical = high recall!)\n",
    "\n",
    "**üöÄ Practice Exercise (Do before Day 2!):**\n",
    "\n",
    "Imagine you're building an AI content moderator for social media:\n",
    "- Test set: 1000 posts\n",
    "- Toxic: 100, Safe: 900\n",
    "- Your model: TP=85, FN=15, FP=50, TN=850\n",
    "\n",
    "Calculate all metrics and decide:\n",
    "- Is this good enough for production?\n",
    "- Which metric matters most?\n",
    "- How would you improve it?\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Next Lesson:** Day 2 - Cross-Validation (Make sure your metrics are reliable!)\n",
    "\n",
    "**üí¨ Questions?** Review the ROC curve section, it's powerful for threshold tuning!\n",
    "\n",
    "---\n",
    "\n",
    "*\"In God we trust, all others must bring data... and proper evaluation metrics!\"* üìä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
