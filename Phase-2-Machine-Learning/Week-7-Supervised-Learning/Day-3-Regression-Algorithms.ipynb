{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Day 3: Regression Algorithms\n",
    "\n",
    "**ğŸ¯ Goal:** Master regression algorithms to predict continuous values\n",
    "\n",
    "**â±ï¸ Time:** 60-90 minutes\n",
    "\n",
    "**ğŸŒŸ Why This Matters for AI:**\n",
    "- Regression predicts numbers: prices, sales, temperatures, stock prices\n",
    "- Powers recommendation engines (predict ratings)\n",
    "- Used in RAG systems to score document relevance\n",
    "- Foundation for time-series forecasting in Agentic AI\n",
    "- Critical for multimodal AI that predicts continuous outputs\n",
    "- Essential for understanding modern AI/ML pipelines in 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classification-vs-regression",
   "metadata": {},
   "source": [
    "## ğŸ¤” Classification vs Regression\n",
    "\n",
    "**Classification** (Days 1-2):\n",
    "- Predicts **categories**: Spam/Not Spam, Cat/Dog, Yes/No\n",
    "- Output: Discrete labels\n",
    "- Example: \"Is this email spam?\"\n",
    "\n",
    "**Regression** (Today):\n",
    "- Predicts **numbers**: Price, Temperature, Rating, Age\n",
    "- Output: Continuous values\n",
    "- Example: \"What price should this house sell for?\"\n",
    "\n",
    "| Task | Type | Output |\n",
    "|------|------|--------|\n",
    "| Email spam? | Classification | \"Spam\" or \"Not Spam\" |\n",
    "| House price? | **Regression** | **$350,000** |\n",
    "| Will customer buy? | Classification | \"Yes\" or \"No\" |\n",
    "| Customer rating? | **Regression** | **4.5 stars** |\n",
    "\n",
    "Let's learn regression! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Regression algorithms\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# Datasets\n",
    "from sklearn.datasets import fetch_california_housing, make_regression\n",
    "\n",
    "# Make plots beautiful\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Regression tools loaded!\")\n",
    "print(\"Let's predict some numbers! ğŸ“ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-regression-intro",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Linear Regression: The Foundation\n",
    "\n",
    "**What it does:** Finds the best straight line through data points\n",
    "\n",
    "**How it works:**\n",
    "- You have points on a graph\n",
    "- Draw a line that's closest to all points\n",
    "- Use that line to predict new values\n",
    "\n",
    "**The Formula:**\n",
    "```\n",
    "y = mx + b\n",
    "```\n",
    "- `y` = what we predict (house price)\n",
    "- `x` = what we know (house size)\n",
    "- `m` = slope (how much y changes when x increases)\n",
    "- `b` = intercept (starting value)\n",
    "\n",
    "**Visual:**\n",
    "```\n",
    "Price\n",
    "  |\n",
    "  |        â—\n",
    "  |     â—   â—      â† Points (actual data)\n",
    "  |   â—       â—\n",
    "  | â—  /        â—  â† Line (predictions)\n",
    "  |  /\n",
    "  |/________________ Size\n",
    "```\n",
    "\n",
    "**ğŸ¯ Real AI Use Cases (2024-2025):**\n",
    "- **Price prediction** for e-commerce AI\n",
    "- **Demand forecasting** for inventory management\n",
    "- **User engagement scoring** in recommendation systems\n",
    "- **Feature engineering** for Transformer models\n",
    "- **Baseline models** before complex AI approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: Predict house price from size\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data: house size (sqft) vs price ($1000s)\n",
    "house_size = np.array([500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750])\n",
    "house_price = house_size * 0.15 + 50 + np.random.randn(10) * 20  # Price = size*0.15 + 50 + noise\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(house_size, house_price, s=100, alpha=0.6, color='blue')\n",
    "plt.xlabel('House Size (sqft)', fontsize=12)\n",
    "plt.ylabel('Price ($1000s)', fontsize=12)\n",
    "plt.title('ğŸ  House Size vs Price', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Can we predict price from size? Let's fit a line!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-linear-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for sklearn (needs 2D array)\n",
    "X_simple = house_size.reshape(-1, 1)\n",
    "y_simple = house_price\n",
    "\n",
    "# Create and train linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_simple, y_simple)\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(X_simple)\n",
    "\n",
    "# Extract the equation: y = mx + b\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "\n",
    "print(\"ğŸ“ Linear Regression Equation:\")\n",
    "print(f\"   Price = {slope:.2f} Ã— Size + {intercept:.2f}\")\n",
    "print(f\"\\n   Interpretation:\")\n",
    "print(f\"   â€¢ Each additional sqft adds ${slope:.2f}k to the price\")\n",
    "print(f\"   â€¢ Base price (at 0 sqft): ${intercept:.2f}k\")\n",
    "\n",
    "# Visualize the line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(house_size, house_price, s=100, alpha=0.6, color='blue', label='Actual Prices')\n",
    "plt.plot(house_size, predictions, color='red', linewidth=2, label='Regression Line')\n",
    "plt.xlabel('House Size (sqft)', fontsize=12)\n",
    "plt.ylabel('Price ($1000s)', fontsize=12)\n",
    "plt.title('ğŸ  Linear Regression: Fitting a Line', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Make a prediction\n",
    "new_size = np.array([[1800]])\n",
    "predicted_price = model.predict(new_size)[0]\n",
    "print(f\"\\nğŸ¯ Prediction: A 1800 sqft house should cost ${predicted_price:.1f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics",
   "metadata": {},
   "source": [
    "## ğŸ“Š Evaluating Regression Models\n",
    "\n",
    "Unlike classification (accuracy %), regression uses different metrics:\n",
    "\n",
    "### 1. **RÂ² (R-squared)** - Coefficient of Determination\n",
    "- Range: 0 to 1 (higher is better)\n",
    "- **Interpretation:** How much variance in y is explained by x?\n",
    "- RÂ² = 0.9 means 90% of price variation is explained by size\n",
    "- RÂ² = 1.0 means perfect predictions!\n",
    "\n",
    "### 2. **MSE (Mean Squared Error)**\n",
    "- Average of squared errors\n",
    "- Penalizes large errors heavily\n",
    "- Lower is better\n",
    "\n",
    "### 3. **MAE (Mean Absolute Error)**\n",
    "- Average of absolute errors\n",
    "- Easy to interpret: \"On average, we're off by $X\"\n",
    "- Lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our simple model\n",
    "r2 = r2_score(y_simple, predictions)\n",
    "mse = mean_squared_error(y_simple, predictions)\n",
    "mae = mean_absolute_error(y_simple, predictions)\n",
    "\n",
    "print(\"ğŸ“Š Model Performance:\")\n",
    "print(f\"   RÂ² Score: {r2:.3f} ({r2*100:.1f}% of variance explained)\")\n",
    "print(f\"   MSE: {mse:.2f}\")\n",
    "print(f\"   MAE: {mae:.2f}k (on average, we're off by ${mae:.2f}k)\")\n",
    "\n",
    "if r2 > 0.9:\n",
    "    print(\"\\nâœ… Excellent! The model explains most of the variation.\")\n",
    "elif r2 > 0.7:\n",
    "    print(\"\\nğŸ‘ Good! The model has decent predictive power.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Room for improvement. Maybe try more features?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-dataset",
   "metadata": {},
   "source": [
    "## ğŸ˜ï¸ Real AI Example: California Housing Prices\n",
    "\n",
    "Let's use a **real dataset** to predict house prices!\n",
    "\n",
    "**Dataset:** California Housing (1990 census)\n",
    "- **Features:** Median income, house age, rooms, location, etc.\n",
    "- **Target:** Median house price\n",
    "- **Use Case:** Real estate pricing AI, property valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y_housing = housing.target  # Median house value in $100k\n",
    "\n",
    "print(\"ğŸ  California Housing Dataset:\")\n",
    "print(f\"Samples: {len(X_housing)}\")\n",
    "print(f\"Features: {list(X_housing.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(X_housing.head())\n",
    "print(f\"\\nTarget (price): {y_housing[:5]} (in $100k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('ğŸ˜ï¸ Feature vs Price Relationships', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot key features\n",
    "features_to_plot = ['MedInc', 'HouseAge', 'AveRooms', 'Population']\n",
    "titles = ['Median Income', 'House Age', 'Average Rooms', 'Population']\n",
    "\n",
    "for idx, (feature, title) in enumerate(zip(features_to_plot, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Sample for faster plotting (plot 1000 points)\n",
    "    sample_idx = np.random.choice(len(X_housing), 1000, replace=False)\n",
    "    \n",
    "    ax.scatter(X_housing[feature].iloc[sample_idx], \n",
    "               y_housing[sample_idx],\n",
    "               alpha=0.3, s=20)\n",
    "    ax.set_xlabel(title, fontsize=11)\n",
    "    ax.set_ylabel('Price ($100k)', fontsize=11)\n",
    "    ax.set_title(f'{title} vs Price', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Notice: Higher income â†’ Higher prices! (Strong correlation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = lr.predict(X_train_scaled)\n",
    "y_pred_test = lr.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"ğŸ¯ Linear Regression Results:\")\n",
    "print(f\"   Training RÂ²: {train_r2:.3f}\")\n",
    "print(f\"   Test RÂ²: {test_r2:.3f}\")\n",
    "print(f\"   Test MAE: ${test_mae*100:.2f}k (on average, predictions are off by this much)\")\n",
    "print(f\"\\nğŸ’¡ Model explains {test_r2*100:.1f}% of price variation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.3, s=20)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', linewidth=2, label='Perfect Predictions')\n",
    "plt.xlabel('Actual Price ($100k)', fontsize=12)\n",
    "plt.ylabel('Predicted Price ($100k)', fontsize=12)\n",
    "plt.title('ğŸ¯ Actual vs Predicted House Prices', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Points closer to the red line = better predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features matter most?\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_housing.columns,\n",
    "    'Coefficient': lr.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"ğŸ”¥ Most Important Features (by coefficient magnitude):\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in feature_importance['Coefficient']]\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors)\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('ğŸ¯ Feature Impact on House Price', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Green = Increases price, Red = Decreases price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polynomial-intro",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Polynomial Regression: Capturing Curves\n",
    "\n",
    "**Problem with Linear Regression:**\n",
    "- Only fits straight lines\n",
    "- Real data often has curves!\n",
    "\n",
    "**Solution: Polynomial Regression**\n",
    "- Fits curves by adding powers: x, xÂ², xÂ³\n",
    "- Still linear regression, but with transformed features\n",
    "\n",
    "**Example:**\n",
    "- Linear: `y = 2x + 1`\n",
    "- Quadratic: `y = 2xÂ² + 3x + 1` â† Can model curves!\n",
    "- Cubic: `y = xÂ³ + 2xÂ² + 3x + 1` â† More complex curves\n",
    "\n",
    "**ğŸ¯ Real AI Use Cases:**\n",
    "- **Stock price trends** (curved patterns)\n",
    "- **User engagement curves** (initial spike, then decline)\n",
    "- **Learning curves** in AI model training\n",
    "- **Seasonality patterns** in time-series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polynomial-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate curved data\n",
    "np.random.seed(42)\n",
    "X_curve = np.linspace(0, 10, 50)\n",
    "y_curve = 0.5 * X_curve**2 - 3 * X_curve + 2 + np.random.randn(50) * 3\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_curve, y_curve, s=50, alpha=0.6, label='Data Points')\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('ğŸ“Š Curved Relationship (Linear won\\'t work well here!)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âŒ This data has a curve - linear regression will struggle!\")\n",
    "print(\"âœ… Let's use polynomial regression!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polynomial-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for sklearn\n",
    "X_curve_2d = X_curve.reshape(-1, 1)\n",
    "\n",
    "# Try different polynomial degrees\n",
    "degrees = [1, 2, 3, 5]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('ğŸ¯ Polynomial Regression: Different Degrees', fontsize=16, fontweight='bold')\n",
    "\n",
    "results_poly = {}\n",
    "\n",
    "for idx, degree in enumerate(degrees):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Transform features\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly_features.fit_transform(X_curve_2d)\n",
    "    \n",
    "    # Fit model\n",
    "    model_poly = LinearRegression()\n",
    "    model_poly.fit(X_poly, y_curve)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_poly = model_poly.predict(X_poly)\n",
    "    r2 = r2_score(y_curve, y_pred_poly)\n",
    "    results_poly[degree] = r2\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(X_curve, y_curve, alpha=0.6, s=30, label='Data')\n",
    "    ax.plot(X_curve, y_pred_poly, 'r-', linewidth=2, label=f'Degree {degree} fit')\n",
    "    ax.set_xlabel('X', fontsize=11)\n",
    "    ax.set_ylabel('y', fontsize=11)\n",
    "    ax.set_title(f'Degree {degree} (RÂ² = {r2:.3f})', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Results:\")\n",
    "for degree, r2 in results_poly.items():\n",
    "    print(f\"   Degree {degree}: RÂ² = {r2:.3f}\")\n",
    "    \n",
    "print(\"\\nğŸ’¡ Notice:\")\n",
    "print(\"   â€¢ Degree 1 (linear) underfits - too simple!\")\n",
    "print(\"   â€¢ Degree 2-3 looks good - captures the curve!\")\n",
    "print(\"   â€¢ Degree 5 might overfit - too wiggly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regularization-intro",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Regularization: Ridge and Lasso\n",
    "\n",
    "**Problem: Overfitting**\n",
    "- Model learns training data TOO well\n",
    "- Performs poorly on new data\n",
    "- Common with many features or high-degree polynomials\n",
    "\n",
    "**Solution: Regularization**\n",
    "- Adds penalty for large coefficients\n",
    "- Forces model to be simpler\n",
    "- Better generalization to new data\n",
    "\n",
    "### Ridge Regression (L2 Regularization)\n",
    "- Penalizes sum of squared coefficients\n",
    "- Shrinks all coefficients toward zero\n",
    "- Keeps all features\n",
    "\n",
    "### Lasso Regression (L1 Regularization)\n",
    "- Penalizes sum of absolute coefficients\n",
    "- Can zero out coefficients completely\n",
    "- Automatic feature selection!\n",
    "\n",
    "**ğŸ¯ Real AI Use Cases (2024-2025):**\n",
    "- **Feature selection** for RAG document ranking\n",
    "- **Preventing overfitting** in recommendation systems\n",
    "- **Model compression** for edge AI deployment\n",
    "- **Interpretable models** for AI explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regularization-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with many features (some irrelevant)\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "n_features = 20\n",
    "\n",
    "X_reg, y_reg = make_regression(n_samples=n_samples, n_features=n_features, \n",
    "                                n_informative=5, noise=10, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "print(f\"ğŸ“Š Dataset: {n_samples} samples, {n_features} features\")\n",
    "print(f\"   Only 5 features are truly informative!\")\n",
    "print(f\"   Training: {len(X_train_reg)}, Test: {len(X_test_reg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-regularization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Linear, Ridge, and Lasso\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge (Î±=1.0)': Ridge(alpha=1.0),\n",
    "    'Lasso (Î±=1.0)': Lasso(alpha=1.0)\n",
    "}\n",
    "\n",
    "results_reg = {}\n",
    "coefficients = {}\n",
    "\n",
    "print(\"ğŸ¯ Training and Comparing Models:\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train_reg_scaled, y_train_reg)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train_reg_scaled)\n",
    "    y_pred_test = model.predict(X_test_reg_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_r2 = r2_score(y_train_reg, y_pred_train)\n",
    "    test_r2 = r2_score(y_test_reg, y_pred_test)\n",
    "    \n",
    "    results_reg[name] = {'train_r2': train_r2, 'test_r2': test_r2}\n",
    "    coefficients[name] = model.coef_\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"   Train RÂ²: {train_r2:.3f}\")\n",
    "    print(f\"   Test RÂ²:  {test_r2:.3f}\")\n",
    "    print(f\"   Non-zero coefficients: {np.sum(np.abs(model.coef_) > 0.01)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-regularization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficients\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('ğŸ¯ Coefficient Comparison: Effect of Regularization', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (name, coefs) in enumerate(coefficients.items()):\n",
    "    ax = axes[idx]\n",
    "    ax.bar(range(len(coefs)), coefs, color='steelblue')\n",
    "    ax.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax.set_xlabel('Feature Index', fontsize=11)\n",
    "    ax.set_ylabel('Coefficient Value', fontsize=11)\n",
    "    ax.set_title(name, fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ Observations:\")\n",
    "print(\"   â€¢ Linear Regression: All features have non-zero coefficients\")\n",
    "print(\"   â€¢ Ridge: Coefficients shrunk but all present\")\n",
    "print(\"   â€¢ Lasso: Many coefficients exactly zero! (automatic feature selection)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance\n",
    "comparison_df = pd.DataFrame(results_reg).T\n",
    "comparison_df['Difference'] = comparison_df['train_r2'] - comparison_df['test_r2']\n",
    "\n",
    "print(\"ğŸ“Š Model Comparison:\")\n",
    "print(comparison_df)\n",
    "print(\"\\nğŸ’¡ Smaller difference = Better generalization (less overfitting)\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x_pos - width/2, comparison_df['train_r2'], width, label='Train RÂ²', color='skyblue')\n",
    "ax.bar(x_pos + width/2, comparison_df['test_r2'], width, label='Test RÂ²', color='orange')\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('RÂ² Score', fontsize=12)\n",
    "ax.set_title('ğŸ¯ Train vs Test Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(comparison_df.index)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸŒŸ Regularization (Ridge/Lasso) often gives better test performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-ai-example",
   "metadata": {},
   "source": [
    "## ğŸŒŸ Real AI Example: Sales Forecasting\n",
    "\n",
    "Let's build a **sales prediction system** - critical for business AI in 2024-2025!\n",
    "\n",
    "**Use Case:** E-commerce company predicts daily sales\n",
    "- **Input:** Marketing spend, season, day of week, competitor prices\n",
    "- **Output:** Predicted sales ($)\n",
    "- **Impact:** Optimize inventory, marketing budget, staffing\n",
    "\n",
    "**ğŸ¯ Real Applications:**\n",
    "- **Demand forecasting** for supply chain AI\n",
    "- **Revenue prediction** for financial planning\n",
    "- **Inventory optimization** in Agentic AI systems\n",
    "- **Budget allocation** for marketing AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sales-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic sales dataset\n",
    "np.random.seed(42)\n",
    "n_days = 365\n",
    "\n",
    "# Features\n",
    "marketing_spend = np.random.uniform(500, 5000, n_days)\n",
    "day_of_week = np.random.randint(0, 7, n_days)  # 0=Monday, 6=Sunday\n",
    "is_weekend = (day_of_week >= 5).astype(int)\n",
    "month = np.random.randint(1, 13, n_days)\n",
    "is_holiday_season = ((month == 11) | (month == 12)).astype(int)\n",
    "competitor_price = np.random.uniform(80, 120, n_days)\n",
    "\n",
    "# Sales (influenced by features)\n",
    "base_sales = 10000\n",
    "sales = (\n",
    "    base_sales +\n",
    "    marketing_spend * 2 +  # More marketing = more sales\n",
    "    is_weekend * 3000 +     # Weekend boost\n",
    "    is_holiday_season * 5000 +  # Holiday boost\n",
    "    (120 - competitor_price) * 100 +  # Lower competitor price = lower sales\n",
    "    np.random.randn(n_days) * 2000  # Random noise\n",
    ")\n",
    "\n",
    "# Create dataframe\n",
    "sales_df = pd.DataFrame({\n",
    "    'marketing_spend': marketing_spend,\n",
    "    'day_of_week': day_of_week,\n",
    "    'is_weekend': is_weekend,\n",
    "    'month': month,\n",
    "    'is_holiday_season': is_holiday_season,\n",
    "    'competitor_price': competitor_price,\n",
    "    'sales': sales\n",
    "})\n",
    "\n",
    "print(\"ğŸ’° Sales Forecasting Dataset:\")\n",
    "print(sales_df.head(10))\n",
    "print(f\"\\nAverage daily sales: ${sales_df['sales'].mean():,.0f}\")\n",
    "print(f\"Sales range: ${sales_df['sales'].min():,.0f} to ${sales_df['sales'].max():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sales-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('ğŸ’° Sales Drivers Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Marketing spend vs sales\n",
    "axes[0, 0].scatter(sales_df['marketing_spend'], sales_df['sales'], alpha=0.4, s=20)\n",
    "axes[0, 0].set_xlabel('Marketing Spend ($)')\n",
    "axes[0, 0].set_ylabel('Sales ($)')\n",
    "axes[0, 0].set_title('Marketing Spend vs Sales')\n",
    "\n",
    "# Weekend effect\n",
    "sales_df.boxplot(column='sales', by='is_weekend', ax=axes[0, 1])\n",
    "axes[0, 1].set_xlabel('Weekend (0=No, 1=Yes)')\n",
    "axes[0, 1].set_ylabel('Sales ($)')\n",
    "axes[0, 1].set_title('Weekend Effect on Sales')\n",
    "\n",
    "# Holiday season effect\n",
    "sales_df.boxplot(column='sales', by='is_holiday_season', ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Holiday Season (0=No, 1=Yes)')\n",
    "axes[1, 0].set_ylabel('Sales ($)')\n",
    "axes[1, 0].set_title('Holiday Season Effect')\n",
    "\n",
    "# Competitor price vs sales\n",
    "axes[1, 1].scatter(sales_df['competitor_price'], sales_df['sales'], alpha=0.4, s=20)\n",
    "axes[1, 1].set_xlabel('Competitor Price ($)')\n",
    "axes[1, 1].set_ylabel('Sales ($)')\n",
    "axes[1, 1].set_title('Competitor Price vs Sales')\n",
    "\n",
    "plt.suptitle('')  # Remove auto-generated title\n",
    "fig.suptitle('ğŸ’° Sales Drivers Analysis', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sales-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_sales = sales_df.drop('sales', axis=1)\n",
    "y_sales = sales_df['sales']\n",
    "\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = train_test_split(\n",
    "    X_sales, y_sales, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale\n",
    "scaler_sales = StandardScaler()\n",
    "X_train_sales_scaled = scaler_sales.fit_transform(X_train_sales)\n",
    "X_test_sales_scaled = scaler_sales.transform(X_test_sales)\n",
    "\n",
    "# Train all regression models\n",
    "print(\"ğŸš€ Training Sales Forecasting Models...\\n\")\n",
    "\n",
    "sales_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge (Î±=10)': Ridge(alpha=10),\n",
    "    'Lasso (Î±=10)': Lasso(alpha=10)\n",
    "}\n",
    "\n",
    "sales_results = {}\n",
    "\n",
    "for name, model in sales_models.items():\n",
    "    # Train\n",
    "    model.fit(X_train_sales_scaled, y_train_sales)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_test = model.predict(X_test_sales_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    r2 = r2_score(y_test_sales, y_pred_test)\n",
    "    mae = mean_absolute_error(y_test_sales, y_pred_test)\n",
    "    \n",
    "    sales_results[name] = {'r2': r2, 'mae': mae}\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"   RÂ²: {r2:.3f} ({r2*100:.1f}% of variance explained)\")\n",
    "    print(f\"   MAE: ${mae:,.0f} (average error)\")\n",
    "    print()\n",
    "\n",
    "best_model_name = max(sales_results, key=lambda x: sales_results[x]['r2'])\n",
    "print(f\"ğŸ† Best Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sales-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "best_sales_model = sales_models[best_model_name]\n",
    "y_pred_sales = best_sales_model.predict(X_test_sales_scaled)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0].scatter(y_test_sales, y_pred_sales, alpha=0.5, s=30)\n",
    "axes[0].plot([y_test_sales.min(), y_test_sales.max()], \n",
    "             [y_test_sales.min(), y_test_sales.max()], \n",
    "             'r--', linewidth=2, label='Perfect Predictions')\n",
    "axes[0].set_xlabel('Actual Sales ($)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Sales ($)', fontsize=12)\n",
    "axes[0].set_title('ğŸ¯ Actual vs Predicted Sales', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction errors\n",
    "errors = y_test_sales - y_pred_sales\n",
    "axes[1].hist(errors, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Prediction Error ($)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('ğŸ“Š Distribution of Prediction Errors', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ğŸ“Š Error Statistics:\")\n",
    "print(f\"   Mean error: ${errors.mean():,.0f}\")\n",
    "print(f\"   Std error: ${errors.std():,.0f}\")\n",
    "print(f\"   Most predictions are within Â±${errors.std():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "business-insights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business insights from coefficients\n",
    "feature_impact = pd.DataFrame({\n",
    "    'Feature': X_sales.columns,\n",
    "    'Coefficient': best_sales_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"ğŸ’¡ Business Insights (Feature Impact on Sales):\\n\")\n",
    "print(feature_impact)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in feature_impact['Coefficient']]\n",
    "plt.barh(feature_impact['Feature'], feature_impact['Coefficient'], color=colors)\n",
    "plt.xlabel('Impact on Sales (scaled)', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('ğŸ’° What Drives Sales? Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¯ Actionable Insights:\")\n",
    "print(\"   âœ… Increase marketing spend â†’ Higher sales\")\n",
    "print(\"   âœ… Weekend promotions â†’ Boost sales\")\n",
    "print(\"   âœ… Holiday season campaigns â†’ Big impact\")\n",
    "print(\"   âš ï¸  Monitor competitor pricing â†’ Affects sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise",
   "metadata": {},
   "source": [
    "## ğŸ¯ YOUR TURN: Final Challenge\n",
    "\n",
    "**Challenge:** Predict student exam scores!\n",
    "\n",
    "**Scenario:** Predict final exam score based on study habits\n",
    "\n",
    "**Your Task:**\n",
    "1. Use the dataset below\n",
    "2. Try Linear Regression, Ridge, and Lasso\n",
    "3. Try Polynomial features (degree 2)\n",
    "4. Which approach works best?\n",
    "5. What factors most influence exam scores?\n",
    "\n",
    "Experiment and discover! ğŸ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student exam score dataset\n",
    "np.random.seed(42)\n",
    "n_students = 300\n",
    "\n",
    "student_data = {\n",
    "    'study_hours': np.random.uniform(1, 10, n_students),\n",
    "    'attendance_rate': np.random.uniform(50, 100, n_students),\n",
    "    'previous_score': np.random.uniform(40, 95, n_students),\n",
    "    'sleep_hours': np.random.uniform(4, 9, n_students),\n",
    "    'practice_tests': np.random.randint(0, 10, n_students)\n",
    "}\n",
    "\n",
    "# Exam score (influenced by features)\n",
    "exam_score = (\n",
    "    20 +\n",
    "    student_data['study_hours'] * 5 +\n",
    "    student_data['attendance_rate'] * 0.3 +\n",
    "    student_data['previous_score'] * 0.4 +\n",
    "    student_data['sleep_hours'] * 2 +\n",
    "    student_data['practice_tests'] * 1.5 +\n",
    "    np.random.randn(n_students) * 5\n",
    ")\n",
    "\n",
    "student_data['exam_score'] = np.clip(exam_score, 0, 100)  # Scores between 0-100\n",
    "student_df = pd.DataFrame(student_data)\n",
    "\n",
    "print(\"ğŸ“ Student Exam Score Dataset:\")\n",
    "print(student_df.head(10))\n",
    "print(f\"\\nAverage exam score: {student_df['exam_score'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "# Follow the patterns you learned above\n",
    "\n",
    "# Step 1: Prepare data\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 2: Try different models\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 3: Try polynomial features\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 4: Compare and analyze\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-toggle",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ“– Click for Solution Hints</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Prepare\n",
    "X_student = student_df.drop('exam_score', axis=1)\n",
    "y_student = student_df['exam_score']\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_student, y_student, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 2: Models\n",
    "models = {\n",
    "    'Linear': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "# Train and evaluate each\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    score = r2_score(y_test, model.predict(X_test_scaled))\n",
    "    print(f\"{name}: RÂ² = {score:.3f}\")\n",
    "\n",
    "# Step 3: Polynomial\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "model_poly = Ridge(alpha=1.0)\n",
    "model_poly.fit(X_train_poly, y_train)\n",
    "score_poly = r2_score(y_test, model_poly.predict(X_test_poly))\n",
    "print(f\"Polynomial Ridge: RÂ² = {score_poly:.3f}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "**Today you mastered:**\n",
    "\n",
    "1. **Linear Regression**\n",
    "   - âœ… Foundation of predictive modeling\n",
    "   - âœ… Fast and interpretable\n",
    "   - âœ… Works well for linear relationships\n",
    "   - **Use for:** Baselines, interpretability, simple predictions\n",
    "\n",
    "2. **Polynomial Regression**\n",
    "   - âœ… Captures non-linear patterns\n",
    "   - âœ… Flexible with degree parameter\n",
    "   - âš ï¸ Watch for overfitting at high degrees\n",
    "   - **Use for:** Curved trends, seasonality, growth patterns\n",
    "\n",
    "3. **Ridge & Lasso Regularization**\n",
    "   - âœ… Prevents overfitting\n",
    "   - âœ… Lasso does automatic feature selection\n",
    "   - âœ… Better generalization\n",
    "   - **Use for:** High-dimensional data, feature selection, robustness\n",
    "\n",
    "**ğŸ“Š Evaluation Metrics:**\n",
    "- **RÂ²**: How much variance explained (0 to 1, higher better)\n",
    "- **MAE**: Average prediction error (interpretable)\n",
    "- **MSE**: Penalizes large errors heavily\n",
    "\n",
    "**ğŸŒŸ Real-World Applications (2024-2025):**\n",
    "- **E-commerce:** Price prediction, demand forecasting\n",
    "- **Finance:** Stock trends, revenue prediction\n",
    "- **RAG Systems:** Document relevance scoring\n",
    "- **Agentic AI:** Action value estimation\n",
    "- **Recommender Systems:** Rating prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "algorithm-selection",
   "metadata": {},
   "source": [
    "## ğŸ§­ Algorithm Selection Guide\n",
    "\n",
    "**Which regression algorithm should you use?**\n",
    "\n",
    "| Scenario | Algorithm | Why? |\n",
    "|----------|-----------|------|\n",
    "| **Linear relationship, few features** | Linear Regression | Simple, fast, interpretable |\n",
    "| **Curved patterns** | Polynomial Regression | Captures non-linearity |\n",
    "| **Many features** | Ridge or Lasso | Prevents overfitting |\n",
    "| **Need feature selection** | Lasso | Zeros out irrelevant features |\n",
    "| **Keep all features** | Ridge | Shrinks but keeps all |\n",
    "| **Production system** | Ridge/Lasso | More robust, better generalization |\n",
    "\n",
    "**ğŸ¯ Pro Tip (2024-2025 AI Workflow):**\n",
    "1. Start with **Linear Regression** (baseline)\n",
    "2. Try **Polynomial features** if needed\n",
    "3. Add **Lasso** for feature selection\n",
    "4. Use **Ridge** for final model\n",
    "5. For complex patterns â†’ Try ensemble methods (next week!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## ğŸš€ What's Next?\n",
    "\n",
    "**ğŸ‰ Congratulations! You've completed Week 7: Supervised Learning!**\n",
    "\n",
    "**You now know:**\n",
    "- âœ… 6 Classification algorithms (Day 1-2)\n",
    "- âœ… 4 Regression algorithms (Day 3)\n",
    "- âœ… When to use each algorithm\n",
    "- âœ… How to evaluate and compare models\n",
    "- âœ… Real AI applications in 2024-2025\n",
    "\n",
    "**Practice Exercises:**\n",
    "1. Apply these algorithms to your own datasets\n",
    "2. Try different hyperparameters (Î± values, polynomial degrees)\n",
    "3. Combine techniques (polynomial + regularization)\n",
    "4. Build end-to-end prediction pipelines\n",
    "\n",
    "**Coming in Week 8:**\n",
    "- **Ensemble Methods:** Random Forests, Gradient Boosting, XGBoost\n",
    "- **Model Evaluation:** Cross-validation, ROC curves, calibration\n",
    "- **Hyperparameter Tuning:** Grid search, random search\n",
    "- **Feature Engineering:** Advanced techniques for better models\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Remember:**\n",
    "- Simple models often work best in production!\n",
    "- Linear/Ridge/Lasso are still used in modern AI systems\n",
    "- Understanding these fundamentals makes you better at deep learning\n",
    "- Real AI = Right tool for the job, not always the fanciest model\n",
    "\n",
    "---\n",
    "\n",
    "*You're building a strong foundation in AI/ML. These \"traditional\" algorithms power countless real-world systems and are essential for understanding modern AI!* ğŸŒŸ\n",
    "\n",
    "**Keep coding, keep learning, keep building!** ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
