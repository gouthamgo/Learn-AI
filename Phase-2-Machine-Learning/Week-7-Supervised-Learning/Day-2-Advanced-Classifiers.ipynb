{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 2: Advanced Classification Algorithms\n",
    "\n",
    "**üéØ Goal:** Master advanced classifiers: SVM, KNN, and Naive Bayes\n",
    "\n",
    "**‚è±Ô∏è Time:** 60-90 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- **SVM** powers image classification in computer vision (before deep learning)\n",
    "- **KNN** is used in recommendation systems and anomaly detection\n",
    "- **Naive Bayes** is the engine behind spam filters and text classification\n",
    "- These algorithms complement Transformers in hybrid AI systems (2024-2025)\n",
    "- Foundation for understanding how multimodal AI classifies different data types\n",
    "- Critical for Agentic AI systems that need fast, interpretable decisions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap",
   "metadata": {},
   "source": [
    "## üìö Quick Recap: Day 1\n",
    "\n",
    "Yesterday we learned:\n",
    "- **Logistic Regression** ‚Üí Linear decision boundaries\n",
    "- **Decision Trees** ‚Üí Rule-based decisions\n",
    "- **Random Forests** ‚Üí Ensemble of trees\n",
    "\n",
    "Today we'll explore:\n",
    "- **Support Vector Machines (SVM)** ‚Üí Find the best boundary\n",
    "- **K-Nearest Neighbors (KNN)** ‚Üí Vote by neighbors\n",
    "- **Naive Bayes** ‚Üí Probability-based classification\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Advanced classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# For visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Make plots beautiful\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Advanced AI libraries loaded!\")\n",
    "print(\"Let's build some powerful classifiers! üéØ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "svm-intro",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Support Vector Machines (SVM)\n",
    "\n",
    "**What it does:** Finds the BEST line/boundary to separate categories\n",
    "\n",
    "**How it works:**\n",
    "- Imagine points on a graph: red dots vs blue dots\n",
    "- Draw a line to separate them\n",
    "- SVM finds the line with the **maximum margin** (widest gap) between classes\n",
    "\n",
    "**Visual Analogy:**\n",
    "```\n",
    "Red dots: ‚óè  ‚óè  ‚óè          |          ‚óã  ‚óã  ‚óã :Blue dots\n",
    "              ‚óè  ‚óè         |         ‚óã  ‚óã\n",
    "                ‚óè          |          ‚óã\n",
    "                     [Max Margin]\n",
    "                           ‚Üë\n",
    "                    Decision Boundary\n",
    "```\n",
    "\n",
    "**Best for:**\n",
    "- High-dimensional data (many features)\n",
    "- Clear separation between classes\n",
    "- Works well with small datasets\n",
    "\n",
    "**üéØ Real AI Use Cases (2024-2025):**\n",
    "- **Image classification basics** (facial recognition, object detection)\n",
    "- **Text categorization** in RAG document retrieval\n",
    "- **Anomaly detection** for AI safety systems\n",
    "- **Hybrid AI** combining SVM with Transformers for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svm-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D dataset for visualization\n",
    "np.random.seed(42)\n",
    "\n",
    "# Class 1: Cluster in bottom-left\n",
    "X_class1 = np.random.randn(100, 2) + np.array([-2, -2])\n",
    "y_class1 = np.zeros(100)\n",
    "\n",
    "# Class 2: Cluster in top-right\n",
    "X_class2 = np.random.randn(100, 2) + np.array([2, 2])\n",
    "y_class2 = np.ones(100)\n",
    "\n",
    "# Combine\n",
    "X_2d = np.vstack([X_class1, X_class2])\n",
    "y_2d = np.hstack([y_class1, y_class2])\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_2d[y_2d == 0, 0], X_2d[y_2d == 0, 1], \n",
    "            c='red', marker='o', label='Class 0', alpha=0.6, s=50)\n",
    "plt.scatter(X_2d[y_2d == 1, 0], X_2d[y_2d == 1, 1], \n",
    "            c='blue', marker='s', label='Class 1', alpha=0.6, s=50)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('üéØ SVM Dataset: Can we separate these classes?', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Dataset created with 2 clearly separable classes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svm-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(\n",
    "    X_2d, y_2d, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features (important for SVM!)\n",
    "scaler = StandardScaler()\n",
    "X_train_svm_scaled = scaler.fit_transform(X_train_svm)\n",
    "X_test_svm_scaled = scaler.transform(X_test_svm)\n",
    "\n",
    "# Train SVM with linear kernel\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_linear.fit(X_train_svm_scaled, y_train_svm)\n",
    "\n",
    "# Predictions\n",
    "y_pred_svm = svm_linear.predict(X_test_svm_scaled)\n",
    "accuracy_svm = accuracy_score(y_test_svm, y_pred_svm)\n",
    "\n",
    "print(\"üéØ Support Vector Machine Results:\")\n",
    "print(f\"Accuracy: {accuracy_svm:.2%}\")\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test_svm, y_pred_svm, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svm-visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    h = 0.02  # step size in mesh\n",
    "    \n",
    "    # Create mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FFAAAA', '#AAAAFF']))\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='red', marker='o', \n",
    "                label='Class 0', edgecolors='k', s=50)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', marker='s', \n",
    "                label='Class 1', edgecolors='k', s=50)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_train_svm_scaled, y_train_svm, svm_linear, \n",
    "                       'üéØ SVM Decision Boundary (Linear Kernel)')\n",
    "\n",
    "print(\"üìä The shaded regions show where SVM predicts each class!\")\n",
    "print(\"The line in the middle is the decision boundary with maximum margin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "svm-kernels",
   "metadata": {},
   "source": [
    "### üî• SVM Kernels: Handling Complex Patterns\n",
    "\n",
    "What if data isn't linearly separable? Use **kernels**!\n",
    "\n",
    "**Kernels transform data to higher dimensions:**\n",
    "- **Linear:** Straight line (what we just did)\n",
    "- **RBF (Radial Basis Function):** Curved boundaries (most popular)\n",
    "- **Polynomial:** Curved, more complex\n",
    "\n",
    "Think of it like:\n",
    "- Can't separate with a flat sheet of paper? ‚Üí Bend the paper! (RBF kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svm-rbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linearly separable data (circles)\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X_circles, y_circles = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_circles[y_circles == 0, 0], X_circles[y_circles == 0, 1],\n",
    "            c='red', marker='o', label='Class 0', alpha=0.6, s=50)\n",
    "plt.scatter(X_circles[y_circles == 1, 0], X_circles[y_circles == 1, 1],\n",
    "            c='blue', marker='s', label='Class 1', alpha=0.6, s=50)\n",
    "plt.title('üîµ Non-Linear Dataset: Circles', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚ùå A straight line can't separate these circles!\")\n",
    "print(\"‚úÖ But SVM with RBF kernel can! Let's see...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svm-rbf-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale\n",
    "X_train_circ, X_test_circ, y_train_circ, y_test_circ = train_test_split(\n",
    "    X_circles, y_circles, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_circ = StandardScaler()\n",
    "X_train_circ_scaled = scaler_circ.fit_transform(X_train_circ)\n",
    "X_test_circ_scaled = scaler_circ.transform(X_test_circ)\n",
    "\n",
    "# Train SVM with RBF kernel\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train_circ_scaled, y_train_circ)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_rbf = svm_rbf.predict(X_test_circ_scaled)\n",
    "accuracy_rbf = accuracy_score(y_test_circ, y_pred_rbf)\n",
    "\n",
    "print(\"üéØ SVM with RBF Kernel:\")\n",
    "print(f\"Accuracy: {accuracy_rbf:.2%}\")\n",
    "\n",
    "# Visualize\n",
    "plot_decision_boundary(X_train_circ_scaled, y_train_circ, svm_rbf,\n",
    "                       'üéØ SVM with RBF Kernel: Curved Decision Boundary')\n",
    "\n",
    "print(\"\\nüåü Amazing! SVM perfectly separated the circles with a curved boundary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knn-intro",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ K-Nearest Neighbors (KNN)\n",
    "\n",
    "**What it does:** Classifies based on the majority vote of nearby neighbors\n",
    "\n",
    "**How it works:**\n",
    "1. Given a new point, find the K closest training points\n",
    "2. Count their labels\n",
    "3. Majority vote wins!\n",
    "\n",
    "**Analogy:**\n",
    "\"Tell me who your friends are, and I'll tell you who you are!\"\n",
    "- If K=5, look at 5 nearest neighbors\n",
    "- If 3 are \"red\" and 2 are \"blue\" ‚Üí Predict \"red\"\n",
    "\n",
    "**Best for:**\n",
    "- Simple and intuitive\n",
    "- No training required (lazy learning)\n",
    "- Works well with small datasets\n",
    "\n",
    "**üéØ Real AI Use Cases (2024-2025):**\n",
    "- **Recommendation systems** (Netflix, Spotify: find similar users)\n",
    "- **Anomaly detection** (fraud detection, cybersecurity)\n",
    "- **Semantic search** in RAG systems (find similar documents)\n",
    "- **Image similarity** in multimodal AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "X_knn = np.vstack([\n",
    "    np.random.randn(50, 2) + [2, 2],   # Class 0\n",
    "    np.random.randn(50, 2) + [-2, -2]  # Class 1\n",
    "])\n",
    "y_knn = np.array([0] * 50 + [1] * 50)\n",
    "\n",
    "# Split\n",
    "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(\n",
    "    X_knn, y_knn, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale (KNN is distance-based, so scaling is important!)\n",
    "scaler_knn = StandardScaler()\n",
    "X_train_knn_scaled = scaler_knn.fit_transform(X_train_knn)\n",
    "X_test_knn_scaled = scaler_knn.transform(X_test_knn)\n",
    "\n",
    "# Try different values of K\n",
    "k_values = [1, 3, 5, 10, 20]\n",
    "results_knn = {}\n",
    "\n",
    "print(\"üîç Testing different K values:\\n\")\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_knn_scaled, y_train_knn)\n",
    "    accuracy = accuracy_score(y_test_knn, knn.predict(X_test_knn_scaled))\n",
    "    results_knn[k] = accuracy\n",
    "    print(f\"K={k:2d} ‚Üí Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Visualize impact of K\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(results_knn.keys()), list(results_knn.values()), \n",
    "         marker='o', linewidth=2, markersize=10)\n",
    "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('üéØ KNN: Impact of K on Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k = max(results_knn, key=results_knn.get)\n",
    "print(f\"\\nüèÜ Best K value: {best_k} with {results_knn[best_k]:.2%} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn-visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary with best K\n",
    "knn_best = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_best.fit(X_train_knn_scaled, y_train_knn)\n",
    "\n",
    "plot_decision_boundary(X_train_knn_scaled, y_train_knn, knn_best,\n",
    "                       f'üéØ KNN Decision Boundary (K={best_k})')\n",
    "\n",
    "print(f\"üìä Each point is classified based on its {best_k} nearest neighbors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naive-bayes-intro",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Naive Bayes\n",
    "\n",
    "**What it does:** Uses probability and Bayes' Theorem to classify\n",
    "\n",
    "**How it works:**\n",
    "- Calculates: \"What's the probability this email is spam, given these words?\"\n",
    "- Uses Bayes' Theorem: P(Spam|Words) = P(Words|Spam) √ó P(Spam) / P(Words)\n",
    "\n",
    "**Why \"Naive\"?**\n",
    "- Assumes features are independent (naive assumption)\n",
    "- Example: Assumes \"free\" and \"money\" appear independently\n",
    "- In reality, they often appear together in spam!\n",
    "- But it works surprisingly well anyway!\n",
    "\n",
    "**Best for:**\n",
    "- Text classification (spam, sentiment)\n",
    "- Fast training and prediction\n",
    "- Works well with high-dimensional data\n",
    "\n",
    "**üéØ Real AI Use Cases (2024-2025):**\n",
    "- **Email spam filtering** (still used by Gmail, Outlook)\n",
    "- **Sentiment analysis** for social media monitoring\n",
    "- **Document classification** in RAG retrieval\n",
    "- **Real-time classification** in streaming AI systems\n",
    "- **Text preprocessing** for Transformer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive-bayes-spam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spam detection dataset (similar to Day 1)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_emails = 1000\n",
    "\n",
    "spam_data = {\n",
    "    'word_freq_free': np.concatenate([\n",
    "        np.random.exponential(2, 400),   # Spam\n",
    "        np.random.exponential(0.3, 600)  # Not spam\n",
    "    ]),\n",
    "    'word_freq_money': np.concatenate([\n",
    "        np.random.exponential(1.8, 400),\n",
    "        np.random.exponential(0.2, 600)\n",
    "    ]),\n",
    "    'word_freq_winner': np.concatenate([\n",
    "        np.random.exponential(1.5, 400),\n",
    "        np.random.exponential(0.1, 600)\n",
    "    ]),\n",
    "    'word_freq_click': np.concatenate([\n",
    "        np.random.exponential(1.2, 400),\n",
    "        np.random.exponential(0.15, 600)\n",
    "    ]),\n",
    "    'exclamation_marks': np.concatenate([\n",
    "        np.random.poisson(4, 400),\n",
    "        np.random.poisson(0.8, 600)\n",
    "    ]),\n",
    "    'is_spam': [1] * 400 + [0] * 600\n",
    "}\n",
    "\n",
    "spam_df = pd.DataFrame(spam_data)\n",
    "spam_df = spam_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"üìß Spam Detection Dataset:\")\n",
    "print(spam_df.head())\n",
    "print(f\"\\nSpam: {spam_df['is_spam'].sum()}, Not Spam: {(spam_df['is_spam'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive-bayes-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_spam = spam_df.drop('is_spam', axis=1)\n",
    "y_spam = spam_df['is_spam']\n",
    "\n",
    "X_train_spam, X_test_spam, y_train_spam, y_test_spam = train_test_split(\n",
    "    X_spam, y_spam, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Gaussian Naive Bayes (for continuous features)\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_spam, y_train_spam)\n",
    "\n",
    "# Predictions\n",
    "y_pred_nb = nb.predict(X_test_spam)\n",
    "y_pred_proba_nb = nb.predict_proba(X_test_spam)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "accuracy_nb = accuracy_score(y_test_spam, y_pred_nb)\n",
    "\n",
    "print(\"üéØ Naive Bayes Results:\")\n",
    "print(f\"Accuracy: {accuracy_nb:.2%}\")\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test_spam, y_pred_nb, target_names=['Not Spam', 'Spam']))\n",
    "\n",
    "# Show probability predictions\n",
    "print(\"\\nüîç Sample Predictions with Probabilities:\")\n",
    "for i in range(5):\n",
    "    actual = \"Spam\" if y_test_spam.iloc[i] == 1 else \"Not Spam\"\n",
    "    predicted = \"Spam\" if y_pred_nb[i] == 1 else \"Not Spam\"\n",
    "    prob_spam = y_pred_proba_nb[i]\n",
    "    print(f\"Email {i+1}: Actual={actual:8s} | Predicted={predicted:8s} | P(Spam)={prob_spam:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-ai-example",
   "metadata": {},
   "source": [
    "## üåü Real AI Example: Image Classification Basics\n",
    "\n",
    "Let's use these algorithms for **basic image classification** - a foundation for computer vision!\n",
    "\n",
    "**Scenario:** Classify handwritten digits (0-9) - the \"Hello World\" of image AI\n",
    "\n",
    "We'll use the famous **MNIST-like dataset** with 8√ó8 pixel images.\n",
    "\n",
    "**üéØ Why This Matters:**\n",
    "- Foundation for OCR (Optical Character Recognition)\n",
    "- Used in document processing for RAG systems\n",
    "- Building block for multimodal AI (text + images)\n",
    "- Understanding before jumping to deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digits-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X_digits = digits.data  # 8x8 images flattened to 64 features\n",
    "y_digits = digits.target  # Labels 0-9\n",
    "\n",
    "print(\"üñºÔ∏è Digits Dataset Loaded!\")\n",
    "print(f\"Number of images: {len(X_digits)}\")\n",
    "print(f\"Image size: 8x8 pixels = {X_digits.shape[1]} features\")\n",
    "print(f\"Classes: {np.unique(y_digits)}\")\n",
    "\n",
    "# Visualize some digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('üî¢ Sample Handwritten Digits', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='gray')\n",
    "    ax.set_title(f'Label: {digits.target[i]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digits-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_dig, X_test_dig, y_train_dig, y_test_dig = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_dig = StandardScaler()\n",
    "X_train_dig_scaled = scaler_dig.fit_transform(X_train_dig)\n",
    "X_test_dig_scaled = scaler_dig.transform(X_test_dig)\n",
    "\n",
    "# Train all classifiers\n",
    "print(\"üöÄ Training Advanced Classifiers on Digit Images...\\n\")\n",
    "\n",
    "classifiers = {\n",
    "    'SVM (Linear)': SVC(kernel='linear', random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
    "    'KNN (K=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "results_digits = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Train\n",
    "    clf.fit(X_train_dig_scaled, y_train_dig)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = clf.predict(X_test_dig_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test_dig, y_pred)\n",
    "    results_digits[name] = accuracy\n",
    "    \n",
    "    print(f\"{name:20s} ‚Üí Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "print(\"\\nüèÜ Best Model:\", max(results_digits, key=results_digits.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digits-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models visually\n",
    "plt.figure(figsize=(12, 6))\n",
    "models = list(results_digits.keys())\n",
    "accuracies = list(results_digits.values())\n",
    "\n",
    "bars = plt.bar(models, accuracies, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('üéØ Image Classification: Algorithm Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=15)\n",
    "plt.ylim(0.8, 1.0)\n",
    "\n",
    "# Add accuracy labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1%}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä All algorithms perform well on this image classification task!\")\n",
    "print(\"SVM with RBF kernel often wins for image data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Show some predictions\n",
    "best_model = SVC(kernel='rbf', random_state=42)\n",
    "best_model.fit(X_train_dig_scaled, y_train_dig)\n",
    "predictions = best_model.predict(X_test_dig_scaled)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('üéØ SVM Predictions on Test Images', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Reshape back to 8x8 for visualization\n",
    "    img = X_test_dig[i].reshape(8, 8)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    \n",
    "    actual = y_test_dig.iloc[i] if isinstance(y_test_dig, pd.Series) else y_test_dig[i]\n",
    "    predicted = predictions[i]\n",
    "    \n",
    "    color = 'green' if actual == predicted else 'red'\n",
    "    ax.set_title(f'Pred: {predicted}, True: {actual}', color=color, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Green = Correct prediction\")\n",
    "print(\"‚ùå Red = Incorrect prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Exercise 1\n",
    "\n",
    "**Challenge:** Build a wine quality classifier!\n",
    "\n",
    "**Scenario:** Classify wine as \"Good\" or \"Bad\" based on chemical properties\n",
    "\n",
    "**Your Task:**\n",
    "1. Use the wine dataset below\n",
    "2. Train SVM, KNN, and Naive Bayes\n",
    "3. Compare their accuracy\n",
    "4. Which works best for this problem?\n",
    "\n",
    "Experiment and learn! üç∑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine quality dataset\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = (wine.target > 0).astype(int)  # Binary: 0 vs (1 or 2)\n",
    "\n",
    "print(\"üç∑ Wine Dataset:\")\n",
    "print(f\"Samples: {len(X_wine)}\")\n",
    "print(f\"Features: {wine.feature_names}\")\n",
    "print(f\"Classes: 0 (Type 0), 1 (Type 1 or 2)\")\n",
    "print(f\"\\nClass distribution: {np.bincount(y_wine)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "# Hint: Follow the pattern from digits classification\n",
    "\n",
    "# Step 1: Split data\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 2: Scale features\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 3: Train classifiers (SVM, KNN, Naive Bayes)\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 4: Compare results\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-toggle",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Split\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
    "    X_wine, y_wine, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Scale\n",
    "scaler_wine = StandardScaler()\n",
    "X_train_wine_scaled = scaler_wine.fit_transform(X_train_wine)\n",
    "X_test_wine_scaled = scaler_wine.transform(X_test_wine)\n",
    "\n",
    "# Step 3: Train\n",
    "wine_classifiers = {\n",
    "    'SVM': SVC(kernel='rbf', random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "wine_results = {}\n",
    "for name, clf in wine_classifiers.items():\n",
    "    clf.fit(X_train_wine_scaled, y_train_wine)\n",
    "    acc = accuracy_score(y_test_wine, clf.predict(X_test_wine_scaled))\n",
    "    wine_results[name] = acc\n",
    "    print(f\"{name}: {acc:.2%}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## üìä Algorithm Comparison Summary\n",
    "\n",
    "| Algorithm | Strengths | Weaknesses | Best For |\n",
    "|-----------|-----------|------------|----------|\n",
    "| **SVM** | Excellent for high dimensions, handles non-linear patterns with kernels | Slow on large datasets, needs feature scaling | Image classification, text categorization |\n",
    "| **KNN** | Simple, no training needed, works for any data shape | Slow predictions, sensitive to scale | Recommendation systems, anomaly detection |\n",
    "| **Naive Bayes** | Very fast, works with high dimensions | Assumes independence (rarely true) | Spam filtering, sentiment analysis |\n",
    "\n",
    "**üéØ When to Use Each (2024-2025 AI Context):**\n",
    "\n",
    "- **SVM**: Pre-processing for Transformers, hybrid AI systems, when you need high accuracy\n",
    "- **KNN**: Similarity search in RAG systems, real-time recommendations\n",
    "- **Naive Bayes**: Fast text classification, streaming data, resource-constrained systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**Today you mastered:**\n",
    "\n",
    "1. **Support Vector Machines (SVM)**\n",
    "   - ‚úÖ Finds optimal decision boundary\n",
    "   - ‚úÖ Kernels handle non-linear patterns\n",
    "   - ‚úÖ Great for images and high-dimensional data\n",
    "   - **Use in:** Image classification, text categorization, RAG document filtering\n",
    "\n",
    "2. **K-Nearest Neighbors (KNN)**\n",
    "   - ‚úÖ Simple and intuitive\n",
    "   - ‚úÖ No training phase\n",
    "   - ‚ùå Slow predictions on large data\n",
    "   - **Use in:** Recommendation systems, similarity search, anomaly detection\n",
    "\n",
    "3. **Naive Bayes**\n",
    "   - ‚úÖ Super fast training and prediction\n",
    "   - ‚úÖ Works great for text\n",
    "   - ‚úÖ Provides probabilities\n",
    "   - **Use in:** Spam filters, sentiment analysis, document classification\n",
    "\n",
    "**üåü Real-World Impact:**\n",
    "- These algorithms power Gmail's spam filter (Naive Bayes)\n",
    "- Netflix recommendations use KNN-like approaches\n",
    "- Early face recognition used SVM (before deep learning)\n",
    "- Modern RAG systems combine these for document retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "**Practice Ideas:**\n",
    "1. Try different SVM kernels (linear, RBF, polynomial) on the digits dataset\n",
    "2. Experiment with different K values in KNN\n",
    "3. Compare Naive Bayes with yesterday's Logistic Regression\n",
    "\n",
    "**Coming Tomorrow:**\n",
    "- **Day 3:** Regression Algorithms (predict continuous values!)\n",
    "  - Linear Regression deep dive\n",
    "  - Polynomial Regression\n",
    "  - Ridge and Lasso regularization\n",
    "  - Real AI examples: price prediction, forecasting\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Amazing Work!** You now know 6 powerful classification algorithms!\n",
    "\n",
    "**üí¨ Pro Tip:** In real AI projects (2024-2025), you often:\n",
    "1. Start with simple models (Logistic Regression, Naive Bayes)\n",
    "2. Try ensemble methods (Random Forest)\n",
    "3. Use SVM for complex patterns\n",
    "4. Finally, deep learning for maximum accuracy\n",
    "\n",
    "---\n",
    "\n",
    "*These \"traditional\" algorithms are still essential in modern AI systems - they're faster, more interpretable, and often perform just as well as complex models!* üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
