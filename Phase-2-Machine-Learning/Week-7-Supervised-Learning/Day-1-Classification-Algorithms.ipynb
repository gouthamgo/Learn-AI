{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 1: Classification Algorithms\n",
    "\n",
    "**üéØ Goal:** Master the fundamental classification algorithms used in modern AI systems\n",
    "\n",
    "**‚è±Ô∏è Time:** 60-90 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Classification powers spam detection, sentiment analysis, and content moderation\n",
    "- Used in RAG systems to classify document relevance\n",
    "- Foundation for Agentic AI decision-making (which action to take?)\n",
    "- Pre-processing step before Transformer models\n",
    "- Essential for building production AI systems in 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-is-classification",
   "metadata": {},
   "source": [
    "## ü§î What is Classification?\n",
    "\n",
    "**Classification = Assigning items to categories**\n",
    "\n",
    "Real-world examples:\n",
    "- Email: Spam or Not Spam?\n",
    "- Review: Positive, Negative, or Neutral?\n",
    "- Image: Cat, Dog, or Bird?\n",
    "- Customer: Will Buy or Won't Buy?\n",
    "\n",
    "Think of it as a **sorting machine**:\n",
    "- Input: Data (email text, customer info, image pixels)\n",
    "- Output: Category label (spam/not spam, buy/won't buy)\n",
    "\n",
    "Let's build classification models! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tools we need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers we'll use\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Make plots look nice\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"Let's build some AI classifiers! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-intro",
   "metadata": {},
   "source": [
    "## üìä Our Dataset: Email Spam Detection\n",
    "\n",
    "We'll build a **spam classifier** - a real AI application used by Gmail, Outlook, etc!\n",
    "\n",
    "**Features:**\n",
    "- `word_freq_money`: How often \"money\" appears\n",
    "- `word_freq_free`: How often \"free\" appears\n",
    "- `word_freq_winner`: How often \"winner\" appears\n",
    "- `capital_run_length`: Longest sequence of CAPS\n",
    "- `exclamation_marks`: Number of !!!\n",
    "\n",
    "**Target:**\n",
    "- `is_spam`: 1 = Spam, 0 = Not Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistic spam detection dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate features\n",
    "# Spam emails have more \"money\", \"free\", caps, etc.\n",
    "data = {\n",
    "    'word_freq_money': np.concatenate([\n",
    "        np.random.exponential(2, 400),  # Spam\n",
    "        np.random.exponential(0.3, 600)  # Not spam\n",
    "    ]),\n",
    "    'word_freq_free': np.concatenate([\n",
    "        np.random.exponential(1.5, 400),\n",
    "        np.random.exponential(0.2, 600)\n",
    "    ]),\n",
    "    'word_freq_winner': np.concatenate([\n",
    "        np.random.exponential(1, 400),\n",
    "        np.random.exponential(0.1, 600)\n",
    "    ]),\n",
    "    'capital_run_length': np.concatenate([\n",
    "        np.random.poisson(8, 400),\n",
    "        np.random.poisson(2, 600)\n",
    "    ]),\n",
    "    'exclamation_marks': np.concatenate([\n",
    "        np.random.poisson(5, 400),\n",
    "        np.random.poisson(1, 600)\n",
    "    ]),\n",
    "    'is_spam': [1] * 400 + [0] * 600  # 40% spam, 60% not spam\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Shuffle the data\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"üìß Spam Detection Dataset Created!\")\n",
    "print(f\"Total emails: {len(df)}\")\n",
    "print(f\"Spam emails: {df['is_spam'].sum()}\")\n",
    "print(f\"Not spam: {(df['is_spam'] == 0).sum()}\")\n",
    "print(\"\\nFirst few emails:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference between spam and not spam\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('üìä Spam vs Not Spam: Feature Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "features = ['word_freq_money', 'word_freq_free', 'word_freq_winner', \n",
    "            'capital_run_length', 'exclamation_marks']\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    df[df['is_spam'] == 1][feature].hist(ax=ax, alpha=0.6, label='Spam', bins=20, color='red')\n",
    "    df[df['is_spam'] == 0][feature].hist(ax=ax, alpha=0.6, label='Not Spam', bins=20, color='green')\n",
    "    \n",
    "    ax.set_title(feature.replace('_', ' ').title())\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Notice how spam emails have higher values for suspicious words!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare-data",
   "metadata": {},
   "source": [
    "## üîß Prepare Data for Training\n",
    "\n",
    "Before training, we need to:\n",
    "1. **Split data**: Training set (80%) and Test set (20%)\n",
    "2. **Scale features**: Normalize values for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df.drop('is_spam', axis=1)\n",
    "y = df['is_spam']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale the features (important for many algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Data prepared!\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "algo-1-logistic",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Logistic Regression\n",
    "\n",
    "**What it does:** Calculates the probability that something belongs to a category\n",
    "\n",
    "**How it works:**\n",
    "- Draws a line (or curve) to separate categories\n",
    "- Outputs probability: 0.0 (definitely not spam) to 1.0 (definitely spam)\n",
    "- If probability > 0.5 ‚Üí Spam, else ‚Üí Not Spam\n",
    "\n",
    "**Best for:**\n",
    "- Fast predictions\n",
    "- When you need probability scores\n",
    "- Binary classification (2 categories)\n",
    "\n",
    "**üéØ Real AI Use Cases:**\n",
    "- **Sentiment analysis** in social media monitoring (2024 trend)\n",
    "- **Document classification** in RAG systems\n",
    "- **Action classification** for Agentic AI (which action to take?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logistic-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba_log = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
    "\n",
    "print(\"üéØ Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_log:.2%}\")\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_log, target_names=['Not Spam', 'Spam']))\n",
    "\n",
    "# Show some predictions with probabilities\n",
    "print(\"\\nüîç Sample Predictions (first 5 test emails):\")\n",
    "for i in range(5):\n",
    "    actual = \"Spam\" if y_test.iloc[i] == 1 else \"Not Spam\"\n",
    "    predicted = \"Spam\" if y_pred_log[i] == 1 else \"Not Spam\"\n",
    "    confidence = y_pred_proba_log[i] if y_pred_log[i] == 1 else 1 - y_pred_proba_log[i]\n",
    "    print(f\"Email {i+1}: Actual={actual}, Predicted={predicted}, Confidence={confidence:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "algo-2-decision-tree",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Decision Trees\n",
    "\n",
    "**What it does:** Makes decisions by asking yes/no questions\n",
    "\n",
    "**How it works:**\n",
    "- Like a flowchart: \"Does it have 'money' > 2 times?\"\n",
    "  - Yes ‚Üí \"Does it have CAPS?\"\n",
    "    - Yes ‚Üí SPAM\n",
    "    - No ‚Üí Check more...\n",
    "\n",
    "**Best for:**\n",
    "- Easy to understand and visualize\n",
    "- Handles non-linear patterns\n",
    "- No need to scale data\n",
    "\n",
    "**üéØ Real AI Use Cases:**\n",
    "- **Customer segmentation** for personalized AI\n",
    "- **Fraud detection** in financial AI systems\n",
    "- **Rule extraction** from multimodal AI outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decision-tree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "dt.fit(X_train, y_train)  # No scaling needed!\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "print(\"üå≥ Decision Tree Results:\")\n",
    "print(f\"Accuracy: {accuracy_dt:.2%}\")\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt, target_names=['Not Spam', 'Spam']))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': dt.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüî• Most Important Features:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('üéØ Decision Tree: Feature Importance for Spam Detection', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Higher importance = More useful for detecting spam!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "algo-3-random-forest",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Random Forests\n",
    "\n",
    "**What it does:** Combines many decision trees for better accuracy\n",
    "\n",
    "**How it works:**\n",
    "- Creates 100+ decision trees (a \"forest\")\n",
    "- Each tree votes on the prediction\n",
    "- Final prediction = majority vote\n",
    "\n",
    "**Analogy:** Instead of asking 1 expert, ask 100 experts and use the majority opinion!\n",
    "\n",
    "**Best for:**\n",
    "- High accuracy\n",
    "- Robust to overfitting\n",
    "- Handles complex patterns\n",
    "\n",
    "**üéØ Real AI Use Cases:**\n",
    "- **Content moderation** on social media platforms\n",
    "- **Query routing** in RAG systems (which documents to retrieve?)\n",
    "- **Ensemble methods** combined with Transformers in 2024-2025\n",
    "- **Feature extraction** for Agentic AI decision-making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"üå≤ Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.2%}\")\n",
    "print(f\"Number of trees: {rf.n_estimators}\")\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Not Spam', 'Spam']))\n",
    "\n",
    "# Feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüî• Most Important Features:\")\n",
    "print(rf_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-models",
   "metadata": {},
   "source": [
    "## üìä Compare All Models\n",
    "\n",
    "Let's see which algorithm performs best on our spam detection task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'Accuracy': [accuracy_log, accuracy_dt, accuracy_rf]\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"üèÜ Model Comparison:\")\n",
    "print(results.to_string(index=False))\n",
    "print(f\"\\nü•á Best Model: {results.iloc[0]['Model']} with {results.iloc[0]['Accuracy']:.2%} accuracy\")\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(results['Model'], results['Accuracy'], color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('üéØ Classification Algorithm Comparison: Spam Detection', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0.7, 1.0)\n",
    "\n",
    "# Add accuracy labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2%}',\n",
    "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-matrix",
   "metadata": {},
   "source": [
    "## üéØ Confusion Matrix: Understanding Errors\n",
    "\n",
    "A confusion matrix shows:\n",
    "- **True Positives (TP)**: Correctly identified spam\n",
    "- **True Negatives (TN)**: Correctly identified not spam\n",
    "- **False Positives (FP)**: Incorrectly flagged as spam (legitimate email marked spam!)\n",
    "- **False Negatives (FN)**: Missed spam (spam reached inbox!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', y_pred_log),\n",
    "    ('Decision Tree', y_pred_dt),\n",
    "    ('Random Forest', y_pred_rf)\n",
    "]\n",
    "\n",
    "for idx, (name, predictions) in enumerate(models):\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Not Spam', 'Spam'],\n",
    "                yticklabels=['Not Spam', 'Spam'])\n",
    "    axes[idx].set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=10)\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Reading the Confusion Matrix:\")\n",
    "print(\"  Top-left: Correctly identified NOT spam\")\n",
    "print(\"  Bottom-right: Correctly identified spam\")\n",
    "print(\"  Top-right: False alarm (legitimate email marked spam) ‚ö†Ô∏è\")\n",
    "print(\"  Bottom-left: Missed spam (spam reached inbox) ‚ö†Ô∏è\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-ai-example",
   "metadata": {},
   "source": [
    "## üåü Real AI Example: Sentiment Analysis Pipeline\n",
    "\n",
    "Let's build a **sentiment classifier** for product reviews - used in 2024-2025 AI systems!\n",
    "\n",
    "**Use Case:** E-commerce platforms analyze millions of reviews to:\n",
    "- Route negative reviews to customer service (Agentic AI)\n",
    "- Identify trending products from positive sentiment\n",
    "- Feed into RAG systems for customer support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentiment analysis dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate review features\n",
    "n_reviews = 800\n",
    "\n",
    "sentiment_data = {\n",
    "    'positive_words': np.concatenate([\n",
    "        np.random.poisson(8, 400),   # Positive reviews\n",
    "        np.random.poisson(2, 400)    # Negative reviews\n",
    "    ]),\n",
    "    'negative_words': np.concatenate([\n",
    "        np.random.poisson(1, 400),   # Positive reviews\n",
    "        np.random.poisson(6, 400)    # Negative reviews\n",
    "    ]),\n",
    "    'exclamation_marks': np.concatenate([\n",
    "        np.random.poisson(3, 400),\n",
    "        np.random.poisson(2, 400)\n",
    "    ]),\n",
    "    'review_length': np.concatenate([\n",
    "        np.random.normal(150, 30, 400),\n",
    "        np.random.normal(100, 25, 400)\n",
    "    ]),\n",
    "    'rating_stars': np.concatenate([\n",
    "        np.random.choice([4, 5], 400, p=[0.3, 0.7]),\n",
    "        np.random.choice([1, 2, 3], 400, p=[0.5, 0.3, 0.2])\n",
    "    ]),\n",
    "    'sentiment': ['Positive'] * 400 + ['Negative'] * 400\n",
    "}\n",
    "\n",
    "sentiment_df = pd.DataFrame(sentiment_data)\n",
    "sentiment_df = sentiment_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Convert sentiment to binary\n",
    "sentiment_df['sentiment_binary'] = (sentiment_df['sentiment'] == 'Positive').astype(int)\n",
    "\n",
    "print(\"üí¨ Sentiment Analysis Dataset Created!\")\n",
    "print(f\"Total reviews: {len(sentiment_df)}\")\n",
    "print(f\"Positive: {(sentiment_df['sentiment'] == 'Positive').sum()}\")\n",
    "print(f\"Negative: {(sentiment_df['sentiment'] == 'Negative').sum()}\")\n",
    "print(\"\\nSample reviews:\")\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sentiment classifier\n",
    "X_sent = sentiment_df.drop(['sentiment', 'sentiment_binary'], axis=1)\n",
    "y_sent = sentiment_df['sentiment_binary']\n",
    "\n",
    "X_train_sent, X_test_sent, y_train_sent, y_test_sent = train_test_split(\n",
    "    X_sent, y_sent, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Try all three algorithms\n",
    "print(\"üöÄ Training Sentiment Classifiers...\\n\")\n",
    "\n",
    "# Logistic Regression\n",
    "sent_log = LogisticRegression(random_state=42)\n",
    "sent_log.fit(X_train_sent, y_train_sent)\n",
    "sent_log_acc = accuracy_score(y_test_sent, sent_log.predict(X_test_sent))\n",
    "\n",
    "# Decision Tree\n",
    "sent_dt = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "sent_dt.fit(X_train_sent, y_train_sent)\n",
    "sent_dt_acc = accuracy_score(y_test_sent, sent_dt.predict(X_test_sent))\n",
    "\n",
    "# Random Forest\n",
    "sent_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "sent_rf.fit(X_train_sent, y_train_sent)\n",
    "sent_rf_acc = accuracy_score(y_test_sent, sent_rf.predict(X_test_sent))\n",
    "\n",
    "print(\"üéØ Sentiment Analysis Results:\")\n",
    "print(f\"  Logistic Regression: {sent_log_acc:.2%}\")\n",
    "print(f\"  Decision Tree: {sent_dt_acc:.2%}\")\n",
    "print(f\"  Random Forest: {sent_rf_acc:.2%}\")\n",
    "\n",
    "# Demo: Classify sample reviews\n",
    "print(\"\\nüí¨ Sample Predictions:\")\n",
    "sample_reviews = X_test_sent.head()\n",
    "predictions = sent_rf.predict(sample_reviews)\n",
    "\n",
    "for i, (idx, row) in enumerate(sample_reviews.iterrows()):\n",
    "    sentiment = \"üòä Positive\" if predictions[i] == 1 else \"üòû Negative\"\n",
    "    actual = \"üòä Positive\" if y_test_sent.iloc[i] == 1 else \"üòû Negative\"\n",
    "    print(f\"\\nReview {i+1}:\")\n",
    "    print(f\"  Positive words: {int(row['positive_words'])}, Negative words: {int(row['negative_words'])}\")\n",
    "    print(f\"  Stars: {int(row['rating_stars'])}‚≠ê\")\n",
    "    print(f\"  Predicted: {sentiment}  |  Actual: {actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Exercise 1\n",
    "\n",
    "**Challenge:** Build a customer churn predictor!\n",
    "\n",
    "**Scenario:** Predict if a customer will cancel their subscription\n",
    "\n",
    "**Your Task:**\n",
    "1. Use the dataset below\n",
    "2. Train all 3 classifiers (Logistic Regression, Decision Tree, Random Forest)\n",
    "3. Compare their accuracy\n",
    "4. Which model works best?\n",
    "\n",
    "Don't worry - experiment and learn! üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer churn dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "churn_data = {\n",
    "    'months_subscribed': np.concatenate([\n",
    "        np.random.randint(1, 6, 300),    # Will churn (short subscription)\n",
    "        np.random.randint(12, 60, 500)   # Won't churn (long subscription)\n",
    "    ]),\n",
    "    'monthly_usage_hours': np.concatenate([\n",
    "        np.random.randint(1, 10, 300),   # Will churn (low usage)\n",
    "        np.random.randint(20, 80, 500)   # Won't churn (high usage)\n",
    "    ]),\n",
    "    'support_tickets': np.concatenate([\n",
    "        np.random.poisson(5, 300),       # Will churn (many complaints)\n",
    "        np.random.poisson(1, 500)        # Won't churn (few complaints)\n",
    "    ]),\n",
    "    'payment_failures': np.concatenate([\n",
    "        np.random.poisson(2, 300),\n",
    "        np.random.poisson(0.2, 500)\n",
    "    ]),\n",
    "    'will_churn': [1] * 300 + [0] * 500  # 1 = Will cancel, 0 = Will stay\n",
    "}\n",
    "\n",
    "churn_df = pd.DataFrame(churn_data)\n",
    "churn_df = churn_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"üìä Customer Churn Dataset:\")\n",
    "print(churn_df.head())\n",
    "print(f\"\\nTotal customers: {len(churn_df)}\")\n",
    "print(f\"Will churn: {churn_df['will_churn'].sum()}\")\n",
    "print(f\"Will stay: {(churn_df['will_churn'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "# Hint: Follow the same steps as spam detection above\n",
    "\n",
    "# Step 1: Separate X and y\n",
    "X_churn = # YOUR CODE\n",
    "y_churn = # YOUR CODE\n",
    "\n",
    "# Step 2: Split data\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 3: Train models\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 4: Compare accuracy\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-toggle",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Separate X and y\n",
    "X_churn = churn_df.drop('will_churn', axis=1)\n",
    "y_churn = churn_df['will_churn']\n",
    "\n",
    "# Step 2: Split data\n",
    "X_train_ch, X_test_ch, y_train_ch, y_test_ch = train_test_split(\n",
    "    X_churn, y_churn, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Train models\n",
    "models_churn = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results_churn = {}\n",
    "for name, model in models_churn.items():\n",
    "    model.fit(X_train_ch, y_train_ch)\n",
    "    accuracy = accuracy_score(y_test_ch, model.predict(X_test_ch))\n",
    "    results_churn[name] = accuracy\n",
    "    print(f\"{name}: {accuracy:.2%}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**You just learned:**\n",
    "\n",
    "1. **Logistic Regression**\n",
    "   - ‚úÖ Fast and simple\n",
    "   - ‚úÖ Outputs probabilities\n",
    "   - ‚ùå Limited to linear patterns\n",
    "   - **Use for:** Quick baselines, probability scores\n",
    "\n",
    "2. **Decision Trees**\n",
    "   - ‚úÖ Easy to understand\n",
    "   - ‚úÖ Handles non-linear patterns\n",
    "   - ‚ùå Can overfit\n",
    "   - **Use for:** Interpretability, feature importance\n",
    "\n",
    "3. **Random Forests**\n",
    "   - ‚úÖ High accuracy\n",
    "   - ‚úÖ Robust and reliable\n",
    "   - ‚ùå Slower, less interpretable\n",
    "   - **Use for:** Production systems, complex patterns\n",
    "\n",
    "**üåü Real-World AI Applications (2024-2025):**\n",
    "- **RAG Systems:** Classify document relevance before retrieval\n",
    "- **Agentic AI:** Route user queries to the right action/agent\n",
    "- **Content Moderation:** Classify toxic content on social media\n",
    "- **Sentiment Analysis:** Analyze customer feedback at scale\n",
    "- **Multimodal AI:** Pre-filter data before expensive Transformer processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "**Practice Exercises:**\n",
    "1. Try adjusting `max_depth` in Decision Trees - what happens?\n",
    "2. Change `n_estimators` in Random Forest (50, 200, 500)\n",
    "3. Create your own dataset and train classifiers\n",
    "\n",
    "**Coming Next:**\n",
    "- **Day 2:** Advanced Classifiers (SVM, KNN, Naive Bayes)\n",
    "- **Day 3:** Regression Algorithms (predict continuous values!)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You can now build spam filters, sentiment analyzers, and churn predictors - all real AI applications!\n",
    "\n",
    "**üí¨ Questions?** Review the notebook, experiment with the code, and see what happens when you change parameters!\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: Every AI system (including ChatGPT, Claude, and modern RAG systems) uses these fundamental algorithms somewhere in their pipeline!* üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
