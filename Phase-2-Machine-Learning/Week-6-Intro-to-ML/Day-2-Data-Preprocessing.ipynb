{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Day 2: Data Preprocessing\n",
    "\n",
    "**üéØ Goal:** Learn to clean and prepare data for machine learning models\n",
    "\n",
    "**‚è±Ô∏è Time:** 45-60 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- \"Garbage in, garbage out\" - Bad data = Bad models\n",
    "- 80% of ML work is data preparation!\n",
    "- RAG systems need properly processed documents for accurate retrieval\n",
    "- Real-world data is ALWAYS messy - you must learn to clean it\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Why Data Preprocessing?\n",
    "\n",
    "Real-world data has problems:\n",
    "- **Missing values** (blank cells) ‚ùå\n",
    "- **Different scales** (age: 25, salary: 50000) üìè\n",
    "- **Text categories** (\"Red\", \"Blue\") that models can't read üé®\n",
    "- **Outliers** (extreme values) üìä\n",
    "\n",
    "**Machine Learning models need:**\n",
    "- ‚úÖ No missing values\n",
    "- ‚úÖ Numbers on similar scales\n",
    "- ‚úÖ Categories converted to numbers\n",
    "- ‚úÖ Clean, consistent data\n",
    "\n",
    "**Today, we'll fix all these issues!** üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn pandas numpy matplotlib --quiet\n",
    "\n",
    "print(\"‚úÖ Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"üìö All libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Let's Create a Messy Dataset\n",
    "\n",
    "This dataset represents customer data with **real-world problems**:\n",
    "- Missing values (some data is blank)\n",
    "- Different scales (age vs income)\n",
    "- Text categories (country names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messy dataset\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],\n",
    "    'Age': [25, 30, np.nan, 28, 35, 29],  # Missing value!\n",
    "    'Country': ['USA', 'UK', 'USA', 'Canada', 'UK', np.nan],  # Missing value!\n",
    "    'Salary': [50000, 60000, 55000, np.nan, 70000, 58000],  # Missing value!\n",
    "    'Purchased': [0, 1, 0, 1, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"üìä Our Messy Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n‚ö†Ô∏è Notice the 'NaN' values? That's missing data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 1: Explore the Data\n",
    "\n",
    "Always understand your data first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the dataset\n",
    "print(\"üìã Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n‚ùì Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nüìä Statistical Summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 2: Handle Missing Values\n",
    "\n",
    "**Three strategies:**\n",
    "1. **Delete rows** with missing values (lose data ‚ùå)\n",
    "2. **Fill with mean/median** for numbers ‚úÖ\n",
    "3. **Fill with mode** (most common value) for categories ‚úÖ\n",
    "\n",
    "We'll use strategy 2 and 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to work with\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Handle missing numerical values (Age, Salary)\n",
    "# Strategy: Fill with MEAN (average)\n",
    "\n",
    "# For Age\n",
    "imputer_age = SimpleImputer(strategy='mean')\n",
    "df_clean['Age'] = imputer_age.fit_transform(df_clean[['Age']])\n",
    "\n",
    "# For Salary\n",
    "imputer_salary = SimpleImputer(strategy='mean')\n",
    "df_clean['Salary'] = imputer_salary.fit_transform(df_clean[['Salary']])\n",
    "\n",
    "print(\"‚úÖ Filled missing Age with mean age\")\n",
    "print(\"‚úÖ Filled missing Salary with mean salary\")\n",
    "print(\"\\nüìä After filling numerical values:\")\n",
    "print(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing categorical values (Country)\n",
    "# Strategy: Fill with MOST FREQUENT value\n",
    "\n",
    "imputer_country = SimpleImputer(strategy='most_frequent')\n",
    "df_clean['Country'] = imputer_country.fit_transform(df_clean[['Country']]).ravel()\n",
    "\n",
    "print(\"‚úÖ Filled missing Country with most frequent country\")\n",
    "print(\"\\nüìä After filling ALL missing values:\")\n",
    "print(df_clean)\n",
    "print(\"\\n‚ú® No more NaN values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Step 3: Encode Categorical Variables\n",
    "\n",
    "**Problem:** ML models only understand numbers, not text!\n",
    "\n",
    "**Solution:** Convert categories to numbers\n",
    "\n",
    "**Label Encoding:** USA=0, UK=1, Canada=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'Country' column\n",
    "label_encoder = LabelEncoder()\n",
    "df_clean['Country_Encoded'] = label_encoder.fit_transform(df_clean['Country'])\n",
    "\n",
    "print(\"üé® Country Encoding:\")\n",
    "print(df_clean[['Country', 'Country_Encoded']])\n",
    "print(\"\\nüìù Encoding mapping:\")\n",
    "for i, country in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {country} ‚Üí {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìè Step 4: Feature Scaling\n",
    "\n",
    "**Problem:** Features have different scales!\n",
    "- Age: 25-35\n",
    "- Salary: 50,000-70,000\n",
    "\n",
    "**Why this matters:** \n",
    "- Models think Salary is MORE important (bigger numbers!)\n",
    "- We need to put everything on the same scale\n",
    "\n",
    "**Two methods:**\n",
    "1. **Standardization** (mean=0, std=1) ‚Üê Most common\n",
    "2. **Normalization** (scale to 0-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Standardization (StandardScaler)\n",
    "\n",
    "**Formula:** `(value - mean) / standard_deviation`\n",
    "\n",
    "**Result:** Mean = 0, Standard Deviation = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features to scale\n",
    "features_to_scale = ['Age', 'Salary']\n",
    "\n",
    "# Create scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform\n",
    "df_clean[['Age_Scaled', 'Salary_Scaled']] = scaler.fit_transform(\n",
    "    df_clean[features_to_scale]\n",
    ")\n",
    "\n",
    "print(\"üìè Before and After Standardization:\")\n",
    "print(df_clean[['Age', 'Age_Scaled', 'Salary', 'Salary_Scaled']])\n",
    "print(\"\\n‚úÖ Now Age and Salary are on the same scale!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Normalization (MinMaxScaler)\n",
    "\n",
    "**Formula:** `(value - min) / (max - min)`\n",
    "\n",
    "**Result:** All values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MinMax scaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize Age and Salary\n",
    "df_clean[['Age_Normalized', 'Salary_Normalized']] = minmax_scaler.fit_transform(\n",
    "    df_clean[['Age', 'Salary']]\n",
    ")\n",
    "\n",
    "print(\"üìè Normalized Values (0 to 1):\")\n",
    "print(df_clean[['Age', 'Age_Normalized', 'Salary', 'Salary_Normalized']])\n",
    "print(\"\\n‚úÖ All values now between 0 and 1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ When to Use Each Scaling Method?\n",
    "\n",
    "**StandardScaler (Standardization):**\n",
    "- ‚úÖ Most algorithms (Linear Regression, SVM, Neural Networks)\n",
    "- ‚úÖ When features follow normal distribution\n",
    "- ‚úÖ Default choice!\n",
    "\n",
    "**MinMaxScaler (Normalization):**\n",
    "- ‚úÖ Neural networks with bounded activation functions\n",
    "- ‚úÖ Image processing (pixels already 0-255)\n",
    "- ‚úÖ When you need specific range (0-1)\n",
    "\n",
    "**No Scaling Needed:**\n",
    "- ‚ùå Tree-based models (Decision Trees, Random Forest)\n",
    "- ‚ùå Already same scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ Step 5: Train/Test Split\n",
    "\n",
    "**Golden Rule:** NEVER test on training data!\n",
    "\n",
    "**Process:**\n",
    "1. Split data FIRST (before scaling!)\n",
    "2. Fit scaler on training data\n",
    "3. Transform BOTH training and test data\n",
    "\n",
    "**Why?** To prevent data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "X = df_clean[['Age', 'Salary', 'Country_Encoded']].values\n",
    "y = df_clean['Purchased'].values\n",
    "\n",
    "print(\"üìä Features (X):\")\n",
    "print(X)\n",
    "print(\"\\nüéØ Target (y):\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"üîÄ Data Split Complete!\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "\n",
    "print(\"\\nüìä Training data:\")\n",
    "print(X_train)\n",
    "print(\"\\nüìä Testing data:\")\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è IMPORTANT: The Right Way to Scale\n",
    "\n",
    "**WRONG ‚ùå:**\n",
    "```python\n",
    "# Scale all data, then split\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test = split(X_scaled)  # DATA LEAKAGE!\n",
    "```\n",
    "\n",
    "**RIGHT ‚úÖ:**\n",
    "```python\n",
    "# Split first, then scale\n",
    "X_train, X_test = split(X)\n",
    "scaler.fit(X_train)  # Learn from training only\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RIGHT way to scale\n",
    "scaler_final = StandardScaler()\n",
    "\n",
    "# Fit on training data ONLY\n",
    "scaler_final.fit(X_train)\n",
    "\n",
    "# Transform both sets\n",
    "X_train_scaled = scaler_final.transform(X_train)\n",
    "X_test_scaled = scaler_final.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Scaled correctly!\")\n",
    "print(\"\\nüìä Scaled Training Data:\")\n",
    "print(X_train_scaled)\n",
    "print(\"\\nüìä Scaled Testing Data:\")\n",
    "print(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Real AI Example: Preparing Data for RAG Systems\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** systems like ChatGPT with documents need clean data!\n",
    "\n",
    "**Scenario:** You're building a RAG system to answer questions about products.\n",
    "\n",
    "**Your data has:**\n",
    "- Missing descriptions\n",
    "- Different price scales\n",
    "- Category names (Electronics, Books, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product data for RAG system\n",
    "products_data = {\n",
    "    'Product': ['Laptop', 'Book', 'Phone', 'Tablet', 'Headphones'],\n",
    "    'Category': ['Electronics', 'Books', 'Electronics', 'Electronics', np.nan],\n",
    "    'Price': [1200, 25, np.nan, 800, 150],\n",
    "    'Rating': [4.5, 4.8, 4.2, np.nan, 4.0],\n",
    "    'Stock': [50, 200, 100, 75, 150]\n",
    "}\n",
    "\n",
    "products_df = pd.DataFrame(products_data)\n",
    "\n",
    "print(\"üõçÔ∏è RAG System - Product Data (BEFORE cleaning):\")\n",
    "print(products_df)\n",
    "print(\"\\n‚ö†Ô∏è Issues: Missing Category, Price, and Rating!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data for RAG system\n",
    "products_clean = products_df.copy()\n",
    "\n",
    "# 1. Fill missing Category with most frequent\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "products_clean['Category'] = cat_imputer.fit_transform(\n",
    "    products_clean[['Category']]\n",
    ").ravel()\n",
    "\n",
    "# 2. Fill missing Price with median (better for prices with outliers)\n",
    "price_imputer = SimpleImputer(strategy='median')\n",
    "products_clean['Price'] = price_imputer.fit_transform(\n",
    "    products_clean[['Price']]\n",
    ")\n",
    "\n",
    "# 3. Fill missing Rating with mean\n",
    "rating_imputer = SimpleImputer(strategy='mean')\n",
    "products_clean['Rating'] = rating_imputer.fit_transform(\n",
    "    products_clean[['Rating']]\n",
    ")\n",
    "\n",
    "# 4. Encode Category\n",
    "cat_encoder = LabelEncoder()\n",
    "products_clean['Category_Encoded'] = cat_encoder.fit_transform(\n",
    "    products_clean['Category']\n",
    ")\n",
    "\n",
    "# 5. Scale numerical features\n",
    "scaler_products = StandardScaler()\n",
    "products_clean[['Price_Scaled', 'Rating_Scaled', 'Stock_Scaled']] = scaler_products.fit_transform(\n",
    "    products_clean[['Price', 'Rating', 'Stock']]\n",
    ")\n",
    "\n",
    "print(\"‚ú® RAG System - Product Data (AFTER cleaning):\")\n",
    "print(products_clean)\n",
    "print(\"\\n‚úÖ Ready for RAG system!\")\n",
    "print(\"\\nüìä This clean data can now be:\")\n",
    "print(\"  1. Converted to embeddings (vector representations)\")\n",
    "print(\"  2. Stored in a vector database\")\n",
    "print(\"  3. Retrieved when user asks questions\")\n",
    "print(\"  4. Used by LLM to generate accurate answers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Interactive Exercise\n",
    "\n",
    "**Challenge:** Prepare this messy employee dataset for ML!\n",
    "\n",
    "**Tasks:**\n",
    "1. Handle missing values\n",
    "2. Encode the 'Department' column\n",
    "3. Scale 'Age' and 'Salary'\n",
    "4. Split into train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employee dataset\n",
    "employee_data = {\n",
    "    'Name': ['John', 'Sarah', 'Mike', 'Emily', 'David'],\n",
    "    'Age': [28, np.nan, 35, 42, 31],\n",
    "    'Department': ['Sales', 'IT', np.nan, 'Sales', 'IT'],\n",
    "    'Salary': [50000, 70000, 60000, np.nan, 65000],\n",
    "    'Promoted': [0, 1, 0, 1, 0]  # Target variable\n",
    "}\n",
    "\n",
    "employee_df = pd.DataFrame(employee_data)\n",
    "\n",
    "print(\"üìä Employee Dataset (MESSY):\")\n",
    "print(employee_df)\n",
    "print(\"\\nüéØ YOUR TASK: Clean this data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "\n",
    "# Step 1: Handle missing Age (use mean)\n",
    "# TODO: Create imputer and fill missing Age\n",
    "\n",
    "# Step 2: Handle missing Department (use most_frequent)\n",
    "# TODO: Create imputer and fill missing Department\n",
    "\n",
    "# Step 3: Handle missing Salary (use median)\n",
    "# TODO: Create imputer and fill missing Salary\n",
    "\n",
    "# Step 4: Encode Department\n",
    "# TODO: Use LabelEncoder\n",
    "\n",
    "# Step 5: Scale Age and Salary\n",
    "# TODO: Use StandardScaler\n",
    "\n",
    "# Step 6: Create X and y, then split\n",
    "# TODO: train_test_split\n",
    "\n",
    "print(\"Complete the TODOs above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution (Try on your own first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create copy\n",
    "employee_clean = employee_df.copy()\n",
    "\n",
    "# Step 1: Fill missing Age\n",
    "age_imputer = SimpleImputer(strategy='mean')\n",
    "employee_clean['Age'] = age_imputer.fit_transform(employee_clean[['Age']])\n",
    "\n",
    "# Step 2: Fill missing Department\n",
    "dept_imputer = SimpleImputer(strategy='most_frequent')\n",
    "employee_clean['Department'] = dept_imputer.fit_transform(\n",
    "    employee_clean[['Department']]\n",
    ").ravel()\n",
    "\n",
    "# Step 3: Fill missing Salary\n",
    "salary_imputer = SimpleImputer(strategy='median')\n",
    "employee_clean['Salary'] = salary_imputer.fit_transform(employee_clean[['Salary']])\n",
    "\n",
    "# Step 4: Encode Department\n",
    "dept_encoder = LabelEncoder()\n",
    "employee_clean['Department_Encoded'] = dept_encoder.fit_transform(\n",
    "    employee_clean['Department']\n",
    ")\n",
    "\n",
    "# Step 5: Scale Age and Salary\n",
    "scaler_emp = StandardScaler()\n",
    "employee_clean[['Age_Scaled', 'Salary_Scaled']] = scaler_emp.fit_transform(\n",
    "    employee_clean[['Age', 'Salary']]\n",
    ")\n",
    "\n",
    "print(\"‚ú® Cleaned Employee Data:\")\n",
    "print(employee_clean)\n",
    "\n",
    "# Step 6: Prepare for ML\n",
    "X = employee_clean[['Age_Scaled', 'Salary_Scaled', 'Department_Encoded']].values\n",
    "y = employee_clean['Promoted'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data ready for ML!\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Data Preprocessing Checklist\n",
    "\n",
    "**Before training ANY ML model:**\n",
    "\n",
    "**1. Explore Data** üîç\n",
    "- [ ] Check data types (`df.info()`)\n",
    "- [ ] Look for missing values (`df.isnull().sum()`)\n",
    "- [ ] Check statistics (`df.describe()`)\n",
    "\n",
    "**2. Handle Missing Values** üõ†Ô∏è\n",
    "- [ ] Numerical: Use mean/median\n",
    "- [ ] Categorical: Use most_frequent\n",
    "- [ ] Or drop rows (if very few missing)\n",
    "\n",
    "**3. Encode Categories** üé®\n",
    "- [ ] Use LabelEncoder for ordinal data\n",
    "- [ ] Use OneHotEncoder for nominal data\n",
    "\n",
    "**4. Scale Features** üìè\n",
    "- [ ] StandardScaler for most algorithms\n",
    "- [ ] MinMaxScaler for neural networks\n",
    "- [ ] Skip for tree-based models\n",
    "\n",
    "**5. Split Data** üîÄ\n",
    "- [ ] Split BEFORE scaling\n",
    "- [ ] Fit scaler on training only\n",
    "- [ ] Transform both sets\n",
    "\n",
    "**6. Verify** ‚úÖ\n",
    "- [ ] No missing values\n",
    "- [ ] All numerical features\n",
    "- [ ] Similar scales\n",
    "- [ ] Ready for ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- ‚úÖ Why data preprocessing is crucial (80% of ML work!)\n",
    "- ‚úÖ How to handle missing values (imputation)\n",
    "- ‚úÖ How to encode categorical variables\n",
    "- ‚úÖ Feature scaling (Standardization vs Normalization)\n",
    "- ‚úÖ Proper train/test split workflow\n",
    "- ‚úÖ How to prepare data for RAG systems\n",
    "\n",
    "**üéØ Practice Exercise (Do this before Day 3!):**\n",
    "\n",
    "Download a real dataset from Kaggle and clean it:\n",
    "1. Titanic dataset (classification)\n",
    "2. House prices (regression)\n",
    "\n",
    "Practice the entire preprocessing pipeline!\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Next Lesson:** Day 3 - Building Your First ML Model (Linear Regression)\n",
    "\n",
    "**üí¨ Key Takeaway:**\n",
    "\n",
    "*\"Garbage in, garbage out\" - Even the best ML algorithm can't fix bad data. Spend time on preprocessing, and your models will thank you!* üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**üîó Connections to Modern AI:**\n",
    "- **RAG Systems**: Clean document data ‚Üí Better retrieval ‚Üí Accurate answers\n",
    "- **LLMs**: Massive text preprocessing before training\n",
    "- **Multimodal AI**: Normalize images, scale audio, encode text\n",
    "- **Agentic AI**: Clean sensor data for decision making"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
