{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 2: Gradient Boosting (XGBoost, LightGBM, CatBoost)\n",
    "\n",
    "**üéØ Goal:** Master the gradient boosting algorithms that dominate Kaggle and production ML\n",
    "\n",
    "**‚è±Ô∏è Time:** 90-120 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- XGBoost/LightGBM power most Kaggle winning solutions (2020-2024)\n",
    "- Used in production at Google, Microsoft, Amazon for ranking systems\n",
    "- Critical for recommendation engines, search ranking, and ad serving\n",
    "- Powers real-time ML inference in Agentic AI systems\n",
    "- Combines with neural networks in modern hybrid AI architectures\n",
    "- Essential skill for ML engineers in 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradient-boosting-intro",
   "metadata": {},
   "source": [
    "## ü§î What is Gradient Boosting?\n",
    "\n",
    "**Evolution of Boosting:**\n",
    "- **Day 1:** AdaBoost ‚Üí Adjusts weights on training examples\n",
    "- **Today:** Gradient Boosting ‚Üí Directly optimizes loss function\n",
    "\n",
    "**How Gradient Boosting Works:**\n",
    "1. Start with a weak model (e.g., predicts average)\n",
    "2. Calculate errors (residuals)\n",
    "3. Train new model to predict these errors\n",
    "4. Add new model to ensemble\n",
    "5. Repeat! Each model corrects previous mistakes\n",
    "\n",
    "**Analogy:**\n",
    "- Imagine you're trying to hit a target\n",
    "- First throw: Miss by 10 feet left\n",
    "- Second throw: Aim 10 feet right to correct\n",
    "- Third throw: Correct remaining error\n",
    "- Eventually: Hit bullseye! üéØ\n",
    "\n",
    "**Why \"Gradient\"?**\n",
    "- Uses gradient descent to minimize loss\n",
    "- Same math as training neural networks!\n",
    "\n",
    "**Modern Implementations:**\n",
    "1. **XGBoost** (eXtreme Gradient Boosting) - Most popular\n",
    "2. **LightGBM** (Light Gradient Boosting Machine) - Fastest\n",
    "3. **CatBoost** (Categorical Boosting) - Best for categorical features\n",
    "\n",
    "Let's master them all! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Gradient Boosting implementations\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# For timing\n",
    "import time\n",
    "\n",
    "# Make plots beautiful\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"Let's master gradient boosting! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-intro",
   "metadata": {},
   "source": [
    "## üìä Our Dataset: Click-Through Rate (CTR) Prediction\n",
    "\n",
    "**Real AI Application:** Predicting if a user will click on an ad or recommendation\n",
    "\n",
    "**Scenario:** You're building an AI system for ad targeting (like Google Ads, Facebook Ads)\n",
    "\n",
    "**Business Impact:**\n",
    "- 1% improvement = Millions in revenue\n",
    "- Better CTR prediction = Better ad placement = Happier users + advertisers\n",
    "\n",
    "**Features:**\n",
    "- `user_age`: User's age\n",
    "- `user_gender`: Gender (0 = F, 1 = M)\n",
    "- `device_type`: Mobile=0, Desktop=1, Tablet=2\n",
    "- `hour_of_day`: Hour when ad shown (0-23)\n",
    "- `ad_position`: Position on page (1-10)\n",
    "- `user_past_clicks`: Historical CTR for this user\n",
    "- `ad_category_match`: User interests match ad category (0-1)\n",
    "- `page_load_time`: Page load time in seconds\n",
    "\n",
    "**Target:**\n",
    "- `clicked`: 1 = User clicked, 0 = Didn't click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic CTR prediction dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 10000\n",
    "\n",
    "# Generate features\n",
    "# Clicked ads have different patterns\n",
    "data = {\n",
    "    'user_age': np.concatenate([\n",
    "        np.random.normal(30, 8, 2000),    # Clickers: younger\n",
    "        np.random.normal(45, 12, 8000)    # Non-clickers: older\n",
    "    ]),\n",
    "    'user_gender': np.random.binomial(1, 0.5, n_samples),\n",
    "    'device_type': np.concatenate([\n",
    "        np.random.choice([0, 1, 2], 2000, p=[0.6, 0.3, 0.1]),  # Clickers: more mobile\n",
    "        np.random.choice([0, 1, 2], 8000, p=[0.4, 0.5, 0.1])\n",
    "    ]),\n",
    "    'hour_of_day': np.concatenate([\n",
    "        np.random.choice(range(24), 2000, p=[0.01]*6 + [0.08]*12 + [0.04]*6),  # Peak hours\n",
    "        np.random.choice(range(24), 8000, p=[0.04]*24)  # Random\n",
    "    ]),\n",
    "    'ad_position': np.concatenate([\n",
    "        np.random.choice(range(1, 11), 2000, p=[0.3, 0.25, 0.2, 0.1, 0.05, 0.04, 0.03, 0.02, 0.01, 0.0]),\n",
    "        np.random.choice(range(1, 11), 8000, p=[0.1]*10)\n",
    "    ]),\n",
    "    'user_past_clicks': np.concatenate([\n",
    "        np.random.beta(5, 2, 2000),       # Clickers: high historical CTR\n",
    "        np.random.beta(2, 8, 8000)        # Non-clickers: low historical CTR\n",
    "    ]),\n",
    "    'ad_category_match': np.concatenate([\n",
    "        np.random.beta(8, 2, 2000),       # Clickers: good match\n",
    "        np.random.beta(3, 5, 8000)        # Non-clickers: poor match\n",
    "    ]),\n",
    "    'page_load_time': np.concatenate([\n",
    "        np.random.exponential(0.5, 2000),  # Clickers: fast load\n",
    "        np.random.exponential(1.5, 8000)   # Non-clickers: slow load\n",
    "    ]),\n",
    "    'clicked': [1] * 2000 + [0] * 8000  # 20% CTR (realistic for ads)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Shuffle\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"üéØ Click-Through Rate (CTR) Dataset Created!\")\n",
    "print(f\"Total impressions: {len(df):,}\")\n",
    "print(f\"Clicks: {df['clicked'].sum():,}\")\n",
    "print(f\"CTR: {df['clicked'].mean():.1%}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df.drop('clicked', axis=1)\n",
    "y = df['clicked']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data prepared!\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Features: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline",
   "metadata": {},
   "source": [
    "## üìä Baseline: Sklearn Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sklearn-gb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn's Gradient Boosting (baseline)\n",
    "print(\"üå≥ Training Sklearn Gradient Boosting...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "gb_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "y_pred_proba_gb = gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "gb_acc = accuracy_score(y_test, y_pred_gb)\n",
    "gb_auc = roc_auc_score(y_test, y_pred_proba_gb)\n",
    "\n",
    "print(f\"‚úÖ Training completed in {gb_time:.2f} seconds\")\n",
    "print(f\"\\nAccuracy: {gb_acc:.2%}\")\n",
    "print(f\"ROC AUC: {gb_auc:.4f}\")\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb, target_names=['No Click', 'Click']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgboost-intro",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "**What is XGBoost?**\n",
    "- Most popular gradient boosting library\n",
    "- Dominated Kaggle competitions (2015-2020)\n",
    "- Used in production at major tech companies\n",
    "\n",
    "**Key Advantages:**\n",
    "- ‚úÖ **Regularization**: L1/L2 regularization prevents overfitting\n",
    "- ‚úÖ **Parallel processing**: Much faster than sklearn\n",
    "- ‚úÖ **Handles missing values**: Built-in missing value handling\n",
    "- ‚úÖ **Tree pruning**: Smarter tree building\n",
    "- ‚úÖ **Cross-validation**: Built-in CV support\n",
    "\n",
    "**üéØ Real AI Use Cases (2024-2025):**\n",
    "- **Recommendation systems**: Netflix, Spotify, YouTube ranking\n",
    "- **Search ranking**: Google search result ordering\n",
    "- **Fraud detection**: Real-time transaction scoring\n",
    "- **Ad targeting**: Click prediction (what we're doing!)\n",
    "- **Risk assessment**: Credit scoring, insurance pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgboost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Classifier\n",
    "print(\"üöÄ Training XGBoost...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "xgb_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "xgb_auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "\n",
    "print(f\"‚úÖ Training completed in {xgb_time:.2f} seconds\")\n",
    "print(f\"‚ö° Speedup vs Sklearn: {gb_time/xgb_time:.1f}x faster\")\n",
    "print(f\"\\nAccuracy: {xgb_acc:.2%}\")\n",
    "print(f\"ROC AUC: {xgb_auc:.4f}\")\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['No Click', 'Click']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': xgb_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"üî• XGBoost Feature Importance:\")\n",
    "print(feature_importance_xgb)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_xgb['Feature'], feature_importance_xgb['Importance'], color='#e74c3c')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('üéØ XGBoost: Most Important Features for CTR Prediction', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Top predictors: user_past_clicks and ad_category_match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightgbm-intro",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "**What is LightGBM?**\n",
    "- Developed by Microsoft\n",
    "- **Fastest** gradient boosting implementation\n",
    "- Winner of many recent Kaggle competitions (2020-2024)\n",
    "\n",
    "**Key Innovations:**\n",
    "- ‚úÖ **Leaf-wise tree growth**: Grows trees differently (faster, more accurate)\n",
    "- ‚úÖ **Histogram-based algorithm**: Bins continuous features (massive speedup)\n",
    "- ‚úÖ **GPU support**: Can train on GPU for even more speed\n",
    "- ‚úÖ **Handles large datasets**: Billions of rows, millions of features\n",
    "- ‚úÖ **Lower memory usage**: More efficient than XGBoost\n",
    "\n",
    "**When to use LightGBM:**\n",
    "- Large datasets (> 10,000 rows)\n",
    "- Need fast training\n",
    "- Limited memory\n",
    "\n",
    "**üéØ Real AI Use Cases:**\n",
    "- **Real-time ranking**: Millisecond-latency predictions\n",
    "- **Large-scale recommendations**: Train on billions of interactions\n",
    "- **Time series forecasting**: Fast retraining for Agentic AI\n",
    "- **Online learning**: Update models frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightgbm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Classifier\n",
    "print(\"‚ö° Training LightGBM...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    verbose=-1  # Suppress warnings\n",
    ")\n",
    "\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "lgb_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred_lgb = lgb_clf.predict(X_test)\n",
    "y_pred_proba_lgb = lgb_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "lgb_acc = accuracy_score(y_test, y_pred_lgb)\n",
    "lgb_auc = roc_auc_score(y_test, y_pred_proba_lgb)\n",
    "\n",
    "print(f\"‚úÖ Training completed in {lgb_time:.2f} seconds\")\n",
    "print(f\"‚ö° Speedup vs Sklearn: {gb_time/lgb_time:.1f}x faster\")\n",
    "print(f\"‚ö° Speedup vs XGBoost: {xgb_time/lgb_time:.1f}x faster\")\n",
    "print(f\"\\nAccuracy: {lgb_acc:.2%}\")\n",
    "print(f\"ROC AUC: {lgb_auc:.4f}\")\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lgb, target_names=['No Click', 'Click']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catboost-intro",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ CatBoost (Categorical Boosting)\n",
    "\n",
    "**What is CatBoost?**\n",
    "- Developed by Yandex (Russian Google)\n",
    "- **Best** at handling categorical features\n",
    "- State-of-the-art on many benchmarks\n",
    "\n",
    "**Key Advantages:**\n",
    "- ‚úÖ **Automatic categorical encoding**: No need for one-hot encoding\n",
    "- ‚úÖ **Robust to overfitting**: Ordered boosting prevents target leakage\n",
    "- ‚úÖ **Great default parameters**: Works well out-of-the-box\n",
    "- ‚úÖ **GPU support**: Fast GPU training\n",
    "- ‚úÖ **Symmetric trees**: More robust, less prone to overfitting\n",
    "\n",
    "**When to use CatBoost:**\n",
    "- Many categorical features\n",
    "- Want good results without tuning\n",
    "- Need robust model (production)\n",
    "\n",
    "**üéØ Real AI Use Cases:**\n",
    "- **User segmentation**: Many categorical user attributes\n",
    "- **Product recommendations**: Categories, brands, tags\n",
    "- **Text classification**: Combined with TF-IDF or embeddings\n",
    "- **Multimodal AI**: Combining structured + unstructured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catboost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Classifier\n",
    "print(\"üê± Training CatBoost...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cat_clf = CatBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=3,\n",
    "    random_state=42,\n",
    "    verbose=0  # Suppress output\n",
    ")\n",
    "\n",
    "cat_clf.fit(X_train, y_train)\n",
    "cat_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred_cat = cat_clf.predict(X_test)\n",
    "y_pred_proba_cat = cat_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "cat_acc = accuracy_score(y_test, y_pred_cat)\n",
    "cat_auc = roc_auc_score(y_test, y_pred_proba_cat)\n",
    "\n",
    "print(f\"‚úÖ Training completed in {cat_time:.2f} seconds\")\n",
    "print(f\"\\nAccuracy: {cat_acc:.2%}\")\n",
    "print(f\"ROC AUC: {cat_auc:.4f}\")\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_cat, target_names=['No Click', 'Click']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-all",
   "metadata": {},
   "source": [
    "## üìä Compare All Gradient Boosting Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Sklearn GB', 'XGBoost', 'LightGBM', 'CatBoost'],\n",
    "    'Accuracy': [gb_acc, xgb_acc, lgb_acc, cat_acc],\n",
    "    'ROC AUC': [gb_auc, xgb_auc, lgb_auc, cat_auc],\n",
    "    'Training Time (s)': [gb_time, xgb_time, lgb_time, cat_time]\n",
    "}).sort_values('ROC AUC', ascending=False)\n",
    "\n",
    "print(\"üèÜ Gradient Boosting Comparison:\\n\")\n",
    "print(results.to_string(index=False))\n",
    "print(f\"\\nü•á Best Accuracy: {results.iloc[0]['Model']}\")\n",
    "print(f\"‚ö° Fastest: {results.nsmallest(1, 'Training Time (s)').iloc[0]['Model']}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(results['Model'], results['ROC AUC'], color=['#95a5a6', '#e74c3c', '#2ecc71', '#3498db'])\n",
    "axes[0].set_ylabel('ROC AUC Score', fontsize=12)\n",
    "axes[0].set_title('üéØ Model Performance (ROC AUC)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(0.8, 1.0)\n",
    "for i, v in enumerate(results['ROC AUC']):\n",
    "    axes[0].text(i, v, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Speed comparison\n",
    "axes[1].bar(results['Model'], results['Training Time (s)'], color=['#95a5a6', '#e74c3c', '#2ecc71', '#3498db'])\n",
    "axes[1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "axes[1].set_title('‚ö° Training Speed', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(results['Training Time (s)']):\n",
    "    axes[1].text(i, v, f'{v:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roc-curve",
   "metadata": {},
   "source": [
    "## üìà ROC Curves: Visual Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-roc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "models = [\n",
    "    ('Sklearn GB', y_pred_proba_gb, gb_auc),\n",
    "    ('XGBoost', y_pred_proba_xgb, xgb_auc),\n",
    "    ('LightGBM', y_pred_proba_lgb, lgb_auc),\n",
    "    ('CatBoost', y_pred_proba_cat, cat_auc)\n",
    "]\n",
    "\n",
    "for name, y_pred_proba, auc in models:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.4f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('üìä ROC Curves: Gradient Boosting Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Higher AUC = Better classifier!\")\n",
    "print(\"All gradient boosting methods significantly outperform random guessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyperparameter-tuning",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Hyperparameter Tuning: Getting the Best Results\n",
    "\n",
    "**Key Hyperparameters for Gradient Boosting:**\n",
    "\n",
    "1. **n_estimators**: Number of trees\n",
    "   - More trees = Better fit, but slower and risk overfitting\n",
    "   - Typical: 100-1000\n",
    "\n",
    "2. **learning_rate**: How much each tree contributes\n",
    "   - Lower = More trees needed, but better generalization\n",
    "   - Typical: 0.01-0.3\n",
    "\n",
    "3. **max_depth**: Maximum tree depth\n",
    "   - Deeper = More complex patterns, but overfitting risk\n",
    "   - Typical: 3-10\n",
    "\n",
    "4. **subsample**: Fraction of samples for each tree\n",
    "   - < 1.0 = Stochastic boosting (prevents overfitting)\n",
    "   - Typical: 0.8-1.0\n",
    "\n",
    "Let's tune XGBoost for maximum performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tune-xgboost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "print(\"üîß Tuning XGBoost hyperparameters...\\n\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_tuned = xgb.XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_tuned,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nüèÜ Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"üìä Best CV ROC AUC: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Test set performance\n",
    "y_pred_tuned = grid_search.best_estimator_.predict(X_test)\n",
    "y_pred_proba_tuned = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "tuned_auc = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "\n",
    "print(f\"\\n‚úÖ Test Set ROC AUC (tuned): {tuned_auc:.4f}\")\n",
    "print(f\"üìà Improvement: {(tuned_auc - xgb_auc):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-world-example",
   "metadata": {},
   "source": [
    "## üåü Real AI Example: Kaggle Competition Technique\n",
    "\n",
    "**Scenario:** You're competing in a Kaggle competition (or building production ML)\n",
    "\n",
    "**Pro Technique: Ensemble of Gradient Boosting Models**\n",
    "- Train XGBoost, LightGBM, and CatBoost\n",
    "- Average their predictions\n",
    "- Often beats any single model!\n",
    "\n",
    "**Why does this work?**\n",
    "- Each implementation has different strengths\n",
    "- Averaging reduces variance\n",
    "- Captures different patterns\n",
    "\n",
    "This technique wins Kaggle competitions! üèÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble-boosting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble of gradient boosting models (Kaggle technique)\n",
    "print(\"üèÜ Creating Gradient Boosting Ensemble (Kaggle Pro Technique)\\n\")\n",
    "\n",
    "# Get probabilities from all models\n",
    "proba_xgb = xgb_clf.predict_proba(X_test)[:, 1]\n",
    "proba_lgb = lgb_clf.predict_proba(X_test)[:, 1]\n",
    "proba_cat = cat_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Average predictions (equal weighting)\n",
    "proba_ensemble = (proba_xgb + proba_lgb + proba_cat) / 3\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_pred_ensemble = (proba_ensemble > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "ensemble_acc = accuracy_score(y_test, y_pred_ensemble)\n",
    "ensemble_auc = roc_auc_score(y_test, proba_ensemble)\n",
    "\n",
    "print(\"üìä Ensemble Results:\")\n",
    "print(f\"Accuracy: {ensemble_acc:.2%}\")\n",
    "print(f\"ROC AUC: {ensemble_auc:.4f}\")\n",
    "print(f\"\\nüìà Comparison to individual models:\")\n",
    "print(f\"  XGBoost: {xgb_auc:.4f}\")\n",
    "print(f\"  LightGBM: {lgb_auc:.4f}\")\n",
    "print(f\"  CatBoost: {cat_auc:.4f}\")\n",
    "print(f\"  Ensemble: {ensemble_auc:.4f} ‚≠ê\")\n",
    "\n",
    "if ensemble_auc > max(xgb_auc, lgb_auc, cat_auc):\n",
    "    print(\"\\nüéâ Ensemble beats all individual models!\")\n",
    "else:\n",
    "    print(\"\\nüí° Ensemble performs competitively with best individual model!\")\n",
    "\n",
    "print(\"\\nüèÜ This is exactly how Kaggle winners build solutions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production-deployment",
   "metadata": {},
   "source": [
    "## üöÄ Production ML: Deployment Considerations\n",
    "\n",
    "**When deploying gradient boosting in production:**\n",
    "\n",
    "**1. Choose Based on Requirements:**\n",
    "- **Speed critical?** ‚Üí LightGBM\n",
    "- **Categorical features?** ‚Üí CatBoost\n",
    "- **General purpose?** ‚Üí XGBoost\n",
    "\n",
    "**2. Model Size:**\n",
    "- Fewer trees = Smaller model = Faster inference\n",
    "- Balance: accuracy vs latency\n",
    "\n",
    "**3. Monitoring:**\n",
    "- Track prediction latency\n",
    "- Monitor feature distributions (drift)\n",
    "- A/B test new models\n",
    "\n",
    "**4. Real-World Systems (2024-2025):**\n",
    "- **RAG ranking**: LightGBM for speed\n",
    "- **Agentic AI routing**: XGBoost for reliability\n",
    "- **Recommendation systems**: Ensemble for maximum accuracy\n",
    "- **Fraud detection**: CatBoost for robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare inference speed (critical for production)\n",
    "print(\"‚ö° Inference Speed Comparison (1000 predictions)\\n\")\n",
    "\n",
    "sample_data = X_test.head(1000)\n",
    "\n",
    "models = [\n",
    "    ('XGBoost', xgb_clf),\n",
    "    ('LightGBM', lgb_clf),\n",
    "    ('CatBoost', cat_clf)\n",
    "]\n",
    "\n",
    "inference_times = []\n",
    "\n",
    "for name, model in models:\n",
    "    start = time.time()\n",
    "    _ = model.predict(sample_data)\n",
    "    elapsed = time.time() - start\n",
    "    inference_times.append(elapsed)\n",
    "    print(f\"{name}: {elapsed*1000:.2f} ms ({elapsed*1000/1000:.3f} ms per prediction)\")\n",
    "\n",
    "# Calculate predictions per second\n",
    "print(\"\\nüìä Predictions per second:\")\n",
    "for (name, _), inf_time in zip(models, inference_times):\n",
    "    pps = 1000 / inf_time\n",
    "    print(f\"  {name}: {pps:,.0f} predictions/sec\")\n",
    "\n",
    "print(\"\\nüí° All gradient boosting methods are fast enough for real-time production use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Exercise 1 - Customer Churn Prediction\n",
    "\n",
    "**Challenge:** Build a gradient boosting model to predict customer churn!\n",
    "\n",
    "**Scenario:** Subscription service wants to predict which customers will cancel\n",
    "\n",
    "**Your Task:**\n",
    "1. Train XGBoost model\n",
    "2. Train LightGBM model\n",
    "3. Compare their ROC AUC scores\n",
    "4. Which performs better?\n",
    "\n",
    "Let's build it! üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer churn dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "n_customers = 5000\n",
    "\n",
    "churn_data = {\n",
    "    'months_subscribed': np.concatenate([\n",
    "        np.random.exponential(3, 1500),   # Churners: short subscription\n",
    "        np.random.exponential(24, 3500)   # Retained: long subscription\n",
    "    ]),\n",
    "    'monthly_usage_minutes': np.concatenate([\n",
    "        np.random.exponential(50, 1500),\n",
    "        np.random.exponential(200, 3500)\n",
    "    ]),\n",
    "    'support_tickets': np.concatenate([\n",
    "        np.random.poisson(5, 1500),\n",
    "        np.random.poisson(1, 3500)\n",
    "    ]),\n",
    "    'payment_failures': np.concatenate([\n",
    "        np.random.poisson(2, 1500),\n",
    "        np.random.poisson(0.1, 3500)\n",
    "    ]),\n",
    "    'feature_usage_pct': np.concatenate([\n",
    "        np.random.beta(2, 5, 1500),\n",
    "        np.random.beta(5, 2, 3500)\n",
    "    ]),\n",
    "    'will_churn': [1] * 1500 + [0] * 3500\n",
    "}\n",
    "\n",
    "churn_df = pd.DataFrame(churn_data)\n",
    "churn_df = churn_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"üìä Customer Churn Dataset:\")\n",
    "print(churn_df.head())\n",
    "print(f\"\\nTotal customers: {len(churn_df)}\")\n",
    "print(f\"Will churn: {churn_df['will_churn'].sum()}\")\n",
    "print(f\"Churn rate: {churn_df['will_churn'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "# Hint: Follow the same pattern as above\n",
    "\n",
    "# Step 1: Prepare data\n",
    "X_churn = # YOUR CODE\n",
    "y_churn = # YOUR CODE\n",
    "\n",
    "# Step 2: Split data\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 3: Train XGBoost\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 4: Train LightGBM\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 5: Compare ROC AUC\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Prepare data\n",
    "X_churn = churn_df.drop('will_churn', axis=1)\n",
    "y_churn = churn_df['will_churn']\n",
    "\n",
    "# Step 2: Split\n",
    "X_train_ch, X_test_ch, y_train_ch, y_test_ch = train_test_split(\n",
    "    X_churn, y_churn, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: XGBoost\n",
    "xgb_churn = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "xgb_churn.fit(X_train_ch, y_train_ch)\n",
    "xgb_churn_auc = roc_auc_score(y_test_ch, xgb_churn.predict_proba(X_test_ch)[:, 1])\n",
    "\n",
    "# Step 4: LightGBM\n",
    "lgb_churn = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "lgb_churn.fit(X_train_ch, y_train_ch)\n",
    "lgb_churn_auc = roc_auc_score(y_test_ch, lgb_churn.predict_proba(X_test_ch)[:, 1])\n",
    "\n",
    "# Step 5: Compare\n",
    "print(f\"XGBoost ROC AUC: {xgb_churn_auc:.4f}\")\n",
    "print(f\"LightGBM ROC AUC: {lgb_churn_auc:.4f}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**You just mastered:**\n",
    "\n",
    "### **Gradient Boosting Algorithms:**\n",
    "\n",
    "1. **XGBoost**\n",
    "   - ‚úÖ Most popular, battle-tested\n",
    "   - ‚úÖ Great regularization\n",
    "   - ‚úÖ Excellent default parameters\n",
    "   - **Use for:** General purpose, production ML\n",
    "\n",
    "2. **LightGBM**\n",
    "   - ‚úÖ Fastest training and inference\n",
    "   - ‚úÖ Best for large datasets\n",
    "   - ‚úÖ Lower memory usage\n",
    "   - **Use for:** Real-time systems, big data\n",
    "\n",
    "3. **CatBoost**\n",
    "   - ‚úÖ Best for categorical features\n",
    "   - ‚úÖ Most robust to overfitting\n",
    "   - ‚úÖ Great out-of-the-box performance\n",
    "   - **Use for:** Categorical data, minimal tuning\n",
    "\n",
    "### **Pro Techniques:**\n",
    "- **Ensemble of boosting models** ‚Üí Kaggle-winning technique\n",
    "- **Hyperparameter tuning** ‚Üí GridSearchCV for optimization\n",
    "- **ROC AUC** ‚Üí Better metric than accuracy for imbalanced data\n",
    "\n",
    "**üåü Real-World Applications (2024-2025):**\n",
    "- **Kaggle competitions**: 90%+ of winning solutions use gradient boosting\n",
    "- **Search ranking**: Google, Bing use XGBoost/LightGBM\n",
    "- **Recommendation systems**: Netflix, Spotify, YouTube\n",
    "- **Ad targeting**: Facebook, Google Ads CTR prediction\n",
    "- **Fraud detection**: Real-time transaction scoring\n",
    "- **RAG systems**: Document ranking and filtering\n",
    "- **Agentic AI**: Query routing and decision-making\n",
    "\n",
    "**Quick Decision Guide:**\n",
    "- **Speed critical?** ‚Üí LightGBM\n",
    "- **Lots of categories?** ‚Üí CatBoost\n",
    "- **General purpose?** ‚Üí XGBoost\n",
    "- **Maximum accuracy?** ‚Üí Ensemble all three!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "**Practice Exercises:**\n",
    "1. Try different `learning_rate` values (0.01, 0.05, 0.1, 0.3)\n",
    "2. Experiment with `n_estimators` (50, 100, 200, 500)\n",
    "3. Create your own ensemble with weighted averaging\n",
    "\n",
    "**Coming Next:**\n",
    "- **Day 3:** Advanced ML Techniques (Feature Engineering, Stacking, Pipelines)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now know the gradient boosting algorithms that power:\n",
    "- Top Kaggle solutions\n",
    "- Production ML at FAANG companies\n",
    "- Modern AI ranking and recommendation systems\n",
    "\n",
    "**üí¨ Pro Tip:** In practice, try XGBoost, LightGBM, and CatBoost on your problem. The best one varies by dataset!\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: Most real-world ML is gradient boosting (for tabular data) + Transformers (for text/images). You now know the gradient boosting side!* üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
