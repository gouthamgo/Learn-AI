{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Day 3: Advanced ML Techniques (Feature Engineering, Stacking, Pipelines)\n",
    "\n",
    "**ðŸŽ¯ Goal:** Master advanced techniques for building production-ready ML systems\n",
    "\n",
    "**â±ï¸ Time:** 90-120 minutes\n",
    "\n",
    "**ðŸŒŸ Why This Matters for AI:**\n",
    "- Feature engineering often provides bigger gains than algorithm choice\n",
    "- ML pipelines are essential for production deployment\n",
    "- Stacking/blending techniques power top Kaggle solutions\n",
    "- Critical for building reliable Agentic AI systems\n",
    "- Required knowledge for ML engineering roles in 2024-2025\n",
    "- Foundation for combining traditional ML with modern neural networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ What You'll Learn Today\n",
    "\n",
    "**1. Feature Engineering**\n",
    "- Creating powerful features from raw data\n",
    "- Feature selection and importance\n",
    "- Handling missing values and outliers\n",
    "\n",
    "**2. Model Stacking & Blending**\n",
    "- Combining multiple models intelligently\n",
    "- Stacked generalization\n",
    "- Kaggle-winning techniques\n",
    "\n",
    "**3. ML Pipelines**\n",
    "- Building reproducible workflows\n",
    "- Preventing data leakage\n",
    "- Production deployment patterns\n",
    "\n",
    "**4. Real AI Example**\n",
    "- Building production ML pipeline for Agentic AI\n",
    "- End-to-end workflow\n",
    "\n",
    "Let's become ML engineers! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "\n",
    "# ML Pipeline components\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Models for stacking\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    StackingClassifier,\n",
    "    VotingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Make plots beautiful\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"Let's build production ML systems! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-intro",
   "metadata": {},
   "source": [
    "## ðŸ“Š Our Dataset: User Intent Classification for Agentic AI\n",
    "\n",
    "**Real AI Application:** Building an intent classifier for an AI agent system\n",
    "\n",
    "**Scenario:** You're building an Agentic AI that routes user requests to specialized agents:\n",
    "- **Customer Support Agent**: Handles complaints, questions\n",
    "- **Sales Agent**: Handles purchases, pricing\n",
    "- **Technical Agent**: Handles bugs, API issues\n",
    "- **General Agent**: Everything else\n",
    "\n",
    "**Challenge:** Classify user intent from message features\n",
    "\n",
    "**Features:**\n",
    "- `message_length`: Number of words\n",
    "- `has_question_mark`: Contains '?'\n",
    "- `urgent_keywords`: Contains urgent words (\"ASAP\", \"urgent\")\n",
    "- `technical_keywords`: Contains tech terms\n",
    "- `sentiment_score`: Negative to positive (-1 to 1)\n",
    "- `time_of_day`: Hour message sent\n",
    "- `user_history_count`: Previous messages\n",
    "\n",
    "**Target:**\n",
    "- `intent`: 0=General, 1=Support, 2=Sales, 3=Technical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic intent classification dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 3000\n",
    "samples_per_class = n_samples // 4\n",
    "\n",
    "def create_class_data(class_id):\n",
    "    \"\"\"Generate features for each intent class\"\"\"\n",
    "    if class_id == 0:  # General\n",
    "        return {\n",
    "            'message_length': np.random.poisson(15, samples_per_class),\n",
    "            'has_question_mark': np.random.binomial(1, 0.3, samples_per_class),\n",
    "            'urgent_keywords': np.random.binomial(1, 0.1, samples_per_class),\n",
    "            'technical_keywords': np.random.binomial(1, 0.2, samples_per_class),\n",
    "            'sentiment_score': np.random.normal(0, 0.3, samples_per_class),\n",
    "            'time_of_day': np.random.choice(24, samples_per_class),\n",
    "            'user_history_count': np.random.poisson(5, samples_per_class)\n",
    "        }\n",
    "    elif class_id == 1:  # Support\n",
    "        return {\n",
    "            'message_length': np.random.poisson(25, samples_per_class),\n",
    "            'has_question_mark': np.random.binomial(1, 0.7, samples_per_class),\n",
    "            'urgent_keywords': np.random.binomial(1, 0.5, samples_per_class),\n",
    "            'technical_keywords': np.random.binomial(1, 0.3, samples_per_class),\n",
    "            'sentiment_score': np.random.normal(-0.3, 0.4, samples_per_class),\n",
    "            'time_of_day': np.random.choice(24, samples_per_class),\n",
    "            'user_history_count': np.random.poisson(3, samples_per_class)\n",
    "        }\n",
    "    elif class_id == 2:  # Sales\n",
    "        return {\n",
    "            'message_length': np.random.poisson(20, samples_per_class),\n",
    "            'has_question_mark': np.random.binomial(1, 0.5, samples_per_class),\n",
    "            'urgent_keywords': np.random.binomial(1, 0.2, samples_per_class),\n",
    "            'technical_keywords': np.random.binomial(1, 0.1, samples_per_class),\n",
    "            'sentiment_score': np.random.normal(0.2, 0.3, samples_per_class),\n",
    "            'time_of_day': np.random.choice(range(9, 18), samples_per_class),  # Business hours\n",
    "            'user_history_count': np.random.poisson(2, samples_per_class)\n",
    "        }\n",
    "    else:  # Technical\n",
    "        return {\n",
    "            'message_length': np.random.poisson(30, samples_per_class),\n",
    "            'has_question_mark': np.random.binomial(1, 0.4, samples_per_class),\n",
    "            'urgent_keywords': np.random.binomial(1, 0.4, samples_per_class),\n",
    "            'technical_keywords': np.random.binomial(1, 0.9, samples_per_class),\n",
    "            'sentiment_score': np.random.normal(-0.1, 0.3, samples_per_class),\n",
    "            'time_of_day': np.random.choice(24, samples_per_class),\n",
    "            'user_history_count': np.random.poisson(8, samples_per_class)\n",
    "        }\n",
    "\n",
    "# Create data for all classes\n",
    "all_data = []\n",
    "for class_id in range(4):\n",
    "    class_data = create_class_data(class_id)\n",
    "    class_data['intent'] = [class_id] * samples_per_class\n",
    "    all_data.append(pd.DataFrame(class_data))\n",
    "\n",
    "# Combine and shuffle\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "intent_names = {0: 'General', 1: 'Support', 2: 'Sales', 3: 'Technical'}\n",
    "\n",
    "print(\"ðŸ¤– Agentic AI Intent Classification Dataset Created!\")\n",
    "print(f\"Total messages: {len(df):,}\")\n",
    "print(\"\\nIntent distribution:\")\n",
    "for intent_id, count in df['intent'].value_counts().sort_index().items():\n",
    "    print(f\"  {intent_names[intent_id]}: {count}\")\n",
    "print(\"\\nFirst few messages:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering-intro",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Part 1: Feature Engineering\n",
    "\n",
    "**What is Feature Engineering?**\n",
    "- Creating new features from existing ones\n",
    "- Often more impactful than choosing the \"best\" algorithm!\n",
    "\n",
    "**Common Techniques:**\n",
    "1. **Polynomial features**: Create interactions (e.g., feature1 Ã— feature2)\n",
    "2. **Domain knowledge**: Engineer features based on problem understanding\n",
    "3. **Aggregations**: Mean, max, count, etc.\n",
    "4. **Binning**: Convert continuous to categorical\n",
    "\n",
    "**ðŸŽ¯ Real AI Impact:**\n",
    "- \"Feature engineering is the key to winning Kaggle competitions\" - Kaggle Grandmasters\n",
    "- At Google/Facebook: Teams spend 80% time on features, 20% on algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Create powerful new features\n",
    "print(\"ðŸ”§ Engineering new features...\\n\")\n",
    "\n",
    "df_engineered = df.copy()\n",
    "\n",
    "# 1. Interaction features\n",
    "df_engineered['urgent_technical'] = df['urgent_keywords'] * df['technical_keywords']\n",
    "df_engineered['question_length'] = df['has_question_mark'] * df['message_length']\n",
    "\n",
    "# 2. Ratio features\n",
    "df_engineered['keywords_per_word'] = (df['urgent_keywords'] + df['technical_keywords']) / (df['message_length'] + 1)\n",
    "\n",
    "# 3. Binning continuous features\n",
    "df_engineered['message_length_bin'] = pd.cut(df['message_length'], bins=[0, 10, 20, 30, 100], labels=[0, 1, 2, 3])\n",
    "df_engineered['message_length_bin'] = df_engineered['message_length_bin'].astype(int)\n",
    "\n",
    "# 4. Time-based features\n",
    "df_engineered['is_business_hours'] = ((df['time_of_day'] >= 9) & (df['time_of_day'] <= 17)).astype(int)\n",
    "df_engineered['is_night'] = ((df['time_of_day'] >= 22) | (df['time_of_day'] <= 6)).astype(int)\n",
    "\n",
    "# 5. User behavior features\n",
    "df_engineered['is_new_user'] = (df['user_history_count'] <= 2).astype(int)\n",
    "df_engineered['is_power_user'] = (df['user_history_count'] >= 10).astype(int)\n",
    "\n",
    "# 6. Sentiment features\n",
    "df_engineered['is_negative'] = (df['sentiment_score'] < -0.2).astype(int)\n",
    "df_engineered['is_positive'] = (df['sentiment_score'] > 0.2).astype(int)\n",
    "\n",
    "print(\"âœ… Engineered Features Created:\")\n",
    "new_features = [col for col in df_engineered.columns if col not in df.columns]\n",
    "for feat in new_features:\n",
    "    print(f\"  â€¢ {feat}\")\n",
    "\n",
    "print(f\"\\nOriginal features: {len(df.columns) - 1}\")\n",
    "print(f\"Total features now: {len(df_engineered.columns) - 1}\")\n",
    "print(f\"New features added: {len(new_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance with and without engineered features\n",
    "print(\"ðŸ“Š Comparing: Original vs Engineered Features\\n\")\n",
    "\n",
    "# Prepare both datasets\n",
    "X_original = df.drop('intent', axis=1)\n",
    "X_engineered = df_engineered.drop('intent', axis=1)\n",
    "y = df['intent']\n",
    "\n",
    "# Split\n",
    "X_orig_train, X_orig_test, y_train, y_test = train_test_split(\n",
    "    X_original, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_eng_train, X_eng_test, _, _ = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train models\n",
    "rf_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_original.fit(X_orig_train, y_train)\n",
    "acc_original = accuracy_score(y_test, rf_original.predict(X_orig_test))\n",
    "\n",
    "rf_engineered = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_engineered.fit(X_eng_train, y_train)\n",
    "acc_engineered = accuracy_score(y_test, rf_engineered.predict(X_eng_test))\n",
    "\n",
    "print(f\"Original Features Accuracy: {acc_original:.2%}\")\n",
    "print(f\"Engineered Features Accuracy: {acc_engineered:.2%}\")\n",
    "print(f\"\\nðŸ“ˆ Improvement: {(acc_engineered - acc_original):.2%}\")\n",
    "\n",
    "if acc_engineered > acc_original:\n",
    "    print(\"\\nðŸŽ‰ Feature engineering improved our model!\")\n",
    "else:\n",
    "    print(\"\\nðŸ’¡ Original features were already quite good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-selection-intro",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Feature Selection: Choosing the Best Features\n",
    "\n",
    "**Why Feature Selection?**\n",
    "- Too many features â†’ Overfitting, slower training\n",
    "- Remove irrelevant/redundant features\n",
    "- Simpler models â†’ Better production performance\n",
    "\n",
    "**Methods:**\n",
    "1. **Filter methods**: Statistical tests (fast)\n",
    "2. **Wrapper methods**: Try feature subsets (accurate, slow)\n",
    "3. **Embedded methods**: Feature importance from models\n",
    "\n",
    "Let's try all three!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Filter method (SelectKBest)\n",
    "print(\"ðŸ” Feature Selection Methods\\n\")\n",
    "print(\"1ï¸âƒ£ Filter Method: SelectKBest\\n\")\n",
    "\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X_eng_train, y_train)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X_eng_train.columns[selector.get_support()].tolist()\n",
    "print(\"Top 10 features by statistical test:\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "# Method 2: Embedded method (Random Forest importance)\n",
    "print(\"\\n2ï¸âƒ£ Embedded Method: Random Forest Feature Importance\\n\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_eng_train.columns,\n",
    "    'Importance': rf_engineered.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 features by Random Forest:\")\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='steelblue')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('ðŸŽ¯ Top 10 Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stacking-intro",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Part 2: Model Stacking & Blending\n",
    "\n",
    "**What is Stacking?**\n",
    "- Train multiple diverse models (Level 0)\n",
    "- Use their predictions as features for another model (Level 1)\n",
    "- Level 1 model learns how to best combine Level 0 predictions\n",
    "\n",
    "**Stacking vs Voting:**\n",
    "- **Voting**: Simple average or majority vote\n",
    "- **Stacking**: Learns optimal way to combine models\n",
    "\n",
    "**Why Stacking Works:**\n",
    "- Different models make different errors\n",
    "- Meta-model learns which model to trust when\n",
    "- Often 1-2% improvement (huge in competitions!)\n",
    "\n",
    "**ðŸŽ¯ Real AI Use Cases:**\n",
    "- **Kaggle**: Almost all top solutions use stacking\n",
    "- **Production**: Combine traditional ML + neural networks\n",
    "- **Netflix Prize**: Winning solution stacked 100+ models\n",
    "- **Modern RAG**: Stack retrieval scores from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stacking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build stacked ensemble\n",
    "print(\"ðŸ—ï¸ Building Stacked Ensemble\\n\")\n",
    "\n",
    "# Level 0 models (base models) - diverse algorithms\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')),\n",
    "    ('lgb', lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1))\n",
    "]\n",
    "\n",
    "# Level 1 model (meta-model)\n",
    "meta_model = LogisticRegression(multi_class='multinomial', random_state=42, max_iter=1000)\n",
    "\n",
    "# Create stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5  # 5-fold cross-validation to avoid overfitting\n",
    ")\n",
    "\n",
    "print(\"Training stacked ensemble...\")\n",
    "stacking_clf.fit(X_eng_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_stack = stacking_clf.predict(X_eng_test)\n",
    "acc_stack = accuracy_score(y_test, y_pred_stack)\n",
    "\n",
    "print(f\"\\nâœ… Stacking Ensemble Accuracy: {acc_stack:.2%}\")\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_stack, target_names=list(intent_names.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-stacking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare individual models vs stacking\n",
    "print(\"ðŸ“Š Comparing Individual Models vs Stacking\\n\")\n",
    "\n",
    "# Train individual models\n",
    "results = []\n",
    "\n",
    "for name, model in base_models:\n",
    "    model.fit(X_eng_train, y_train)\n",
    "    acc = accuracy_score(y_test, model.predict(X_eng_test))\n",
    "    results.append({'Model': name.upper(), 'Accuracy': acc})\n",
    "\n",
    "results.append({'Model': 'STACKING', 'Accuracy': acc_stack})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "bars = plt.bar(results_df['Model'], results_df['Accuracy'], color=colors)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('ðŸ† Individual Models vs Stacking Ensemble', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0.7, 1.0)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2%}',\n",
    "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if results_df.iloc[0]['Model'] == 'STACKING':\n",
    "    print(\"\\nðŸŽ‰ Stacking beats all individual models!\")\n",
    "else:\n",
    "    print(\"\\nðŸ’¡ Stacking performs competitively with the best individual model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipelines-intro",
   "metadata": {},
   "source": [
    "## ðŸ”§ Part 3: ML Pipelines\n",
    "\n",
    "**What is an ML Pipeline?**\n",
    "- Automates the entire ML workflow\n",
    "- Preprocessing â†’ Feature Engineering â†’ Model Training\n",
    "- Ensures reproducibility and prevents data leakage\n",
    "\n",
    "**Why Pipelines Matter:**\n",
    "- âœ… **Prevent data leakage**: Fit only on training data\n",
    "- âœ… **Reproducible**: Same steps every time\n",
    "- âœ… **Production-ready**: Easy to deploy\n",
    "- âœ… **Clean code**: No manual step tracking\n",
    "\n",
    "**ðŸŽ¯ Real AI Production:**\n",
    "- Every production ML system uses pipelines\n",
    "- Required for MLOps and model monitoring\n",
    "- Essential for A/B testing new models\n",
    "- Critical for Agentic AI systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a basic ML pipeline\n",
    "print(\"ðŸ”§ Building ML Pipeline\\n\")\n",
    "\n",
    "# Define pipeline steps\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(f_classif, k=10)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Pipeline Steps:\")\n",
    "for i, (name, step) in enumerate(pipeline.steps, 1):\n",
    "    print(f\"  {i}. {name}: {step.__class__.__name__}\")\n",
    "\n",
    "# Train pipeline (all steps executed automatically)\n",
    "print(\"\\nTraining pipeline...\")\n",
    "pipeline.fit(X_eng_train, y_train)\n",
    "\n",
    "# Predict (preprocessing applied automatically)\n",
    "y_pred_pipeline = pipeline.predict(X_eng_test)\n",
    "acc_pipeline = accuracy_score(y_test, y_pred_pipeline)\n",
    "\n",
    "print(f\"\\nâœ… Pipeline Accuracy: {acc_pipeline:.2%}\")\n",
    "print(\"\\nðŸ’¡ Pipeline automatically applied all transformations to test data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-advanced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced pipeline with custom transformations\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer for feature engineering\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # Add engineered features\n",
    "        X_new['urgent_technical'] = X['urgent_keywords'] * X['technical_keywords']\n",
    "        X_new['question_length'] = X['has_question_mark'] * X['message_length']\n",
    "        X_new['is_business_hours'] = ((X['time_of_day'] >= 9) & (X['time_of_day'] <= 17)).astype(int)\n",
    "        X_new['is_negative'] = (X['sentiment_score'] < -0.2).astype(int)\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "# Build advanced pipeline\n",
    "advanced_pipeline = Pipeline([\n",
    "    ('feature_engineering', FeatureEngineer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'))\n",
    "])\n",
    "\n",
    "print(\"ðŸš€ Advanced Pipeline with Custom Feature Engineering\\n\")\n",
    "print(\"Pipeline Steps:\")\n",
    "for i, (name, step) in enumerate(advanced_pipeline.steps, 1):\n",
    "    print(f\"  {i}. {name}: {step.__class__.__name__}\")\n",
    "\n",
    "# Train\n",
    "advanced_pipeline.fit(X_orig_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_adv = advanced_pipeline.predict(X_orig_test)\n",
    "acc_adv = accuracy_score(y_test, y_pred_adv)\n",
    "\n",
    "print(f\"\\nâœ… Advanced Pipeline Accuracy: {acc_adv:.2%}\")\n",
    "print(\"\\nðŸ’¡ This pipeline is production-ready! Can be saved and deployed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production-pipeline",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Real AI Example: Production ML Pipeline for Agentic AI\n",
    "\n",
    "**Full Production Pipeline:**\n",
    "1. Feature engineering\n",
    "2. Preprocessing and scaling\n",
    "3. Feature selection\n",
    "4. Stacked ensemble\n",
    "5. Model versioning and monitoring\n",
    "\n",
    "**This is what you'd deploy in a real Agentic AI system!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready pipeline for Agentic AI\n",
    "print(\"ðŸ¤– Building Production ML Pipeline for Agentic AI\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Feature Engineering Pipeline\n",
    "feature_pipeline = Pipeline([\n",
    "    ('engineer', FeatureEngineer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Step 2: Prepare data\n",
    "X_processed_train = feature_pipeline.fit_transform(X_orig_train)\n",
    "X_processed_test = feature_pipeline.transform(X_orig_test)\n",
    "\n",
    "# Step 3: Build stacked ensemble with diverse models\n",
    "production_base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)),\n",
    "    ('xgb', xgb.XGBClassifier(n_estimators=200, learning_rate=0.1, random_state=42, eval_metric='mlogloss')),\n",
    "    ('lgb', lgb.LGBMClassifier(n_estimators=200, learning_rate=0.1, random_state=42, verbose=-1))\n",
    "]\n",
    "\n",
    "production_meta_model = xgb.XGBClassifier(\n",
    "    n_estimators=50, \n",
    "    learning_rate=0.05, \n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "production_model = StackingClassifier(\n",
    "    estimators=production_base_models,\n",
    "    final_estimator=production_meta_model,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "print(\"Training production model...\")\n",
    "print(\"  Base Models: Random Forest, XGBoost, LightGBM\")\n",
    "print(\"  Meta Model: XGBoost\")\n",
    "print(\"  Cross-Validation: 5-fold\")\n",
    "print(\"\\nThis may take a moment...\\n\")\n",
    "\n",
    "production_model.fit(X_processed_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate\n",
    "y_pred_prod = production_model.predict(X_processed_test)\n",
    "acc_prod = accuracy_score(y_test, y_pred_prod)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nðŸŽ¯ PRODUCTION MODEL RESULTS\\n\")\n",
    "print(f\"Overall Accuracy: {acc_prod:.2%}\")\n",
    "print(\"\\nðŸ“Š Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_prod, target_names=list(intent_names.values())))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_prod)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=list(intent_names.values()),\n",
    "            yticklabels=list(intent_names.values()))\n",
    "plt.title('ðŸŽ¯ Production Model: Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual Intent', fontsize=12)\n",
    "plt.xlabel('Predicted Intent', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Production pipeline is ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deployment-example",
   "metadata": {},
   "source": [
    "## ðŸš€ Deployment Simulation: How This Works in Production\n",
    "\n",
    "**Real-world deployment flow:**\n",
    "\n",
    "1. **User sends message** â†’ \"Help! My API is returning 500 errors\"\n",
    "2. **Extract features** â†’ message_length=7, has_question_mark=0, technical_keywords=1, etc.\n",
    "3. **Pipeline processes** â†’ Feature engineering, scaling, prediction\n",
    "4. **Route to agent** â†’ Technical Agent\n",
    "5. **Agent responds** â†’ \"I'll help with your API error...\"\n",
    "\n",
    "Let's simulate this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deployment-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate production inference\n",
    "print(\"ðŸ¤– PRODUCTION DEPLOYMENT SIMULATION\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulate new incoming messages\n",
    "new_messages = pd.DataFrame([\n",
    "    {\n",
    "        'message_length': 7,\n",
    "        'has_question_mark': 0,\n",
    "        'urgent_keywords': 1,\n",
    "        'technical_keywords': 1,\n",
    "        'sentiment_score': -0.4,\n",
    "        'time_of_day': 14,\n",
    "        'user_history_count': 5,\n",
    "        'description': 'API returning 500 errors'\n",
    "    },\n",
    "    {\n",
    "        'message_length': 12,\n",
    "        'has_question_mark': 1,\n",
    "        'urgent_keywords': 0,\n",
    "        'technical_keywords': 0,\n",
    "        'sentiment_score': 0.3,\n",
    "        'time_of_day': 11,\n",
    "        'user_history_count': 1,\n",
    "        'description': 'How much does the pro plan cost?'\n",
    "    },\n",
    "    {\n",
    "        'message_length': 20,\n",
    "        'has_question_mark': 1,\n",
    "        'urgent_keywords': 1,\n",
    "        'technical_keywords': 0,\n",
    "        'sentiment_score': -0.5,\n",
    "        'time_of_day': 16,\n",
    "        'user_history_count': 3,\n",
    "        'description': 'URGENT: Need refund, service not working!'\n",
    "    },\n",
    "    {\n",
    "        'message_length': 8,\n",
    "        'has_question_mark': 1,\n",
    "        'urgent_keywords': 0,\n",
    "        'technical_keywords': 0,\n",
    "        'sentiment_score': 0.1,\n",
    "        'time_of_day': 10,\n",
    "        'user_history_count': 2,\n",
    "        'description': 'What are your business hours?'\n",
    "    }\n",
    "])\n",
    "\n",
    "# Extract features for prediction\n",
    "new_messages_features = new_messages.drop('description', axis=1)\n",
    "\n",
    "# Apply production pipeline\n",
    "new_messages_processed = feature_pipeline.transform(new_messages_features)\n",
    "predictions = production_model.predict(new_messages_processed)\n",
    "prediction_proba = production_model.predict_proba(new_messages_processed)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nðŸ“¨ INCOMING MESSAGES & ROUTING DECISIONS:\\n\")\n",
    "\n",
    "for i, (idx, msg) in enumerate(new_messages.iterrows()):\n",
    "    predicted_intent = intent_names[predictions[i]]\n",
    "    confidence = prediction_proba[i][predictions[i]]\n",
    "    \n",
    "    print(f\"Message {i+1}: \\\"{msg['description']}\\\"\")\n",
    "    print(f\"  â†’ Routed to: {predicted_intent} Agent\")\n",
    "    print(f\"  â†’ Confidence: {confidence:.1%}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâœ… Production system successfully routing messages to correct agents!\")\n",
    "print(\"\\nðŸ’¡ In production, this would:\")\n",
    "print(\"  1. Run in < 10ms for real-time routing\")\n",
    "print(\"  2. Log all predictions for monitoring\")\n",
    "print(\"  3. Track accuracy and retrain periodically\")\n",
    "print(\"  4. Handle thousands of requests per second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ YOUR TURN: Exercise - Build Your Own Production Pipeline\n",
    "\n",
    "**Challenge:** Create a complete production pipeline for a new problem!\n",
    "\n",
    "**Problem:** Predict whether a support ticket needs escalation\n",
    "\n",
    "**Your Task:**\n",
    "1. Engineer at least 3 new features\n",
    "2. Build a pipeline with preprocessing + model\n",
    "3. Use cross-validation to evaluate\n",
    "4. Report accuracy\n",
    "\n",
    "Use the dataset below. Good luck! ðŸ’ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticket escalation dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "n_tickets = 2000\n",
    "\n",
    "ticket_data = {\n",
    "    'response_time_hours': np.concatenate([\n",
    "        np.random.exponential(24, 600),   # Escalated: slow response\n",
    "        np.random.exponential(4, 1400)    # Not escalated: fast response\n",
    "    ]),\n",
    "    'num_messages': np.concatenate([\n",
    "        np.random.poisson(10, 600),\n",
    "        np.random.poisson(3, 1400)\n",
    "    ]),\n",
    "    'customer_satisfaction': np.concatenate([\n",
    "        np.random.beta(2, 5, 600) * 5,    # Escalated: low satisfaction\n",
    "        np.random.beta(5, 2, 1400) * 5    # Not escalated: high satisfaction\n",
    "    ]),\n",
    "    'priority_level': np.concatenate([\n",
    "        np.random.choice([1, 2, 3], 600, p=[0.6, 0.3, 0.1]),\n",
    "        np.random.choice([1, 2, 3], 1400, p=[0.1, 0.3, 0.6])\n",
    "    ]),\n",
    "    'needs_escalation': [1] * 600 + [0] * 1400\n",
    "}\n",
    "\n",
    "ticket_df = pd.DataFrame(ticket_data)\n",
    "ticket_df = ticket_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"ðŸŽ« Support Ticket Escalation Dataset:\")\n",
    "print(ticket_df.head())\n",
    "print(f\"\\nTotal tickets: {len(ticket_df)}\")\n",
    "print(f\"Needs escalation: {ticket_df['needs_escalation'].sum()}\")\n",
    "print(f\"Escalation rate: {ticket_df['needs_escalation'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "# Build a production pipeline for ticket escalation prediction\n",
    "\n",
    "# Step 1: Engineer features\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 2: Create pipeline\n",
    "# YOUR CODE\n",
    "\n",
    "# Step 3: Train and evaluate\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ“– Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Feature Engineering\n",
    "ticket_df['messages_per_hour'] = ticket_df['num_messages'] / (ticket_df['response_time_hours'] + 1)\n",
    "ticket_df['is_high_priority'] = (ticket_df['priority_level'] == 1).astype(int)\n",
    "ticket_df['is_unsatisfied'] = (ticket_df['customer_satisfaction'] < 2.5).astype(int)\n",
    "ticket_df['slow_response'] = (ticket_df['response_time_hours'] > 12).astype(int)\n",
    "\n",
    "# Step 2: Prepare data\n",
    "X = ticket_df.drop('needs_escalation', axis=1)\n",
    "y = ticket_df['needs_escalation']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Build pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Step 4: Train and evaluate\n",
    "pipeline.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, pipeline.predict(X_test))\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "**You just mastered advanced ML techniques!**\n",
    "\n",
    "### **1. Feature Engineering**\n",
    "   - âœ… Often more impactful than algorithm choice\n",
    "   - âœ… Domain knowledge is key\n",
    "   - âœ… Create interactions, ratios, aggregations\n",
    "   - **Pro Tip:** Spend 80% time on features, 20% on algorithms\n",
    "\n",
    "### **2. Feature Selection**\n",
    "   - âœ… Reduces overfitting\n",
    "   - âœ… Faster training and inference\n",
    "   - âœ… Better interpretability\n",
    "   - **Methods:** Filter, Wrapper, Embedded\n",
    "\n",
    "### **3. Model Stacking**\n",
    "   - âœ… Combines diverse models\n",
    "   - âœ… Learns optimal combination\n",
    "   - âœ… 1-2% improvement (huge in production!)\n",
    "   - **Use when:** Maximum accuracy needed (Kaggle, critical systems)\n",
    "\n",
    "### **4. ML Pipelines**\n",
    "   - âœ… Prevents data leakage\n",
    "   - âœ… Reproducible and maintainable\n",
    "   - âœ… Production-ready\n",
    "   - **Essential for:** All production ML systems\n",
    "\n",
    "**ðŸŒŸ Real-World Applications (2024-2025):**\n",
    "\n",
    "- **Kaggle Competitions**: Stacking + feature engineering = winning solutions\n",
    "- **Production ML**: Pipelines are mandatory for deployment\n",
    "- **Agentic AI**: Intent classification, query routing, decision-making\n",
    "- **RAG Systems**: Combining multiple retrieval signals via stacking\n",
    "- **Hybrid AI**: Combining traditional ML + neural networks\n",
    "- **MLOps**: Pipelines enable monitoring, A/B testing, retraining\n",
    "\n",
    "**Production Checklist:**\n",
    "- âœ… Feature engineering pipeline\n",
    "- âœ… Proper train/test split (no leakage!)\n",
    "- âœ… Cross-validation for robust evaluation\n",
    "- âœ… Ensemble or stacking for accuracy\n",
    "- âœ… sklearn Pipeline for deployment\n",
    "- âœ… Model versioning and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## ðŸš€ Next Steps & Career Path\n",
    "\n",
    "**You've completed Week 10! You now know:**\n",
    "- Ensemble methods (Bagging, Boosting, Random Forests)\n",
    "- Gradient boosting (XGBoost, LightGBM, CatBoost)\n",
    "- Advanced techniques (Feature engineering, Stacking, Pipelines)\n",
    "\n",
    "**Practice Projects:**\n",
    "1. **Kaggle Competition**: Apply stacking to a real competition\n",
    "2. **Build RAG System**: Use ensemble for document ranking\n",
    "3. **Agentic AI**: Build multi-agent routing system\n",
    "\n",
    "**Real-World Applications:**\n",
    "- **Data Science Role**: You can now build production ML systems\n",
    "- **ML Engineer**: You understand deployment pipelines\n",
    "- **AI Engineer**: You can combine ML with LLMs/Transformers\n",
    "\n",
    "**Continue Learning:**\n",
    "- **Deep Learning**: Neural networks, Transformers, LLMs\n",
    "- **MLOps**: Model deployment, monitoring, retraining\n",
    "- **Specialized AI**: Computer Vision, NLP, Reinforcement Learning\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Congratulations!** You now have professional-level ML skills:\n",
    "- Top Kaggle techniques\n",
    "- Production ML patterns\n",
    "- Modern AI system building\n",
    "\n",
    "**ðŸ’¬ The Journey:**\n",
    "- Week 1-5: Python fundamentals\n",
    "- Week 6-8: Machine learning basics\n",
    "- Week 9: Model evaluation\n",
    "- **Week 10**: Advanced ML (you are here!)\n",
    "- Next: Deep Learning, Transformers, and beyond!\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: Modern AI = Traditional ML + Deep Learning. You now master the ML side. The combination of both makes you a complete AI engineer in 2024-2025!* ðŸŒŸ\n",
    "\n",
    "**Keep building, keep learning!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
