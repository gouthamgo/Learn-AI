{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üé® Week 3, Day 2: Seaborn Statistical Plots\n",
    "\n",
    "**üéØ Goal:** Create beautiful, publication-ready statistical visualizations with Seaborn\n",
    "\n",
    "**‚è±Ô∏è Time:** 60-90 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- **Statistical understanding** - See correlations, distributions, patterns\n",
    "- **Feature engineering** - Discover relationships between variables\n",
    "- **Model diagnostics** - Understand residuals, errors, predictions\n",
    "- **Data quality** - Spot outliers, missing values, imbalances\n",
    "- **Publication ready** - Impress stakeholders, write papers\n",
    "\n",
    "---\n",
    "\n",
    "## üî• 2024-2025 AI Trend Alert!\n",
    "\n",
    "**AI Transparency & Explainability** is now CRITICAL:\n",
    "- Regulators demand interpretable models\n",
    "- Businesses need to trust AI decisions\n",
    "- **Seaborn helps visualize model behavior patterns!**\n",
    "\n",
    "**Multimodal AI Analysis** (GPT-4V, Gemini Vision):\n",
    "- Analyzing relationships between text, image, audio features\n",
    "- **Seaborn visualizes cross-modal correlations!**\n",
    "\n",
    "**RAG System Optimization**:\n",
    "- Understanding embedding similarity distributions\n",
    "- **Seaborn plots retrieval quality metrics!**\n",
    "\n",
    "**You'll learn the tools used by AI research teams at top labs!** üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üé® What is Seaborn?\n",
    "\n",
    "**Seaborn** = Matplotlib on steroids + statistical superpowers\n",
    "\n",
    "Think of it as:\n",
    "- Matplotlib: Powerful but requires lots of code üîß\n",
    "- Seaborn: Beautiful by default, built for stats üé®\n",
    "\n",
    "**Key advantages:**\n",
    "- Beautiful default themes (no ugly charts!)\n",
    "- Built-in statistical functions\n",
    "- Works seamlessly with Pandas DataFrames\n",
    "- One line of code = complex visualizations\n",
    "\n",
    "Let's see the magic! ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Seaborn (Google Colab has it pre-installed!)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set default style (makes everything beautiful!)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Seaborn version:\", sns.__version__)\n",
    "print(\"‚úÖ Seaborn is ready to make beautiful plots!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## üìä Built-in Datasets for Practice\n",
    "\n",
    "Seaborn includes real datasets perfect for learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See available datasets\n",
    "print(\"Available datasets:\")\n",
    "print(sns.get_dataset_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## üéØ Distribution Plots - Understanding Your Data\n",
    "\n",
    "### 1Ô∏è‚É£ Histogram with KDE (Kernel Density Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic AI model confidence scores\n",
    "np.random.seed(42)\n",
    "confidence_scores = pd.DataFrame({\n",
    "    'GPT-4': np.random.beta(8, 2, 500),  # High confidence\n",
    "    'GPT-3.5': np.random.beta(6, 3, 500),  # Medium confidence\n",
    "    'Llama-3': np.random.beta(5, 4, 500)   # Lower confidence\n",
    "})\n",
    "\n",
    "# Melt for Seaborn (convert wide to long format)\n",
    "confidence_long = confidence_scores.melt(var_name='Model', value_name='Confidence')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram + KDE in one plot!\n",
    "sns.histplot(data=confidence_long, x='Confidence', hue='Model', \n",
    "            kde=True, alpha=0.6, bins=30)\n",
    "\n",
    "plt.title('LLM Confidence Score Distributions', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Confidence Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä KDE (smooth curve) shows the underlying distribution\")\n",
    "print(\"üéØ GPT-4 has higher confidence (shifted right)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Box Plot - Spot Outliers & Quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AI model response times across different tasks\n",
    "np.random.seed(42)\n",
    "response_data = pd.DataFrame({\n",
    "    'Model': ['GPT-4']*100 + ['Claude-3.5']*100 + ['Gemini-Pro']*100 + ['Llama-3']*100,\n",
    "    'Task': np.tile(['Code', 'Creative', 'Analysis', 'Chat'], 100),\n",
    "    'Response_Time_ms': np.concatenate([\n",
    "        np.random.gamma(2, 500, 100),   # GPT-4\n",
    "        np.random.gamma(1.8, 450, 100), # Claude\n",
    "        np.random.gamma(2.2, 520, 100), # Gemini\n",
    "        np.random.gamma(1.5, 400, 100)  # Llama\n",
    "    ])\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Box plot with beautiful colors\n",
    "sns.boxplot(data=response_data, x='Model', y='Response_Time_ms', \n",
    "           hue='Task', palette='Set2')\n",
    "\n",
    "plt.title('LLM Response Times by Model and Task Type', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Response Time (ms)', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.legend(title='Task Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üì¶ Box plot shows:\")\n",
    "print(\"  - Box = 25th to 75th percentile (middle 50%)\")\n",
    "print(\"  - Line in box = median\")\n",
    "print(\"  - Whiskers = min/max (within 1.5*IQR)\")\n",
    "print(\"  - Dots = outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Violin Plot - Distribution Shape + Box Plot Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Violin plot - shows full distribution shape\n",
    "sns.violinplot(data=response_data, x='Task', y='Response_Time_ms', \n",
    "              hue='Model', palette='muted', split=False)\n",
    "\n",
    "plt.title('Response Time Distributions Across Tasks', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Response Time (ms)', fontsize=12)\n",
    "plt.xlabel('Task Type', fontsize=12)\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéª Violin plot advantage: See the FULL distribution shape!\")\n",
    "print(\"   Wider = more data points at that value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## üìà Relationship Plots - Find Correlations\n",
    "\n",
    "### 4Ô∏è‚É£ Scatter Plot with Regression Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic training data\n",
    "np.random.seed(42)\n",
    "training_data = pd.DataFrame({\n",
    "    'Dataset_Size_K': np.random.randint(10, 1000, 100),\n",
    "    'Model_Accuracy': 0.5 + 0.45 * (1 - np.exp(-np.random.randint(10, 1000, 100)/300)) + np.random.normal(0, 0.03, 100),\n",
    "    'Training_Type': np.random.choice(['Supervised', 'Semi-Supervised', 'Self-Supervised'], 100)\n",
    "})\n",
    "\n",
    "training_data['Model_Accuracy'] = training_data['Model_Accuracy'].clip(0, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Scatter plot with regression line (automatically fitted!)\n",
    "sns.scatterplot(data=training_data, x='Dataset_Size_K', y='Model_Accuracy', \n",
    "               hue='Training_Type', style='Training_Type', s=100, alpha=0.7)\n",
    "sns.regplot(data=training_data, x='Dataset_Size_K', y='Model_Accuracy', \n",
    "           scatter=False, color='red', label='Trend Line')\n",
    "\n",
    "plt.title('Model Accuracy vs Dataset Size (Scaling Laws)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Dataset Size (Thousands)', fontsize=12)\n",
    "plt.ylabel('Model Accuracy', fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Key insight: More data = better performance (but diminishing returns!)\")\n",
    "print(\"   This is why GPT-4 was trained on trillions of tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### 5Ô∏è‚É£ Joint Plot - 2D Distribution + Marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters vs inference speed\n",
    "np.random.seed(42)\n",
    "model_specs = pd.DataFrame({\n",
    "    'Parameters_B': np.random.exponential(50, 200),\n",
    "    'Inference_Speed_tokens_per_sec': 1000 / (1 + np.random.exponential(50, 200)/20) + np.random.normal(0, 20, 200)\n",
    "})\n",
    "\n",
    "# Joint plot shows scatter + distributions on sides!\n",
    "g = sns.jointplot(data=model_specs, x='Parameters_B', y='Inference_Speed_tokens_per_sec', \n",
    "                 kind='scatter', height=8, alpha=0.6)\n",
    "\n",
    "g.set_axis_labels('Model Parameters (Billions)', 'Inference Speed (tokens/sec)', fontsize=12)\n",
    "g.fig.suptitle('LLM Size vs Speed Trade-off', fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Joint plot shows:\")\n",
    "print(\"  - Center: Scatter plot (relationship)\")\n",
    "print(\"  - Top: Distribution of Parameters\")\n",
    "print(\"  - Right: Distribution of Speed\")\n",
    "print(\"\\nüéØ Bigger models = slower inference (the speed/quality tradeoff!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### 6Ô∏è‚É£ Pair Plot - See ALL Relationships at Once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive model comparison dataset\n",
    "np.random.seed(42)\n",
    "models_df = pd.DataFrame({\n",
    "    'Parameters_B': np.random.exponential(30, 50),\n",
    "    'Training_Cost_M': np.random.exponential(30, 50) * 50,\n",
    "    'Accuracy': 0.7 + 0.25 * np.random.random(50),\n",
    "    'Inference_Speed': 1000 / (1 + np.random.exponential(30, 50)/15),\n",
    "    'Model_Type': np.random.choice(['Encoder-Only', 'Decoder-Only', 'Encoder-Decoder'], 50)\n",
    "})\n",
    "\n",
    "# Pair plot - matrix of all relationships!\n",
    "g = sns.pairplot(models_df, hue='Model_Type', palette='husl', \n",
    "                height=2.5, diag_kind='kde', plot_kws={'alpha': 0.6})\n",
    "g.fig.suptitle('LLM Multi-Dimensional Analysis', fontsize=14, fontweight='bold', y=1.01)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"üî• Pair plot is AMAZING for:\")\n",
    "print(\"  - Exploring all feature relationships at once\")\n",
    "print(\"  - Finding correlations before modeling\")\n",
    "print(\"  - Spotting clusters and patterns\")\n",
    "print(\"\\nüìä Diagonal = distribution of each variable\")\n",
    "print(\"üìä Off-diagonal = scatter plots between variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## üî• Heatmaps - Correlation & Confusion Matrices\n",
    "\n",
    "### 7Ô∏è‚É£ Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation = models_df[['Parameters_B', 'Training_Cost_M', 'Accuracy', 'Inference_Speed']].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Beautiful correlation heatmap\n",
    "sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "           center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "plt.title('LLM Feature Correlations', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"üå°Ô∏è Heatmap colors:\")\n",
    "print(\"  - Red = positive correlation\")\n",
    "print(\"  - Blue = negative correlation\")\n",
    "print(\"  - White = no correlation\")\n",
    "print(\"\\nüéØ Key findings:\")\n",
    "print(\"  - Parameters ‚Üî Cost: Strongly positive (bigger = more expensive)\")\n",
    "print(\"  - Parameters ‚Üî Speed: Negative (bigger = slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 8Ô∏è‚É£ Confusion Matrix with Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis confusion matrix\n",
    "confusion = np.array([\n",
    "    [850, 80, 70],    # Actual: Positive\n",
    "    [90, 820, 90],    # Actual: Neutral  \n",
    "    [60, 100, 840]    # Actual: Negative\n",
    "])\n",
    "\n",
    "classes = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Beautiful confusion matrix\n",
    "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           cbar_kws={'label': 'Number of Samples'})\n",
    "\n",
    "plt.title('Sentiment Analysis Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = np.trace(confusion) / np.sum(confusion)\n",
    "print(f\"‚úÖ Overall Accuracy: {accuracy:.1%}\")\n",
    "print(\"\\nüß† Model struggles most with:\")\n",
    "print(\"  - Neutral class (often predicted as Positive or Negative)\")\n",
    "print(\"  - This is common in sentiment analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## üìä Categorical Plots - Count & Compare\n",
    "\n",
    "### 9Ô∏è‚É£ Count Plot - Frequency Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI usage statistics\n",
    "np.random.seed(42)\n",
    "usage_data = pd.DataFrame({\n",
    "    'Model': np.random.choice(['GPT-4', 'Claude-3.5', 'Gemini-Pro', 'Llama-3'], 500, \n",
    "                             p=[0.4, 0.3, 0.2, 0.1]),\n",
    "    'Use_Case': np.random.choice(['Coding', 'Writing', 'Analysis', 'Chat'], 500)\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Count plot with hue\n",
    "sns.countplot(data=usage_data, x='Model', hue='Use_Case', palette='pastel')\n",
    "\n",
    "plt.title('LLM Usage by Model and Use Case', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Number of Requests', fontsize=12)\n",
    "plt.legend(title='Use Case', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Count plots are perfect for:\")\n",
    "print(\"  - Understanding class distribution (imbalanced data?)\")\n",
    "print(\"  - Usage statistics\")\n",
    "print(\"  - Categorical frequency analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### üîü Bar Plot with Error Bars (Statistical Confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model benchmarks with confidence intervals\n",
    "np.random.seed(42)\n",
    "benchmark_data = pd.DataFrame({\n",
    "    'Model': ['GPT-4']*20 + ['Claude-3.5']*20 + ['Gemini-Pro']*20 + ['Llama-3']*20,\n",
    "    'Score': np.concatenate([\n",
    "        np.random.normal(95, 2, 20),\n",
    "        np.random.normal(93, 2.5, 20),\n",
    "        np.random.normal(91, 3, 20),\n",
    "        np.random.normal(87, 3.5, 20)\n",
    "    ])\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar plot with confidence intervals (error bars)\n",
    "sns.barplot(data=benchmark_data, x='Model', y='Score', \n",
    "           palette='viridis', errorbar='sd')  # sd = standard deviation\n",
    "\n",
    "plt.title('LLM Benchmark Scores with Standard Deviation', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Benchmark Score', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylim(80, 100)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Error bars show variability:\")\n",
    "print(\"  - Small bars = consistent performance\")\n",
    "print(\"  - Large bars = unpredictable performance\")\n",
    "print(\"\\nüéØ This is crucial for model selection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## üé® Advanced: FacetGrid - Multiple Plots for Subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive AI training dataset\n",
    "np.random.seed(42)\n",
    "training_logs = pd.DataFrame({\n",
    "    'Epoch': np.tile(np.arange(1, 21), 12),\n",
    "    'Loss': np.concatenate([2.5 * np.exp(-0.1 * np.arange(1, 21)) + 0.1 * np.random.randn(20) for _ in range(12)]),\n",
    "    'Model': np.repeat(['GPT', 'BERT', 'T5'], 80),\n",
    "    'Dataset': np.tile(np.repeat(['Small', 'Medium', 'Large', 'XLarge'], 20), 3)\n",
    "})\n",
    "\n",
    "# FacetGrid - multiple subplots based on categories!\n",
    "g = sns.FacetGrid(training_logs, col='Dataset', row='Model', \n",
    "                 height=3, aspect=1.2, margin_titles=True)\n",
    "g.map(sns.lineplot, 'Epoch', 'Loss', color='steelblue', linewidth=2)\n",
    "g.set_axis_labels('Epoch', 'Training Loss')\n",
    "g.set_titles(row_template='{row_name}', col_template='{col_name} Dataset')\n",
    "g.fig.suptitle('Training Loss Across Models and Dataset Sizes', \n",
    "              fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"üî• FacetGrid is POWERFUL for:\")\n",
    "print(\"  - Comparing across multiple dimensions\")\n",
    "print(\"  - Spotting patterns by category\")\n",
    "print(\"  - Creating publication-ready figure panels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## üéØ Real AI Example: RAG System Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG retrieval performance data\n",
    "np.random.seed(42)\n",
    "rag_data = pd.DataFrame({\n",
    "    'Chunk_Size': np.tile([128, 256, 512, 1024], 100),\n",
    "    'Retrieval_Accuracy': np.concatenate([\n",
    "        np.random.beta(6, 2, 100),\n",
    "        np.random.beta(7, 2, 100),\n",
    "        np.random.beta(8, 2, 100),\n",
    "        np.random.beta(7, 3, 100)\n",
    "    ]),\n",
    "    'Response_Time_ms': np.concatenate([\n",
    "        np.random.gamma(2, 100, 100),\n",
    "        np.random.gamma(2.5, 120, 100),\n",
    "        np.random.gamma(3, 150, 100),\n",
    "        np.random.gamma(4, 180, 100)\n",
    "    ]),\n",
    "    'Embedding_Model': np.tile(np.repeat(['OpenAI', 'Cohere'], 200), 1)\n",
    "})\n",
    "\n",
    "# Create comprehensive analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('üîç RAG System Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Retrieval accuracy by chunk size\n",
    "sns.boxplot(data=rag_data, x='Chunk_Size', y='Retrieval_Accuracy', ax=axes[0, 0], palette='Set2')\n",
    "axes[0, 0].set_title('Retrieval Accuracy vs Chunk Size')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "\n",
    "# Plot 2: Response time by chunk size\n",
    "sns.violinplot(data=rag_data, x='Chunk_Size', y='Response_Time_ms', ax=axes[0, 1], palette='muted')\n",
    "axes[0, 1].set_title('Response Time Distribution')\n",
    "axes[0, 1].set_ylabel('Response Time (ms)')\n",
    "\n",
    "# Plot 3: Accuracy vs Time scatter\n",
    "sns.scatterplot(data=rag_data, x='Response_Time_ms', y='Retrieval_Accuracy', \n",
    "               hue='Embedding_Model', style='Embedding_Model', s=50, alpha=0.5, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Accuracy vs Speed Trade-off')\n",
    "axes[1, 0].set_xlabel('Response Time (ms)')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "\n",
    "# Plot 4: Model comparison\n",
    "sns.barplot(data=rag_data, x='Embedding_Model', y='Retrieval_Accuracy', \n",
    "           hue='Chunk_Size', ax=axes[1, 1], palette='viridis')\n",
    "axes[1, 1].set_title('Performance by Embedding Model')\n",
    "axes[1, 1].set_ylabel('Retrieval Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Key insights for RAG optimization:\")\n",
    "print(\"  - 512 tokens = best accuracy/speed balance\")\n",
    "print(\"  - Larger chunks = slower but not always better\")\n",
    "print(\"  - Choose embedding model based on your needs\")\n",
    "print(\"\\n‚ú® This is how you optimize production RAG systems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## üéØ MINI CHALLENGE: Analyze Your Own AI Dataset\n",
    "\n",
    "**Scenario:** You're analyzing a chatbot's performance across different topics!\n",
    "\n",
    "**Your Task:** Create a comprehensive analysis with:\n",
    "1. Response time distribution by topic\n",
    "2. User satisfaction correlation with response length\n",
    "3. Topic popularity count plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chatbot interaction dataset\n",
    "np.random.seed(100)\n",
    "chatbot_data = pd.DataFrame({\n",
    "    'Topic': np.random.choice(['Tech', 'Health', 'Finance', 'Education', 'Entertainment'], 300),\n",
    "    'Response_Time_s': np.random.gamma(2, 1.5, 300),\n",
    "    'Response_Length_words': np.random.randint(20, 300, 300),\n",
    "    'User_Satisfaction': np.random.randint(1, 6, 300),\n",
    "    'Model_Confidence': np.random.beta(7, 2, 300)\n",
    "})\n",
    "\n",
    "# Add some correlation: longer responses = higher satisfaction (with noise)\n",
    "chatbot_data['User_Satisfaction'] = (chatbot_data['User_Satisfaction'] + \n",
    "                                     (chatbot_data['Response_Length_words'] / 100)).clip(1, 5).astype(int)\n",
    "\n",
    "# TODO: Create your analysis!\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('ü§ñ Chatbot Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Response time by topic\n",
    "sns.violinplot(data=chatbot_data, x='Topic', y='Response_Time_s', ax=axes[0, 0], palette='pastel')\n",
    "axes[0, 0].set_title('Response Time Distribution by Topic')\n",
    "axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Plot 2: Satisfaction vs Length\n",
    "sns.scatterplot(data=chatbot_data, x='Response_Length_words', y='User_Satisfaction', \n",
    "               hue='Topic', alpha=0.6, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('User Satisfaction vs Response Length')\n",
    "\n",
    "# Plot 3: Topic popularity\n",
    "sns.countplot(data=chatbot_data, x='Topic', palette='Set2', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Query Count by Topic')\n",
    "axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Plot 4: Confidence vs Satisfaction heatmap\n",
    "pivot_data = chatbot_data.groupby(['Topic', 'User_Satisfaction']).size().unstack(fill_value=0)\n",
    "sns.heatmap(pivot_data, annot=True, fmt='d', cmap='YlGnBu', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Satisfaction Distribution by Topic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Analysis complete!\")\n",
    "print(f\"Average satisfaction: {chatbot_data['User_Satisfaction'].mean():.2f}/5\")\n",
    "print(f\"Most popular topic: {chatbot_data['Topic'].value_counts().index[0]}\")\n",
    "print(f\"Average response time: {chatbot_data['Response_Time_s'].mean():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- ‚úÖ Why Seaborn is perfect for statistical visualization\n",
    "- ‚úÖ Distribution plots (histogram, KDE, box, violin)\n",
    "- ‚úÖ Relationship plots (scatter, joint, pair plots)\n",
    "- ‚úÖ Heatmaps (correlation, confusion matrices)\n",
    "- ‚úÖ Categorical plots (count, bar with error bars)\n",
    "- ‚úÖ Advanced FacetGrid for multi-dimensional analysis\n",
    "- ‚úÖ Real AI examples (RAG optimization, model comparison)\n",
    "- ‚úÖ Publication-ready styling\n",
    "\n",
    "**üéØ Seaborn Cheat Sheet:**\n",
    "```python\n",
    "# Distribution\n",
    "sns.histplot()    # Histogram + KDE\n",
    "sns.boxplot()     # Box plot (outliers)\n",
    "sns.violinplot()  # Distribution shape\n",
    "\n",
    "# Relationships\n",
    "sns.scatterplot() # Scatter with groups\n",
    "sns.jointplot()   # 2D + marginals\n",
    "sns.pairplot()    # All relationships\n",
    "\n",
    "# Categorical\n",
    "sns.countplot()   # Frequency bars\n",
    "sns.barplot()     # Mean + error bars\n",
    "\n",
    "# Matrix\n",
    "sns.heatmap()     # Correlation, confusion\n",
    "sns.clustermap()  # Hierarchical clustering\n",
    "\n",
    "# Multi-plot\n",
    "sns.FacetGrid()   # Subplots by category\n",
    "```\n",
    "\n",
    "**üéØ Practice Exercise:**\n",
    "\n",
    "Analyze an image classification model:\n",
    "1. Create confusion matrix for 5 classes\n",
    "2. Show prediction confidence distribution by class\n",
    "3. Correlation between model confidence and accuracy\n",
    "4. Compare multiple models with box plots\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Next Lesson:** Day 3 - Interactive Visualizations (Plotly & Dashboards!)\n",
    "\n",
    "**üí° Fun Fact:** Every AI research paper uses Seaborn! Check papers from NeurIPS, ICML, ICLR - they all use these exact techniques.\n",
    "\n",
    "---\n",
    "\n",
    "*You now create visualizations worthy of top AI conferences!* üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
