{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Day 3: Statistical Testing for AI/ML\n",
    "\n",
    "**üéØ Goal:** Learn to compare models scientifically and make data-driven decisions\n",
    "\n",
    "**‚è±Ô∏è Time:** 45-60 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- How do you know if Model A is REALLY better than Model B?\n",
    "- Is the difference due to skill or just random luck?\n",
    "- Statistical testing provides the answer!\n",
    "- Essential for A/B testing, model deployment decisions, research papers\n",
    "- Critical for 2024-2025 AI: Comparing RAG systems, Agentic AI performance, Multimodal models\n",
    "\n",
    "**Real examples today:**\n",
    "- A/B test two LLM prompts\n",
    "- Compare RAG retrieval systems statistically\n",
    "- Determine if model improvements are significant\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö What is Statistical Testing?\n",
    "\n",
    "**Statistical Testing** answers the question: *\"Is this difference real or just random chance?\"*\n",
    "\n",
    "**Everyday example:**\n",
    "- You flip a coin 10 times, get 7 heads and 3 tails\n",
    "- Is the coin unfair or did you just get lucky?\n",
    "- Statistical testing tells you!\n",
    "\n",
    "**AI example:**\n",
    "- Model A: 85% accuracy\n",
    "- Model B: 87% accuracy\n",
    "- Is Model B REALLY better or just got lucky on this test set?\n",
    "- Statistical testing tells you!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "# Make plots look nice\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 1: Hypothesis Testing Fundamentals\n",
    "\n",
    "**The Scientific Method for AI:**\n",
    "\n",
    "1. **Null Hypothesis (H‚ÇÄ):** \"No difference\" - the boring answer\n",
    "   - Example: \"Both models have the same accuracy\"\n",
    "\n",
    "2. **Alternative Hypothesis (H‚ÇÅ):** \"There IS a difference\" - what we want to prove\n",
    "   - Example: \"Model B is better than Model A\"\n",
    "\n",
    "3. **P-value:** Probability of seeing results this extreme if H‚ÇÄ is true\n",
    "   - Low p-value (< 0.05) = Reject H‚ÇÄ = difference is REAL\n",
    "   - High p-value (> 0.05) = Can't reject H‚ÇÄ = might be random\n",
    "\n",
    "4. **Significance Level (Œ±):** Usually 0.05 (5%)\n",
    "   - If p-value < 0.05 ‚Üí \"statistically significant\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example: Is This Model Better Than Random?\n",
    "\n",
    "**Scenario:** Binary classifier (yes/no prediction). Random guessing = 50% accuracy.\n",
    "\n",
    "Your model got 60% accuracy on 100 examples. Is it REALLY better than random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "n_samples = 100\n",
    "correct_predictions = 60  # 60% accuracy\n",
    "null_hypothesis_probability = 0.5  # Random guessing = 50%\n",
    "\n",
    "# Binomial test: Is 60/100 significantly different from 50/100?\n",
    "p_value = stats.binom_test(correct_predictions, n_samples, null_hypothesis_probability, alternative='greater')\n",
    "\n",
    "print(\"üéØ HYPOTHESIS TEST: Is Model Better Than Random?\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nH‚ÇÄ (Null):        Model is just guessing randomly (50%)\")\n",
    "print(f\"H‚ÇÅ (Alternative): Model is better than random\")\n",
    "print(f\"\\nObserved accuracy: {correct_predictions}/{n_samples} = {correct_predictions/n_samples:.0%}\")\n",
    "print(f\"Expected (random): {null_hypothesis_probability:.0%}\")\n",
    "print(f\"\\nP-value: {p_value:.4f}\")\n",
    "print(f\"Significance level: 0.05\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úÖ CONCLUSION: Reject H‚ÇÄ\")\n",
    "    print(f\"   The model IS significantly better than random! (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå CONCLUSION: Cannot reject H‚ÇÄ\")\n",
    "    print(f\"   The model might just be getting lucky (p > 0.05)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the P-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate what we'd expect from random guessing\n",
    "np.random.seed(42)\n",
    "random_results = np.random.binomial(n_samples, 0.5, 10000)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(random_results, bins=30, color='lightblue', edgecolor='black', alpha=0.7, density=True)\n",
    "plt.axvline(50, color='green', linestyle='--', linewidth=2, label='Expected (random guessing = 50)')\n",
    "plt.axvline(correct_predictions, color='red', linestyle='--', linewidth=2, label=f'Our model = {correct_predictions}')\n",
    "plt.xlabel('Number of Correct Predictions (out of 100)', fontsize=12)\n",
    "plt.ylabel('Probability Density', fontsize=12)\n",
    "plt.title('Distribution Under Null Hypothesis (Random Guessing)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üí° The red line shows our model's performance\")\n",
    "print(f\"üí° It's in the tail of the distribution ‚Üí unlikely to happen by chance\")\n",
    "print(f\"üí° P-value = area in the tail beyond the red line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Test Your Model\n",
    "\n",
    "**Scenario:** You built a sentiment classifier. Test if it's better than random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your classifier's results\n",
    "n_reviews = 200\n",
    "correct_classifications = 130  # 65% accuracy\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Run binomial test (random = 50%)\n",
    "p_value = stats.binom_test(correct_classifications, n_reviews, 0.5, alternative='greater')\n",
    "\n",
    "# 2. Print results\n",
    "print(f\"Observed: {correct_classifications}/{n_reviews} = {correct_classifications/n_reviews:.0%}\")\n",
    "print(f\"P-value: {p_value:.6f}\")\n",
    "\n",
    "# 3. Interpret (is p < 0.05?)\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úÖ Statistically significant! Your classifier works!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Not significant. Might need more training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 2: Comparing Two Models (T-Test)\n",
    "\n",
    "**Most common scenario in AI:** Which of two models is better?\n",
    "\n",
    "**T-test** compares the means of two groups.\n",
    "\n",
    "**Types:**\n",
    "- **Paired t-test:** Same data, two different models (MOST COMMON in ML)\n",
    "- **Independent t-test:** Different datasets\n",
    "\n",
    "Let's compare two models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Two models tested on 30 different tasks\n",
    "np.random.seed(42)\n",
    "\n",
    "# Model A: Older model\n",
    "model_a_scores = np.random.normal(loc=0.82, scale=0.05, size=30)  # Mean 82%, std 5%\n",
    "\n",
    "# Model B: Your new model (slightly better)\n",
    "model_b_scores = np.random.normal(loc=0.86, scale=0.05, size=30)  # Mean 86%, std 5%\n",
    "\n",
    "# Perform paired t-test\n",
    "t_statistic, p_value = stats.ttest_rel(model_b_scores, model_a_scores)\n",
    "\n",
    "print(\"ü§ñ COMPARING TWO AI MODELS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel A (baseline):\")\n",
    "print(f\"  Mean: {np.mean(model_a_scores):.1%}\")\n",
    "print(f\"  Std:  {np.std(model_a_scores):.1%}\")\n",
    "\n",
    "print(f\"\\nModel B (new model):\")\n",
    "print(f\"  Mean: {np.mean(model_b_scores):.1%}\")\n",
    "print(f\"  Std:  {np.std(model_b_scores):.1%}\")\n",
    "\n",
    "print(f\"\\nDifference: {np.mean(model_b_scores) - np.mean(model_a_scores):.1%}\")\n",
    "\n",
    "print(f\"\\nüìä Statistical Test:\")\n",
    "print(f\"  T-statistic: {t_statistic:.3f}\")\n",
    "print(f\"  P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úÖ SIGNIFICANT! Model B is statistically better than Model A\")\n",
    "    print(f\"   Deploy Model B with confidence!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå NOT SIGNIFICANT. Difference might be due to chance\")\n",
    "    print(f\"   Need more testing or the improvement is too small\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "axes[0].boxplot([model_a_scores, model_b_scores], labels=['Model A', 'Model B'])\n",
    "axes[0].set_ylabel('Accuracy Score', fontsize=12)\n",
    "axes[0].set_title('Model Performance Comparison\\n(Box Plot)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Histogram overlay\n",
    "axes[1].hist(model_a_scores, bins=15, alpha=0.6, label='Model A', color='blue', edgecolor='black')\n",
    "axes[1].hist(model_b_scores, bins=15, alpha=0.6, label='Model B', color='orange', edgecolor='black')\n",
    "axes[1].axvline(np.mean(model_a_scores), color='blue', linestyle='--', linewidth=2)\n",
    "axes[1].axvline(np.mean(model_b_scores), color='orange', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Accuracy Score', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Score Distributions\\n(Histogram)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üí° Even though Model B appears better visually...\")\n",
    "print(f\"üí° We need statistical testing to be SURE it's not just luck!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Compare RAG Systems\n",
    "\n",
    "**Scenario:** You're testing two RAG (Retrieval-Augmented Generation) configurations.\n",
    "\n",
    "Which one retrieves more relevant documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two RAG systems tested on 25 queries\n",
    "np.random.seed(123)\n",
    "\n",
    "# RAG System 1: Basic embedding\n",
    "rag_system_1 = np.random.normal(loc=0.75, scale=0.08, size=25)\n",
    "\n",
    "# RAG System 2: Fine-tuned embedding\n",
    "rag_system_2 = np.random.normal(loc=0.80, scale=0.08, size=25)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Calculate means\n",
    "mean_1 = np.mean(rag_system_1)\n",
    "mean_2 = np.mean(rag_system_2)\n",
    "\n",
    "# 2. Run paired t-test\n",
    "t_stat, p_val = stats.ttest_rel(rag_system_2, rag_system_1)\n",
    "\n",
    "# 3. Print results\n",
    "print(f\"RAG System 1 (Basic):      {mean_1:.1%}\")\n",
    "print(f\"RAG System 2 (Fine-tuned): {mean_2:.1%}\")\n",
    "print(f\"\\nImprovement: {mean_2 - mean_1:.1%}\")\n",
    "print(f\"P-value: {p_val:.4f}\")\n",
    "\n",
    "# 4. Decide which to use\n",
    "if p_val < 0.05:\n",
    "    print(f\"\\n‚úÖ Use RAG System 2! Significantly better (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"\\nü§î No significant difference. Stick with System 1 (simpler)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 3: Confidence Intervals\n",
    "\n",
    "**Confidence Interval:** Range where the true value likely falls\n",
    "\n",
    "**95% Confidence Interval:** We're 95% confident the true value is in this range\n",
    "\n",
    "**Why it matters:**\n",
    "- Model shows 85% accuracy ‚Üí but true accuracy might be 82-88%\n",
    "- Confidence intervals show uncertainty\n",
    "- Critical for reporting model performance honestly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Model tested on 100 samples\n",
    "np.random.seed(42)\n",
    "model_scores = np.random.normal(loc=0.85, scale=0.10, size=100)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "mean_score = np.mean(model_scores)\n",
    "std_error = stats.sem(model_scores)  # Standard error of the mean\n",
    "confidence_interval = stats.t.interval(\n",
    "    confidence=0.95,\n",
    "    df=len(model_scores)-1,\n",
    "    loc=mean_score,\n",
    "    scale=std_error\n",
    ")\n",
    "\n",
    "print(\"üìä MODEL PERFORMANCE WITH CONFIDENCE INTERVAL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMean accuracy: {mean_score:.1%}\")\n",
    "print(f\"\\n95% Confidence Interval: [{confidence_interval[0]:.1%}, {confidence_interval[1]:.1%}]\")\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"   We are 95% confident the TRUE accuracy is between\")\n",
    "print(f\"   {confidence_interval[0]:.1%} and {confidence_interval[1]:.1%}\")\n",
    "print(f\"\\nüí° When reporting to stakeholders:\")\n",
    "print(f\"   DON'T say: 'The model is {mean_score:.1%} accurate'\")\n",
    "print(f\"   DO say: 'The model is {mean_score:.1%} accurate (95% CI: {confidence_interval[0]:.1%}-{confidence_interval[1]:.1%})'\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 3 models with confidence intervals\n",
    "np.random.seed(42)\n",
    "\n",
    "models = {\n",
    "    'Model A\\n(Baseline)': np.random.normal(0.80, 0.08, 50),\n",
    "    'Model B\\n(Improved)': np.random.normal(0.85, 0.08, 50),\n",
    "    'Model C\\n(SOTA)': np.random.normal(0.88, 0.08, 50),\n",
    "}\n",
    "\n",
    "# Calculate means and CIs\n",
    "means = []\n",
    "cis = []\n",
    "names = []\n",
    "\n",
    "for name, scores in models.items():\n",
    "    mean = np.mean(scores)\n",
    "    se = stats.sem(scores)\n",
    "    ci = stats.t.interval(0.95, len(scores)-1, loc=mean, scale=se)\n",
    "    \n",
    "    names.append(name)\n",
    "    means.append(mean)\n",
    "    cis.append((ci[1] - mean, mean - ci[0]))  # Error bars\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_pos = np.arange(len(names))\n",
    "plt.errorbar(x_pos, means, yerr=np.array(cis).T, fmt='o', markersize=10, \n",
    "             capsize=10, capthick=2, linewidth=2, color='darkblue')\n",
    "plt.xticks(x_pos, names, fontsize=11)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Comparison with 95% Confidence Intervals', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.axhline(y=0.85, color='red', linestyle='--', alpha=0.3, label='Target: 85%')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Error bars show uncertainty\")\n",
    "print(\"üí° If confidence intervals DON'T overlap ‚Üí significant difference\")\n",
    "print(\"üí° If they DO overlap ‚Üí difference might not be significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 4: A/B Testing for AI Models\n",
    "\n",
    "**A/B Testing:** Compare two versions in real-world conditions\n",
    "\n",
    "**Common in AI:**\n",
    "- Testing two different prompts for LLMs\n",
    "- Comparing two RAG configurations\n",
    "- Testing two recommendation algorithms\n",
    "- Evaluating two chatbot personalities\n",
    "\n",
    "**Let's run a real A/B test!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: A/B Testing LLM Prompts\n",
    "\n",
    "**Scenario:** You have two prompts for a customer service chatbot.\n",
    "\n",
    "Which one gets better user satisfaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate A/B test data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Group A: Standard prompt (500 users)\n",
    "# Satisfaction score: 1-5 stars\n",
    "group_a_satisfaction = np.random.choice([1, 2, 3, 4, 5], size=500, p=[0.05, 0.10, 0.25, 0.40, 0.20])\n",
    "\n",
    "# Group B: Improved prompt (500 users)\n",
    "# Slightly higher satisfaction\n",
    "group_b_satisfaction = np.random.choice([1, 2, 3, 4, 5], size=500, p=[0.03, 0.07, 0.20, 0.45, 0.25])\n",
    "\n",
    "# Calculate metrics\n",
    "mean_a = np.mean(group_a_satisfaction)\n",
    "mean_b = np.mean(group_b_satisfaction)\n",
    "\n",
    "# Statistical test\n",
    "t_stat, p_value = stats.ttest_ind(group_b_satisfaction, group_a_satisfaction)\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "pooled_std = np.sqrt((np.std(group_a_satisfaction)**2 + np.std(group_b_satisfaction)**2) / 2)\n",
    "cohens_d = (mean_b - mean_a) / pooled_std\n",
    "\n",
    "print(\"üß™ A/B TEST RESULTS: LLM Prompt Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nGroup A (Standard Prompt):\")\n",
    "print(f\"  Users: {len(group_a_satisfaction)}\")\n",
    "print(f\"  Average satisfaction: {mean_a:.2f} / 5 stars\")\n",
    "print(f\"  Std: {np.std(group_a_satisfaction):.2f}\")\n",
    "\n",
    "print(f\"\\nGroup B (Improved Prompt):\")\n",
    "print(f\"  Users: {len(group_b_satisfaction)}\")\n",
    "print(f\"  Average satisfaction: {mean_b:.2f} / 5 stars\")\n",
    "print(f\"  Std: {np.std(group_b_satisfaction):.2f}\")\n",
    "\n",
    "print(f\"\\nüìä Statistical Analysis:\")\n",
    "print(f\"  Difference: +{mean_b - mean_a:.2f} stars ({(mean_b - mean_a)/mean_a * 100:.1f}% improvement)\")\n",
    "print(f\"  P-value: {p_value:.4f}\")\n",
    "print(f\"  Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ DECISION:\")\n",
    "if p_value < 0.05:\n",
    "    print(f\"  ‚úÖ STATISTICALLY SIGNIFICANT! (p < 0.05)\")\n",
    "    print(f\"  ‚úÖ Improved prompt is REALLY better\")\n",
    "    print(f\"  ‚úÖ RECOMMENDATION: Deploy improved prompt to all users\")\n",
    "else:\n",
    "    print(f\"  ‚ùå NOT significant (p > 0.05)\")\n",
    "    print(f\"  ‚ùå Difference might be random\")\n",
    "    print(f\"  ‚ùå RECOMMENDATION: Keep testing or stick with standard prompt\")\n",
    "\n",
    "# Interpret effect size\n",
    "if abs(cohens_d) < 0.2:\n",
    "    effect_interpretation = \"small\"\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    effect_interpretation = \"medium\"\n",
    "else:\n",
    "    effect_interpretation = \"large\"\n",
    "\n",
    "print(f\"\\nüí° Effect size is {effect_interpretation} (d = {cohens_d:.3f})\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing A/B Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Distribution comparison\n",
    "axes[0].hist(group_a_satisfaction, bins=5, alpha=0.6, label='Group A (Standard)', \n",
    "             color='blue', edgecolor='black', density=True)\n",
    "axes[0].hist(group_b_satisfaction, bins=5, alpha=0.6, label='Group B (Improved)', \n",
    "             color='orange', edgecolor='black', density=True)\n",
    "axes[0].set_xlabel('Satisfaction (1-5 stars)', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Distribution Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Box plot\n",
    "axes[1].boxplot([group_a_satisfaction, group_b_satisfaction], \n",
    "                labels=['Group A\\n(Standard)', 'Group B\\n(Improved)'])\n",
    "axes[1].set_ylabel('Satisfaction (1-5 stars)', fontsize=11)\n",
    "axes[1].set_title('Box Plot Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Mean comparison with error bars\n",
    "se_a = stats.sem(group_a_satisfaction)\n",
    "se_b = stats.sem(group_b_satisfaction)\n",
    "ci_a = stats.t.interval(0.95, len(group_a_satisfaction)-1, loc=mean_a, scale=se_a)\n",
    "ci_b = stats.t.interval(0.95, len(group_b_satisfaction)-1, loc=mean_b, scale=se_b)\n",
    "\n",
    "groups = ['Group A\\n(Standard)', 'Group B\\n(Improved)']\n",
    "means_ab = [mean_a, mean_b]\n",
    "errors = [(mean_a - ci_a[0], ci_a[1] - mean_a), (mean_b - ci_b[0], ci_b[1] - mean_b)]\n",
    "\n",
    "axes[2].bar(groups, means_ab, yerr=np.array(errors).T, capsize=10, \n",
    "            color=['blue', 'orange'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[2].set_ylabel('Average Satisfaction', fontsize=11)\n",
    "axes[2].set_title('Mean with 95% CI', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylim(0, 5)\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, (group, mean) in enumerate(zip(groups, means_ab)):\n",
    "    axes[2].text(i, mean + 0.15, f'{mean:.2f}', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: A/B Test Two RAG Configurations\n",
    "\n",
    "**Scenario:** Testing two RAG chunk sizes for retrieval quality.\n",
    "\n",
    "Run a complete A/B test analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B test data: Relevance scores (0-100)\n",
    "np.random.seed(456)\n",
    "\n",
    "# Configuration A: 512 token chunks (200 queries)\n",
    "config_a_scores = np.random.normal(loc=78, scale=12, size=200)\n",
    "\n",
    "# Configuration B: 256 token chunks (200 queries)\n",
    "config_b_scores = np.random.normal(loc=82, scale=12, size=200)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Calculate means\n",
    "mean_config_a = np.mean(config_a_scores)\n",
    "mean_config_b = np.mean(config_b_scores)\n",
    "\n",
    "# 2. Run t-test\n",
    "t_stat, p_val = stats.ttest_ind(config_b_scores, config_a_scores)\n",
    "\n",
    "# 3. Calculate effect size\n",
    "pooled_std = np.sqrt((np.std(config_a_scores)**2 + np.std(config_b_scores)**2) / 2)\n",
    "cohens_d = (mean_config_b - mean_config_a) / pooled_std\n",
    "\n",
    "# 4. Print comprehensive results\n",
    "print(\"üîç RAG CONFIGURATION A/B TEST\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nConfig A (512 tokens): {mean_config_a:.1f}\")\n",
    "print(f\"Config B (256 tokens): {mean_config_b:.1f}\")\n",
    "print(f\"\\nImprovement: {mean_config_b - mean_config_a:.1f} points\")\n",
    "print(f\"P-value: {p_val:.4f}\")\n",
    "print(f\"Effect size: {cohens_d:.3f}\")\n",
    "\n",
    "# 5. Make recommendation\n",
    "if p_val < 0.05:\n",
    "    print(f\"\\n‚úÖ Use Config B (256 tokens)! Significantly better.\")\n",
    "else:\n",
    "    print(f\"\\nü§î No significant difference. Use Config A (simpler).\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ REAL AI EXAMPLE: Comparing Multimodal Models Statistically\n",
    "\n",
    "**Scenario:** You're evaluating 3 multimodal models (vision + text) for image captioning.\n",
    "\n",
    "Let's perform a complete statistical comparison!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic multimodal model performance data\n",
    "np.random.seed(42)\n",
    "n_images = 100\n",
    "\n",
    "# Model 1: CLIP-based (baseline)\n",
    "clip_scores = np.random.normal(loc=72, scale=8, size=n_images)\n",
    "\n",
    "# Model 2: Fine-tuned BLIP\n",
    "blip_scores = np.random.normal(loc=78, scale=7, size=n_images)\n",
    "\n",
    "# Model 3: Custom transformer\n",
    "custom_scores = np.random.normal(loc=81, scale=9, size=n_images)\n",
    "\n",
    "models_data = {\n",
    "    'CLIP (Baseline)': clip_scores,\n",
    "    'BLIP (Fine-tuned)': blip_scores,\n",
    "    'Custom Transformer': custom_scores\n",
    "}\n",
    "\n",
    "print(\"üñºÔ∏è MULTIMODAL MODEL COMPARISON (Image Captioning)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTest set: {n_images} images\")\n",
    "print(f\"Metric: BLEU score (0-100)\\n\")\n",
    "\n",
    "# Calculate statistics for each model\n",
    "results = {}\n",
    "for model_name, scores in models_data.items():\n",
    "    mean = np.mean(scores)\n",
    "    std = np.std(scores)\n",
    "    se = stats.sem(scores)\n",
    "    ci = stats.t.interval(0.95, len(scores)-1, loc=mean, scale=se)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'ci': ci,\n",
    "        'scores': scores\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Mean BLEU: {mean:.2f}\")\n",
    "    print(f\"  Std Dev:   {std:.2f}\")\n",
    "    print(f\"  95% CI:    [{ci[0]:.2f}, {ci[1]:.2f}]\\n\")\n",
    "\n",
    "# Pairwise comparisons\n",
    "print(\"\\nüìä PAIRWISE STATISTICAL TESTS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparisons = [\n",
    "    ('BLIP (Fine-tuned)', 'CLIP (Baseline)'),\n",
    "    ('Custom Transformer', 'CLIP (Baseline)'),\n",
    "    ('Custom Transformer', 'BLIP (Fine-tuned)')\n",
    "]\n",
    "\n",
    "for model1, model2 in comparisons:\n",
    "    t_stat, p_val = stats.ttest_rel(results[model1]['scores'], results[model2]['scores'])\n",
    "    diff = results[model1]['mean'] - results[model2]['mean']\n",
    "    \n",
    "    print(f\"\\n{model1} vs {model2}:\")\n",
    "    print(f\"  Difference: {diff:+.2f} points\")\n",
    "    print(f\"  T-statistic: {t_stat:.3f}\")\n",
    "    print(f\"  P-value: {p_val:.4f}\")\n",
    "    \n",
    "    if p_val < 0.05:\n",
    "        winner = model1 if diff > 0 else model2\n",
    "        print(f\"  ‚úÖ {winner} is SIGNIFICANTLY better (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå No significant difference (p > 0.05)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nüéØ FINAL RECOMMENDATION:\")\n",
    "best_model = max(results.items(), key=lambda x: x[1]['mean'])[0]\n",
    "print(f\"\\n  Deploy: {best_model}\")\n",
    "print(f\"  Mean BLEU: {results[best_model]['mean']:.2f}\")\n",
    "print(f\"  95% CI: [{results[best_model]['ci'][0]:.2f}, {results[best_model]['ci'][1]:.2f}]\")\n",
    "print(\"\\n  ‚úÖ Highest performance\")\n",
    "print(\"  ‚úÖ Statistically validated\")\n",
    "print(\"  ‚úÖ Ready for production\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication-ready visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Box plots\n",
    "axes[0, 0].boxplot([results[m]['scores'] for m in models_data.keys()], \n",
    "                    labels=['CLIP', 'BLIP', 'Custom'])\n",
    "axes[0, 0].set_ylabel('BLEU Score', fontsize=11)\n",
    "axes[0, 0].set_title('Score Distribution (Box Plot)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Violin plots (shows distribution shape)\n",
    "positions = [1, 2, 3]\n",
    "parts = axes[0, 1].violinplot([results[m]['scores'] for m in models_data.keys()], \n",
    "                               positions=positions, showmeans=True, showmedians=True)\n",
    "axes[0, 1].set_xticks(positions)\n",
    "axes[0, 1].set_xticklabels(['CLIP', 'BLIP', 'Custom'])\n",
    "axes[0, 1].set_ylabel('BLEU Score', fontsize=11)\n",
    "axes[0, 1].set_title('Score Distribution (Violin Plot)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Mean comparison with confidence intervals\n",
    "model_names = list(models_data.keys())\n",
    "means = [results[m]['mean'] for m in model_names]\n",
    "errors = [(results[m]['mean'] - results[m]['ci'][0], \n",
    "           results[m]['ci'][1] - results[m]['mean']) for m in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "axes[1, 0].bar(x_pos, means, yerr=np.array(errors).T, capsize=10, \n",
    "               color=['skyblue', 'lightcoral', 'lightgreen'], \n",
    "               edgecolor='black', linewidth=2, alpha=0.8)\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(['CLIP', 'BLIP', 'Custom'])\n",
    "axes[1, 0].set_ylabel('Mean BLEU Score', fontsize=11)\n",
    "axes[1, 0].set_title('Mean Performance with 95% CI', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, mean in enumerate(means):\n",
    "    axes[1, 0].text(i, mean + 2, f'{mean:.1f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 4. Overlapping histograms\n",
    "for model_name, color in zip(model_names, ['blue', 'red', 'green']):\n",
    "    axes[1, 1].hist(results[model_name]['scores'], bins=15, alpha=0.5, \n",
    "                    label=model_name.split()[0], color=color, edgecolor='black')\n",
    "    axes[1, 1].axvline(results[model_name]['mean'], color=color, \n",
    "                       linestyle='--', linewidth=2)\n",
    "\n",
    "axes[1, 1].set_xlabel('BLEU Score', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 1].set_title('Score Distributions (Histogram)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° This is how you present model comparisons in research papers!\")\n",
    "print(\"üí° Multiple visualizations + statistical tests = robust conclusions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Summary & Key Takeaways\n",
    "\n",
    "**You just learned:**\n",
    "\n",
    "### Hypothesis Testing:\n",
    "- ‚úÖ Null hypothesis (H‚ÇÄ) vs Alternative hypothesis (H‚ÇÅ)\n",
    "- ‚úÖ P-value: probability results are due to chance\n",
    "- ‚úÖ p < 0.05 = statistically significant\n",
    "- ‚úÖ Used to validate if improvements are real\n",
    "\n",
    "### Comparing Models:\n",
    "- ‚úÖ T-test compares means of two groups\n",
    "- ‚úÖ Paired t-test for same data, different models\n",
    "- ‚úÖ Independent t-test for different datasets\n",
    "- ‚úÖ Essential for choosing best model\n",
    "\n",
    "### Confidence Intervals:\n",
    "- ‚úÖ Range where true value likely falls\n",
    "- ‚úÖ 95% CI = 95% confident true value is in range\n",
    "- ‚úÖ Shows uncertainty in estimates\n",
    "- ‚úÖ Critical for honest reporting\n",
    "\n",
    "### A/B Testing:\n",
    "- ‚úÖ Compare two versions in production\n",
    "- ‚úÖ Used for prompts, models, configurations\n",
    "- ‚úÖ Statistical validation of improvements\n",
    "- ‚úÖ Effect size shows practical significance\n",
    "\n",
    "### Real Applications:\n",
    "- ‚úÖ Compared multimodal models scientifically\n",
    "- ‚úÖ A/B tested LLM prompts\n",
    "- ‚úÖ Validated RAG configurations\n",
    "- ‚úÖ Made data-driven deployment decisions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Final Challenge: Complete Model Evaluation\n",
    "\n",
    "**Scenario:** You're presenting to stakeholders. They want to know:\n",
    "1. Which model to deploy?\n",
    "2. Is it REALLY better?\n",
    "3. How confident are you?\n",
    "\n",
    "Perform a complete analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Agentic AI systems tested on 50 tasks\n",
    "np.random.seed(789)\n",
    "\n",
    "agent_current = np.random.normal(loc=75, scale=10, size=50)  # Current production agent\n",
    "agent_new = np.random.normal(loc=80, scale=10, size=50)      # New improved agent\n",
    "\n",
    "# YOUR COMPLETE ANALYSIS:\n",
    "\n",
    "print(\"ü§ñ AGENTIC AI SYSTEM EVALUATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Descriptive statistics\n",
    "print(\"\\n1Ô∏è‚É£ DESCRIPTIVE STATISTICS:\\n\")\n",
    "for name, scores in [('Current Agent', agent_current), ('New Agent', agent_new)]:\n",
    "    mean = np.mean(scores)\n",
    "    std = np.std(scores)\n",
    "    se = stats.sem(scores)\n",
    "    ci = stats.t.interval(0.95, len(scores)-1, loc=mean, scale=se)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean:   {mean:.2f}\")\n",
    "    print(f\"  Std:    {std:.2f}\")\n",
    "    print(f\"  95% CI: [{ci[0]:.2f}, {ci[1]:.2f}]\\n\")\n",
    "\n",
    "# 2. Statistical test\n",
    "print(\"2Ô∏è‚É£ STATISTICAL TEST (Paired T-Test):\\n\")\n",
    "t_stat, p_val = stats.ttest_rel(agent_new, agent_current)\n",
    "print(f\"  T-statistic: {t_stat:.3f}\")\n",
    "print(f\"  P-value: {p_val:.4f}\")\n",
    "print(f\"  Significant: {'YES ‚úÖ' if p_val < 0.05 else 'NO ‚ùå'}\\n\")\n",
    "\n",
    "# 3. Effect size\n",
    "print(\"3Ô∏è‚É£ EFFECT SIZE:\\n\")\n",
    "diff = np.mean(agent_new) - np.mean(agent_current)\n",
    "pooled_std = np.sqrt((np.std(agent_current)**2 + np.std(agent_new)**2) / 2)\n",
    "cohens_d = diff / pooled_std\n",
    "print(f\"  Improvement: {diff:.2f} points ({diff/np.mean(agent_current)*100:.1f}%)\")\n",
    "print(f\"  Cohen's d: {cohens_d:.3f}\")\n",
    "if abs(cohens_d) < 0.2:\n",
    "    print(f\"  Interpretation: Small effect\\n\")\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    print(f\"  Interpretation: Medium effect\\n\")\n",
    "else:\n",
    "    print(f\"  Interpretation: Large effect\\n\")\n",
    "\n",
    "# 4. Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].boxplot([agent_current, agent_new], labels=['Current', 'New'])\n",
    "axes[0].set_ylabel('Performance Score')\n",
    "axes[0].set_title('Agent Comparison', fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].hist(agent_current, bins=15, alpha=0.6, label='Current', color='blue', edgecolor='black')\n",
    "axes[1].hist(agent_new, bins=15, alpha=0.6, label='New', color='orange', edgecolor='black')\n",
    "axes[1].set_xlabel('Performance Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Score Distributions', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Final recommendation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"4Ô∏è‚É£ RECOMMENDATION FOR STAKEHOLDERS:\\n\")\n",
    "if p_val < 0.05:\n",
    "    print(\"  ‚úÖ DEPLOY NEW AGENT\")\n",
    "    print(f\"  ‚úÖ {diff:.1f} point improvement ({diff/np.mean(agent_current)*100:.1f}% better)\")\n",
    "    print(f\"  ‚úÖ Statistically significant (p = {p_val:.4f})\")\n",
    "    print(f\"  ‚úÖ 95% confident improvement is real\")\n",
    "    print(\"\\n  Expected impact: Better user experience, higher success rate\")\n",
    "else:\n",
    "    print(\"  ü§î KEEP CURRENT AGENT\")\n",
    "    print(f\"  ü§î Improvement not statistically significant (p = {p_val:.4f})\")\n",
    "    print(\"  ü§î Difference might be due to random variation\")\n",
    "    print(\"\\n  Recommendation: Collect more data or improve new agent further\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüí° This is how professionals make deployment decisions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Why This Matters in 2024-2025\n",
    "\n",
    "**Statistical testing is CRITICAL for modern AI:**\n",
    "\n",
    "1. **RAG Systems:**\n",
    "   - A/B test different chunk sizes\n",
    "   - Compare embedding models statistically\n",
    "   - Validate retrieval improvements\n",
    "   - Optimize re-ranking strategies\n",
    "\n",
    "2. **Agentic AI:**\n",
    "   - Compare agent architectures\n",
    "   - Validate decision-making improvements\n",
    "   - Test different prompting strategies\n",
    "   - Measure reliability statistically\n",
    "\n",
    "3. **Multimodal Models:**\n",
    "   - Compare fusion strategies\n",
    "   - Test different modality combinations\n",
    "   - Validate cross-modal improvements\n",
    "   - Optimize attention mechanisms\n",
    "\n",
    "4. **LLM Development:**\n",
    "   - A/B test prompts\n",
    "   - Compare fine-tuning approaches\n",
    "   - Validate RLHF improvements\n",
    "   - Test different sampling strategies\n",
    "\n",
    "**Bottom line:** Without statistical testing, you're guessing. With it, you KNOW! üéØ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Week 5 Complete!\n",
    "\n",
    "**üéâ Congratulations! You've mastered:**\n",
    "\n",
    "**Day 1: Descriptive Statistics**\n",
    "- Mean, median, mode\n",
    "- Variance and standard deviation\n",
    "- Distributions and histograms\n",
    "\n",
    "**Day 2: Probability Theory**\n",
    "- Probability fundamentals\n",
    "- Conditional probability and Bayes' Theorem\n",
    "- Probability distributions\n",
    "- Built a Naive Bayes classifier!\n",
    "\n",
    "**Day 3: Statistical Testing**\n",
    "- Hypothesis testing\n",
    "- T-tests and p-values\n",
    "- Confidence intervals\n",
    "- A/B testing for AI models\n",
    "\n",
    "**You can now:**\n",
    "- ‚úÖ Analyze any AI dataset scientifically\n",
    "- ‚úÖ Compare models statistically\n",
    "- ‚úÖ Make data-driven deployment decisions\n",
    "- ‚úÖ Present results with confidence\n",
    "- ‚úÖ Understand modern AI research papers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ What's Next?\n",
    "\n",
    "**Statistics is the foundation. Now you're ready for:**\n",
    "\n",
    "- **Week 6:** Linear Algebra (matrices, vectors, transformations)\n",
    "- **Week 7:** Calculus for ML (gradients, optimization)\n",
    "- **Week 8:** NumPy & Data Processing\n",
    "- **Phase 2:** Machine Learning Algorithms\n",
    "- **Phase 3:** Deep Learning & Neural Networks\n",
    "- **Phase 4:** Modern AI (Transformers, RAG, Agents!)\n",
    "\n",
    "**Keep going! You're building the foundation for an AI career!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**üí¨ Questions?** Review all three notebooks, practice with real datasets!\n",
    "\n",
    "*Remember: Statistics isn't just numbers - it's the scientific method for AI!* üìä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
