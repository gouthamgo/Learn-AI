{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ² Day 2: Probability Theory for AI/ML\n",
    "\n",
    "**ğŸ¯ Goal:** Master probability concepts that power modern AI systems\n",
    "\n",
    "**â±ï¸ Time:** 45-60 minutes\n",
    "\n",
    "**ğŸŒŸ Why This Matters for AI:**\n",
    "- ALL machine learning is built on probability theory\n",
    "- Neural networks output probabilities, not certainties\n",
    "- Bayesian methods power spam filters, recommendation systems, RAG systems\n",
    "- Understanding probability = understanding how AI \"thinks\"\n",
    "- Critical for 2024-2025 AI: Agentic AI decision-making, RAG relevance scoring, Multimodal fusion\n",
    "\n",
    "**Real examples today:**\n",
    "- Build a spam classifier using Naive Bayes\n",
    "- Understand RAG retrieval probability\n",
    "- Analyze multimodal model confidence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š What is Probability?\n",
    "\n",
    "**Probability** measures how likely something is to happen.\n",
    "\n",
    "**Scale:** 0 (impossible) to 1 (certain)\n",
    "- 0.0 = 0% chance (never happens)\n",
    "- 0.5 = 50% chance (coin flip)\n",
    "- 1.0 = 100% chance (always happens)\n",
    "\n",
    "**AI Example:**\n",
    "- Image classifier says: \"This is a cat with 0.95 probability\"\n",
    "- Means: 95% confident it's a cat, 5% uncertain\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Make plots look nice\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Part 1: Probability Fundamentals\n",
    "\n",
    "### Basic Probability Formula\n",
    "\n",
    "**P(Event) = Number of favorable outcomes / Total number of outcomes**\n",
    "\n",
    "Let's start with a simple example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: AI model predictions on 100 images\n",
    "predictions = ['cat'] * 60 + ['dog'] * 30 + ['bird'] * 10  # 60 cats, 30 dogs, 10 birds\n",
    "\n",
    "total = len(predictions)\n",
    "cat_count = predictions.count('cat')\n",
    "dog_count = predictions.count('dog')\n",
    "bird_count = predictions.count('bird')\n",
    "\n",
    "# Calculate probabilities\n",
    "p_cat = cat_count / total\n",
    "p_dog = dog_count / total\n",
    "p_bird = bird_count / total\n",
    "\n",
    "print(\"ğŸ–¼ï¸ Model Predictions on 100 Images:\")\n",
    "print(f\"\\nP(Cat)  = {cat_count}/{total} = {p_cat:.2f} ({p_cat:.0%})\")\n",
    "print(f\"P(Dog)  = {dog_count}/{total} = {p_dog:.2f} ({p_dog:.0%})\")\n",
    "print(f\"P(Bird) = {bird_count}/{total} = {p_bird:.2f} ({p_bird:.0%})\")\n",
    "print(f\"\\nâœ… Sum of all probabilities: {p_cat + p_dog + p_bird:.2f} (should always = 1.0!)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "classes = ['Cat', 'Dog', 'Bird']\n",
    "probabilities = [p_cat, p_dog, p_bird]\n",
    "plt.bar(classes, probabilities, color=['orange', 'brown', 'skyblue'], edgecolor='black')\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.title('Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1)\n",
    "for i, (c, p) in enumerate(zip(classes, probabilities)):\n",
    "    plt.text(i, p + 0.02, f'{p:.0%}', ha='center', fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Probability Rules\n",
    "\n",
    "1. **Range:** 0 â‰¤ P(Event) â‰¤ 1\n",
    "2. **Sum Rule:** All probabilities must sum to 1.0\n",
    "3. **Complement Rule:** P(not A) = 1 - P(A)\n",
    "\n",
    "Let's see these in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Spam detection\n",
    "p_spam = 0.30  # 30% of emails are spam\n",
    "p_not_spam = 1 - p_spam  # Complement rule\n",
    "\n",
    "print(\"ğŸ“§ Email Classification:\")\n",
    "print(f\"P(Spam)     = {p_spam:.0%}\")\n",
    "print(f\"P(Not Spam) = {p_not_spam:.0%}\")\n",
    "print(f\"\\nâœ… Sum: {p_spam + p_not_spam:.1f} âœ“\")\n",
    "print(f\"\\nğŸ’¡ If 30% are spam, then 70% are NOT spam (complement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ YOUR TURN: Calculate Probabilities\n",
    "\n",
    "**Scenario:** A RAG system retrieves 50 documents. Calculate retrieval probabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG retrieval results\n",
    "retrieval_results = ['relevant'] * 35 + ['partially_relevant'] * 10 + ['not_relevant'] * 5\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Calculate total\n",
    "total = len(retrieval_results)\n",
    "\n",
    "# 2. Count each category\n",
    "relevant_count = retrieval_results.count('relevant')\n",
    "partial_count = retrieval_results.count('partially_relevant')\n",
    "not_relevant_count = retrieval_results.count('not_relevant')\n",
    "\n",
    "# 3. Calculate probabilities\n",
    "p_relevant = relevant_count / total\n",
    "p_partial = partial_count / total\n",
    "p_not_relevant = not_relevant_count / total\n",
    "\n",
    "# 4. Print results\n",
    "print(f\"P(Relevant)           = {p_relevant:.0%}\")\n",
    "print(f\"P(Partially Relevant) = {p_partial:.0%}\")\n",
    "print(f\"P(Not Relevant)       = {p_not_relevant:.0%}\")\n",
    "print(f\"\\nSum: {p_relevant + p_partial + p_not_relevant:.1f} âœ“\")\n",
    "print(f\"\\nğŸ’¡ Your RAG system has {p_relevant:.0%} precision - pretty good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— Part 2: Conditional Probability\n",
    "\n",
    "**Conditional Probability:** Probability of A given that B has happened\n",
    "\n",
    "**Notation:** P(A|B) - \"Probability of A given B\"\n",
    "\n",
    "**Formula:** P(A|B) = P(A and B) / P(B)\n",
    "\n",
    "**AI Example:** What's the probability an email is spam GIVEN it contains the word \"free\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Email spam detection\n",
    "# We analyzed 1000 emails\n",
    "\n",
    "total_emails = 1000\n",
    "spam_emails = 300        # 300 are spam\n",
    "contains_free = 400      # 400 contain \"free\"\n",
    "spam_and_free = 250      # 250 are spam AND contain \"free\"\n",
    "\n",
    "# Calculate probabilities\n",
    "p_spam = spam_emails / total_emails\n",
    "p_free = contains_free / total_emails\n",
    "p_spam_and_free = spam_and_free / total_emails\n",
    "\n",
    "# Conditional probability: P(Spam | Contains \"free\")\n",
    "p_spam_given_free = p_spam_and_free / p_free\n",
    "\n",
    "print(\"ğŸ“§ Email Analysis (1000 emails):\")\n",
    "print(f\"\\nP(Spam)                    = {p_spam:.0%}\")\n",
    "print(f\"P(Contains 'free')         = {p_free:.0%}\")\n",
    "print(f\"P(Spam AND Contains 'free')= {p_spam_and_free:.0%}\")\n",
    "print(f\"\\nğŸ¯ P(Spam | Contains 'free') = {p_spam_given_free:.0%}\")\n",
    "print(f\"\\nğŸ’¡ If email contains 'free', it's {p_spam_given_free:.0%} likely to be spam!\")\n",
    "print(f\"ğŸ’¡ Compare to base rate of {p_spam:.0%} - 'free' is a strong spam indicator!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Conditional Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual representation\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create bars\n",
    "categories = ['Overall\\nSpam Rate', 'Spam Rate\\nGiven \"free\"']\n",
    "rates = [p_spam, p_spam_given_free]\n",
    "colors = ['lightcoral', 'darkred']\n",
    "\n",
    "bars = ax.bar(categories, rates, color=colors, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Probability', fontsize=12)\n",
    "ax.set_title('Impact of \"free\" on Spam Probability', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, rate in zip(bars, rates):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{rate:.0%}', ha='center', va='bottom', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"ğŸ’¡ The word 'free' increases spam probability by {(p_spam_given_free/p_spam - 1)*100:.0f}%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Part 3: Bayes' Theorem (The Foundation of AI!)\n",
    "\n",
    "**Bayes' Theorem** is one of the most important formulas in AI/ML!\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "P(A|B) = P(B|A) Ã— P(A) / P(B)\n",
    "```\n",
    "\n",
    "**In plain English:**\n",
    "- P(A|B) = Posterior probability (what we want to find)\n",
    "- P(B|A) = Likelihood (how likely is evidence given hypothesis)\n",
    "- P(A) = Prior probability (what we knew before seeing evidence)\n",
    "- P(B) = Evidence probability (normalizing constant)\n",
    "\n",
    "**AI Translation (Email Spam):**\n",
    "```\n",
    "P(Spam|\"free\") = P(\"free\"|Spam) Ã— P(Spam) / P(\"free\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Bayes' Theorem to calculate spam probability\n",
    "\n",
    "# Given information:\n",
    "p_spam = 0.30                    # Prior: 30% of emails are spam\n",
    "p_free_given_spam = 0.80         # Likelihood: 80% of spam contains \"free\"\n",
    "p_free_given_not_spam = 0.20     # 20% of legitimate emails contain \"free\"\n",
    "\n",
    "# Calculate P(\"free\") using law of total probability\n",
    "p_not_spam = 1 - p_spam\n",
    "p_free = (p_free_given_spam * p_spam) + (p_free_given_not_spam * p_not_spam)\n",
    "\n",
    "# Apply Bayes' Theorem\n",
    "p_spam_given_free = (p_free_given_spam * p_spam) / p_free\n",
    "\n",
    "print(\"ğŸ§  BAYES' THEOREM CALCULATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nğŸ“Š Given:\")\n",
    "print(f\"   P(Spam)           = {p_spam:.0%}  [Prior]\")\n",
    "print(f\"   P('free'|Spam)    = {p_free_given_spam:.0%}  [Likelihood]\")\n",
    "print(f\"   P('free'|Not Spam)= {p_free_given_not_spam:.0%}\")\n",
    "\n",
    "print(f\"\\nğŸ” Calculate:\")\n",
    "print(f\"   P('free')         = {p_free:.0%}  [Evidence]\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Result (using Bayes' Theorem):\")\n",
    "print(f\"   P(Spam|'free')    = {p_spam_given_free:.0%}  [Posterior]\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interpretation:\")\n",
    "print(f\"   - Started with {p_spam:.0%} spam rate (prior)\")\n",
    "print(f\"   - Saw word 'free' (evidence)\")\n",
    "print(f\"   - Updated belief to {p_spam_given_free:.0%} spam rate (posterior)\")\n",
    "print(f\"   - This is how AI learns from data! ğŸš€\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ YOUR TURN: Apply Bayes' Theorem\n",
    "\n",
    "**Scenario:** Multimodal AI classifying content as \"safe\" or \"unsafe\" for kids.\n",
    "\n",
    "Use Bayes' Theorem to find: P(Unsafe | Contains violence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given:\n",
    "p_unsafe = 0.15                      # Prior: 15% of content is unsafe\n",
    "p_violence_given_unsafe = 0.70       # 70% of unsafe content has violence\n",
    "p_violence_given_safe = 0.05         # 5% of safe content has violence (cartoons, games)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Calculate P(safe)\n",
    "p_safe = 1 - p_unsafe\n",
    "\n",
    "# 2. Calculate P(violence) using law of total probability\n",
    "p_violence = (p_violence_given_unsafe * p_unsafe) + (p_violence_given_safe * p_safe)\n",
    "\n",
    "# 3. Apply Bayes' Theorem to find P(unsafe | violence)\n",
    "p_unsafe_given_violence = (p_violence_given_unsafe * p_unsafe) / p_violence\n",
    "\n",
    "# 4. Print results\n",
    "print(f\"P(Unsafe)             = {p_unsafe:.0%}\")\n",
    "print(f\"P(Violence)           = {p_violence:.0%}\")\n",
    "print(f\"P(Unsafe | Violence)  = {p_unsafe_given_violence:.0%}\")\n",
    "print(f\"\\nğŸ’¡ If content has violence, it's {p_unsafe_given_violence:.0%} likely to be unsafe!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Part 4: Probability Distributions\n",
    "\n",
    "**Probability Distribution:** Shows all possible values and their probabilities\n",
    "\n",
    "We'll cover 3 essential distributions for AI/ML:\n",
    "1. **Uniform** - All outcomes equally likely\n",
    "2. **Normal (Gaussian)** - Bell curve, most common in nature\n",
    "3. **Binomial** - Success/failure experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1ï¸âƒ£ Uniform Distribution\n",
    "\n",
    "**All outcomes have equal probability**\n",
    "\n",
    "**Examples:** Dice roll, random initialization of neural network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Random weight initialization in neural networks\n",
    "np.random.seed(42)\n",
    "weights = np.random.uniform(low=-1, high=1, size=1000)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(weights, bins=30, color='skyblue', edgecolor='black', alpha=0.7, density=True)\n",
    "plt.xlabel('Weight Value', fontsize=12)\n",
    "plt.ylabel('Probability Density', fontsize=12)\n",
    "plt.title('Uniform Distribution: Neural Network Weight Initialization', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', label='Expected density (1/range = 1/2 = 0.5)')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"ğŸ’¡ All values between -1 and 1 are equally likely\")\n",
    "print(f\"ğŸ’¡ This prevents bias in initial neural network weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2ï¸âƒ£ Normal (Gaussian) Distribution\n",
    "\n",
    "**The most important distribution in AI/ML!**\n",
    "\n",
    "**Properties:**\n",
    "- Bell-shaped, symmetric\n",
    "- Defined by mean (Î¼) and standard deviation (Ïƒ)\n",
    "- 68-95-99.7 rule\n",
    "\n",
    "**AI Examples:** Model errors, feature values, gradient distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Model prediction errors\n",
    "np.random.seed(42)\n",
    "errors = np.random.normal(loc=0, scale=2, size=1000)  # mean=0, std=2\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(errors, bins=40, color='lightgreen', edgecolor='black', alpha=0.7, density=True)\n",
    "plt.xlabel('Prediction Error', fontsize=12)\n",
    "plt.ylabel('Probability Density', fontsize=12)\n",
    "plt.title('Normal Distribution: Model Errors', fontsize=14, fontweight='bold')\n",
    "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Mean = 0')\n",
    "plt.axvline(-2, color='blue', linestyle='--', alpha=0.5, label='Â±1 std')\n",
    "plt.axvline(2, color='blue', linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 68-95-99.7 rule visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "x = np.linspace(-8, 8, 1000)\n",
    "y = (1/(2*np.sqrt(2*np.pi))) * np.exp(-0.5*((x-0)/2)**2)\n",
    "plt.plot(x, y, 'b-', linewidth=2)\n",
    "plt.fill_between(x, y, where=(x >= -2) & (x <= 2), alpha=0.3, label='68% (Â±1Ïƒ)')\n",
    "plt.fill_between(x, y, where=(x >= -4) & (x <= 4), alpha=0.2, label='95% (Â±2Ïƒ)')\n",
    "plt.xlabel('Value', fontsize=12)\n",
    "plt.ylabel('Probability Density', fontsize=12)\n",
    "plt.title('68-95-99.7 Rule', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ Normal distribution appears everywhere in ML:\")\n",
    "print(\"   - Model prediction errors\")\n",
    "print(\"   - Feature distributions (after normalization)\")\n",
    "print(\"   - Neural network gradients\")\n",
    "print(\"   - Noise in data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3ï¸âƒ£ Binomial Distribution\n",
    "\n",
    "**Models repeated yes/no experiments**\n",
    "\n",
    "**Parameters:**\n",
    "- n = number of trials\n",
    "- p = probability of success\n",
    "\n",
    "**AI Example:** Out of 100 predictions, how many will be correct if accuracy is 85%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Model prediction outcomes\n",
    "n_predictions = 100  # Make 100 predictions\n",
    "p_correct = 0.85     # Model is 85% accurate\n",
    "\n",
    "# Generate possible outcomes\n",
    "np.random.seed(42)\n",
    "outcomes = np.random.binomial(n=n_predictions, p=p_correct, size=10000)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(outcomes, bins=20, color='purple', edgecolor='black', alpha=0.7, density=True)\n",
    "plt.axvline(outcomes.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {outcomes.mean():.1f}')\n",
    "plt.axvline(n_predictions * p_correct, color='green', linestyle='--', linewidth=2, label=f'Expected: {n_predictions * p_correct:.1f}')\n",
    "plt.xlabel('Number of Correct Predictions (out of 100)', fontsize=12)\n",
    "plt.ylabel('Probability Density', fontsize=12)\n",
    "plt.title('Binomial Distribution: Model Predictions', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"ğŸ’¡ If model is {p_correct:.0%} accurate over 100 predictions:\")\n",
    "print(f\"   - Expected correct: {n_predictions * p_correct:.0f}\")\n",
    "print(f\"   - Actual range: typically {outcomes.mean() - outcomes.std():.0f} to {outcomes.mean() + outcomes.std():.0f}\")\n",
    "print(f\"   - This variability is NORMAL - not every batch will be exactly 85%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ REAL AI EXAMPLE: Building a Naive Bayes Spam Classifier\n",
    "\n",
    "**Let's build a REAL spam classifier using Bayes' Theorem!**\n",
    "\n",
    "This is how Gmail, Outlook, and other email services filter spam.\n",
    "\n",
    "**How it works:**\n",
    "1. Learn probability of words appearing in spam vs legitimate emails\n",
    "2. Use Bayes' Theorem to calculate spam probability for new email\n",
    "3. Classify based on threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: emails labeled as spam or not spam\n",
    "training_emails = [\n",
    "    # (email text, is_spam)\n",
    "    (\"free money now\", True),\n",
    "    (\"hi mom how are you\", False),\n",
    "    (\"free free free offer\", True),\n",
    "    (\"meeting tomorrow at 3pm\", False),\n",
    "    (\"claim your free prize\", True),\n",
    "    (\"project deadline reminder\", False),\n",
    "    (\"you won the lottery\", True),\n",
    "    (\"lunch plans today\", False),\n",
    "    (\"free viagra now\", True),\n",
    "    (\"quarterly report attached\", False),\n",
    "]\n",
    "\n",
    "# Separate spam and not spam\n",
    "spam_emails = [email for email, is_spam in training_emails if is_spam]\n",
    "not_spam_emails = [email for email, is_spam in training_emails if not is_spam]\n",
    "\n",
    "print(\"ğŸ“§ TRAINING DATA:\")\n",
    "print(f\"Total emails: {len(training_emails)}\")\n",
    "print(f\"Spam: {len(spam_emails)}\")\n",
    "print(f\"Not Spam: {len(not_spam_emails)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word probabilities\n",
    "def get_word_counts(emails):\n",
    "    \"\"\"Count word frequencies in emails\"\"\"\n",
    "    word_counts = Counter()\n",
    "    for email in emails:\n",
    "        words = email.lower().split()\n",
    "        word_counts.update(words)\n",
    "    return word_counts\n",
    "\n",
    "spam_word_counts = get_word_counts(spam_emails)\n",
    "not_spam_word_counts = get_word_counts(not_spam_emails)\n",
    "\n",
    "# Calculate prior probabilities\n",
    "p_spam = len(spam_emails) / len(training_emails)\n",
    "p_not_spam = len(not_spam_emails) / len(training_emails)\n",
    "\n",
    "print(\"\\nğŸ“Š LEARNED PROBABILITIES:\")\n",
    "print(f\"\\nPriors:\")\n",
    "print(f\"P(Spam)     = {p_spam:.0%}\")\n",
    "print(f\"P(Not Spam) = {p_not_spam:.0%}\")\n",
    "\n",
    "print(f\"\\nTop spam words:\")\n",
    "for word, count in spam_word_counts.most_common(5):\n",
    "    print(f\"  '{word}': {count} times\")\n",
    "\n",
    "print(f\"\\nTop legitimate words:\")\n",
    "for word, count in not_spam_word_counts.most_common(5):\n",
    "    print(f\"  '{word}': {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "def classify_email(email, spam_words, not_spam_words, p_spam, p_not_spam):\n",
    "    \"\"\"\n",
    "    Classify email using Naive Bayes\n",
    "    Returns: (is_spam, spam_probability)\n",
    "    \"\"\"\n",
    "    words = email.lower().split()\n",
    "    \n",
    "    # Calculate total words (for probability estimation)\n",
    "    total_spam_words = sum(spam_words.values())\n",
    "    total_not_spam_words = sum(not_spam_words.values())\n",
    "    \n",
    "    # Start with prior probabilities\n",
    "    spam_score = p_spam\n",
    "    not_spam_score = p_not_spam\n",
    "    \n",
    "    # Multiply by likelihood of each word (Naive Bayes assumption: words are independent)\n",
    "    for word in words:\n",
    "        # Use Laplace smoothing (+1) to handle unseen words\n",
    "        p_word_given_spam = (spam_words.get(word, 0) + 1) / (total_spam_words + len(spam_words))\n",
    "        p_word_given_not_spam = (not_spam_words.get(word, 0) + 1) / (total_not_spam_words + len(not_spam_words))\n",
    "        \n",
    "        spam_score *= p_word_given_spam\n",
    "        not_spam_score *= p_word_given_not_spam\n",
    "    \n",
    "    # Normalize to get probability\n",
    "    spam_probability = spam_score / (spam_score + not_spam_score)\n",
    "    \n",
    "    return spam_probability > 0.5, spam_probability\n",
    "\n",
    "# Test the classifier!\n",
    "test_emails = [\n",
    "    \"free offer click now\",           # Should be spam\n",
    "    \"meeting notes from yesterday\",   # Should be not spam\n",
    "    \"you won free money\",             # Should be spam\n",
    "    \"lunch at noon tomorrow\",         # Should be not spam\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ§ª TESTING THE CLASSIFIER:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for email in test_emails:\n",
    "    is_spam, spam_prob = classify_email(email, spam_word_counts, not_spam_word_counts, p_spam, p_not_spam)\n",
    "    print(f\"\\nEmail: '{email}'\")\n",
    "    print(f\"Prediction: {'SPAM ğŸš«' if is_spam else 'LEGITIMATE âœ…'}\")\n",
    "    print(f\"Spam probability: {spam_prob:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ’¡ This is the SAME algorithm Gmail used in early 2000s!\")\n",
    "print(\"ğŸ’¡ Modern spam filters use deep learning, but Naive Bayes still works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ CHALLENGE: Build a Sentiment Classifier\n",
    "\n",
    "**Your turn!** Build a Naive Bayes classifier for sentiment analysis (positive vs negative reviews).\n",
    "\n",
    "This is used by companies to analyze customer feedback, social media, product reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: movie reviews\n",
    "training_reviews = [\n",
    "    (\"amazing movie loved it\", \"positive\"),\n",
    "    (\"terrible waste of time\", \"negative\"),\n",
    "    (\"best film ever great acting\", \"positive\"),\n",
    "    (\"boring and slow\", \"negative\"),\n",
    "    (\"fantastic story wonderful\", \"positive\"),\n",
    "    (\"awful worst movie ever\", \"negative\"),\n",
    "    (\"brilliant masterpiece\", \"positive\"),\n",
    "    (\"disappointing and bad\", \"negative\"),\n",
    "]\n",
    "\n",
    "# Test reviews\n",
    "test_reviews = [\n",
    "    \"great movie loved it\",\n",
    "    \"terrible and boring\",\n",
    "    \"amazing acting wonderful\",\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Separate positive and negative reviews\n",
    "positive_reviews = [review for review, sentiment in training_reviews if sentiment == \"positive\"]\n",
    "negative_reviews = [review for review, sentiment in training_reviews if sentiment == \"negative\"]\n",
    "\n",
    "# 2. Get word counts\n",
    "positive_word_counts = get_word_counts(positive_reviews)\n",
    "negative_word_counts = get_word_counts(negative_reviews)\n",
    "\n",
    "# 3. Calculate priors\n",
    "p_positive = len(positive_reviews) / len(training_reviews)\n",
    "p_negative = len(negative_reviews) / len(training_reviews)\n",
    "\n",
    "# 4. Classify test reviews\n",
    "print(\"ğŸ¬ SENTIMENT ANALYSIS RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "for review in test_reviews:\n",
    "    # Calculate scores (similar to spam classifier)\n",
    "    words = review.lower().split()\n",
    "    \n",
    "    total_pos = sum(positive_word_counts.values())\n",
    "    total_neg = sum(negative_word_counts.values())\n",
    "    \n",
    "    pos_score = p_positive\n",
    "    neg_score = p_negative\n",
    "    \n",
    "    for word in words:\n",
    "        p_word_pos = (positive_word_counts.get(word, 0) + 1) / (total_pos + len(positive_word_counts))\n",
    "        p_word_neg = (negative_word_counts.get(word, 0) + 1) / (total_neg + len(negative_word_counts))\n",
    "        pos_score *= p_word_pos\n",
    "        neg_score *= p_word_neg\n",
    "    \n",
    "    pos_prob = pos_score / (pos_score + neg_score)\n",
    "    \n",
    "    print(f\"\\nReview: '{review}'\")\n",
    "    print(f\"Sentiment: {'POSITIVE ğŸ˜Š' if pos_prob > 0.5 else 'NEGATIVE ğŸ˜'}\")\n",
    "    print(f\"Confidence: {max(pos_prob, 1-pos_prob):.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ’¡ You just built a sentiment analyzer used by real companies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Summary & Key Takeaways\n",
    "\n",
    "**You just learned:**\n",
    "\n",
    "### Probability Fundamentals:\n",
    "- âœ… Probability measures likelihood (0 to 1)\n",
    "- âœ… P(Event) = favorable outcomes / total outcomes\n",
    "- âœ… Probabilities must sum to 1.0\n",
    "- âœ… Complement rule: P(not A) = 1 - P(A)\n",
    "\n",
    "### Conditional Probability:\n",
    "- âœ… P(A|B) = probability of A given B happened\n",
    "- âœ… Used to update beliefs based on evidence\n",
    "- âœ… Foundation for all supervised learning\n",
    "\n",
    "### Bayes' Theorem:\n",
    "- âœ… P(A|B) = P(B|A) Ã— P(A) / P(B)\n",
    "- âœ… Updates prior beliefs with evidence â†’ posterior\n",
    "- âœ… THE foundation of probabilistic AI\n",
    "\n",
    "### Probability Distributions:\n",
    "- âœ… **Uniform** - Equal probabilities\n",
    "- âœ… **Normal** - Bell curve, most common in ML\n",
    "- âœ… **Binomial** - Success/failure experiments\n",
    "\n",
    "### Real Applications:\n",
    "- âœ… Built a Naive Bayes spam classifier\n",
    "- âœ… Built a sentiment analysis system\n",
    "- âœ… Used in Gmail, Netflix, Amazon, Google!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Practice Exercises\n",
    "\n",
    "**Before Day 3, complete these:**\n",
    "\n",
    "1. **RAG Relevance Classifier:**\n",
    "   - Build a Naive Bayes classifier that predicts if a retrieved document is relevant\n",
    "   - Use word frequency from \"relevant\" vs \"not relevant\" documents\n",
    "   - Test on new documents\n",
    "\n",
    "2. **Multimodal Confidence:**\n",
    "   - Simulate a multimodal model combining vision + text predictions\n",
    "   - Use Bayes' Theorem to combine probabilities from both modalities\n",
    "   - Compare combined vs individual predictions\n",
    "\n",
    "3. **Agent Decision Making:**\n",
    "   - Create an Agentic AI that decides actions based on probability\n",
    "   - Use conditional probability to update beliefs\n",
    "   - Simulate decision-making under uncertainty\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Why This Matters in 2024-2025\n",
    "\n",
    "**Probability theory powers ALL modern AI:**\n",
    "\n",
    "1. **RAG Systems:**\n",
    "   - Retrieval scoring uses probability\n",
    "   - Relevance ranking based on Bayesian updating\n",
    "   - Combining multiple retrievers probabilistically\n",
    "\n",
    "2. **Agentic AI:**\n",
    "   - Agents make decisions under uncertainty\n",
    "   - Action selection based on probability\n",
    "   - Learning from outcomes (Bayesian reinforcement learning)\n",
    "\n",
    "3. **Multimodal Models:**\n",
    "   - Combine vision, text, audio probabilities\n",
    "   - Cross-modal attention uses probability distributions\n",
    "   - Uncertainty quantification across modalities\n",
    "\n",
    "4. **LLMs (GPT, Claude, etc.):**\n",
    "   - Output probability distributions over tokens\n",
    "   - Sampling strategies use probability\n",
    "   - Temperature controls probability sharpness\n",
    "\n",
    "**You can't understand modern AI without understanding probability!** ğŸ¯\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Next Lesson\n",
    "\n",
    "**Day 3: Statistical Testing**\n",
    "- Hypothesis testing\n",
    "- P-values and confidence intervals\n",
    "- A/B testing for ML models\n",
    "- Compare model performance scientifically\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¬ Questions?** Review Bayes' Theorem - it's the most important concept here!\n",
    "\n",
    "*Remember: All of machine learning is just applied probability theory!* ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
