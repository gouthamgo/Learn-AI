{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Day 1: Descriptive Statistics for AI/ML\n",
    "\n",
    "**üéØ Goal:** Master statistical measures to understand and describe your AI data\n",
    "\n",
    "**‚è±Ô∏è Time:** 45-60 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Statistics is the foundation of ALL machine learning\n",
    "- Understand your data before training models (garbage in = garbage out!)\n",
    "- Evaluate model performance with statistical metrics\n",
    "- Critical for 2024-2025 AI: RAG systems, Agentic AI, Multimodal models all rely on statistics\n",
    "- Data scientists spend 80% of their time analyzing data with these exact techniques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö What is Descriptive Statistics?\n",
    "\n",
    "**Descriptive statistics** summarizes and describes data in a meaningful way.\n",
    "\n",
    "Imagine you're training an AI model:\n",
    "- You have 10,000 training examples\n",
    "- Each example has features (numbers, measurements)\n",
    "- How do you understand what your data looks like?\n",
    "\n",
    "**That's where descriptive statistics comes in!** üéØ\n",
    "\n",
    "We'll learn 3 key concepts:\n",
    "1. **Measures of Central Tendency** (mean, median, mode) - \"What's typical?\"\n",
    "2. **Measures of Spread** (variance, standard deviation) - \"How spread out is the data?\"\n",
    "3. **Distributions** (visualizing data patterns) - \"What does the data look like?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup: Import Libraries\n",
    "\n",
    "We'll use:\n",
    "- `numpy` - Fast numerical computations\n",
    "- `matplotlib` - Data visualization\n",
    "- `statistics` - Built-in Python statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "# Make plots look nice\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìè Part 1: Measures of Central Tendency\n",
    "\n",
    "**Central tendency** answers: *\"What's the typical or middle value?\"*\n",
    "\n",
    "### 1Ô∏è‚É£ Mean (Average)\n",
    "\n",
    "**Formula:** Sum all values √∑ Number of values\n",
    "\n",
    "**When to use:** Most common measure, good for normally distributed data\n",
    "\n",
    "**AI Example:** Average model accuracy across training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Model accuracy scores over 10 training epochs\n",
    "accuracy_scores = [0.72, 0.78, 0.81, 0.85, 0.87, 0.89, 0.90, 0.91, 0.92, 0.93]\n",
    "\n",
    "# Calculate mean\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "print(\"ü§ñ Model Accuracy Scores:\", accuracy_scores)\n",
    "print(f\"üìä Average Accuracy: {mean_accuracy:.2%}\")\n",
    "print(f\"\\nüí° Interpretation: On average, the model is {mean_accuracy:.1%} accurate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Median (Middle Value)\n",
    "\n",
    "**Definition:** The middle value when data is sorted\n",
    "\n",
    "**When to use:** Better than mean when you have outliers (extreme values)\n",
    "\n",
    "**AI Example:** Median response time for AI chatbot (handles slow outliers better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Chatbot response times in seconds (with some slow outliers)\n",
    "response_times = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 15.0, 20.0]  # Last two are outliers!\n",
    "\n",
    "# Calculate mean vs median\n",
    "mean_time = np.mean(response_times)\n",
    "median_time = np.median(response_times)\n",
    "\n",
    "print(\"‚è±Ô∏è Response Times (seconds):\", response_times)\n",
    "print(f\"\\nüìä Mean (Average): {mean_time:.2f} seconds\")\n",
    "print(f\"üìä Median (Middle): {median_time:.2f} seconds\")\n",
    "print(f\"\\nüí° See the difference? Outliers (15.0, 20.0) made the mean misleading!\")\n",
    "print(f\"üí° Median ({median_time:.2f}s) better represents typical response time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Mode (Most Frequent Value)\n",
    "\n",
    "**Definition:** The value that appears most often\n",
    "\n",
    "**When to use:** Categorical data or finding most common outcomes\n",
    "\n",
    "**AI Example:** Most common prediction class in a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Image classification predictions over 20 images\n",
    "predictions = ['cat', 'dog', 'cat', 'bird', 'cat', 'dog', 'cat', 'cat', \n",
    "               'bird', 'cat', 'dog', 'cat', 'cat', 'bird', 'cat', 'dog', \n",
    "               'cat', 'cat', 'dog', 'cat']\n",
    "\n",
    "# Calculate mode\n",
    "mode_prediction = statistics.mode(predictions)\n",
    "\n",
    "print(\"üñºÔ∏è Model Predictions:\", predictions)\n",
    "print(f\"\\nüìä Mode (Most Common): {mode_prediction}\")\n",
    "print(f\"üí° The model predicts '{mode_prediction}' most frequently\")\n",
    "print(f\"üí° This might indicate class imbalance in your training data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Calculate Central Tendency\n",
    "\n",
    "**Scenario:** You're analyzing a RAG (Retrieval-Augmented Generation) system's retrieval scores.\n",
    "\n",
    "Calculate mean, median, and interpret the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG retrieval relevance scores (0-100)\n",
    "retrieval_scores = [85, 92, 78, 88, 95, 90, 87, 15, 91, 89]  # One outlier at 15!\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Calculate the mean\n",
    "mean_score = np.mean(retrieval_scores)\n",
    "\n",
    "# 2. Calculate the median\n",
    "median_score = np.median(retrieval_scores)\n",
    "\n",
    "# 3. Print both\n",
    "print(f\"Mean Score: {mean_score:.2f}\")\n",
    "print(f\"Median Score: {median_score:.2f}\")\n",
    "\n",
    "# 4. Which one is better for this data and why?\n",
    "print(f\"\\nüí° The median ({median_score:.2f}) is better because it's not affected by the outlier (15)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìê Part 2: Measures of Spread (Variability)\n",
    "\n",
    "**Spread** answers: *\"How much do values differ from each other?\"*\n",
    "\n",
    "Two models can have the same average accuracy but very different consistency!\n",
    "\n",
    "### 1Ô∏è‚É£ Variance\n",
    "\n",
    "**Definition:** Average of squared differences from the mean\n",
    "\n",
    "**Formula:** Œ£(x - mean)¬≤ / n\n",
    "\n",
    "**AI Use:** Measures model stability/consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two different AI models with SAME average accuracy but different consistency\n",
    "model_a_accuracy = [0.85, 0.86, 0.85, 0.86, 0.85, 0.86, 0.85, 0.86, 0.85, 0.86]  # Very consistent\n",
    "model_b_accuracy = [0.70, 0.95, 0.75, 0.90, 0.80, 0.88, 0.82, 0.92, 0.78, 0.95]  # Very inconsistent\n",
    "\n",
    "# Calculate means\n",
    "mean_a = np.mean(model_a_accuracy)\n",
    "mean_b = np.mean(model_b_accuracy)\n",
    "\n",
    "# Calculate variances\n",
    "var_a = np.var(model_a_accuracy)\n",
    "var_b = np.var(model_b_accuracy)\n",
    "\n",
    "print(\"üìä Model A (Consistent):\")\n",
    "print(f\"   Mean Accuracy: {mean_a:.2%}\")\n",
    "print(f\"   Variance: {var_a:.6f}\")\n",
    "\n",
    "print(\"\\nüìä Model B (Inconsistent):\")\n",
    "print(f\"   Mean Accuracy: {mean_b:.2%}\")\n",
    "print(f\"   Variance: {var_b:.6f}\")\n",
    "\n",
    "print(f\"\\nüí° Both models have ~85% average accuracy\")\n",
    "print(f\"üí° But Model B has {var_b/var_a:.1f}x higher variance = less reliable!\")\n",
    "print(f\"üí° In production, you'd choose Model A (lower variance = more predictable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Standard Deviation (Most Important!)\n",
    "\n",
    "**Definition:** Square root of variance\n",
    "\n",
    "**Why better than variance?** Same units as original data (easier to interpret)\n",
    "\n",
    "**Rule of thumb (Normal distribution):**\n",
    "- 68% of data within 1 standard deviation of mean\n",
    "- 95% of data within 2 standard deviations of mean\n",
    "- 99.7% of data within 3 standard deviations of mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Training loss values during model training\n",
    "training_losses = [2.5, 2.3, 2.1, 2.0, 1.9, 1.8, 1.7, 1.6, 1.5, 1.4]\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_loss = np.mean(training_losses)\n",
    "std_loss = np.std(training_losses)\n",
    "\n",
    "print(\"üìâ Training Losses:\", training_losses)\n",
    "print(f\"\\nüìä Mean Loss: {mean_loss:.2f}\")\n",
    "print(f\"üìä Standard Deviation: {std_loss:.2f}\")\n",
    "print(f\"\\nüí° Typical loss values range from {mean_loss - std_loss:.2f} to {mean_loss + std_loss:.2f}\")\n",
    "print(f\"üí° Lower std = more stable training process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Analyze Model Variability\n",
    "\n",
    "**Scenario:** You're testing two multimodal AI models (combining vision + text) for image captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU scores (0-100) for two multimodal models\n",
    "model_vision_text_a = [78, 82, 79, 81, 80, 82, 79, 81, 80, 82]\n",
    "model_vision_text_b = [65, 85, 70, 90, 68, 88, 72, 86, 75, 92]\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Calculate mean for both models\n",
    "mean_a = np.mean(model_vision_text_a)\n",
    "mean_b = np.mean(model_vision_text_b)\n",
    "\n",
    "# 2. Calculate standard deviation for both\n",
    "std_a = np.std(model_vision_text_a)\n",
    "std_b = np.std(model_vision_text_b)\n",
    "\n",
    "# 3. Print results\n",
    "print(f\"Model A: Mean={mean_a:.2f}, Std={std_a:.2f}\")\n",
    "print(f\"Model B: Mean={mean_b:.2f}, Std={std_b:.2f}\")\n",
    "\n",
    "# 4. Which model would you deploy and why?\n",
    "print(f\"\\nüí° Choose Model A: Similar mean but {std_b/std_a:.1f}x lower variance = more reliable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 3: Distributions and Histograms\n",
    "\n",
    "**Distribution:** How values are spread across possible values\n",
    "\n",
    "**Histogram:** Visual representation of distribution\n",
    "\n",
    "**Why it matters:**\n",
    "- Identify data patterns\n",
    "- Detect outliers\n",
    "- Understand data shape (normal, skewed, bimodal)\n",
    "- Critical for feature engineering in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Your First Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: 1000 model prediction confidence scores (0-100)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "confidence_scores = np.random.normal(loc=75, scale=10, size=1000)  # Normal distribution\n",
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(confidence_scores, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(np.mean(confidence_scores), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(confidence_scores):.2f}')\n",
    "plt.axvline(np.median(confidence_scores), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(confidence_scores):.2f}')\n",
    "\n",
    "plt.xlabel('Confidence Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Model Confidence Scores', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Mean: {np.mean(confidence_scores):.2f}\")\n",
    "print(f\"üìä Median: {np.median(confidence_scores):.2f}\")\n",
    "print(f\"üìä Std Dev: {np.std(confidence_scores):.2f}\")\n",
    "print(f\"\\nüí° This is a NORMAL distribution - most common in AI/ML!\")\n",
    "print(f\"üí° Symmetric, bell-shaped, mean ‚âà median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Different Distribution Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 different distributions\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Normal distribution (symmetric)\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=1000)\n",
    "\n",
    "# 2. Right-skewed (long tail on right) - common in real-world data\n",
    "skewed_data = np.random.exponential(scale=20, size=1000)\n",
    "\n",
    "# 3. Bimodal (two peaks) - might indicate two different user groups\n",
    "bimodal_data = np.concatenate([np.random.normal(30, 5, 500), np.random.normal(70, 5, 500)])\n",
    "\n",
    "# Plot all three\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(normal_data, bins=30, color='lightblue', edgecolor='black')\n",
    "axes[0].set_title('Normal Distribution\\n(Symmetric)', fontweight='bold')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(skewed_data, bins=30, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_title('Right-Skewed Distribution\\n(Long right tail)', fontweight='bold')\n",
    "axes[1].set_xlabel('Value')\n",
    "\n",
    "axes[2].hist(bimodal_data, bins=30, color='lightgreen', edgecolor='black')\n",
    "axes[2].set_title('Bimodal Distribution\\n(Two peaks)', fontweight='bold')\n",
    "axes[2].set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Understanding these patterns helps you:\")\n",
    "print(\"   1. Choose the right ML algorithm\")\n",
    "print(\"   2. Decide if data needs transformation\")\n",
    "print(\"   3. Identify potential data quality issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ REAL AI EXAMPLE: Analyzing Model Performance Metrics\n",
    "\n",
    "**Scenario:** You're building an **Agentic AI** system (AI agents that can take actions). You need to analyze how well different agents perform across various tasks.\n",
    "\n",
    "Let's use everything we learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance scores for 3 different AI agents across 50 tasks\n",
    "np.random.seed(42)\n",
    "\n",
    "agent_gpt = np.random.normal(loc=85, scale=5, size=50)    # Consistent, high performer\n",
    "agent_claude = np.random.normal(loc=87, scale=3, size=50) # More consistent, slightly better\n",
    "agent_custom = np.random.normal(loc=82, scale=12, size=50) # Less consistent\n",
    "\n",
    "# Calculate all statistics\n",
    "agents = {\n",
    "    'GPT Agent': agent_gpt,\n",
    "    'Claude Agent': agent_claude,\n",
    "    'Custom Agent': agent_custom\n",
    "}\n",
    "\n",
    "print(\"ü§ñ AGENTIC AI PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, scores in agents.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean Score:      {np.mean(scores):.2f}\")\n",
    "    print(f\"  Median Score:    {np.median(scores):.2f}\")\n",
    "    print(f\"  Std Deviation:   {np.std(scores):.2f}\")\n",
    "    print(f\"  Min-Max:         {np.min(scores):.2f} - {np.max(scores):.2f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Box plot for comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot([agent_gpt, agent_claude, agent_custom], labels=['GPT', 'Claude', 'Custom'])\n",
    "plt.ylabel('Performance Score', fontsize=12)\n",
    "plt.title('Agent Performance Comparison\\n(Box Plot)', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Histograms\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(agent_gpt, bins=15, alpha=0.5, label='GPT', color='blue')\n",
    "plt.hist(agent_claude, bins=15, alpha=0.5, label='Claude', color='orange')\n",
    "plt.hist(agent_custom, bins=15, alpha=0.5, label='Custom', color='green')\n",
    "plt.xlabel('Performance Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Performance Distribution\\n(Histogram)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° DECISION: Choose Claude Agent!\")\n",
    "print(\"   ‚úÖ Highest mean (87)\")\n",
    "print(\"   ‚úÖ Lowest std dev (3) = most reliable\")\n",
    "print(\"   ‚úÖ Tightest distribution = predictable performance\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ CHALLENGE: Analyze Transformer Model Attention Scores\n",
    "\n",
    "**Scenario:** You're analyzing attention scores from a Transformer model (like GPT, BERT) to understand how it focuses on different tokens.\n",
    "\n",
    "Complete the analysis below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention weights from a transformer layer (0-1 scale)\n",
    "np.random.seed(123)\n",
    "attention_weights = np.random.beta(a=2, b=5, size=200)  # Beta distribution (common for attention)\n",
    "\n",
    "# YOUR TASK: Complete this analysis\n",
    "\n",
    "# 1. Calculate mean, median, std\n",
    "mean_attention = np.mean(attention_weights)\n",
    "median_attention = np.median(attention_weights)\n",
    "std_attention = np.std(attention_weights)\n",
    "\n",
    "print(\"üîç Transformer Attention Analysis\")\n",
    "print(f\"Mean:   {mean_attention:.3f}\")\n",
    "print(f\"Median: {median_attention:.3f}\")\n",
    "print(f\"Std:    {std_attention:.3f}\")\n",
    "\n",
    "# 2. Create a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(attention_weights, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(mean_attention, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_attention:.3f}')\n",
    "plt.xlabel('Attention Weight', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Transformer Attention Weights', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 3. Interpretation\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"   - Most attention weights are LOW (right-skewed distribution)\")\n",
    "print(f\"   - Model focuses on a FEW important tokens (sparse attention)\")\n",
    "print(f\"   - This is NORMAL for transformers - they learn to be selective!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Summary & Key Takeaways\n",
    "\n",
    "**You just learned:**\n",
    "\n",
    "### Measures of Central Tendency:\n",
    "- ‚úÖ **Mean** - Average value (sensitive to outliers)\n",
    "- ‚úÖ **Median** - Middle value (robust to outliers)\n",
    "- ‚úÖ **Mode** - Most frequent value\n",
    "\n",
    "### Measures of Spread:\n",
    "- ‚úÖ **Variance** - Average squared deviation\n",
    "- ‚úÖ **Standard Deviation** - Square root of variance (same units as data)\n",
    "- ‚úÖ Lower spread = more consistent/reliable model\n",
    "\n",
    "### Distributions:\n",
    "- ‚úÖ **Histograms** - Visualize data patterns\n",
    "- ‚úÖ **Normal, Skewed, Bimodal** - Different distribution shapes\n",
    "- ‚úÖ Understanding shape guides ML decisions\n",
    "\n",
    "### Real AI Applications:\n",
    "- ‚úÖ Model performance evaluation\n",
    "- ‚úÖ Agentic AI comparison\n",
    "- ‚úÖ Transformer attention analysis\n",
    "- ‚úÖ RAG system optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Practice Exercises\n",
    "\n",
    "**Before Day 2, complete these exercises:**\n",
    "\n",
    "1. **RAG Retrieval Analysis:**\n",
    "   - Generate 100 random retrieval scores (use `np.random.normal`)\n",
    "   - Calculate all statistics\n",
    "   - Create a histogram\n",
    "   - Add mean and median lines\n",
    "\n",
    "2. **Multimodal Model Comparison:**\n",
    "   - Create data for 2 multimodal models (vision+text+audio)\n",
    "   - One should be consistent, one inconsistent\n",
    "   - Calculate statistics\n",
    "   - Decide which to deploy\n",
    "\n",
    "3. **Outlier Detection:**\n",
    "   - Create a dataset with deliberate outliers\n",
    "   - Show how mean vs median differ\n",
    "   - Visualize with histogram\n",
    "   - Explain when to use each measure\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Real-World Connection: Why This Matters in 2024-2025\n",
    "\n",
    "**These exact techniques are used daily by AI engineers for:**\n",
    "\n",
    "1. **RAG Systems (2024 breakthrough):**\n",
    "   - Analyze retrieval score distributions\n",
    "   - Optimize chunk size using statistical metrics\n",
    "   - Compare different embedding models\n",
    "\n",
    "2. **Agentic AI (2025 trend):**\n",
    "   - Measure agent reliability (low variance = good!)\n",
    "   - Compare agent performance across tasks\n",
    "   - Detect anomalous agent behavior\n",
    "\n",
    "3. **Multimodal Models:**\n",
    "   - Analyze attention distributions across modalities\n",
    "   - Balance vision/text/audio contributions\n",
    "   - Evaluate cross-modal alignment\n",
    "\n",
    "4. **Transformer Models:**\n",
    "   - Analyze attention weight distributions\n",
    "   - Understand token importance\n",
    "   - Debug model behavior\n",
    "\n",
    "**Bottom line:** Statistics is not optional - it's ESSENTIAL for AI/ML! üéØ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Next Lesson\n",
    "\n",
    "**Day 2: Probability Theory**\n",
    "- Probability fundamentals\n",
    "- Conditional probability and Bayes' theorem\n",
    "- Probability distributions\n",
    "- Build a Naive Bayes classifier!\n",
    "\n",
    "---\n",
    "\n",
    "**üí¨ Questions?** Review this notebook, experiment with different datasets, break things and fix them!\n",
    "\n",
    "*Remember: Statistics is the language of AI. Master it, and you master AI!* üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
