{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üé≤ Week 4, Day 3: Probability & Statistics for AI\n",
    "\n",
    "**üéØ Goal:** Master probability and statistics - the language of uncertainty in AI\n",
    "\n",
    "**‚è±Ô∏è Time:** 60-90 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- **AI is probabilistic** - Models output probabilities, not certainties\n",
    "- **Bayesian reasoning** - Update beliefs with new evidence\n",
    "- **Statistical inference** - Make decisions from data\n",
    "- **Uncertainty quantification** - Know when your model is confident\n",
    "- **A/B testing** - Prove your AI improvements work\n",
    "\n",
    "---\n",
    "\n",
    "## üî• 2024-2025 AI Trend Alert!\n",
    "\n",
    "**Large Language Models are Probability Machines**:\n",
    "- GPT-4 predicts: \"Next token has 60% chance of being 'the'\"\n",
    "- **Temperature = controlling randomness/creativity!**\n",
    "- Sampling strategies: Top-p, Top-k = probability filtering\n",
    "\n",
    "**Uncertainty in AI** is now CRITICAL:\n",
    "- Healthcare AI: \"90% confidence this is cancer\"\n",
    "- Self-driving: \"Probability of obstacle ahead\"\n",
    "- **Knowing WHEN to trust AI = understanding probability!**\n",
    "\n",
    "**Bayesian Deep Learning**:\n",
    "- Neural networks with uncertainty estimates\n",
    "- **Used in drug discovery, medical diagnosis**\n",
    "\n",
    "**You'll learn the math behind AI decision-making!** üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üé≤ What is Probability?\n",
    "\n",
    "**Probability** = Math of uncertainty and randomness\n",
    "\n",
    "Think of it as:\n",
    "- Certainty: \"The sun will rise tomorrow\" (100%) ‚òÄÔ∏è\n",
    "- Probability: \"It will rain tomorrow\" (30%) üåßÔ∏è\n",
    "- Impossibility: \"I'll grow wings\" (0%) ü¶Ö\n",
    "\n",
    "**In AI:**\n",
    "- \"This email is spam with 92% probability\"\n",
    "- \"Customer will buy with 65% probability\"\n",
    "- \"Next word is 'the' with 40% probability\"\n",
    "\n",
    "Let's explore! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"‚úÖ Ready to explore probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## üìä Basic Probability Concepts\n",
    "\n",
    "### Key Rules:\n",
    "1. **Probability range**: 0 ‚â§ P(A) ‚â§ 1\n",
    "2. **Sum rule**: P(A or B) = P(A) + P(B) - P(A and B)\n",
    "3. **Product rule**: P(A and B) = P(A) √ó P(B|A)\n",
    "4. **Complement**: P(not A) = 1 - P(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate coin flips\n",
    "n_flips = 10000\n",
    "flips = np.random.choice(['Heads', 'Tails'], size=n_flips)\n",
    "\n",
    "# Count outcomes\n",
    "heads_count = np.sum(flips == 'Heads')\n",
    "tails_count = np.sum(flips == 'Tails')\n",
    "\n",
    "# Calculate probabilities\n",
    "p_heads = heads_count / n_flips\n",
    "p_tails = tails_count / n_flips\n",
    "\n",
    "print(f\"üé≤ Coin Flip Simulation ({n_flips:,} flips)\\n\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Heads: {heads_count:,} ({p_heads:.3f})\")\n",
    "print(f\"Tails: {tails_count:,} ({p_tails:.3f})\")\n",
    "print(f\"\\n‚úÖ P(Heads) + P(Tails) = {p_heads + p_tails:.3f} (should be 1.0)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Heads', 'Tails'], [heads_count, tails_count], color=['blue', 'orange'])\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Coin Flip Results ({n_flips:,} flips)', fontweight='bold')\n",
    "plt.axhline(y=n_flips/2, color='red', linestyle='--', label='Expected (50%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüß† Law of Large Numbers:\")\n",
    "print(\"   More trials ‚Üí Closer to theoretical probability (0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## üéØ Conditional Probability - The Heart of AI\n",
    "\n",
    "**P(A|B)** = Probability of A given B has occurred\n",
    "\n",
    "**Formula:** P(A|B) = P(A and B) / P(B)\n",
    "\n",
    "**AI Examples:**\n",
    "- P(spam | contains \"lottery\") - Email filtering\n",
    "- P(click | shown ad) - Click-through rate\n",
    "- P(churn | low engagement) - Customer retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email spam classifier example\n",
    "print(\"üìß EMAIL SPAM CLASSIFICATION\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset statistics (simulated)\n",
    "total_emails = 10000\n",
    "spam_emails = 3000\n",
    "ham_emails = 7000\n",
    "\n",
    "# Word \"free\" statistics\n",
    "free_in_spam = 2400  # 80% of spam contains \"free\"\n",
    "free_in_ham = 700    # 10% of ham contains \"free\"\n",
    "\n",
    "# Calculate probabilities\n",
    "P_spam = spam_emails / total_emails\n",
    "P_ham = ham_emails / total_emails\n",
    "P_free_given_spam = free_in_spam / spam_emails\n",
    "P_free_given_ham = free_in_ham / ham_emails\n",
    "\n",
    "print(f\"Dataset: {total_emails:,} emails\")\n",
    "print(f\"  Spam: {spam_emails:,} ({P_spam:.1%})\")\n",
    "print(f\"  Ham:  {ham_emails:,} ({P_ham:.1%})\")\n",
    "print()\n",
    "print(f\"Word 'free' statistics:\")\n",
    "print(f\"  P('free' | spam) = {P_free_given_spam:.1%}\")\n",
    "print(f\"  P('free' | ham)  = {P_free_given_ham:.1%}\")\n",
    "\n",
    "# Bayes' Theorem: P(spam | \"free\")\n",
    "P_free = (free_in_spam + free_in_ham) / total_emails\n",
    "P_spam_given_free = (P_free_given_spam * P_spam) / P_free\n",
    "\n",
    "print(f\"\\nüéØ If email contains 'free':\")\n",
    "print(f\"  P(spam | 'free') = {P_spam_given_free:.1%}\")\n",
    "print(f\"  P(ham  | 'free') = {1-P_spam_given_free:.1%}\")\n",
    "\n",
    "print(\"\\n‚ú® This is the foundation of Naive Bayes classifiers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## üîî Probability Distributions\n",
    "\n",
    "### 1Ô∏è‚É£ Uniform Distribution - All outcomes equally likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice roll - uniform distribution\n",
    "dice_rolls = np.random.randint(1, 7, size=10000)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(dice_rolls, bins=np.arange(0.5, 7.5, 1), \n",
    "        edgecolor='black', alpha=0.7, density=True)\n",
    "plt.xlabel('Dice Value', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.title('Uniform Distribution (Fair Dice)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(1, 7))\n",
    "plt.axhline(y=1/6, color='red', linestyle='--', label='Expected (1/6)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üé≤ Each outcome has equal probability: 1/6 ‚âà 16.67%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Bernoulli Distribution - Binary outcomes (success/failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click-through rate (CTR) simulation\n",
    "p_click = 0.03  # 3% CTR\n",
    "n_impressions = 10000\n",
    "\n",
    "clicks = np.random.binomial(1, p_click, n_impressions)\n",
    "total_clicks = np.sum(clicks)\n",
    "\n",
    "print(f\"üì± Ad Click Simulation\\n\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Impressions: {n_impressions:,}\")\n",
    "print(f\"Clicks: {total_clicks} ({total_clicks/n_impressions:.2%})\")\n",
    "print(f\"Expected: {p_click:.1%}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['No Click', 'Click'], \n",
    "       [n_impressions - total_clicks, total_clicks],\n",
    "       color=['lightcoral', 'lightgreen'])\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Bernoulli Distribution (CTR = {p_click:.1%})', fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüß† AI Applications:\")\n",
    "print(\"  - Binary classification (spam/ham, fraud/legit)\")\n",
    "print(\"  - Conversion prediction\")\n",
    "print(\"  - A/B testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Normal (Gaussian) Distribution - THE MOST IMPORTANT! üîî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Height distribution (classic example)\n",
    "mean_height = 170  # cm\n",
    "std_height = 10    # cm\n",
    "heights = np.random.normal(mean_height, std_height, 10000)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram + KDE\n",
    "plt.hist(heights, bins=50, density=True, alpha=0.6, \n",
    "        edgecolor='black', label='Data')\n",
    "\n",
    "# Theoretical normal distribution\n",
    "x = np.linspace(130, 210, 100)\n",
    "pdf = stats.norm.pdf(x, mean_height, std_height)\n",
    "plt.plot(x, pdf, 'r-', linewidth=2, label='Normal PDF')\n",
    "\n",
    "# Mark mean and standard deviations\n",
    "plt.axvline(mean_height, color='green', linestyle='--', linewidth=2, label='Mean')\n",
    "plt.axvline(mean_height - std_height, color='orange', linestyle=':', linewidth=2, label='¬±1 std')\n",
    "plt.axvline(mean_height + std_height, color='orange', linestyle=':', linewidth=2)\n",
    "plt.axvline(mean_height - 2*std_height, color='purple', linestyle=':', linewidth=2, label='¬±2 std')\n",
    "plt.axvline(mean_height + 2*std_height, color='purple', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Height (cm)', fontsize=12)\n",
    "plt.ylabel('Probability Density', fontsize=12)\n",
    "plt.title(f'Normal Distribution (Œº={mean_height}, œÉ={std_height})', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä 68-95-99.7 Rule (Empirical Rule):\\n\")\n",
    "within_1std = np.sum((heights >= mean_height - std_height) & \n",
    "                    (heights <= mean_height + std_height)) / len(heights)\n",
    "within_2std = np.sum((heights >= mean_height - 2*std_height) & \n",
    "                    (heights <= mean_height + 2*std_height)) / len(heights)\n",
    "within_3std = np.sum((heights >= mean_height - 3*std_height) & \n",
    "                    (heights <= mean_height + 3*std_height)) / len(heights)\n",
    "\n",
    "print(f\"  Within ¬±1œÉ: {within_1std:.1%} (theory: 68%)\")\n",
    "print(f\"  Within ¬±2œÉ: {within_2std:.1%} (theory: 95%)\")\n",
    "print(f\"  Within ¬±3œÉ: {within_3std:.1%} (theory: 99.7%)\")\n",
    "\n",
    "print(\"\\nüß† Why it matters for AI:\")\n",
    "print(\"  - Many natural phenomena are normally distributed\")\n",
    "print(\"  - Central Limit Theorem: Averages ‚Üí Normal\")\n",
    "print(\"  - Neural network weight initialization\")\n",
    "print(\"  - Noise in data\")\n",
    "print(\"  - Bayesian inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## üìä Descriptive Statistics - Understanding Data\n",
    "\n",
    "### Measures of Central Tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI model prediction errors (residuals)\n",
    "np.random.seed(42)\n",
    "errors = np.concatenate([\n",
    "    np.random.normal(0, 1, 950),  # Most errors are small\n",
    "    np.random.normal(0, 5, 50)    # Some large outliers\n",
    "])\n",
    "\n",
    "# Calculate statistics\n",
    "mean = np.mean(errors)\n",
    "median = np.median(errors)\n",
    "mode = stats.mode(errors.round(1), keepdims=True)[0][0]\n",
    "std = np.std(errors)\n",
    "variance = np.var(errors)\n",
    "\n",
    "print(\"üìä MODEL PREDICTION ERRORS ANALYSIS\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean:     {mean:8.3f}  (average error)\")\n",
    "print(f\"Median:   {median:8.3f}  (middle value, robust to outliers)\")\n",
    "print(f\"Std Dev:  {std:8.3f}  (spread of errors)\")\n",
    "print(f\"Variance: {variance:8.3f}  (squared std dev)\")\n",
    "print(f\"Min:      {errors.min():8.3f}\")\n",
    "print(f\"Max:      {errors.max():8.3f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean:.2f}')\n",
    "plt.axvline(median, color='green', linestyle='--', linewidth=2, label=f'Median: {median:.2f}')\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Error Distribution', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(errors, vert=True)\n",
    "plt.ylabel('Error')\n",
    "plt.title('Box Plot (shows quartiles & outliers)', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key insights:\")\n",
    "print(\"  - Most errors are small (good!)\")\n",
    "print(\"  - Some large outliers exist\")\n",
    "print(\"  - Median is closer to typical value than mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## üîó Correlation - Relationship Between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated data\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "\n",
    "# Positive correlation\n",
    "study_hours = np.random.uniform(0, 10, n)\n",
    "test_score = 40 + 5 * study_hours + np.random.normal(0, 5, n)\n",
    "\n",
    "# Negative correlation\n",
    "social_media_hours = np.random.uniform(0, 8, n)\n",
    "productivity = 100 - 8 * social_media_hours + np.random.normal(0, 10, n)\n",
    "\n",
    "# Calculate correlations\n",
    "corr_positive = np.corrcoef(study_hours, test_score)[0, 1]\n",
    "corr_negative = np.corrcoef(social_media_hours, productivity)[0, 1]\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Positive correlation\n",
    "axes[0].scatter(study_hours, test_score, alpha=0.6)\n",
    "axes[0].plot(np.unique(study_hours), \n",
    "            np.poly1d(np.polyfit(study_hours, test_score, 1))(np.unique(study_hours)),\n",
    "            color='red', linewidth=2, label='Trend line')\n",
    "axes[0].set_xlabel('Study Hours')\n",
    "axes[0].set_ylabel('Test Score')\n",
    "axes[0].set_title(f'Positive Correlation (r = {corr_positive:.2f})', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Negative correlation\n",
    "axes[1].scatter(social_media_hours, productivity, alpha=0.6, color='orange')\n",
    "axes[1].plot(np.unique(social_media_hours), \n",
    "            np.poly1d(np.polyfit(social_media_hours, productivity, 1))(np.unique(social_media_hours)),\n",
    "            color='red', linewidth=2, label='Trend line')\n",
    "axes[1].set_xlabel('Social Media Hours')\n",
    "axes[1].set_ylabel('Productivity')\n",
    "axes[1].set_title(f'Negative Correlation (r = {corr_negative:.2f})', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Correlation Coefficient (r):\\n\")\n",
    "print(\"  r = +1: Perfect positive correlation\")\n",
    "print(\"  r = 0:  No correlation\")\n",
    "print(\"  r = -1: Perfect negative correlation\")\n",
    "print(\"\\nüß† AI Applications:\")\n",
    "print(\"  - Feature selection (remove correlated features)\")\n",
    "print(\"  - Understanding relationships in data\")\n",
    "print(\"  - Multicollinearity detection\")\n",
    "print(\"\\n‚ö†Ô∏è Warning: Correlation ‚â† Causation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## üéØ Central Limit Theorem - Why Normal Distribution is Everywhere!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate CLT with dice rolls\n",
    "sample_sizes = [1, 2, 5, 30]\n",
    "n_samples = 10000\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, n in enumerate(sample_sizes):\n",
    "    # Roll dice and take average\n",
    "    samples = np.random.randint(1, 7, size=(n_samples, n))\n",
    "    sample_means = samples.mean(axis=1)\n",
    "    \n",
    "    # Plot histogram\n",
    "    axes[idx].hist(sample_means, bins=30, density=True, \n",
    "                  edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    # Overlay normal distribution\n",
    "    x = np.linspace(sample_means.min(), sample_means.max(), 100)\n",
    "    pdf = stats.norm.pdf(x, sample_means.mean(), sample_means.std())\n",
    "    axes[idx].plot(x, pdf, 'r-', linewidth=2, label='Normal fit')\n",
    "    \n",
    "    axes[idx].set_title(f'Average of {n} dice roll(s)', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Sample Mean')\n",
    "    axes[idx].set_ylabel('Density')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Central Limit Theorem in Action', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîî Central Limit Theorem:\\n\")\n",
    "print(\"  The average of many random samples tends toward a normal distribution,\")\n",
    "print(\"  regardless of the original distribution!\")\n",
    "print(\"\\nüß† Why it matters:\")\n",
    "print(\"  - Explains why normal distribution is everywhere\")\n",
    "print(\"  - Foundation of statistical inference\")\n",
    "print(\"  - Enables hypothesis testing\")\n",
    "print(\"  - Used in confidence intervals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## üß™ Hypothesis Testing - Making Decisions with Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B test: Did the new AI model improve performance?\n",
    "print(\"üß™ A/B TEST: MODEL IMPROVEMENT\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model A (old)\n",
    "np.random.seed(42)\n",
    "model_a_accuracy = np.random.normal(0.85, 0.03, 1000)  # Mean: 85%\n",
    "\n",
    "# Model B (new - supposedly better)\n",
    "model_b_accuracy = np.random.normal(0.87, 0.03, 1000)  # Mean: 87%\n",
    "\n",
    "# Statistics\n",
    "mean_a = model_a_accuracy.mean()\n",
    "mean_b = model_b_accuracy.mean()\n",
    "std_a = model_a_accuracy.std()\n",
    "std_b = model_b_accuracy.std()\n",
    "\n",
    "print(f\"Model A (Baseline):\")\n",
    "print(f\"  Mean accuracy: {mean_a:.3f} ¬± {std_a:.3f}\")\n",
    "print(f\"\\nModel B (New):\")\n",
    "print(f\"  Mean accuracy: {mean_b:.3f} ¬± {std_b:.3f}\")\n",
    "print(f\"\\nDifference: {mean_b - mean_a:.3f} ({(mean_b-mean_a)/mean_a*100:+.1f}%)\")\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(model_b_accuracy, model_a_accuracy)\n",
    "\n",
    "print(f\"\\nüìä Statistical Test (t-test):\")\n",
    "print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "print(f\"  p-value: {p_value:.6f}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(f\"\\n‚úÖ Result: SIGNIFICANT (p < {alpha})\")\n",
    "    print(\"   Model B is statistically better than Model A!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Result: NOT SIGNIFICANT (p >= {alpha})\")\n",
    "    print(\"   Cannot conclude Model B is better.\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(model_a_accuracy, bins=30, alpha=0.6, label='Model A', density=True)\n",
    "plt.hist(model_b_accuracy, bins=30, alpha=0.6, label='Model B', density=True)\n",
    "plt.axvline(mean_a, color='blue', linestyle='--', linewidth=2, label=f'Mean A: {mean_a:.3f}')\n",
    "plt.axvline(mean_b, color='orange', linestyle='--', linewidth=2, label=f'Mean B: {mean_b:.3f}')\n",
    "plt.xlabel('Accuracy', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('A/B Test: Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Interpretation:\")\n",
    "print(f\"  p-value = {p_value:.6f}\")\n",
    "print(f\"  This means: {p_value*100:.4f}% chance of seeing this difference by random chance\")\n",
    "print(f\"  Since p < 0.05, we're >95% confident the improvement is real!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## üéØ Real AI Example: Confidence Intervals in Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate AI model predictions with uncertainty\n",
    "np.random.seed(42)\n",
    "\n",
    "# True relationship: y = 2x + 1 + noise\n",
    "X = np.linspace(0, 10, 50)\n",
    "y_true = 2 * X + 1\n",
    "y_observed = y_true + np.random.normal(0, 2, len(X))\n",
    "\n",
    "# Model predictions (with uncertainty)\n",
    "y_pred = 2 * X + 1\n",
    "std_pred = 2  # Prediction uncertainty\n",
    "\n",
    "# 95% confidence interval\n",
    "ci_lower = y_pred - 1.96 * std_pred\n",
    "ci_upper = y_pred + 1.96 * std_pred\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Data points\n",
    "plt.scatter(X, y_observed, alpha=0.6, s=50, label='Observed data')\n",
    "\n",
    "# Model prediction\n",
    "plt.plot(X, y_pred, 'r-', linewidth=2, label='Model prediction')\n",
    "\n",
    "# Confidence interval\n",
    "plt.fill_between(X, ci_lower, ci_upper, alpha=0.2, label='95% Confidence Interval')\n",
    "\n",
    "plt.xlabel('Input (X)', fontsize=12)\n",
    "plt.ylabel('Output (y)', fontsize=12)\n",
    "plt.title('AI Predictions with Uncertainty (Confidence Intervals)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate coverage\n",
    "in_ci = np.sum((y_observed >= ci_lower) & (y_observed <= ci_upper))\n",
    "coverage = in_ci / len(y_observed)\n",
    "\n",
    "print(\"üìä Confidence Interval Analysis:\\n\")\n",
    "print(f\"  95% CI coverage: {coverage:.1%} ({in_ci}/{len(y_observed)} points)\")\n",
    "print(f\"  Expected: ~95%\")\n",
    "print(\"\\nüß† This tells us:\")\n",
    "print(\"  - We're 95% confident the true value lies in the shaded region\")\n",
    "print(\"  - Wider intervals = more uncertainty\")\n",
    "print(\"  - Critical for high-stakes AI (healthcare, finance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## üéØ MINI CHALLENGE: Bayesian Spam Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a Naive Bayes spam classifier!\n",
    "\n",
    "print(\"üìß NAIVE BAYES SPAM FILTER\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training data statistics\n",
    "total_emails = 10000\n",
    "spam_count = 3000\n",
    "ham_count = 7000\n",
    "\n",
    "# Word frequencies in spam vs ham\n",
    "word_stats = {\n",
    "    'free': {'spam': 2400/3000, 'ham': 700/7000},\n",
    "    'winner': {'spam': 1800/3000, 'ham': 140/7000},\n",
    "    'meeting': {'spam': 150/3000, 'ham': 3500/7000},\n",
    "    'money': {'spam': 2100/3000, 'ham': 350/7000}\n",
    "}\n",
    "\n",
    "# Prior probabilities\n",
    "P_spam = spam_count / total_emails\n",
    "P_ham = ham_count / total_emails\n",
    "\n",
    "print(f\"Training data: {total_emails:,} emails\")\n",
    "print(f\"  P(spam) = {P_spam:.2f}\")\n",
    "print(f\"  P(ham)  = {P_ham:.2f}\")\n",
    "\n",
    "# Test email\n",
    "test_email = ['free', 'winner', 'money']  # Looks spammy!\n",
    "\n",
    "print(f\"\\nüì® Test email contains: {test_email}\")\n",
    "\n",
    "# TODO: Calculate P(spam|email) using Bayes' theorem\n",
    "# Naive assumption: Words are independent\n",
    "\n",
    "# P(words|spam)\n",
    "P_words_given_spam = P_spam\n",
    "for word in test_email:\n",
    "    P_words_given_spam *= word_stats[word]['spam']\n",
    "\n",
    "# P(words|ham)\n",
    "P_words_given_ham = P_ham\n",
    "for word in test_email:\n",
    "    P_words_given_ham *= word_stats[word]['ham']\n",
    "\n",
    "# Normalize (Bayes' theorem)\n",
    "total_prob = P_words_given_spam + P_words_given_ham\n",
    "P_spam_given_words = P_words_given_spam / total_prob\n",
    "P_ham_given_words = P_words_given_ham / total_prob\n",
    "\n",
    "print(\"\\nüìä Probability Calculations:\\n\")\n",
    "for word in test_email:\n",
    "    print(f\"  P('{word}' | spam) = {word_stats[word]['spam']:.2%}\")\n",
    "    print(f\"  P('{word}' | ham)  = {word_stats[word]['ham']:.2%}\")\n",
    "    print()\n",
    "\n",
    "print(\"üéØ Final Classification:\\n\")\n",
    "print(f\"  P(spam | email) = {P_spam_given_words:.4f} ({P_spam_given_words*100:.2f}%)\")\n",
    "print(f\"  P(ham  | email) = {P_ham_given_words:.4f} ({P_ham_given_words*100:.2f}%)\")\n",
    "\n",
    "if P_spam_given_words > 0.5:\n",
    "    print(f\"\\n‚úÖ Classification: SPAM (confidence: {P_spam_given_words*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Classification: HAM (confidence: {P_ham_given_words*100:.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['SPAM', 'HAM'], [P_spam_given_words, P_ham_given_words], \n",
    "       color=['red', 'green'], alpha=0.7)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.title('Email Classification Results', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1)\n",
    "for i, (label, prob) in enumerate([('SPAM', P_spam_given_words), \n",
    "                                   ('HAM', P_ham_given_words)]):\n",
    "    plt.text(i, prob + 0.02, f'{prob*100:.1f}%', \n",
    "            ha='center', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ú® This is the foundation of spam filters, sentiment analysis, and more!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- ‚úÖ Probability fundamentals (rules, conditional probability)\n",
    "- ‚úÖ Bayes' Theorem (the heart of probabilistic AI)\n",
    "- ‚úÖ Key distributions (Uniform, Bernoulli, Normal)\n",
    "- ‚úÖ Descriptive statistics (mean, median, std, correlation)\n",
    "- ‚úÖ Central Limit Theorem (why normal is everywhere)\n",
    "- ‚úÖ Hypothesis testing (A/B tests, p-values)\n",
    "- ‚úÖ Confidence intervals (quantifying uncertainty)\n",
    "- ‚úÖ Built a Naive Bayes spam filter!\n",
    "\n",
    "**üéØ Probability & Statistics Cheat Sheet:**\n",
    "```python\n",
    "# Probability\n",
    "P(A and B) = P(A) √ó P(B|A)  # Product rule\n",
    "P(A or B) = P(A) + P(B) - P(A and B)  # Sum rule\n",
    "P(A|B) = P(B|A) √ó P(A) / P(B)  # Bayes' theorem\n",
    "\n",
    "# Distributions\n",
    "np.random.uniform()     # Uniform\n",
    "np.random.binomial()    # Bernoulli/Binomial\n",
    "np.random.normal()      # Gaussian/Normal\n",
    "np.random.poisson()     # Poisson\n",
    "\n",
    "# Statistics\n",
    "np.mean(), np.median(), np.std()  # Central tendency\n",
    "np.corrcoef()                     # Correlation\n",
    "stats.ttest_ind()                 # Hypothesis test\n",
    "```\n",
    "\n",
    "**üß† Key Insights:**\n",
    "- AI is fundamentally probabilistic\n",
    "- Uncertainty quantification is critical\n",
    "- Normal distribution appears everywhere (CLT)\n",
    "- Bayes' theorem = updating beliefs with evidence\n",
    "- Always test if improvements are statistically significant!\n",
    "\n",
    "**üéØ Practice Exercise:**\n",
    "\n",
    "Build a complete Bayesian classifier:\n",
    "1. Collect word frequencies from real emails\n",
    "2. Implement smoothing (handle unseen words)\n",
    "3. Calculate accuracy on test set\n",
    "4. Create confusion matrix\n",
    "5. Compare with different threshold values\n",
    "6. Visualize decision boundaries\n",
    "\n",
    "---\n",
    "\n",
    "**üéä WEEK 4 COMPLETE!**\n",
    "\n",
    "You've mastered the math foundations of AI:\n",
    "- Day 1: Linear Algebra (neural network operations)\n",
    "- Day 2: Calculus (how AI learns)\n",
    "- Day 3: Probability (handling uncertainty)\n",
    "\n",
    "**üìö Next Steps:**\n",
    "- Week 5: Introduction to Machine Learning\n",
    "- Week 6: Neural Networks from Scratch\n",
    "- Week 7: Deep Learning with PyTorch\n",
    "\n",
    "**üí° Fun Facts:** \n",
    "- Naive Bayes (1700s math!) still powers modern spam filters\n",
    "- LLMs are probabilistic: They predict P(next_token | context)\n",
    "- Temperature parameter = controlling output randomness\n",
    "- All of AI: Optimize probabilities to make better predictions!\n",
    "\n",
    "---\n",
    "\n",
    "**üèÜ ACHIEVEMENT UNLOCKED: Math Master!**\n",
    "\n",
    "*You now understand the mathematical foundations of ALL modern AI!* üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**Phase 1 Foundations COMPLETE!** üéâ\n",
    "\n",
    "You're ready to build real AI systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
