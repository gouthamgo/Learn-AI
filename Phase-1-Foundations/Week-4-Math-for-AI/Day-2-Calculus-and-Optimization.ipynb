{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üìà Week 4, Day 2: Calculus & Optimization\n",
    "\n",
    "**üéØ Goal:** Master the math that makes AI learn - derivatives, gradients, and optimization\n",
    "\n",
    "**‚è±Ô∏è Time:** 60-90 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- **Training IS optimization** - Finding best model parameters\n",
    "- **Backpropagation** = Chain rule of calculus\n",
    "- **Gradient descent** = Follow the slope downhill\n",
    "- **Learning rate** = How big a step to take\n",
    "- **Every AI model** uses these concepts!\n",
    "\n",
    "---\n",
    "\n",
    "## üî• 2024-2025 AI Trend Alert!\n",
    "\n",
    "**Large Language Model Training**:\n",
    "- GPT-4: Optimizing 1.8 TRILLION parameters\n",
    "- **Gradient descent in billion-dimensional space!**\n",
    "- Advanced optimizers: Adam, AdamW, Lion\n",
    "\n",
    "**Fine-tuning Revolution**:\n",
    "- LoRA, QLoRA = Efficient optimization\n",
    "- **Understanding gradients = Understanding fine-tuning!**\n",
    "\n",
    "**Scaling Laws**:\n",
    "- Loss curves follow power laws\n",
    "- **Calculus predicts how much data/compute you need!**\n",
    "\n",
    "**You'll learn how ChatGPT, Claude, Gemini were trained!** üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üìä Why Calculus?\n",
    "\n",
    "**Calculus** = Math of change and optimization\n",
    "\n",
    "Think of it as:\n",
    "- Algebra: Fixed values (y = 2x + 1) üìè\n",
    "- Calculus: Rates of change (how fast is y changing?) üìà\n",
    "\n",
    "**Real AI example:**\n",
    "```python\n",
    "# Training loop (simplified)\n",
    "for epoch in range(1000):\n",
    "    loss = compute_loss(model, data)\n",
    "    gradient = compute_gradient(loss)  # ‚Üê CALCULUS!\n",
    "    weights = weights - learning_rate * gradient  # ‚Üê OPTIMIZATION!\n",
    "```\n",
    "\n",
    "Let's understand the magic! ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"‚úÖ Ready to learn how AI learns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## üìê Derivatives - Rate of Change\n",
    "\n",
    "### What is a Derivative?\n",
    "\n",
    "**Derivative** = Slope of a function at a point = \"How fast is it changing?\"\n",
    "\n",
    "**Notation:**\n",
    "- f'(x) = derivative of f with respect to x\n",
    "- df/dx = same thing, different notation\n",
    "\n",
    "**In AI:**\n",
    "- How much does loss change when we change a weight?\n",
    "- Which direction should we update parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize derivative as slope\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = x**2  # Function: f(x) = x¬≤\n",
    "\n",
    "# Pick a point\n",
    "x0 = 1.5\n",
    "y0 = x0**2\n",
    "slope = 2 * x0  # Derivative: f'(x) = 2x\n",
    "\n",
    "# Tangent line\n",
    "tangent_y = slope * (x - x0) + y0\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='f(x) = x¬≤')\n",
    "plt.plot(x, tangent_y, 'r--', linewidth=2, label=f'Tangent (slope = {slope:.1f})')\n",
    "plt.plot(x0, y0, 'go', markersize=10, label=f'Point ({x0}, {y0:.2f})')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Derivative = Slope of Tangent Line', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"At x = {x0}:\")\n",
    "print(f\"  Function value: f({x0}) = {y0:.2f}\")\n",
    "print(f\"  Derivative: f'({x0}) = {slope:.1f}\")\n",
    "print(f\"  Meaning: At this point, y increases by {slope:.1f} for every unit increase in x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## üî¢ Common Derivative Rules\n",
    "\n",
    "**You don't need to memorize - NumPy/PyTorch computes them automatically!**\n",
    "\n",
    "But understanding helps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common derivatives\n",
    "print(\"üìê COMMON DERIVATIVE RULES\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rules = [\n",
    "    (\"Constant\", \"f(x) = c\", \"f'(x) = 0\"),\n",
    "    (\"Linear\", \"f(x) = x\", \"f'(x) = 1\"),\n",
    "    (\"Power\", \"f(x) = x¬≤\", \"f'(x) = 2x\"),\n",
    "    (\"Power (general)\", \"f(x) = x‚Åø\", \"f'(x) = n¬∑x‚Åø‚Åª¬π\"),\n",
    "    (\"Exponential\", \"f(x) = eÀ£\", \"f'(x) = eÀ£\"),\n",
    "    (\"Natural log\", \"f(x) = ln(x)\", \"f'(x) = 1/x\"),\n",
    "    (\"Sum\", \"f(x) = g(x) + h(x)\", \"f'(x) = g'(x) + h'(x)\"),\n",
    "    (\"Product\", \"f(x) = g(x)¬∑h(x)\", \"f'(x) = g'(x)¬∑h(x) + g(x)¬∑h'(x)\"),\n",
    "    (\"Chain rule\", \"f(g(x))\", \"f'(g(x))¬∑g'(x)\")\n",
    "]\n",
    "\n",
    "for name, func, deriv in rules:\n",
    "    print(f\"{name:20} {func:20} ‚Üí {deriv}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üß† Chain rule is CRITICAL for backpropagation!\")\n",
    "print(\"   Neural networks = nested functions\")\n",
    "print(\"   Chain rule = how to compute gradients through layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## üìä Partial Derivatives & Gradients\n",
    "\n",
    "**Partial derivative** = Derivative with respect to ONE variable (others held constant)\n",
    "\n",
    "**Gradient** = Vector of all partial derivatives\n",
    "\n",
    "**In AI:** Functions have MILLIONS of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function with 2 variables: f(x,y) = x¬≤ + y¬≤ (bowl shape)\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Partial derivatives\n",
    "def df_dx(x, y):\n",
    "    return 2*x  # ‚àÇf/‚àÇx\n",
    "\n",
    "def df_dy(x, y):\n",
    "    return 2*y  # ‚àÇf/‚àÇy\n",
    "\n",
    "# Gradient vector\n",
    "def gradient(x, y):\n",
    "    return np.array([df_dx(x, y), df_dy(x, y)])\n",
    "\n",
    "# Visualize the function\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('f(x,y) = x¬≤ + y¬≤ (3D View)', fontweight='bold')\n",
    "\n",
    "# Contour plot with gradient vectors\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Plot gradient vectors at several points\n",
    "points = [(-2, -2), (-2, 2), (2, -2), (2, 2), (-1, 0), (0, 1.5)]\n",
    "for px, py in points:\n",
    "    grad = gradient(px, py)\n",
    "    ax2.arrow(px, py, -grad[0]*0.2, -grad[1]*0.2, \n",
    "             head_width=0.15, head_length=0.1, fc='red', ec='red')\n",
    "    ax2.plot(px, py, 'ro', markersize=8)\n",
    "\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Contour Plot with Gradient Vectors (red arrows)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key insights:\")\n",
    "print(\"  - Gradient points in direction of STEEPEST ASCENT\")\n",
    "print(\"  - Negative gradient points toward MINIMUM\")\n",
    "print(\"  - At minimum (0,0), gradient = [0,0]\")\n",
    "print(\"\\nüß† This is the foundation of gradient descent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## üéØ Gradient Descent - How AI Learns!\n",
    "\n",
    "**Algorithm:**\n",
    "1. Start with random parameters\n",
    "2. Compute loss (how wrong is the model?)\n",
    "3. Compute gradient (which direction to improve?)\n",
    "4. Update parameters: `w = w - learning_rate √ó gradient`\n",
    "5. Repeat until converged!\n",
    "\n",
    "**This is LITERALLY how neural networks train!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent on f(x) = x¬≤ + 10\n",
    "def f(x):\n",
    "    return x**2 + 10\n",
    "\n",
    "def df(x):\n",
    "    return 2*x\n",
    "\n",
    "# Gradient descent\n",
    "x = 5.0  # Start far from minimum\n",
    "learning_rate = 0.1\n",
    "num_iterations = 20\n",
    "\n",
    "history = {'x': [x], 'f': [f(x)]}\n",
    "\n",
    "print(\"üöÄ GRADIENT DESCENT IN ACTION\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting at x = {x}, f(x) = {f(x):.2f}\")\n",
    "print(f\"Learning rate = {learning_rate}\")\n",
    "print(f\"\\nIteration | x       | f(x)    | gradient\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    gradient = df(x)\n",
    "    x = x - learning_rate * gradient  # UPDATE RULE!\n",
    "    \n",
    "    history['x'].append(x)\n",
    "    history['f'].append(f(x))\n",
    "    \n",
    "    if i < 10 or i == num_iterations - 1:  # Print first 10 and last\n",
    "        print(f\"{i+1:9} | {x:7.4f} | {f(x):7.4f} | {gradient:7.4f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚úÖ Converged to x = {x:.6f}, f(x) = {f(x):.6f}\")\n",
    "print(f\"   True minimum: x = 0, f(x) = 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent path\n",
    "x_range = np.linspace(-6, 6, 100)\n",
    "y_range = f(x_range)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot function\n",
    "plt.plot(x_range, y_range, 'b-', linewidth=2, label='f(x) = x¬≤ + 10')\n",
    "\n",
    "# Plot gradient descent path\n",
    "plt.plot(history['x'], history['f'], 'ro-', markersize=8, linewidth=2, \n",
    "        label='Gradient Descent Path', alpha=0.7)\n",
    "\n",
    "# Highlight start and end\n",
    "plt.plot(history['x'][0], history['f'][0], 'g*', markersize=20, label='Start')\n",
    "plt.plot(history['x'][-1], history['f'][-1], 'r*', markersize=20, label='End')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Gradient Descent: Finding the Minimum', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Notice how:\")\n",
    "print(\"  - Steps are LARGE when far from minimum (steep slope)\")\n",
    "print(\"  - Steps get SMALLER as we approach minimum (flatter slope)\")\n",
    "print(\"  - This is why gradient descent is efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## üéöÔ∏è Learning Rate - The Most Important Hyperparameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for idx, lr in enumerate(learning_rates, 1):\n",
    "    plt.subplot(2, 2, idx)\n",
    "    \n",
    "    # Run gradient descent\n",
    "    x = 5.0\n",
    "    history = {'x': [x], 'f': [f(x)]}\n",
    "    \n",
    "    for i in range(20):\n",
    "        gradient = df(x)\n",
    "        x = x - lr * gradient\n",
    "        history['x'].append(x)\n",
    "        history['f'].append(f(x))\n",
    "    \n",
    "    # Plot\n",
    "    x_range = np.linspace(-6, 6, 100)\n",
    "    plt.plot(x_range, f(x_range), 'b-', linewidth=2, alpha=0.5)\n",
    "    plt.plot(history['x'], history['f'], 'ro-', markersize=6)\n",
    "    plt.plot(history['x'][0], history['f'][0], 'g*', markersize=15)\n",
    "    plt.plot(history['x'][-1], history['f'][-1], 'r*', markersize=15)\n",
    "    \n",
    "    plt.title(f'Learning Rate = {lr}', fontweight='bold')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(0, 40)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéöÔ∏è Learning Rate Effects:\\n\")\n",
    "print(\"  Too small (0.01): Slow convergence, many iterations needed\")\n",
    "print(\"  Just right (0.1): Fast, stable convergence\")\n",
    "print(\"  Too large (0.5): Overshooting, oscillations\")\n",
    "print(\"  Way too large (1.0): Divergence! Explodes!\")\n",
    "print(\"\\nüß† This is why tuning learning rate is crucial for AI training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## üß† Real Example: Training a Simple Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data: y = 3x + 2 + noise\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randn(100, 1) * 2\n",
    "y_train = 3 * X_train + 2 + np.random.randn(100, 1) * 0.5\n",
    "\n",
    "# Visualize data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, alpha=0.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Training Data (y = 3x + 2 + noise)', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Goal: Learn a line y = w*x + b that fits this data\")\n",
    "print(\"   True values: w = 3, b = 2\")\n",
    "print(\"   We'll use gradient descent to find them!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression with gradient descent\n",
    "print(\"üöÄ TRAINING LINEAR MODEL WITH GRADIENT DESCENT\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize parameters randomly\n",
    "w = np.random.randn()\n",
    "b = np.random.randn()\n",
    "print(f\"Initial parameters: w = {w:.3f}, b = {b:.3f}\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Track history\n",
    "loss_history = []\n",
    "w_history = [w]\n",
    "b_history = [b]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: predictions\n",
    "    y_pred = w * X_train + b\n",
    "    \n",
    "    # Compute loss (Mean Squared Error)\n",
    "    loss = np.mean((y_pred - y_train)**2)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Compute gradients (partial derivatives)\n",
    "    dw = (2/len(X_train)) * np.sum((y_pred - y_train) * X_train)\n",
    "    db = (2/len(X_train)) * np.sum(y_pred - y_train)\n",
    "    \n",
    "    # Update parameters (gradient descent!)\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "    \n",
    "    w_history.append(w)\n",
    "    b_history.append(b)\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 20 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch:3d}: Loss = {loss:.4f}, w = {w:.3f}, b = {b:.3f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚úÖ Final parameters: w = {w:.3f}, b = {b:.3f}\")\n",
    "print(f\"   True parameters:  w = 3.000, b = 2.000\")\n",
    "print(f\"   Error: w_error = {abs(w-3):.3f}, b_error = {abs(b-2):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0].plot(loss_history, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Loss Curve (Training Progress)', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Parameter evolution\n",
    "axes[1].plot(w_history, label='w (slope)', linewidth=2)\n",
    "axes[1].plot(b_history, label='b (intercept)', linewidth=2)\n",
    "axes[1].axhline(y=3, color='blue', linestyle='--', alpha=0.5, label='True w')\n",
    "axes[1].axhline(y=2, color='orange', linestyle='--', alpha=0.5, label='True b')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Parameter Value')\n",
    "axes[1].set_title('Parameter Convergence', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Final fit\n",
    "axes[2].scatter(X_train, y_train, alpha=0.5, label='Data')\n",
    "x_line = np.linspace(X_train.min(), X_train.max(), 100)\n",
    "y_line = w * x_line + b\n",
    "axes[2].plot(x_line, y_line, 'r-', linewidth=2, label=f'Learned: y={w:.2f}x+{b:.2f}')\n",
    "axes[2].set_xlabel('X')\n",
    "axes[2].set_ylabel('y')\n",
    "axes[2].set_title('Final Model Fit', fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ The model learned the relationship from data!\")\n",
    "print(\"   This is EXACTLY how neural networks train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## üîÑ Backpropagation - Chain Rule in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2-layer neural network (forward & backward pass)\n",
    "print(\"üß† BACKPROPAGATION EXAMPLE (2-Layer Network)\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Input\n",
    "x = np.array([[0.5, 0.8]])\n",
    "y_true = np.array([[0.9]])\n",
    "\n",
    "# Initialize weights (small random values)\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(2, 3) * 0.1  # Layer 1: 2 ‚Üí 3\n",
    "b1 = np.zeros((1, 3))\n",
    "W2 = np.random.randn(3, 1) * 0.1  # Layer 2: 3 ‚Üí 1\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "# Activation function (sigmoid)\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# FORWARD PASS\n",
    "print(\"FORWARD PASS:\")\n",
    "z1 = x @ W1 + b1\n",
    "a1 = sigmoid(z1)\n",
    "print(f\"  Layer 1: z1 = x @ W1 + b1\")\n",
    "print(f\"  Layer 1 output (a1): {a1}\")\n",
    "\n",
    "z2 = a1 @ W2 + b2\n",
    "a2 = sigmoid(z2)\n",
    "print(f\"  Layer 2: z2 = a1 @ W2 + b2\")\n",
    "print(f\"  Final output (a2): {a2}\")\n",
    "print(f\"  True value: {y_true}\")\n",
    "\n",
    "# Loss\n",
    "loss = 0.5 * (a2 - y_true)**2\n",
    "print(f\"  Loss: {loss[0,0]:.6f}\\n\")\n",
    "\n",
    "# BACKWARD PASS (Backpropagation!)\n",
    "print(\"BACKWARD PASS (computing gradients):\")\n",
    "\n",
    "# Output layer gradients\n",
    "dL_da2 = a2 - y_true  # Derivative of loss w.r.t. output\n",
    "dL_dz2 = dL_da2 * sigmoid_derivative(z2)  # Chain rule!\n",
    "dL_dW2 = a1.T @ dL_dz2  # Gradient for W2\n",
    "dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)  # Gradient for b2\n",
    "\n",
    "print(f\"  ‚àÇL/‚àÇW2 shape: {dL_dW2.shape}\")\n",
    "print(f\"  ‚àÇL/‚àÇb2 shape: {dL_db2.shape}\")\n",
    "\n",
    "# Hidden layer gradients\n",
    "dL_da1 = dL_dz2 @ W2.T  # Backpropagate through W2\n",
    "dL_dz1 = dL_da1 * sigmoid_derivative(z1)  # Chain rule!\n",
    "dL_dW1 = x.T @ dL_dz1  # Gradient for W1\n",
    "dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)  # Gradient for b1\n",
    "\n",
    "print(f\"  ‚àÇL/‚àÇW1 shape: {dL_dW1.shape}\")\n",
    "print(f\"  ‚àÇL/‚àÇb1 shape: {dL_db1.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚ú® This is backpropagation!\")\n",
    "print(\"   - Forward: Compute outputs\")\n",
    "print(\"   - Backward: Compute gradients (chain rule)\")\n",
    "print(\"   - Update: weights = weights - lr * gradients\")\n",
    "print(\"\\nüî• PyTorch/TensorFlow do this automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## üéØ MINI CHALLENGE: Train a Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this neural network training loop!\n",
    "\n",
    "# Generate XOR dataset (classic problem linear models can't solve!)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])  # XOR truth table\n",
    "\n",
    "print(\"üéØ XOR Problem (Requires Non-Linear Model)\\n\")\n",
    "print(\"Input ‚Üí Output\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"{X[i]} ‚Üí {y[i][0]}\")\n",
    "\n",
    "# Initialize network (2 ‚Üí 4 ‚Üí 1)\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(2, 4) * 0.5\n",
    "b1 = np.zeros((1, 4))\n",
    "W2 = np.random.randn(4, 1) * 0.5\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.5\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # TODO: Forward pass\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    # TODO: Compute loss\n",
    "    loss = np.mean((a2 - y)**2)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # TODO: Backward pass (backpropagation)\n",
    "    dL_da2 = 2 * (a2 - y) / len(X)\n",
    "    dL_dz2 = dL_da2 * sigmoid_derivative(z2)\n",
    "    dL_dW2 = a1.T @ dL_dz2\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
    "    \n",
    "    dL_da1 = dL_dz2 @ W2.T\n",
    "    dL_dz1 = dL_da1 * sigmoid_derivative(z1)\n",
    "    dL_dW1 = X.T @ dL_dz1\n",
    "    dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
    "    \n",
    "    # TODO: Update weights\n",
    "    W2 -= learning_rate * dL_dW2\n",
    "    b2 -= learning_rate * dL_db2\n",
    "    W1 -= learning_rate * dL_dW1\n",
    "    b1 -= learning_rate * dL_db1\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 2000 == 0:\n",
    "        print(f\"Epoch {epoch:5d}: Loss = {loss:.6f}\")\n",
    "\n",
    "# Test the trained network\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TRAINED NETWORK PREDICTIONS:\\n\")\n",
    "print(\"Input    | True | Predicted | Rounded\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(len(X)):\n",
    "    z1 = X[i:i+1] @ W1 + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    pred = sigmoid(z2)\n",
    "    print(f\"{X[i]}  |  {y[i][0]}   |   {pred[0,0]:.4f}   |    {round(pred[0,0])}\")\n",
    "\n",
    "print(\"\\nüéâ Network successfully learned XOR!\")\n",
    "print(\"   This requires non-linear activation (sigmoid)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Neural Network Training: XOR Problem', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale to see details\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Notice:\")\n",
    "print(\"  - Loss decreases rapidly at first\")\n",
    "print(\"  - Then slows down (diminishing returns)\")\n",
    "print(\"  - This is typical of neural network training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- ‚úÖ Derivatives (rate of change, slopes)\n",
    "- ‚úÖ Partial derivatives & gradients\n",
    "- ‚úÖ Gradient descent (THE optimization algorithm)\n",
    "- ‚úÖ Learning rate (most important hyperparameter)\n",
    "- ‚úÖ Backpropagation (chain rule for neural networks)\n",
    "- ‚úÖ Trained a linear model from scratch\n",
    "- ‚úÖ Built and trained a neural network for XOR!\n",
    "\n",
    "**üéØ Calculus for AI Cheat Sheet:**\n",
    "```python\n",
    "# Gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = compute_loss(predictions, y_true)\n",
    "    \n",
    "    # Backward pass (compute gradients)\n",
    "    gradients = compute_gradients(loss)\n",
    "    \n",
    "    # Update (gradient descent step)\n",
    "    weights = weights - learning_rate * gradients\n",
    "\n",
    "# Key concepts\n",
    "derivative = slope at a point\n",
    "gradient = vector of partial derivatives\n",
    "learning_rate = step size (tune carefully!)\n",
    "chain_rule = backpropagation through layers\n",
    "```\n",
    "\n",
    "**üß† Key Insights:**\n",
    "- Training = Optimization = Finding minimum loss\n",
    "- Gradient points toward steepest increase\n",
    "- Negative gradient points toward minimum\n",
    "- Learning rate controls step size\n",
    "- Backpropagation = chain rule applied layer by layer\n",
    "\n",
    "**üéØ Practice Exercise:**\n",
    "\n",
    "Train a 3-layer neural network:\n",
    "1. Architecture: 2 ‚Üí 8 ‚Üí 4 ‚Üí 1\n",
    "2. Dataset: Generate non-linear classification data\n",
    "3. Use ReLU activation (try: max(0, x))\n",
    "4. Implement Adam optimizer (advanced gradient descent)\n",
    "5. Plot decision boundary\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Next Lesson:** Day 3 - Probability & Statistics (Understanding Uncertainty!)\n",
    "\n",
    "**üí° Fun Fact:** \n",
    "- GPT-4 training: Gradient descent in 1.8 trillion dimensions!\n",
    "- Took months of compute on thousands of GPUs\n",
    "- Cost: Estimated $100+ million\n",
    "- All using the same principles you just learned!\n",
    "\n",
    "---\n",
    "\n",
    "*You now understand how ChatGPT, Claude, and Gemini learn!* üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
