{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üßÆ Week 4, Day 1: Linear Algebra for AI\n",
    "\n",
    "**üéØ Goal:** Master the math that powers neural networks, transformers, and all modern AI\n",
    "\n",
    "**‚è±Ô∏è Time:** 60-90 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- **Neural networks ARE linear algebra** - Every layer is matrix multiplication\n",
    "- **Transformers** (GPT, BERT, Claude) = Attention matrices\n",
    "- **Word embeddings** = Vectors in high-dimensional space\n",
    "- **Image processing** = Matrix operations on pixel arrays\n",
    "- **Recommendation systems** = Vector similarity\n",
    "\n",
    "---\n",
    "\n",
    "## üî• 2024-2025 AI Trend Alert!\n",
    "\n",
    "**Large Language Models** are PURE linear algebra:\n",
    "- GPT-4: 1.8 trillion parameters = GIANT matrices\n",
    "- **Every token prediction = thousands of matrix multiplications!**\n",
    "- Training = optimizing billion-dimensional spaces\n",
    "\n",
    "**Transformer Architecture** revolutionized AI:\n",
    "- Self-attention = Query, Key, Value MATRICES\n",
    "- **Understanding Q¬∑K·µÄ = Understanding ChatGPT!**\n",
    "\n",
    "**RAG & Vector Databases**:\n",
    "- Text ‚Üí Vectors (embeddings)\n",
    "- Search = Cosine similarity (dot product!)\n",
    "- **Linear algebra makes RAG possible!**\n",
    "\n",
    "**You'll learn the exact math inside ChatGPT, Claude, Gemini!** üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üìä Why Linear Algebra?\n",
    "\n",
    "**Linear algebra** = Math of vectors and matrices\n",
    "\n",
    "Think of it as:\n",
    "- Regular algebra: Single numbers (1 + 2 = 3) üî¢\n",
    "- Linear algebra: Arrays of numbers ([1,2,3] + [4,5,6]) üìä\n",
    "\n",
    "**Real AI example:**\n",
    "```python\n",
    "# A neuron in GPT-4\n",
    "input_vector = [0.5, 0.3, 0.8, ...]  # 12,288 numbers!\n",
    "weight_matrix = [[w‚ÇÅ‚ÇÅ, w‚ÇÅ‚ÇÇ, ...], ...]  # 12,288 √ó 12,288\n",
    "output = matrix_multiply(weights, input)  # One layer!\n",
    "```\n",
    "\n",
    "Let's start with the basics! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"‚úÖ Ready to learn the math inside AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## üìê Vectors - The Building Blocks\n",
    "\n",
    "### What is a Vector?\n",
    "\n",
    "A **vector** = list of numbers with direction and magnitude\n",
    "\n",
    "**In AI:**\n",
    "- Word embeddings: \"cat\" = [0.2, -0.5, 0.8, ...] (300+ dimensions!)\n",
    "- Image pixels: [255, 128, 64, ...] (millions of values)\n",
    "- User preferences: [likes_action, likes_comedy, ...]\n",
    "\n",
    "Let's visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors\n",
    "v1 = np.array([3, 2])  # 2D vector\n",
    "v2 = np.array([1, 4])\n",
    "\n",
    "print(\"Vector v1:\", v1)\n",
    "print(\"Vector v2:\", v2)\n",
    "print(\"\\nShape:\", v1.shape)  # (2,) = 2 elements\n",
    "\n",
    "# Visualize vectors\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.006, label='v1 [3,2]')\n",
    "plt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.006, label='v2 [1,4]')\n",
    "\n",
    "plt.xlim(-1, 5)\n",
    "plt.ylim(-1, 5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('2D Vectors Visualization')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Vectors have:\")\n",
    "print(\"  - Direction (angle)\")\n",
    "print(\"  - Magnitude (length)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## ‚ûï Vector Operations\n",
    "\n",
    "### 1Ô∏è‚É£ Vector Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector addition\n",
    "v1 = np.array([3, 2])\n",
    "v2 = np.array([1, 4])\n",
    "v_sum = v1 + v2\n",
    "\n",
    "print(\"v1 + v2 =\", v_sum)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.006, label='v1')\n",
    "plt.quiver(v1[0], v1[1], v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.006, label='v2')\n",
    "plt.quiver(0, 0, v_sum[0], v_sum[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.008, label='v1+v2')\n",
    "\n",
    "plt.xlim(-1, 6)\n",
    "plt.ylim(-1, 7)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.title('Vector Addition (Head-to-Tail Rule)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüß† AI Example: Word embeddings\")\n",
    "print(\"   'king' - 'man' + 'woman' ‚âà 'queen'\")\n",
    "print(\"   This ACTUALLY works with vector math!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Scalar Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar multiplication (scale a vector)\n",
    "v = np.array([2, 1])\n",
    "v_scaled = 2 * v\n",
    "\n",
    "print(\"v =\", v)\n",
    "print(\"2 * v =\", v_scaled)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.006, label='v')\n",
    "plt.quiver(0, 0, v_scaled[0], v_scaled[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.008, label='2*v')\n",
    "\n",
    "plt.xlim(-1, 5)\n",
    "plt.ylim(-1, 3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.title('Scalar Multiplication (Scaling)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüß† AI Example: Learning rate in gradient descent\")\n",
    "print(\"   new_weights = old_weights - learning_rate * gradient\")\n",
    "print(\"   Scaling the gradient vector!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Dot Product - THE MOST IMPORTANT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product (scalar product)\n",
    "v1 = np.array([2, 3])\n",
    "v2 = np.array([1, 4])\n",
    "\n",
    "dot_product = np.dot(v1, v2)\n",
    "# Or: v1 @ v2  (Python 3.5+)\n",
    "\n",
    "print(\"v1 =\", v1)\n",
    "print(\"v2 =\", v2)\n",
    "print(\"\\nDot product = v1 ¬∑ v2 =\", dot_product)\n",
    "print(\"\\nCalculation: (2√ó1) + (3√ó4) = 2 + 12 = 14\")\n",
    "\n",
    "# Geometric interpretation\n",
    "magnitude_v1 = np.linalg.norm(v1)\n",
    "magnitude_v2 = np.linalg.norm(v2)\n",
    "cos_angle = dot_product / (magnitude_v1 * magnitude_v2)\n",
    "angle_rad = np.arccos(cos_angle)\n",
    "angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "print(f\"\\nüìê Geometric interpretation:\")\n",
    "print(f\"   ||v1|| = {magnitude_v1:.2f}\")\n",
    "print(f\"   ||v2|| = {magnitude_v2:.2f}\")\n",
    "print(f\"   Angle between vectors = {angle_deg:.1f}¬∞\")\n",
    "print(f\"   v1 ¬∑ v2 = ||v1|| √ó ||v2|| √ó cos(Œ∏)\")\n",
    "\n",
    "print(\"\\nüß† AI Applications:\")\n",
    "print(\"   ‚úÖ Neural network forward pass\")\n",
    "print(\"   ‚úÖ Transformer attention mechanism\")\n",
    "print(\"   ‚úÖ Cosine similarity (text search)\")\n",
    "print(\"   ‚úÖ Recommendation systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### üéØ Real AI Example: Text Similarity with Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate word embeddings (simplified)\n",
    "# In reality: 300-1536 dimensions!\n",
    "embeddings = {\n",
    "    'cat': np.array([0.8, 0.3, 0.1, 0.9]),\n",
    "    'dog': np.array([0.7, 0.4, 0.2, 0.8]),\n",
    "    'car': np.array([0.1, 0.9, 0.8, 0.2]),\n",
    "    'kitten': np.array([0.85, 0.25, 0.05, 0.95])\n",
    "}\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate cosine similarity (normalized dot product)\"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Compare 'cat' with all words\n",
    "query_word = 'cat'\n",
    "query_vec = embeddings[query_word]\n",
    "\n",
    "print(f\"üîç Finding words similar to '{query_word}':\\n\")\n",
    "similarities = {}\n",
    "for word, vec in embeddings.items():\n",
    "    if word != query_word:\n",
    "        sim = cosine_similarity(query_vec, vec)\n",
    "        similarities[word] = sim\n",
    "        print(f\"   {word}: {sim:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ Most similar: {max(similarities, key=similarities.get)}\")\n",
    "print(\"\\n‚ú® This is EXACTLY how:\")\n",
    "print(\"   - Google Search finds relevant documents\")\n",
    "print(\"   - ChatGPT retrieves context (RAG)\")\n",
    "print(\"   - Spotify recommends similar songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## üî≤ Matrices - Multiple Vectors Together\n",
    "\n",
    "A **matrix** = 2D array of numbers (rectangular grid)\n",
    "\n",
    "**In AI:**\n",
    "- Neural network weights\n",
    "- Batch of data (rows = samples, columns = features)\n",
    "- Images (rows √ó columns = pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "B = np.array([[2, 0],\n",
    "              [1, 3],\n",
    "              [4, 1]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"Shape: {A.shape} (2 rows, 3 columns)\")\n",
    "\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "print(f\"Shape: {B.shape} (3 rows, 2 columns)\")\n",
    "\n",
    "print(\"\\nüß† AI Example:\")\n",
    "print(\"   A = Batch of 2 samples with 3 features each\")\n",
    "print(\"   B = Weight matrix connecting 3 inputs to 2 outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## üîÑ Matrix Multiplication - THE CORE OF NEURAL NETWORKS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "B = np.array([[2, 0],\n",
    "              [1, 3],\n",
    "              [4, 1]])\n",
    "\n",
    "C = A @ B  # or np.matmul(A, B) or np.dot(A, B)\n",
    "\n",
    "print(\"A @ B = C\\n\")\n",
    "print(f\"({A.shape[0]}√ó{A.shape[1]}) @ ({B.shape[0]}√ó{B.shape[1]}) = ({C.shape[0]}√ó{C.shape[1]})\")\n",
    "print(\"\\nResult C:\")\n",
    "print(C)\n",
    "\n",
    "print(\"\\nüìê How it works:\")\n",
    "print(f\"   C[0,0] = (1√ó2) + (2√ó1) + (3√ó4) = {C[0,0]}\")\n",
    "print(f\"   C[0,1] = (1√ó0) + (2√ó3) + (3√ó1) = {C[0,1]}\")\n",
    "print(f\"   C[1,0] = (4√ó2) + (5√ó1) + (6√ó4) = {C[1,0]}\")\n",
    "print(f\"   C[1,1] = (4√ó0) + (5√ó3) + (6√ó1) = {C[1,1]}\")\n",
    "\n",
    "print(\"\\nüî• KEY RULE: (m√ón) @ (n√óp) = (m√óp)\")\n",
    "print(\"   Inner dimensions MUST match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## üß† Neural Network Layer = Matrix Multiplication!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a simple neural network layer\n",
    "print(\"ü§ñ SIMPLE NEURAL NETWORK LAYER\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Input: 1 sample with 3 features\n",
    "X = np.array([[0.5, 0.8, 0.2]])  # Shape: (1, 3)\n",
    "print(\"Input (1 sample, 3 features):\")\n",
    "print(X)\n",
    "\n",
    "# Weights: 3 inputs ‚Üí 4 neurons\n",
    "W = np.array([[0.1, 0.2, 0.3, 0.4],\n",
    "              [0.5, 0.6, 0.7, 0.8],\n",
    "              [0.9, 1.0, 1.1, 1.2]])  # Shape: (3, 4)\n",
    "print(\"\\nWeights (3‚Üí4 neurons):\")\n",
    "print(W)\n",
    "\n",
    "# Bias\n",
    "b = np.array([0.1, 0.1, 0.1, 0.1])  # Shape: (4,)\n",
    "print(\"\\nBias:\")\n",
    "print(b)\n",
    "\n",
    "# Forward pass: Y = X @ W + b\n",
    "Y = X @ W + b\n",
    "print(\"\\nOutput (1 sample, 4 neurons):\")\n",
    "print(Y)\n",
    "print(f\"Shape: {Y.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚ú® This is LITERALLY how neural networks work!\")\n",
    "print(\"   Every layer: output = input @ weights + bias\")\n",
    "print(\"   GPT-4 does this THOUSANDS of times per token!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## üéØ Transformer Attention Mechanism (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified self-attention (what makes GPT/BERT work!)\n",
    "print(\"üî• TRANSFORMER SELF-ATTENTION (Simplified)\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Input: 3 tokens (words), each with 4-dim embedding\n",
    "X = np.array([[1.0, 0.5, 0.2, 0.8],  # Token 1\n",
    "              [0.3, 0.9, 0.1, 0.6],  # Token 2\n",
    "              [0.7, 0.2, 0.9, 0.4]]) # Token 3\n",
    "print(f\"Input embeddings (3 tokens √ó 4 dims):\\n{X}\\n\")\n",
    "\n",
    "# Weight matrices (normally learned during training)\n",
    "W_Q = np.random.randn(4, 4) * 0.1  # Query\n",
    "W_K = np.random.randn(4, 4) * 0.1  # Key\n",
    "W_V = np.random.randn(4, 4) * 0.1  # Value\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q = X @ W_Q  # (3, 4) @ (4, 4) = (3, 4)\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "print(f\"Q (Queries): {Q.shape}\")\n",
    "print(f\"K (Keys): {K.shape}\")\n",
    "print(f\"V (Values): {V.shape}\\n\")\n",
    "\n",
    "# Attention scores: Q @ K^T\n",
    "attention_scores = Q @ K.T  # (3, 4) @ (4, 3) = (3, 3)\n",
    "print(f\"Attention scores (Q @ K^T):\\n{attention_scores}\\n\")\n",
    "\n",
    "# Softmax to get attention weights (sum to 1)\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(attention_scores)\n",
    "print(f\"Attention weights (after softmax):\\n{attention_weights}\\n\")\n",
    "print(f\"Each row sums to: {attention_weights.sum(axis=1)}\\n\")\n",
    "\n",
    "# Apply attention to values\n",
    "output = attention_weights @ V  # (3, 3) @ (3, 4) = (3, 4)\n",
    "print(f\"Output (attention applied):\\n{output}\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ This is the CORE of GPT, BERT, Claude, Gemini!\")\n",
    "print(\"   - Q @ K^T = 'How much should tokens attend to each other?'\")\n",
    "print(\"   - Softmax = Convert scores to probabilities\")\n",
    "print(\"   - attention @ V = Weighted combination of values\")\n",
    "print(\"\\n‚ú® You just understood transformers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## üîÑ Matrix Transpose & Other Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix transpose (flip rows and columns)\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "A_T = A.T  # or np.transpose(A)\n",
    "\n",
    "print(\"Original A (2√ó3):\")\n",
    "print(A)\n",
    "print(\"\\nTransposed A^T (3√ó2):\")\n",
    "print(A_T)\n",
    "\n",
    "print(\"\\nüß† AI Usage: Attention mechanism needs K^T\")\n",
    "print(\"   Q @ K^T creates attention scores!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other important operations\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "\n",
    "# Determinant\n",
    "det_A = np.linalg.det(A)\n",
    "print(f\"\\nDeterminant: {det_A}\")\n",
    "\n",
    "# Inverse (A √ó A^(-1) = Identity)\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(f\"\\nInverse A^(-1):\")\n",
    "print(A_inv)\n",
    "\n",
    "# Verify: A @ A^(-1) ‚âà I\n",
    "identity = A @ A_inv\n",
    "print(f\"\\nA @ A^(-1) (should be identity):\")\n",
    "print(identity)\n",
    "\n",
    "# Eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "print(f\"\\nEigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors:\\n{eigenvectors}\")\n",
    "\n",
    "print(\"\\nüß† AI Applications:\")\n",
    "print(\"   - Inverse: Solving systems of equations\")\n",
    "print(\"   - Eigenvalues: PCA (dimensionality reduction)\")\n",
    "print(\"   - Eigenvectors: Principal components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## üéØ MINI CHALLENGE: Build a 2-Layer Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this neural network!\n",
    "\n",
    "print(\"üß† 2-LAYER NEURAL NETWORK FROM SCRATCH\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Input: 2 samples, 3 features each\n",
    "X = np.array([[0.5, 0.8, 0.2],\n",
    "              [0.3, 0.6, 0.9]])\n",
    "print(\"Input X (2 samples √ó 3 features):\")\n",
    "print(X)\n",
    "\n",
    "# Layer 1: 3 ‚Üí 4 neurons\n",
    "W1 = np.random.randn(3, 4) * 0.5\n",
    "b1 = np.zeros((1, 4))\n",
    "\n",
    "# TODO: Compute Layer 1 output\n",
    "Z1 = X @ W1 + b1\n",
    "A1 = np.maximum(0, Z1)  # ReLU activation\n",
    "\n",
    "print(f\"\\nLayer 1 output (2 √ó 4):\")\n",
    "print(A1)\n",
    "\n",
    "# Layer 2: 4 ‚Üí 2 neurons (binary classification)\n",
    "W2 = np.random.randn(4, 2) * 0.5\n",
    "b2 = np.zeros((1, 2))\n",
    "\n",
    "# TODO: Compute Layer 2 output\n",
    "Z2 = A1 @ W2 + b2\n",
    "\n",
    "# Softmax activation (convert to probabilities)\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "predictions = softmax(Z2)\n",
    "\n",
    "print(f\"\\nFinal predictions (2 samples √ó 2 classes):\")\n",
    "print(predictions)\n",
    "print(f\"\\nProbabilities sum to: {predictions.sum(axis=1)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚ú® You just built a neural network with linear algebra!\")\n",
    "print(\"\\nArchitecture:\")\n",
    "print(\"   Input (3) ‚Üí Layer1 (4) ‚Üí Layer2 (2) ‚Üí Softmax\")\n",
    "print(\"\\nAll operations:\")\n",
    "print(\"   - Matrix multiplication (X @ W)\")\n",
    "print(\"   - Vector addition (+ b)\")\n",
    "print(\"   - Element-wise operations (ReLU, softmax)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## üéØ Real Example: Image as Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple \"image\" (grayscale)\n",
    "image = np.array([[0, 0, 0, 0, 0],\n",
    "                 [0, 1, 1, 1, 0],\n",
    "                 [0, 1, 0, 1, 0],\n",
    "                 [0, 1, 1, 1, 0],\n",
    "                 [0, 0, 0, 0, 0]])\n",
    "\n",
    "print(\"Image as matrix (5√ó5):\")\n",
    "print(image)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Image as Matrix (0=black, 1=white)')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Simple filter (edge detection)\n",
    "filter_edge = np.array([[-1, -1, -1],\n",
    "                       [-1,  8, -1],\n",
    "                       [-1, -1, -1]])\n",
    "\n",
    "print(\"\\nüß† In CNNs:\")\n",
    "print(\"   - Images = matrices\")\n",
    "print(\"   - Filters = small matrices\")\n",
    "print(\"   - Convolution = matrix multiplication!\")\n",
    "print(\"\\nReal images:\")\n",
    "print(\"   - Grayscale: 28√ó28 (MNIST)\")\n",
    "print(\"   - Color: 224√ó224√ó3 (ImageNet)\")\n",
    "print(\"   - HD: 1920√ó1080√ó3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- ‚úÖ Vectors (the building blocks of AI)\n",
    "- ‚úÖ Vector operations (addition, scaling, dot product)\n",
    "- ‚úÖ Dot product = cosine similarity (text search!)\n",
    "- ‚úÖ Matrices (collections of vectors)\n",
    "- ‚úÖ Matrix multiplication (THE core of neural networks)\n",
    "- ‚úÖ Neural network layers = matrix operations\n",
    "- ‚úÖ Transformer attention = Q @ K^T\n",
    "- ‚úÖ Built a 2-layer neural network from scratch!\n",
    "\n",
    "**üéØ Linear Algebra Cheat Sheet:**\n",
    "```python\n",
    "# Vectors\n",
    "v = np.array([1, 2, 3])\n",
    "v1 + v2              # Addition\n",
    "2 * v                # Scaling\n",
    "np.dot(v1, v2)       # Dot product\n",
    "np.linalg.norm(v)    # Magnitude\n",
    "\n",
    "# Matrices\n",
    "A = np.array([[1,2],[3,4]])\n",
    "A @ B                # Matrix multiplication\n",
    "A.T                  # Transpose\n",
    "np.linalg.inv(A)     # Inverse\n",
    "np.linalg.det(A)     # Determinant\n",
    "\n",
    "# Neural networks\n",
    "output = input @ weights + bias  # Forward pass\n",
    "```\n",
    "\n",
    "**üß† Key Insights:**\n",
    "- Every neural network layer = matrix multiplication\n",
    "- Transformer attention = Q @ K^T @ V\n",
    "- Text similarity = cosine similarity (dot product)\n",
    "- Images = matrices of pixels\n",
    "\n",
    "**üéØ Practice Exercise:**\n",
    "\n",
    "Build a 3-layer neural network:\n",
    "1. Input: 5 features\n",
    "2. Hidden layer 1: 8 neurons (ReLU)\n",
    "3. Hidden layer 2: 4 neurons (ReLU)\n",
    "4. Output: 3 classes (Softmax)\n",
    "5. Test with random input data\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Next Lesson:** Day 2 - Calculus & Optimization (How AI Learns!)\n",
    "\n",
    "**üí° Fun Fact:** \n",
    "- GPT-4 has ~1.8 trillion parameters\n",
    "- Every forward pass = thousands of matrix multiplications\n",
    "- Training = optimizing matrices with TRILLIONS of numbers!\n",
    "\n",
    "---\n",
    "\n",
    "*You now understand the math inside ChatGPT, Claude, and Gemini!* üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
