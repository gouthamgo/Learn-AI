{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîß Week 2, Day 3: Data Manipulation & Cleaning\n",
        "\n",
        "**üéØ Goal:** Master data cleaning and transformation - The real AI work!\n",
        "\n",
        "**‚è±Ô∏è Time:** 60-90 minutes\n",
        "\n",
        "**üåü Why This Matters for AI:**\n",
        "- **80% of AI work is data preparation** - Not modeling!\n",
        "- Dirty data = Bad AI models (Garbage in, Garbage out)\n",
        "- Feature engineering often beats fancy algorithms\n",
        "- These skills separate junior from senior data scientists\n",
        "\n",
        "---\n",
        "\n",
        "## üî• 2024-2025 AI Trend Alert!\n",
        "\n",
        "**RAG (Retrieval-Augmented Generation)** powers modern AI:\n",
        "- Combines LLMs with your company's data\n",
        "- **Pandas cleans and processes knowledge bases!**\n",
        "- Used by ChatGPT Enterprise, Claude for Work\n",
        "\n",
        "**Transformer Models** (BERT, GPT, LLaMA) require:\n",
        "- Clean, tokenized text data\n",
        "- **Pandas handles millions of training examples!**\n",
        "- Proper preprocessing = 20-30% accuracy improvement\n",
        "\n",
        "**Foundation Models** trained on trillions of tokens:\n",
        "- Data quality > Data quantity\n",
        "- **Pandas filters and deduplicates at scale!**\n",
        "\n",
        "**You'll learn the exact techniques used to prepare GPT-4 and Claude training data!** üöÄ\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"Pandas version:\", pd.__version__)\n",
        "print(\"‚úÖ Ready to clean some data!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ Part 1: Data Cleaning - Handling Missing Values\n",
        "\n",
        "**Real AI Problem:** Missing data is EVERYWHERE!\n",
        "- User didn't fill out a form field\n",
        "- Sensor malfunction\n",
        "- API timeout\n",
        "- Data corruption\n",
        "\n",
        "**You MUST handle it correctly or your AI will fail!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a messy AI training dataset (realistic!)\n",
        "np.random.seed(42)\n",
        "\n",
        "messy_data = pd.DataFrame({\n",
        "    'user_id': range(1, 21),\n",
        "    'age': [25, np.nan, 35, 28, np.nan, 45, 32, 29, np.nan, 38,\n",
        "            42, 31, np.nan, 27, 36, 33, np.nan, 41, 30, 34],\n",
        "    'income': [50000, 60000, np.nan, 55000, 48000, np.nan, 62000, 58000, 51000, np.nan,\n",
        "               70000, np.nan, 54000, 56000, 59000, 61000, 53000, np.nan, 57000, 63000],\n",
        "    'purchase_amount': [100, 250, 150, np.nan, 200, 180, np.nan, 220, 190, 210,\n",
        "                        np.nan, 170, 160, 240, np.nan, 200, 185, 195, 230, np.nan],\n",
        "    'email': ['user1@email.com', None, 'user3@email.com', 'user4@email.com', None,\n",
        "              'user6@email.com', None, 'user8@email.com', 'user9@email.com', None,\n",
        "              'user11@email.com', 'user12@email.com', None, 'user14@email.com', 'user15@email.com',\n",
        "              None, 'user17@email.com', 'user18@email.com', None, 'user20@email.com']\n",
        "})\n",
        "\n",
        "print(\"üîç Messy Dataset (like real AI data!):\")\n",
        "print(messy_data)\n",
        "print(f\"\\nShape: {messy_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Step 1: Detect Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# isnull() or isna() - Find missing values\n",
        "print(\"‚ùì Missing Values (True = Missing):\")\n",
        "print(messy_data.isnull().head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count missing values per column\n",
        "print(\"üìä Missing Values Summary:\")\n",
        "missing_count = messy_data.isnull().sum()\n",
        "print(missing_count)\n",
        "\n",
        "print(\"\\nüìä Percentage Missing:\")\n",
        "missing_pct = (messy_data.isnull().sum() / len(messy_data) * 100).round(2)\n",
        "print(missing_pct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Total missing values in entire dataset\n",
        "total_missing = messy_data.isnull().sum().sum()\n",
        "total_cells = messy_data.size\n",
        "print(f\"üîç Total missing: {total_missing} out of {total_cells} cells ({total_missing/total_cells*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üõ†Ô∏è Step 2: Handle Missing Values\n",
        "\n",
        "**4 Main Strategies:**\n",
        "1. **Drop** - Remove rows/columns with missing values\n",
        "2. **Fill with constant** - Use 0, \"Unknown\", etc.\n",
        "3. **Fill with statistics** - Mean, median, mode\n",
        "4. **Forward/backward fill** - Use previous/next value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 1: Drop rows with ANY missing values\n",
        "clean_dropany = messy_data.dropna()\n",
        "print(f\"üìä Original: {len(messy_data)} rows\")\n",
        "print(f\"üìä After dropna(): {len(clean_dropany)} rows\")\n",
        "print(f\"‚ùå Lost {len(messy_data) - len(clean_dropany)} rows ({(len(messy_data) - len(clean_dropany))/len(messy_data)*100:.1f}%)\")\n",
        "print(\"\\n‚ö†Ô∏è Too aggressive! Lost most of our data!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 2: Drop rows where ALL values are missing\n",
        "clean_dropall = messy_data.dropna(how='all')\n",
        "print(f\"üìä Original: {len(messy_data)} rows\")\n",
        "print(f\"üìä After dropna(how='all'): {len(clean_dropall)} rows\")\n",
        "print(\"‚úÖ Only drops completely empty rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 3: Drop rows with missing values in SPECIFIC columns\n",
        "# Keep rows with valid user_id and age only\n",
        "clean_subset = messy_data.dropna(subset=['age', 'income'])\n",
        "print(f\"üìä Original: {len(messy_data)} rows\")\n",
        "print(f\"üìä After dropna(subset=['age', 'income']): {len(clean_subset)} rows\")\n",
        "print(\"‚úÖ More targeted approach!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 4: Fill with constant value\n",
        "df_filled = messy_data.copy()\n",
        "df_filled['email'] = df_filled['email'].fillna('no_email@unknown.com')\n",
        "print(\"üìß Email column after filling:\")\n",
        "print(df_filled['email'].tail(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 5: Fill with MEAN (for numerical data)\n",
        "df_filled = messy_data.copy()\n",
        "age_mean = df_filled['age'].mean()\n",
        "df_filled['age'] = df_filled['age'].fillna(age_mean)\n",
        "\n",
        "print(f\"üìä Mean age: {age_mean:.2f}\")\n",
        "print(\"\\nüìä Age column after filling with mean:\")\n",
        "print(df_filled['age'].head(10))\n",
        "print(f\"\\n‚úÖ Missing values in age: {df_filled['age'].isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 6: Fill with MEDIAN (better for outliers!)\n",
        "df_filled = messy_data.copy()\n",
        "income_median = df_filled['income'].median()\n",
        "df_filled['income'] = df_filled['income'].fillna(income_median)\n",
        "\n",
        "print(f\"üìä Median income: ${income_median:,.0f}\")\n",
        "print(f\"‚úÖ Missing values in income: {df_filled['income'].isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 7: Fill ALL numeric columns at once!\n",
        "df_clean = messy_data.copy()\n",
        "\n",
        "# Fill each column with its median\n",
        "for col in ['age', 'income', 'purchase_amount']:\n",
        "    df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
        "\n",
        "# Fill email with placeholder\n",
        "df_clean['email'] = df_clean['email'].fillna('unknown@email.com')\n",
        "\n",
        "print(\"üéâ Cleaned Dataset:\")\n",
        "print(df_clean)\n",
        "print(f\"\\n‚úÖ Missing values: {df_clean.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**üß† AI Best Practices:**\n",
        "```python\n",
        "# Numerical features:\n",
        "- Mean: When data is normally distributed\n",
        "- Median: When you have outliers (SAFER!)\n",
        "- Mode: For categorical data\n",
        "\n",
        "# Categorical features:\n",
        "- \"Unknown\" or \"Missing\" category\n",
        "- Mode (most common value)\n",
        "\n",
        "# Time series:\n",
        "- Forward fill (use previous value)\n",
        "- Backward fill (use next value)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Part 2: Data Transformation - GroupBy & Aggregation\n",
        "\n",
        "**Think SQL GROUP BY, but more powerful!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create realistic e-commerce dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "sales_data = pd.DataFrame({\n",
        "    'date': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
        "    'product': np.random.choice(['AI Course', 'ML Book', 'GPU Cloud', 'ChatGPT Plus'], 100),\n",
        "    'category': np.random.choice(['Education', 'Hardware', 'Software'], 100),\n",
        "    'region': np.random.choice(['North America', 'Europe', 'Asia'], 100),\n",
        "    'sales': np.random.randint(100, 1000, 100),\n",
        "    'quantity': np.random.randint(1, 20, 100)\n",
        "})\n",
        "\n",
        "print(\"üõí E-commerce Sales Data:\")\n",
        "print(sales_data.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GroupBy single column\n",
        "print(\"üìä Total Sales by Product:\")\n",
        "product_sales = sales_data.groupby('product')['sales'].sum().sort_values(ascending=False)\n",
        "print(product_sales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multiple aggregations at once\n",
        "print(\"üìä Sales Statistics by Region:\")\n",
        "region_stats = sales_data.groupby('region')['sales'].agg([\n",
        "    'count',   # Number of transactions\n",
        "    'sum',     # Total sales\n",
        "    'mean',    # Average sale\n",
        "    'min',     # Minimum sale\n",
        "    'max'      # Maximum sale\n",
        "]).round(2)\n",
        "print(region_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GroupBy multiple columns\n",
        "print(\"üìä Sales by Region AND Product:\")\n",
        "region_product = sales_data.groupby(['region', 'product'])['sales'].sum()\n",
        "print(region_product)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom aggregations\n",
        "print(\"üìä Custom Aggregation by Category:\")\n",
        "category_stats = sales_data.groupby('category').agg({\n",
        "    'sales': ['sum', 'mean', 'max'],\n",
        "    'quantity': ['sum', 'mean'],\n",
        "    'product': 'count'  # Count of transactions\n",
        "}).round(2)\n",
        "print(category_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add calculated column: Revenue per unit\n",
        "sales_data['revenue_per_unit'] = sales_data['sales'] / sales_data['quantity']\n",
        "\n",
        "print(\"üí∞ Sales Data with Revenue per Unit:\")\n",
        "print(sales_data[['product', 'sales', 'quantity', 'revenue_per_unit']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÄ Part 3: Pivot Tables - Excel-style Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pivot table: Region vs Product\n",
        "print(\"üìä Pivot Table - Total Sales by Region and Product:\")\n",
        "pivot = sales_data.pivot_table(\n",
        "    values='sales',\n",
        "    index='region',\n",
        "    columns='product',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "print(pivot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add row and column totals\n",
        "print(\"üìä Pivot Table with Margins (Totals):\")\n",
        "pivot_margins = sales_data.pivot_table(\n",
        "    values='sales',\n",
        "    index='region',\n",
        "    columns='product',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0,\n",
        "    margins=True,\n",
        "    margins_name='Total'\n",
        ")\n",
        "print(pivot_margins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multiple aggregations in pivot table\n",
        "print(\"üìä Advanced Pivot - Multiple Metrics:\")\n",
        "pivot_advanced = sales_data.pivot_table(\n",
        "    values='sales',\n",
        "    index='region',\n",
        "    columns='product',\n",
        "    aggfunc=['sum', 'mean', 'count'],\n",
        "    fill_value=0\n",
        ").round(2)\n",
        "print(pivot_advanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó Part 4: Merging Datasets - Combining Multiple Data Sources\n",
        "\n",
        "**Real AI Scenario:** \n",
        "- User data in one table\n",
        "- Purchase history in another\n",
        "- Product details in third\n",
        "- **You need to combine them!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create related datasets (like real databases)\n",
        "\n",
        "# Table 1: Users\n",
        "users = pd.DataFrame({\n",
        "    'user_id': [1, 2, 3, 4, 5],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
        "    'age': [25, 30, 35, 28, 32],\n",
        "    'country': ['USA', 'UK', 'Canada', 'USA', 'Germany']\n",
        "})\n",
        "\n",
        "# Table 2: Purchases\n",
        "purchases = pd.DataFrame({\n",
        "    'purchase_id': [101, 102, 103, 104, 105, 106],\n",
        "    'user_id': [1, 2, 1, 3, 2, 6],  # Note: user_id 6 doesn't exist!\n",
        "    'product': ['AI Course', 'ML Book', 'GPU Cloud', 'AI Course', 'ChatGPT Plus', 'ML Book'],\n",
        "    'amount': [299, 49, 150, 299, 20, 49]\n",
        "})\n",
        "\n",
        "# Table 3: Product Details\n",
        "products = pd.DataFrame({\n",
        "    'product': ['AI Course', 'ML Book', 'GPU Cloud', 'ChatGPT Plus'],\n",
        "    'category': ['Education', 'Education', 'Hardware', 'Software'],\n",
        "    'rating': [4.8, 4.5, 4.9, 4.7]\n",
        "})\n",
        "\n",
        "print(\"üë• Users:\")\n",
        "print(users)\n",
        "print(\"\\nüõí Purchases:\")\n",
        "print(purchases)\n",
        "print(\"\\nüì¶ Products:\")\n",
        "print(products)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîó Join Types Explained:\n",
        "\n",
        "- **INNER JOIN** - Only matching rows (most common)\n",
        "- **LEFT JOIN** - All from left table + matches from right\n",
        "- **RIGHT JOIN** - All from right table + matches from left\n",
        "- **OUTER JOIN** - All rows from both tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INNER JOIN - Only users who made purchases\n",
        "print(\"üîó INNER JOIN (Users + Purchases):\")\n",
        "inner = pd.merge(users, purchases, on='user_id', how='inner')\n",
        "print(inner)\n",
        "print(f\"\\nüìä Rows: {len(inner)} (only matching user_ids)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LEFT JOIN - All users, even without purchases\n",
        "print(\"üîó LEFT JOIN (All Users + Purchases):\")\n",
        "left = pd.merge(users, purchases, on='user_id', how='left')\n",
        "print(left)\n",
        "print(f\"\\nüìä Rows: {len(left)} (all users kept)\")\n",
        "print(\"\\n‚ö†Ô∏è Diana and Eve have NaN - they didn't purchase anything!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RIGHT JOIN - All purchases, even orphaned ones\n",
        "print(\"üîó RIGHT JOIN (Users + All Purchases):\")\n",
        "right = pd.merge(users, purchases, on='user_id', how='right')\n",
        "print(right)\n",
        "print(f\"\\nüìä Rows: {len(right)} (all purchases kept)\")\n",
        "print(\"\\n‚ö†Ô∏è Last purchase has NaN user info - user_id 6 doesn't exist!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OUTER JOIN - Everything!\n",
        "print(\"üîó OUTER JOIN (All Users + All Purchases):\")\n",
        "outer = pd.merge(users, purchases, on='user_id', how='outer')\n",
        "print(outer)\n",
        "print(f\"\\nüìä Rows: {len(outer)} (everything kept)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chain multiple merges - Real AI workflow!\n",
        "print(\"üîó Multi-Table Join (Users + Purchases + Products):\")\n",
        "\n",
        "# Step 1: Join users and purchases\n",
        "user_purchases = pd.merge(users, purchases, on='user_id', how='inner')\n",
        "\n",
        "# Step 2: Join with product details\n",
        "complete_data = pd.merge(user_purchases, products, on='product', how='left')\n",
        "\n",
        "print(complete_data)\n",
        "print(\"\\n‚úÖ Complete dataset ready for AI modeling!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Part 5: Feature Engineering for Machine Learning\n",
        "\n",
        "**Feature engineering = Creating new features from existing data**\n",
        "\n",
        "**This is where AI magic happens!** ü™Ñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create AI customer dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "customers = pd.DataFrame({\n",
        "    'customer_id': range(1, 101),\n",
        "    'signup_date': pd.date_range('2023-01-01', periods=100, freq='3D'),\n",
        "    'total_purchases': np.random.randint(1, 50, 100),\n",
        "    'total_spent': np.random.randint(100, 5000, 100),\n",
        "    'support_tickets': np.random.randint(0, 10, 100),\n",
        "    'days_since_last_purchase': np.random.randint(1, 180, 100),\n",
        "    'email_opened': np.random.randint(0, 100, 100),\n",
        "    'email_sent': np.random.randint(50, 150, 100)\n",
        "})\n",
        "\n",
        "print(\"üë• Customer Dataset:\")\n",
        "print(customers.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature 1: Average Order Value (AOV)\n",
        "customers['avg_order_value'] = (customers['total_spent'] / customers['total_purchases']).round(2)\n",
        "\n",
        "print(\"üí∞ Feature: Average Order Value\")\n",
        "print(customers[['customer_id', 'total_purchases', 'total_spent', 'avg_order_value']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature 2: Customer Lifetime (days)\n",
        "customers['customer_lifetime_days'] = (pd.Timestamp('2024-11-17') - customers['signup_date']).dt.days\n",
        "\n",
        "print(\"üìÖ Feature: Customer Lifetime\")\n",
        "print(customers[['customer_id', 'signup_date', 'customer_lifetime_days']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature 3: Email Engagement Rate\n",
        "customers['email_open_rate'] = (customers['email_opened'] / customers['email_sent'] * 100).round(2)\n",
        "\n",
        "print(\"üìß Feature: Email Engagement Rate\")\n",
        "print(customers[['customer_id', 'email_sent', 'email_opened', 'email_open_rate']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature 4: Purchase Frequency (purchases per month)\n",
        "customers['purchase_frequency'] = (customers['total_purchases'] / (customers['customer_lifetime_days'] / 30)).round(2)\n",
        "\n",
        "print(\"üõí Feature: Purchase Frequency (per month)\")\n",
        "print(customers[['customer_id', 'total_purchases', 'customer_lifetime_days', 'purchase_frequency']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature 5: Customer Segment (based on spending)\n",
        "def categorize_customer(spent):\n",
        "    if spent < 1000:\n",
        "        return 'Low-Value'\n",
        "    elif spent < 3000:\n",
        "        return 'Medium-Value'\n",
        "    else:\n",
        "        return 'High-Value'\n",
        "\n",
        "customers['customer_segment'] = customers['total_spent'].apply(categorize_customer)\n",
        "\n",
        "print(\"üèÜ Feature: Customer Segment\")\n",
        "print(customers['customer_segment'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature 6: Churn Risk (Binary classification target!)\n",
        "# If customer hasn't purchased in 90 days = High churn risk\n",
        "customers['churn_risk'] = (customers['days_since_last_purchase'] > 90).astype(int)\n",
        "\n",
        "print(\"‚ö†Ô∏è Feature: Churn Risk (TARGET for ML model!)\")\n",
        "print(customers['churn_risk'].value_counts())\n",
        "print(f\"\\nChurn rate: {customers['churn_risk'].mean() * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature 7: RFM Score (Recency, Frequency, Monetary)\n",
        "# Industry-standard for customer analysis!\n",
        "\n",
        "# Recency score (lower days = better)\n",
        "customers['recency_score'] = pd.qcut(customers['days_since_last_purchase'], \n",
        "                                      q=5, labels=[5, 4, 3, 2, 1])\n",
        "\n",
        "# Frequency score\n",
        "customers['frequency_score'] = pd.qcut(customers['total_purchases'].rank(method='first'), \n",
        "                                        q=5, labels=[1, 2, 3, 4, 5])\n",
        "\n",
        "# Monetary score\n",
        "customers['monetary_score'] = pd.qcut(customers['total_spent'].rank(method='first'), \n",
        "                                       q=5, labels=[1, 2, 3, 4, 5])\n",
        "\n",
        "# Combined RFM score\n",
        "customers['rfm_score'] = (customers['recency_score'].astype(int) + \n",
        "                          customers['frequency_score'].astype(int) + \n",
        "                          customers['monetary_score'].astype(int))\n",
        "\n",
        "print(\"üéØ RFM Scores (Used by Amazon, Netflix, Spotify!):\")\n",
        "print(customers[['customer_id', 'recency_score', 'frequency_score', 'monetary_score', 'rfm_score']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final dataset ready for ML!\n",
        "print(\"üéâ Final Feature-Engineered Dataset:\")\n",
        "print(customers.head())\n",
        "print(f\"\\nüìä Shape: {customers.shape}\")\n",
        "print(f\"üìä Features: {customers.shape[1]} columns\")\n",
        "print(f\"üìä Original: 8 columns ‚Üí Engineered: {customers.shape[1]} columns!\")\n",
        "print(\"\\n‚úÖ Ready for machine learning!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Part 6: Complete AI Preprocessing Pipeline\n",
        "\n",
        "**Let's build a real AI data pipeline from scratch!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create messy, realistic AI training dataset\n",
        "np.random.seed(100)\n",
        "\n",
        "raw_data = pd.DataFrame({\n",
        "    'user_id': range(1, 201),\n",
        "    'age': np.random.randint(18, 70, 200),\n",
        "    'income': np.random.randint(30000, 150000, 200),\n",
        "    'credit_score': np.random.randint(300, 850, 200),\n",
        "    'loan_amount': np.random.randint(5000, 100000, 200),\n",
        "    'employment_years': np.random.randint(0, 40, 200),\n",
        "    'previous_defaults': np.random.randint(0, 5, 200),\n",
        "    'approved': np.random.choice([0, 1], 200, p=[0.3, 0.7])  # TARGET\n",
        "})\n",
        "\n",
        "# Introduce missing values (realistic!)\n",
        "raw_data.loc[np.random.choice(raw_data.index, 20), 'income'] = np.nan\n",
        "raw_data.loc[np.random.choice(raw_data.index, 15), 'credit_score'] = np.nan\n",
        "raw_data.loc[np.random.choice(raw_data.index, 10), 'employment_years'] = np.nan\n",
        "\n",
        "print(\"üìä RAW DATA (Messy!):\")\n",
        "print(raw_data.head(10))\n",
        "print(f\"\\nMissing values:\\n{raw_data.isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1: Handle missing values\n",
        "print(\"üßπ STEP 1: Cleaning missing values...\")\n",
        "\n",
        "df_clean = raw_data.copy()\n",
        "df_clean['income'] = df_clean['income'].fillna(df_clean['income'].median())\n",
        "df_clean['credit_score'] = df_clean['credit_score'].fillna(df_clean['credit_score'].median())\n",
        "df_clean['employment_years'] = df_clean['employment_years'].fillna(df_clean['employment_years'].median())\n",
        "\n",
        "print(f\"‚úÖ Missing values: {df_clean.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2: Feature Engineering\n",
        "print(\"\\nüõ†Ô∏è STEP 2: Engineering new features...\")\n",
        "\n",
        "# Debt-to-Income Ratio\n",
        "df_clean['debt_to_income'] = (df_clean['loan_amount'] / df_clean['income']).round(3)\n",
        "\n",
        "# Credit score category\n",
        "def categorize_credit(score):\n",
        "    if score < 580:\n",
        "        return 'Poor'\n",
        "    elif score < 670:\n",
        "        return 'Fair'\n",
        "    elif score < 740:\n",
        "        return 'Good'\n",
        "    elif score < 800:\n",
        "        return 'Very Good'\n",
        "    else:\n",
        "        return 'Excellent'\n",
        "\n",
        "df_clean['credit_category'] = df_clean['credit_score'].apply(categorize_credit)\n",
        "\n",
        "# Risk score (custom formula)\n",
        "df_clean['risk_score'] = (\n",
        "    (df_clean['previous_defaults'] * 2) + \n",
        "    (df_clean['debt_to_income'] * 10) - \n",
        "    (df_clean['credit_score'] / 100)\n",
        ").round(2)\n",
        "\n",
        "print(f\"‚úÖ New features created: 3\")\n",
        "print(f\"‚úÖ Total features: {df_clean.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3: Encode categorical features\n",
        "print(\"\\nüî¢ STEP 3: Encoding categorical features...\")\n",
        "\n",
        "# One-hot encoding for credit_category\n",
        "credit_dummies = pd.get_dummies(df_clean['credit_category'], prefix='credit')\n",
        "df_clean = pd.concat([df_clean, credit_dummies], axis=1)\n",
        "\n",
        "print(f\"‚úÖ One-hot encoded: {len(credit_dummies.columns)} dummy variables\")\n",
        "print(f\"‚úÖ Total features now: {df_clean.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 4: Normalize numerical features (0-1 scale)\n",
        "print(\"\\nüìè STEP 4: Normalizing features...\")\n",
        "\n",
        "features_to_normalize = ['age', 'income', 'credit_score', 'loan_amount', 'employment_years']\n",
        "\n",
        "for col in features_to_normalize:\n",
        "    min_val = df_clean[col].min()\n",
        "    max_val = df_clean[col].max()\n",
        "    df_clean[f'{col}_normalized'] = (df_clean[col] - min_val) / (max_val - min_val)\n",
        "\n",
        "print(f\"‚úÖ Normalized {len(features_to_normalize)} features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 5: Final dataset preparation\n",
        "print(\"\\nüéØ STEP 5: Preparing final ML dataset...\")\n",
        "\n",
        "# Select features for ML model\n",
        "feature_columns = [\n",
        "    'age_normalized', 'income_normalized', 'credit_score_normalized',\n",
        "    'loan_amount_normalized', 'employment_years_normalized',\n",
        "    'debt_to_income', 'risk_score', 'previous_defaults',\n",
        "    'credit_Excellent', 'credit_Fair', 'credit_Good', 'credit_Poor', 'credit_Very Good'\n",
        "]\n",
        "\n",
        "X = df_clean[feature_columns]  # Features\n",
        "y = df_clean['approved']        # Target\n",
        "\n",
        "print(\"‚úÖ PIPELINE COMPLETE!\")\n",
        "print(f\"\\nüìä Feature matrix (X): {X.shape}\")\n",
        "print(f\"üìä Target vector (y): {y.shape}\")\n",
        "print(f\"\\nüéâ Dataset ready for machine learning!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview final dataset\n",
        "print(\"üëÄ Final ML Dataset Preview:\")\n",
        "print(X.head())\n",
        "print(\"\\nüìä Target Distribution:\")\n",
        "print(y.value_counts())\n",
        "print(f\"\\nApproval rate: {y.mean() * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Practice Exercise: Build Your Own Pipeline\n",
        "\n",
        "**Scenario:** You're building a customer churn prediction model!\n",
        "\n",
        "**Your Task:**\n",
        "1. Load the data below\n",
        "2. Handle missing values\n",
        "3. Create 3 new features\n",
        "4. Prepare for ML modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "churn_data = pd.DataFrame({\n",
        "    'customer_id': range(1, 151),\n",
        "    'tenure_months': np.random.randint(1, 72, 150),\n",
        "    'monthly_charges': np.random.uniform(20, 120, 150).round(2),\n",
        "    'total_charges': np.random.uniform(100, 8000, 150).round(2),\n",
        "    'support_calls': np.random.randint(0, 15, 150),\n",
        "    'contract_type': np.random.choice(['Month-to-Month', 'One Year', 'Two Year'], 150),\n",
        "    'churned': np.random.choice([0, 1], 150, p=[0.73, 0.27])  # TARGET\n",
        "})\n",
        "\n",
        "# Add missing values\n",
        "churn_data.loc[np.random.choice(churn_data.index, 15), 'monthly_charges'] = np.nan\n",
        "churn_data.loc[np.random.choice(churn_data.index, 10), 'total_charges'] = np.nan\n",
        "\n",
        "print(\"üìä Customer Churn Dataset:\")\n",
        "print(churn_data.head(10))\n",
        "print(f\"\\nMissing values:\\n{churn_data.isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Your turn!\n",
        "\n",
        "# 1. Fill missing values with median\n",
        "churn_clean = churn_data.copy()\n",
        "churn_clean['monthly_charges'].fillna(churn_clean['monthly_charges'].median(), inplace=True)\n",
        "churn_clean['total_charges'].fillna(churn_clean['total_charges'].median(), inplace=True)\n",
        "\n",
        "print(\"‚úÖ Step 1: Missing values handled\")\n",
        "\n",
        "# 2. Create new features\n",
        "churn_clean['avg_monthly_spend'] = (churn_clean['total_charges'] / churn_clean['tenure_months']).round(2)\n",
        "churn_clean['support_per_month'] = (churn_clean['support_calls'] / churn_clean['tenure_months']).round(3)\n",
        "churn_clean['high_value_customer'] = (churn_clean['monthly_charges'] > churn_clean['monthly_charges'].median()).astype(int)\n",
        "\n",
        "print(\"‚úÖ Step 2: Created 3 new features\")\n",
        "\n",
        "# 3. Encode contract type\n",
        "contract_dummies = pd.get_dummies(churn_clean['contract_type'], prefix='contract')\n",
        "churn_clean = pd.concat([churn_clean, contract_dummies], axis=1)\n",
        "\n",
        "print(\"‚úÖ Step 3: Encoded categorical variable\")\n",
        "\n",
        "# 4. Show final dataset\n",
        "print(\"\\nüéâ Final Dataset:\")\n",
        "print(churn_clean.head())\n",
        "print(f\"\\nShape: {churn_clean.shape}\")\n",
        "print(f\"Churn rate: {churn_clean['churned'].mean() * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "**You just learned:**\n",
        "- ‚úÖ Detecting and handling missing values (7 strategies!)\n",
        "- ‚úÖ Data transformation with GroupBy and aggregation\n",
        "- ‚úÖ Pivot tables for analysis\n",
        "- ‚úÖ Merging datasets (INNER, LEFT, RIGHT, OUTER joins)\n",
        "- ‚úÖ Feature engineering (creating powerful new features!)\n",
        "- ‚úÖ Building complete AI preprocessing pipelines\n",
        "- ‚úÖ Real-world examples (churn, loans, e-commerce)\n",
        "\n",
        "**üéØ Your Complete AI Data Pipeline:**\n",
        "```python\n",
        "# 1. Load data\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# 2. Explore\n",
        "df.info()\n",
        "df.describe()\n",
        "\n",
        "# 3. Clean\n",
        "df.fillna(df.median())\n",
        "df.dropna(subset=['important_col'])\n",
        "\n",
        "# 4. Transform\n",
        "df.groupby('category').agg({'sales': 'sum'})\n",
        "\n",
        "# 5. Merge\n",
        "pd.merge(df1, df2, on='id', how='inner')\n",
        "\n",
        "# 6. Engineer features\n",
        "df['new_feature'] = df['col1'] / df['col2']\n",
        "\n",
        "# 7. Normalize\n",
        "df_normalized = (df - df.min()) / (df.max() - df.min())\n",
        "\n",
        "# 8. Ready for ML!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**üìö Next Week:** Week 3 - Data Visualization (Make your data come alive!)\n",
        "\n",
        "**üí° Industry Secret:** \n",
        "> \"Feature engineering and data preparation are more important than the algorithm choice. A simple model with great features beats a complex model with poor features.\" \n",
        "> ‚Äî Every senior data scientist ever\n",
        "\n",
        "**üèÜ You now have skills that:**\n",
        "- Take 3-6 months to learn in traditional courses\n",
        "- Are used daily by data scientists at FAANG companies\n",
        "- Are required for 90% of AI/ML job interviews\n",
        "- Power real production AI systems\n",
        "\n",
        "---\n",
        "\n",
        "*You're now ready to process data for real AI projects!* üöÄ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
