{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Day 1: Building Your First Neural Network\n",
    "\n",
    "**ğŸ¯ Goal:** Understand neural networks and build one from scratch with NumPy\n",
    "\n",
    "**â±ï¸ Time:** 60-75 minutes\n",
    "\n",
    "**ğŸŒŸ Why This Matters for AI:**\n",
    "- Neural networks power ChatGPT, Claude, GPT-4, and all modern LLMs\n",
    "- Understanding neurons and layers helps you build Transformers, RAG systems, and Agentic AI\n",
    "- This is the foundation of deep learning - everything builds on this!\n",
    "- Multimodal AI (GPT-4V, Gemini) uses neural networks to process images, text, and audio\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ğŸ§  What are Neural Networks?\n",
    "\n",
    "**Neural Networks** are computing systems inspired by the human brain!\n",
    "\n",
    "### ğŸ§¬ Biological Inspiration\n",
    "\n",
    "**Human Brain:**\n",
    "- 86 billion neurons\n",
    "- Each neuron connects to ~7,000 other neurons\n",
    "- Neurons receive signals, process them, and send outputs\n",
    "- Learning happens by strengthening connections!\n",
    "\n",
    "**Artificial Neural Network:**\n",
    "- Artificial neurons (nodes)\n",
    "- Connections between neurons (weights)\n",
    "- Receives inputs, processes them, sends outputs\n",
    "- Learning happens by adjusting weights!\n",
    "\n",
    "```\n",
    "BIOLOGICAL NEURON          â†’          ARTIFICIAL NEURON\n",
    "==================                     ==================\n",
    "Dendrites (inputs)         â†’          Input values (xâ‚, xâ‚‚, ...)\n",
    "Synapses (connections)     â†’          Weights (wâ‚, wâ‚‚, ...)\n",
    "Cell body (processing)     â†’          Summation + Activation\n",
    "Axon (output)              â†’          Output value (y)\n",
    "```\n",
    "\n",
    "### ğŸ¯ Real-World Neural Network Applications (2024-2025):\n",
    "- **ChatGPT & Claude**: Transformer neural networks with billions of parameters\n",
    "- **RAG Systems**: Neural networks encode documents into embeddings\n",
    "- **Image Generation**: Diffusion models (Stable Diffusion, DALL-E)\n",
    "- **Multimodal AI**: GPT-4V uses neural networks for vision + language\n",
    "- **Agentic AI**: Neural networks help agents decide which actions to take"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## ğŸ”¬ Anatomy of a Neural Network\n",
    "\n",
    "### ğŸ§± Core Components:\n",
    "\n",
    "#### 1ï¸âƒ£ **Neurons (Nodes)**\n",
    "The basic unit that receives inputs, processes them, and produces an output\n",
    "\n",
    "#### 2ï¸âƒ£ **Layers**\n",
    "- **Input Layer**: Receives raw data (images, text, numbers)\n",
    "- **Hidden Layers**: Process and transform data (this is where the magic happens!)\n",
    "- **Output Layer**: Produces final predictions\n",
    "\n",
    "#### 3ï¸âƒ£ **Weights**\n",
    "Numbers that control how much influence each input has (like volume knobs!)\n",
    "\n",
    "#### 4ï¸âƒ£ **Bias**\n",
    "An offset value that helps the neuron fit the data better\n",
    "\n",
    "#### 5ï¸âƒ£ **Activation Function**\n",
    "Adds non-linearity - helps the network learn complex patterns!\n",
    "\n",
    "### ğŸ“Š Visual Architecture:\n",
    "\n",
    "```\n",
    "INPUT LAYER    HIDDEN LAYER    OUTPUT LAYER\n",
    "    â—‹              â—‹                â—‹\n",
    "   xâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•± â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•± â•²â”€â”€â”€â”€â”€â”€> Prediction\n",
    "    â—‹            â—‹   â—‹            â—‹   â—‹\n",
    "   xâ‚‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•² â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•² â•±\n",
    "    â—‹              â—‹                â—‹\n",
    "   xâ‚ƒ\n",
    "\n",
    "  [Data]      [Processing]      [Output]\n",
    "```\n",
    "\n",
    "**Each connection has a weight (w)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## âš¡ Activation Functions: The Secret Sauce\n",
    "\n",
    "**Why do we need activation functions?**\n",
    "Without them, neural networks would just be fancy linear regression! Activation functions add **non-linearity**, allowing networks to learn complex patterns.\n",
    "\n",
    "### ğŸ¯ Popular Activation Functions:\n",
    "\n",
    "#### 1ï¸âƒ£ **ReLU (Rectified Linear Unit)** - Most Popular! â­\n",
    "```python\n",
    "ReLU(x) = max(0, x)\n",
    "```\n",
    "- **Output:** 0 if x < 0, else x\n",
    "- **Use:** Hidden layers (default choice in 2024-2025)\n",
    "- **Pros:** Fast, prevents vanishing gradients\n",
    "- **Used in:** GPT, BERT, ResNet, most modern networks\n",
    "\n",
    "#### 2ï¸âƒ£ **Sigmoid** - Classic Function\n",
    "```python\n",
    "Sigmoid(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "- **Output:** Values between 0 and 1\n",
    "- **Use:** Output layer for binary classification\n",
    "- **Pros:** Smooth, interpretable as probability\n",
    "- **Cons:** Can cause vanishing gradients\n",
    "\n",
    "#### 3ï¸âƒ£ **Tanh (Hyperbolic Tangent)**\n",
    "```python\n",
    "Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "```\n",
    "- **Output:** Values between -1 and 1\n",
    "- **Use:** Hidden layers (less common now)\n",
    "- **Pros:** Zero-centered (better than sigmoid)\n",
    "- **Cons:** Still can vanish gradients\n",
    "\n",
    "#### 4ï¸âƒ£ **Softmax** - For Multi-Class\n",
    "```python\n",
    "Softmax(x_i) = e^(x_i) / Î£(e^(x_j))\n",
    "```\n",
    "- **Output:** Probability distribution (sums to 1)\n",
    "- **Use:** Output layer for multi-class classification\n",
    "- **Example:** Classify image as cat (0.7), dog (0.2), bird (0.1)\n",
    "\n",
    "### ğŸ“Š Visual Comparison:\n",
    "```\n",
    "ReLU:     ___/    (turns negative to 0)\n",
    "Sigmoid:  S-curve (squashes to 0-1)\n",
    "Tanh:     S-curve (squashes to -1 to 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## ğŸ”„ Forward Propagation: How Neural Networks Think\n",
    "\n",
    "**Forward Propagation** is the process of passing input data through the network to get a prediction.\n",
    "\n",
    "### ğŸ¯ The Process (Step-by-Step):\n",
    "\n",
    "```\n",
    "Step 1: INPUT\n",
    "   x = [xâ‚, xâ‚‚, xâ‚ƒ] â†’ Raw data\n",
    "\n",
    "Step 2: WEIGHTED SUM (Linear Transformation)\n",
    "   z = (wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + wâ‚ƒÂ·xâ‚ƒ) + b\n",
    "   z = Î£(wáµ¢Â·xáµ¢) + b\n",
    "\n",
    "Step 3: ACTIVATION (Non-linear Transformation)\n",
    "   a = activation(z)\n",
    "   Example: a = ReLU(z) = max(0, z)\n",
    "\n",
    "Step 4: OUTPUT\n",
    "   y = final activation (prediction!)\n",
    "```\n",
    "\n",
    "### ğŸ“ Mathematical Formula:\n",
    "```\n",
    "For a single neuron:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "z = WÂ·X + b          (weighted sum)\n",
    "a = Ïƒ(z)             (activation)\n",
    "\n",
    "Where:\n",
    "- W = weights matrix\n",
    "- X = input vector\n",
    "- b = bias\n",
    "- Ïƒ = activation function\n",
    "```\n",
    "\n",
    "### ğŸ”— Multi-Layer Forward Propagation:\n",
    "```\n",
    "Layer 1: aâ‚ = Ïƒ(Wâ‚Â·X + bâ‚)\n",
    "Layer 2: aâ‚‚ = Ïƒ(Wâ‚‚Â·aâ‚ + bâ‚‚)\n",
    "Layer 3: y = Ïƒ(Wâ‚ƒÂ·aâ‚‚ + bâ‚ƒ)\n",
    "\n",
    "Each layer builds on the previous one!\n",
    "```\n",
    "\n",
    "**This is exactly how ChatGPT processes your input - layer by layer!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## ğŸš€ Let's Build a Neural Network from Scratch!\n",
    "\n",
    "**Project:** Build a neural network to classify if a point is inside or outside a circle\n",
    "\n",
    "**Why this example?**\n",
    "- Simple to visualize\n",
    "- Requires non-linear decision boundary (can't use linear classifier!)\n",
    "- Same principles used in LLMs and image classifiers\n",
    "\n",
    "**Architecture:**\n",
    "- Input Layer: 2 neurons (x, y coordinates)\n",
    "- Hidden Layer: 4 neurons (ReLU activation)\n",
    "- Output Layer: 1 neuron (Sigmoid activation for binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy matplotlib --quiet\n",
    "\n",
    "print(\"âœ… Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ğŸ“š Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Step 1: Implement Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    Returns max(0, x)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    Returns value between 0 and 1\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Tanh activation function\n",
    "    Returns value between -1 and 1\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Test the activation functions\n",
    "test_values = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "print(\"ğŸ”¬ Testing Activation Functions:\")\n",
    "print(f\"Input values: {test_values}\")\n",
    "print(f\"ReLU output:  {relu(test_values)}\")\n",
    "print(f\"Sigmoid output: {sigmoid(test_values)}\")\n",
    "print(f\"Tanh output: {tanh(test_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# ReLU\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x, relu(x), 'b-', linewidth=2)\n",
    "plt.title('ReLU Activation', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Sigmoid\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x, sigmoid(x), 'r-', linewidth=2)\n",
    "plt.title('Sigmoid Activation', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axhline(y=1, color='k', linewidth=0.5, linestyle='--')\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Tanh\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x, tanh(x), 'g-', linewidth=2)\n",
    "plt.title('Tanh Activation', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axhline(y=1, color='k', linewidth=0.5, linestyle='--')\n",
    "plt.axhline(y=-1, color='k', linewidth=0.5, linestyle='--')\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Notice how each activation function transforms the input differently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Step 2: Create Training Data\n",
    "\n",
    "Generate points inside and outside a circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset: points inside and outside a circle\n",
    "def generate_circle_data(n_samples=200):\n",
    "    \"\"\"\n",
    "    Generate random points and label them:\n",
    "    - Inside circle (radius 1): label = 1\n",
    "    - Outside circle: label = 0\n",
    "    \"\"\"\n",
    "    # Generate random points in [-2, 2] x [-2, 2]\n",
    "    X = np.random.randn(n_samples, 2) * 1.5\n",
    "    \n",
    "    # Calculate distance from origin\n",
    "    distances = np.sqrt(X[:, 0]**2 + X[:, 1]**2)\n",
    "    \n",
    "    # Label: 1 if inside circle (radius 1), 0 if outside\n",
    "    y = (distances < 1.0).astype(int).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X_train, y_train = generate_circle_data(200)\n",
    "\n",
    "print(\"ğŸ“Š Dataset created!\")\n",
    "print(f\"Shape of X (features): {X_train.shape}\")\n",
    "print(f\"Shape of y (labels): {y_train.shape}\")\n",
    "print(f\"\\nFirst 5 points:\")\n",
    "print(f\"X: {X_train[:5]}\")\n",
    "print(f\"y: {y_train[:5].ravel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot points inside circle (label = 1)\n",
    "inside = y_train.ravel() == 1\n",
    "plt.scatter(X_train[inside, 0], X_train[inside, 1], \n",
    "            c='blue', label='Inside Circle', alpha=0.6, s=50)\n",
    "\n",
    "# Plot points outside circle (label = 0)\n",
    "outside = y_train.ravel() == 0\n",
    "plt.scatter(X_train[outside, 0], X_train[outside, 1], \n",
    "            c='red', label='Outside Circle', alpha=0.6, s=50)\n",
    "\n",
    "# Draw the circle boundary\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "circle_x = np.cos(theta)\n",
    "circle_y = np.sin(theta)\n",
    "plt.plot(circle_x, circle_y, 'k--', linewidth=2, label='Decision Boundary')\n",
    "\n",
    "plt.xlabel('X coordinate', fontsize=12)\n",
    "plt.ylabel('Y coordinate', fontsize=12)\n",
    "plt.title('Circle Classification Dataset', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ¯ Our goal: Train a neural network to learn this circular boundary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Step 3: Build the Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Simple Neural Network with one hidden layer\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer: 2 neurons (x, y)\n",
    "    - Hidden layer: 4 neurons (ReLU)\n",
    "    - Output layer: 1 neuron (Sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=2, hidden_size=4, output_size=1):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases randomly\n",
    "        \"\"\"\n",
    "        # Xavier initialization (better than random)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        print(\"ğŸ§  Neural Network initialized!\")\n",
    "        print(f\"   Input size: {input_size}\")\n",
    "        print(f\"   Hidden size: {hidden_size}\")\n",
    "        print(f\"   Output size: {output_size}\")\n",
    "        print(f\"   Total parameters: {self.count_parameters()}\")\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total trainable parameters\"\"\"\n",
    "        return (self.W1.size + self.b1.size + \n",
    "                self.W2.size + self.b2.size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (n_samples, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Output predictions (n_samples, 1)\n",
    "        \"\"\"\n",
    "        # Layer 1: Input -> Hidden\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1  # Weighted sum\n",
    "        self.a1 = relu(self.z1)                  # ReLU activation\n",
    "        \n",
    "        # Layer 2: Hidden -> Output\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2  # Weighted sum\n",
    "        self.a2 = sigmoid(self.z2)                     # Sigmoid activation\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Make binary predictions\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            threshold: Classification threshold (default 0.5)\n",
    "        \n",
    "        Returns:\n",
    "            Binary predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return (probabilities > threshold).astype(int)\n",
    "\n",
    "# Create the neural network\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "print(\"\\nğŸ“Š Network Architecture:\")\n",
    "print(f\"   W1 shape: {nn.W1.shape} (Input -> Hidden)\")\n",
    "print(f\"   b1 shape: {nn.b1.shape}\")\n",
    "print(f\"   W2 shape: {nn.W2.shape} (Hidden -> Output)\")\n",
    "print(f\"   b2 shape: {nn.b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Step 4: Test Forward Propagation\n",
    "\n",
    "Let's see what predictions the untrained network makes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward propagation with a few points\n",
    "test_points = np.array([\n",
    "    [0.5, 0.5],   # Inside circle\n",
    "    [2.0, 2.0],   # Outside circle\n",
    "    [0.0, 0.0],   # Center (inside)\n",
    "    [-1.5, 1.5],  # Outside circle\n",
    "])\n",
    "\n",
    "# Get predictions (before training)\n",
    "predictions = nn.forward(test_points)\n",
    "binary_preds = nn.predict(test_points)\n",
    "\n",
    "print(\"ğŸ”® Predictions BEFORE training (random weights):\")\n",
    "print(\"\\nPoint\\t\\tProbability\\tPrediction\\tActual\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "actual_labels = [1, 0, 1, 0]  # True labels\n",
    "for i, point in enumerate(test_points):\n",
    "    dist = np.sqrt(point[0]**2 + point[1]**2)\n",
    "    print(f\"{point}\\t{predictions[i][0]:.4f}\\t\\t{binary_preds[i][0]}\\t\\t{actual_labels[i]}\")\n",
    "\n",
    "print(\"\\nâš ï¸ Random predictions! We need to TRAIN the network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## ğŸ¯ Why This Matters for Modern AI\n",
    "\n",
    "The neural network you just built uses the **exact same principles** as state-of-the-art AI systems!\n",
    "\n",
    "### ğŸ¤– **ChatGPT & Claude (Transformers)**\n",
    "- Use the same forward propagation concept\n",
    "- Just with billions of parameters instead of ~20!\n",
    "- Layers: Input â†’ 96 Transformer Layers â†’ Output\n",
    "- Activation: GELU (advanced ReLU variant)\n",
    "\n",
    "### ğŸ” **RAG Systems**\n",
    "- **Step 1**: Neural network encodes documents into vectors (embeddings)\n",
    "- **Step 2**: Forward propagation to find similar documents\n",
    "- **Step 3**: LLM generates answer using retrieved context\n",
    "\n",
    "### ğŸ¨ **Multimodal AI (GPT-4V, Gemini)**\n",
    "- **Vision Encoder**: Neural network processes images\n",
    "- **Text Encoder**: Neural network processes text\n",
    "- **Fusion Layer**: Combines both modalities\n",
    "- All using forward propagation!\n",
    "\n",
    "### ğŸ¤ **Agentic AI**\n",
    "- Neural networks help agents:\n",
    "  - Perceive environment (inputs)\n",
    "  - Process information (hidden layers)\n",
    "  - Decide actions (outputs)\n",
    "\n",
    "### ğŸ“Š **Scale Comparison:**\n",
    "```\n",
    "Our Network:      ~20 parameters\n",
    "Small CNN:        ~1M parameters\n",
    "BERT:             110M parameters\n",
    "GPT-3:            175B parameters\n",
    "GPT-4:            ~1.7T parameters (estimated)\n",
    "\n",
    "Same architecture, different scale!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## ğŸ¯ YOUR TURN: Interactive Exercise\n",
    "\n",
    "**Challenge:** Build a neural network to classify points in a different pattern!\n",
    "\n",
    "**Task:** Create a network to classify points based on XOR pattern:\n",
    "- Quadrant 1 (x>0, y>0): Class 1\n",
    "- Quadrant 2 (x<0, y>0): Class 0\n",
    "- Quadrant 3 (x<0, y<0): Class 1\n",
    "- Quadrant 4 (x>0, y<0): Class 0\n",
    "\n",
    "This is the famous XOR problem that requires a hidden layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "\n",
    "# Step 1: Create XOR dataset\n",
    "def generate_xor_data(n_samples=200):\n",
    "    \"\"\"\n",
    "    Generate XOR pattern data\n",
    "    TODO: Complete this function\n",
    "    \"\"\"\n",
    "    # Generate random points\n",
    "    X = np.random.randn(n_samples, 2) * 2\n",
    "    \n",
    "    # TODO: Create labels based on XOR logic\n",
    "    # Hint: label = 1 if (x>0 and y>0) OR (x<0 and y<0)\n",
    "    #       label = 0 otherwise\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    y = None  # Replace with your logic\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Step 2: Create and test the network\n",
    "# TODO: Generate XOR data\n",
    "# TODO: Create neural network\n",
    "# TODO: Test forward propagation\n",
    "# TODO: Visualize the data\n",
    "\n",
    "print(\"Complete the TODOs above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### âœ… Solution (Try on your own first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def generate_xor_data(n_samples=200):\n",
    "    \"\"\"Generate XOR pattern data\"\"\"\n",
    "    X = np.random.randn(n_samples, 2) * 2\n",
    "    \n",
    "    # XOR logic: 1 if both positive or both negative\n",
    "    y = ((X[:, 0] > 0) == (X[:, 1] > 0)).astype(int).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X_xor, y_xor = generate_xor_data(200)\n",
    "\n",
    "# Create network\n",
    "xor_nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "class_1 = y_xor.ravel() == 1\n",
    "plt.scatter(X_xor[class_1, 0], X_xor[class_1, 1], \n",
    "            c='blue', label='Class 1', alpha=0.6, s=50)\n",
    "\n",
    "class_0 = y_xor.ravel() == 0\n",
    "plt.scatter(X_xor[class_0, 0], X_xor[class_0, 1], \n",
    "            c='red', label='Class 0', alpha=0.6, s=50)\n",
    "\n",
    "plt.xlabel('X coordinate', fontsize=12)\n",
    "plt.ylabel('Y coordinate', fontsize=12)\n",
    "plt.title('XOR Pattern Dataset', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… XOR network created!\")\n",
    "print(\"ğŸ“Œ Notice: No straight line can separate these classes!\")\n",
    "print(\"   This is why we need neural networks with hidden layers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding the Network Architecture\n",
    "\n",
    "Let's visualize what's happening inside our neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network weights\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Weights from input to hidden layer\n",
    "im1 = axes[0].imshow(nn.W1, cmap='coolwarm', aspect='auto')\n",
    "axes[0].set_title('Weights: Input â†’ Hidden Layer', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Hidden Neurons')\n",
    "axes[0].set_ylabel('Input Features')\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_yticklabels(['x', 'y'])\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Weights from hidden to output layer\n",
    "im2 = axes[1].imshow(nn.W2, cmap='coolwarm', aspect='auto')\n",
    "axes[1].set_title('Weights: Hidden â†’ Output Layer', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Output Neuron')\n",
    "axes[1].set_ylabel('Hidden Neurons')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ¨ Weights Visualization:\")\n",
    "print(\"   - Red = positive weights (strong influence)\")\n",
    "print(\"   - Blue = negative weights (inhibitory)\")\n",
    "print(\"   - White = near zero (weak influence)\")\n",
    "print(\"\\nâš ï¸ These are random weights - we'll learn better ones in Day 2!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- âœ… What neural networks are (inspired by biological neurons)\n",
    "- âœ… Core components: neurons, layers, weights, biases\n",
    "- âœ… Activation functions: ReLU, Sigmoid, Tanh\n",
    "- âœ… Forward propagation (how networks make predictions)\n",
    "- âœ… Built a neural network from scratch with NumPy!\n",
    "- âœ… How these concepts power ChatGPT, RAG, and modern AI\n",
    "\n",
    "### ğŸ¯ Key Takeaways:\n",
    "\n",
    "1. **Neural networks transform data layer by layer**\n",
    "   - Each layer learns different features\n",
    "   - Deep learning = many layers!\n",
    "\n",
    "2. **Activation functions add non-linearity**\n",
    "   - Without them, networks = linear regression\n",
    "   - ReLU is most popular in 2024-2025\n",
    "\n",
    "3. **Forward propagation is just matrix multiplication**\n",
    "   - z = WÂ·X + b (weighted sum)\n",
    "   - a = Ïƒ(z) (activation)\n",
    "\n",
    "4. **Same principles scale to billions of parameters**\n",
    "   - Your network: 20 parameters\n",
    "   - GPT-4: ~1.7 trillion parameters\n",
    "   - Same math, bigger scale!\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ¯ Practice Exercise (Before Day 2):**\n",
    "\n",
    "Build a neural network to classify:\n",
    "- Input: [hours_studied, hours_slept]\n",
    "- Output: Will student pass exam? (0 or 1)\n",
    "\n",
    "Create 20+ training examples and test forward propagation!\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“š Next Lesson:** Day 2 - Training Neural Networks\n",
    "- Backpropagation (how networks learn!)\n",
    "- Gradient descent\n",
    "- Loss functions\n",
    "- Training MNIST classifier\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¬ Remember:**\n",
    "\n",
    "*\"You just built the foundation of modern AI! ChatGPT, Claude, GPT-4 - they all use the same forward propagation you implemented. The only difference? Scale and training data. You're on your way to understanding how AI really works!\"* ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ”— Connections to Modern AI:**\n",
    "- **LLMs**: Stack of neural network layers (96+ layers in GPT-3)\n",
    "- **RAG**: Neural networks encode text into vectors\n",
    "- **Multimodal**: Separate networks for vision/text, combined layers\n",
    "- **Agentic AI**: Networks help agents perceive and decide actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
