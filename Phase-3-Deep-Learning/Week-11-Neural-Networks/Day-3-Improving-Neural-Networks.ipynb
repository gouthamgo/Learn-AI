{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# üìò Day 3: Improving Neural Networks\n",
    "\n",
    "**üéØ Goal:** Learn advanced techniques to build robust, production-ready neural networks\n",
    "\n",
    "**‚è±Ô∏è Time:** 60-75 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Regularization prevents overfitting - crucial for GPT-4 and all LLMs\n",
    "- Dropout and Batch Norm are used in every modern neural network\n",
    "- Learning rate scheduling makes training stable and efficient\n",
    "- These techniques make the difference between research and production AI\n",
    "- Understanding these helps you fine-tune LLMs, build RAG systems, and optimize Agentic AI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## üéØ The Problem: Overfitting\n",
    "\n",
    "**Overfitting** = Model memorizes training data but fails on new data\n",
    "\n",
    "### üéì The Student Analogy:\n",
    "\n",
    "**Good Student (Generalization):**\n",
    "- Understands concepts\n",
    "- Can solve new problems\n",
    "- Applies knowledge flexibly\n",
    "- ‚úÖ This is what we want!\n",
    "\n",
    "**Memorizer (Overfitting):**\n",
    "- Memorizes past exam questions\n",
    "- Fails on slightly different questions\n",
    "- No real understanding\n",
    "- ‚ùå This is overfitting!\n",
    "\n",
    "### üìä Signs of Overfitting:\n",
    "\n",
    "```\n",
    "Training Accuracy: 99% ‚úÖ\n",
    "Test Accuracy:     65% ‚ùå\n",
    "\n",
    "‚Üí Model memorized training data!\n",
    "‚Üí Doesn't generalize to new examples\n",
    "```\n",
    "\n",
    "### üéØ Goal: Balance Fit and Generalization\n",
    "\n",
    "```\n",
    "Underfitting     Just Right      Overfitting\n",
    "    ___           ‚àº‚àº‚àº‚àº             ‚àø‚àø‚àø‚àø‚àø‚àø\n",
    "  Too simple    Good fit       Too complex\n",
    "  High bias    Low bias/var   High variance\n",
    "```\n",
    "\n",
    "### üîç Real-World Impact:\n",
    "\n",
    "**ChatGPT without regularization:**\n",
    "- Would memorize training examples verbatim\n",
    "- Couldn't generate creative responses\n",
    "- Would fail on novel questions\n",
    "\n",
    "**RAG systems without regularization:**\n",
    "- Embeddings wouldn't generalize\n",
    "- Poor retrieval on new documents\n",
    "- Unreliable in production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Regularization Techniques\n",
    "\n",
    "**Regularization** = Techniques to prevent overfitting and improve generalization\n",
    "\n",
    "### 1Ô∏è‚É£ **L2 Regularization (Weight Decay)** ‚≠ê Most Popular!\n",
    "\n",
    "**Idea:** Penalize large weights\n",
    "\n",
    "```python\n",
    "# Regular loss\n",
    "loss = cross_entropy(predictions, labels)\n",
    "\n",
    "# L2 regularized loss\n",
    "loss = cross_entropy(predictions, labels) + Œª √ó Œ£(weights¬≤)\n",
    "                                            ‚Üë\n",
    "                                    Penalty for large weights\n",
    "```\n",
    "\n",
    "**Effect:**\n",
    "- Keeps weights small\n",
    "- Prevents model from relying too much on any single feature\n",
    "- Makes model more robust\n",
    "\n",
    "**Œª (lambda) = regularization strength:**\n",
    "- Œª = 0: No regularization\n",
    "- Œª = 0.01: Weak regularization\n",
    "- Œª = 0.1: Strong regularization\n",
    "\n",
    "**Used in:** GPT, BERT, ResNet, and almost all modern networks!\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **L1 Regularization (Lasso)**\n",
    "\n",
    "**Idea:** Penalize absolute values of weights\n",
    "\n",
    "```python\n",
    "loss = cross_entropy(predictions, labels) + Œª √ó Œ£|weights|\n",
    "```\n",
    "\n",
    "**Effect:**\n",
    "- Drives some weights to exactly zero\n",
    "- Creates sparse models (feature selection)\n",
    "- Useful when you have many irrelevant features\n",
    "\n",
    "**L1 vs L2:**\n",
    "- L1: Some weights ‚Üí 0 (sparsity)\n",
    "- L2: All weights ‚Üí small (shrinkage)\n",
    "- **L2 is more common in deep learning**\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Dropout** üé≤ Revolutionary Technique!\n",
    "\n",
    "**Idea:** Randomly drop neurons during training\n",
    "\n",
    "```\n",
    "Regular Training:\n",
    "‚óã‚îÄ‚îÄ‚óã‚îÄ‚îÄ‚óã\n",
    "‚îÇ  ‚îÇ  ‚îÇ\n",
    "‚óã‚îÄ‚îÄ‚óã‚îÄ‚îÄ‚óã  ‚Üê All neurons active\n",
    "‚îÇ  ‚îÇ  ‚îÇ\n",
    "‚óã‚îÄ‚îÄ‚óã‚îÄ‚îÄ‚óã\n",
    "\n",
    "Dropout (p=0.5):\n",
    "‚óã‚îÄ‚îÄX‚îÄ‚îÄ‚óã\n",
    "‚îÇ     ‚îÇ\n",
    "X‚îÄ‚îÄ‚óã‚îÄ‚îÄ‚óã  ‚Üê 50% randomly dropped\n",
    "‚îÇ  ‚îÇ  \n",
    "‚óã‚îÄ‚îÄ‚óã‚îÄ‚îÄX\n",
    "```\n",
    "\n",
    "**Why it works:**\n",
    "- Forces network to learn redundant representations\n",
    "- Prevents neurons from co-adapting\n",
    "- Like training an ensemble of networks!\n",
    "\n",
    "**Dropout rate (p):**\n",
    "- p = 0.2: Drop 20% of neurons (light regularization)\n",
    "- p = 0.5: Drop 50% of neurons (standard)\n",
    "- p = 0.8: Drop 80% of neurons (heavy regularization)\n",
    "\n",
    "**During inference:** Use all neurons (no dropout!)\n",
    "\n",
    "**Used in:** GPT-3, BERT, almost all modern architectures\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **Early Stopping** ‚è∞\n",
    "\n",
    "**Idea:** Stop training when validation loss stops improving\n",
    "\n",
    "```python\n",
    "best_val_loss = infinity\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    train()\n",
    "    val_loss = validate()\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    if patience > 10:  # No improvement for 10 epochs\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Automatic stopping criterion\n",
    "- Saves training time\n",
    "- Prevents overfitting\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ **Data Augmentation**\n",
    "\n",
    "**Idea:** Create more training data by transforming existing data\n",
    "\n",
    "**For images:**\n",
    "- Rotation, flipping, cropping\n",
    "- Color jittering\n",
    "- Adding noise\n",
    "\n",
    "**For text (LLMs):**\n",
    "- Back-translation\n",
    "- Synonym replacement\n",
    "- Random insertion/deletion\n",
    "\n",
    "**Effect:** More diverse training data ‚Üí better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## üîÑ Batch Normalization\n",
    "\n",
    "**Batch Normalization (BatchNorm)** = Normalize activations between layers\n",
    "\n",
    "### üéØ The Problem: Internal Covariate Shift\n",
    "\n",
    "During training, distributions of layer inputs change:\n",
    "- Layer 1 updates ‚Üí Layer 2 inputs change\n",
    "- Layer 2 updates ‚Üí Layer 3 inputs change\n",
    "- Makes training unstable!\n",
    "\n",
    "### üí° The Solution: Normalize Each Layer\n",
    "\n",
    "```python\n",
    "# For each mini-batch:\n",
    "Œº = mean(batch)\n",
    "œÉ¬≤ = variance(batch)\n",
    "\n",
    "# Normalize\n",
    "x_normalized = (x - Œº) / ‚àö(œÉ¬≤ + Œµ)\n",
    "\n",
    "# Scale and shift (learnable parameters)\n",
    "y = Œ≥ √ó x_normalized + Œ≤\n",
    "```\n",
    "\n",
    "### ‚ú® Benefits:\n",
    "\n",
    "1. **Faster training** (can use higher learning rates)\n",
    "2. **More stable** (reduces sensitivity to initialization)\n",
    "3. **Regularization effect** (slight randomness from batch statistics)\n",
    "4. **Less sensitive to learning rate**\n",
    "\n",
    "### üéØ Where to Use:\n",
    "\n",
    "```\n",
    "Typical Deep Network:\n",
    "\n",
    "Input\n",
    "  ‚Üì\n",
    "Linear\n",
    "  ‚Üì\n",
    "BatchNorm  ‚Üê Add here!\n",
    "  ‚Üì\n",
    "ReLU\n",
    "  ‚Üì\n",
    "Linear\n",
    "  ‚Üì\n",
    "BatchNorm  ‚Üê Add here!\n",
    "  ‚Üì\n",
    "ReLU\n",
    "  ‚Üì\n",
    "Output\n",
    "```\n",
    "\n",
    "### üöÄ Modern Variants:\n",
    "\n",
    "- **Layer Normalization**: Used in Transformers (GPT, BERT)\n",
    "- **Group Normalization**: Good for small batches\n",
    "- **Instance Normalization**: Used in style transfer\n",
    "\n",
    "**GPT-4 uses Layer Normalization!** (variant of BatchNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## üìâ Learning Rate Scheduling\n",
    "\n",
    "**Learning Rate Scheduling** = Adjusting learning rate during training\n",
    "\n",
    "### üéØ Why Adjust Learning Rate?\n",
    "\n",
    "```\n",
    "Early Training:\n",
    "- Large learning rate ‚Üí Fast progress\n",
    "- Far from optimum ‚Üí Big steps OK\n",
    "\n",
    "Late Training:\n",
    "- Small learning rate ‚Üí Fine-tuning\n",
    "- Near optimum ‚Üí Small steps needed\n",
    "```\n",
    "\n",
    "### üìä Popular Schedules:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Step Decay**\n",
    "```python\n",
    "lr = initial_lr √ó 0.1^(epoch // 30)\n",
    "\n",
    "Epochs 0-29:   lr = 0.1\n",
    "Epochs 30-59:  lr = 0.01\n",
    "Epochs 60-89:  lr = 0.001\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ **Exponential Decay**\n",
    "```python\n",
    "lr = initial_lr √ó decay_rate^epoch\n",
    "\n",
    "Example (decay_rate=0.95):\n",
    "Epoch 0:   lr = 0.1\n",
    "Epoch 10:  lr = 0.06\n",
    "Epoch 20:  lr = 0.036\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ **Cosine Annealing** ‚≠ê Very Popular!\n",
    "```python\n",
    "lr = min_lr + (max_lr - min_lr) √ó (1 + cos(œÄ √ó epoch / total_epochs)) / 2\n",
    "\n",
    "Smooth decrease following cosine curve\n",
    "```\n",
    "\n",
    "**Used in:** GPT-3, many Transformer models\n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ **Warm-up + Decay** üî• LLM Favorite!\n",
    "```python\n",
    "if epoch < warmup_epochs:\n",
    "    lr = max_lr √ó (epoch / warmup_epochs)  # Linear warmup\n",
    "else:\n",
    "    lr = decay(epoch)  # Then decay\n",
    "```\n",
    "\n",
    "**Why warmup?**\n",
    "- Prevents instability at start\n",
    "- Especially important for large models\n",
    "- **Used in GPT, BERT, all modern LLMs!**\n",
    "\n",
    "---\n",
    "\n",
    "#### 5Ô∏è‚É£ **ReduceLROnPlateau** üìä Adaptive!\n",
    "```python\n",
    "if validation_loss doesn't improve for N epochs:\n",
    "    lr = lr √ó 0.1\n",
    "```\n",
    "\n",
    "**Adaptive to training progress!**\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Visual Comparison:\n",
    "\n",
    "```\n",
    "Learning Rate Over Time:\n",
    "\n",
    "Step:       ___|\n",
    "               |___|\n",
    "                   |___\n",
    "\n",
    "Exponential: \\___\n",
    "              \\___\n",
    "\n",
    "Cosine:      ‚àº‚àº‚àº‚àº__\n",
    "             (smooth curve)\n",
    "\n",
    "Warmup:      /‚Äæ‚Äæ\\___\n",
    "            ‚Üë   ‚Üë\n",
    "         warmup decay\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## üöÄ Real AI Example: Building Robust Neural Networks\n",
    "\n",
    "**Project:** Build a production-ready digit classifier with all improvements!\n",
    "\n",
    "**Techniques we'll use:**\n",
    "- ‚úÖ L2 Regularization\n",
    "- ‚úÖ Dropout\n",
    "- ‚úÖ Batch Normalization\n",
    "- ‚úÖ Learning Rate Scheduling\n",
    "- ‚úÖ Early Stopping\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy matplotlib scikit-learn --quiet\n",
    "\n",
    "print(\"‚úÖ Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üìö Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Step 1: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X = digits.data / 16.0  # Normalize\n",
    "y = digits.target\n",
    "\n",
    "# One-hot encode labels\n",
    "def to_one_hot(y, num_classes=10):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_one_hot = to_one_hot(y)\n",
    "\n",
    "# Split into train, validation, and test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y_one_hot, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"üìä Data prepared!\")\n",
    "print(f\"Training samples:   {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Test samples:       {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Step 2: Build Improved Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNeuralNetwork:\n",
    "    \"\"\"\n",
    "    Production-ready Neural Network with:\n",
    "    - L2 Regularization\n",
    "    - Dropout\n",
    "    - Batch Normalization\n",
    "    - Learning Rate Scheduling\n",
    "    - Early Stopping\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=64, hidden_size=128, output_size=10,\n",
    "                 dropout_rate=0.3, l2_lambda=0.01):\n",
    "        \n",
    "        # Network parameters\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        \n",
    "        # Initialize weights (Xavier initialization)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        # Batch normalization parameters\n",
    "        self.gamma = np.ones((1, hidden_size))\n",
    "        self.beta = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Running statistics for batch norm (used during inference)\n",
    "        self.running_mean = np.zeros((1, hidden_size))\n",
    "        self.running_var = np.ones((1, hidden_size))\n",
    "        self.momentum = 0.9\n",
    "        \n",
    "        # Training history\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.lr_history = []\n",
    "        \n",
    "        print(\"üß† Improved Neural Network initialized!\")\n",
    "        print(f\"   Architecture: {input_size} ‚Üí {hidden_size} ‚Üí {output_size}\")\n",
    "        print(f\"   Dropout rate: {dropout_rate}\")\n",
    "        print(f\"   L2 lambda: {l2_lambda}\")\n",
    "        print(f\"   Total parameters: {self.count_parameters()}\")\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return (self.W1.size + self.b1.size + \n",
    "                self.W2.size + self.b2.size +\n",
    "                self.gamma.size + self.beta.size)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def batch_norm_forward(self, x, training=True):\n",
    "        \"\"\"Batch normalization forward pass\"\"\"\n",
    "        if training:\n",
    "            # Calculate batch statistics\n",
    "            mean = np.mean(x, axis=0, keepdims=True)\n",
    "            var = np.var(x, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update running statistics\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "            \n",
    "            # Normalize\n",
    "            self.x_normalized = (x - mean) / np.sqrt(var + 1e-8)\n",
    "            \n",
    "            # Store for backward pass\n",
    "            self.bn_mean = mean\n",
    "            self.bn_var = var\n",
    "        else:\n",
    "            # Use running statistics during inference\n",
    "            self.x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + 1e-8)\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * self.x_normalized + self.beta\n",
    "        return out\n",
    "    \n",
    "    def dropout_forward(self, x, training=True):\n",
    "        \"\"\"Dropout forward pass\"\"\"\n",
    "        if training:\n",
    "            # Create dropout mask\n",
    "            self.dropout_mask = (np.random.rand(*x.shape) > self.dropout_rate).astype(float)\n",
    "            # Scale by dropout rate (inverted dropout)\n",
    "            return x * self.dropout_mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            # No dropout during inference\n",
    "            return x\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"Forward propagation with all improvements\"\"\"\n",
    "        # Layer 1: Input ‚Üí Hidden\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.z1_bn = self.batch_norm_forward(self.z1, training)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.a1 = self.relu(self.z1_bn)\n",
    "        \n",
    "        # Dropout\n",
    "        self.a1_dropout = self.dropout_forward(self.a1, training)\n",
    "        \n",
    "        # Layer 2: Hidden ‚Üí Output\n",
    "        self.z2 = np.dot(self.a1_dropout, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Cross-entropy loss with L2 regularization\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        cross_entropy = -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_reg = (self.l2_lambda / 2) * (\n",
    "            np.sum(self.W1 ** 2) + np.sum(self.W2 ** 2)\n",
    "        )\n",
    "        \n",
    "        return cross_entropy + l2_reg\n",
    "    \n",
    "    def backward(self, X, y_true, learning_rate):\n",
    "        \"\"\"Backpropagation with regularization\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y_true\n",
    "        dW2 = np.dot(self.a1_dropout.T, dz2) / m + self.l2_lambda * self.W2  # L2 reg\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients (through dropout)\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        da1_dropout = da1 * self.dropout_mask / (1 - self.dropout_rate)\n",
    "        \n",
    "        # Through ReLU\n",
    "        dz1_bn = da1_dropout * self.relu_derivative(self.a1)\n",
    "        \n",
    "        # Batch norm gradients (simplified)\n",
    "        dgamma = np.sum(dz1_bn * self.x_normalized, axis=0, keepdims=True)\n",
    "        dbeta = np.sum(dz1_bn, axis=0, keepdims=True)\n",
    "        \n",
    "        dz1 = dz1_bn  # Simplified (full batch norm backward is complex)\n",
    "        \n",
    "        dW1 = np.dot(X.T, dz1) / m + self.l2_lambda * self.W1  # L2 reg\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.gamma -= learning_rate * dgamma * 0.1  # Smaller LR for BN\n",
    "        self.beta -= learning_rate * dbeta * 0.1\n",
    "    \n",
    "    def cosine_annealing_lr(self, initial_lr, epoch, total_epochs):\n",
    "        \"\"\"Cosine annealing learning rate schedule\"\"\"\n",
    "        return initial_lr * 0.5 * (1 + np.cos(np.pi * epoch / total_epochs))\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, \n",
    "              epochs=200, initial_lr=0.5, patience=20, verbose=True):\n",
    "        \"\"\"Train with early stopping and LR scheduling\"\"\"\n",
    "        \n",
    "        print(f\"\\nüèãÔ∏è Training started!\")\n",
    "        print(f\"   Epochs: {epochs}\")\n",
    "        print(f\"   Initial LR: {initial_lr}\")\n",
    "        print(f\"   Early stopping patience: {patience}\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Learning rate scheduling (cosine annealing)\n",
    "            lr = self.cosine_annealing_lr(initial_lr, epoch, epochs)\n",
    "            self.lr_history.append(lr)\n",
    "            \n",
    "            # Training\n",
    "            train_pred = self.forward(X_train, training=True)\n",
    "            train_loss = self.compute_loss(y_train, train_pred)\n",
    "            self.backward(X_train, y_train, lr)\n",
    "            self.train_loss_history.append(train_loss)\n",
    "            \n",
    "            # Validation (no dropout, no training mode)\n",
    "            val_pred = self.forward(X_val, training=False)\n",
    "            val_loss = self.compute_loss(y_val, val_pred)\n",
    "            self.val_loss_history.append(val_loss)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best weights\n",
    "                self.best_W1 = self.W1.copy()\n",
    "                self.best_W2 = self.W2.copy()\n",
    "                self.best_b1 = self.b1.copy()\n",
    "                self.best_b2 = self.b2.copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch % 20 == 0 or epoch == epochs - 1):\n",
    "                train_acc = self.evaluate(X_train, y_train)\n",
    "                val_acc = self.evaluate(X_val, y_val)\n",
    "                print(f\"Epoch {epoch:3d} | LR: {lr:.4f} | \"\n",
    "                      f\"Train Loss: {train_loss:.4f} ({train_acc:.1f}%) | \"\n",
    "                      f\"Val Loss: {val_loss:.4f} ({val_acc:.1f}%)\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\n‚è∞ Early stopping at epoch {epoch}!\")\n",
    "                print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "                # Restore best weights\n",
    "                self.W1 = self.best_W1\n",
    "                self.W2 = self.best_W2\n",
    "                self.b1 = self.best_b1\n",
    "                self.b2 = self.best_b2\n",
    "                break\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"‚úÖ Training complete!\\n\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probabilities = self.forward(X, training=False)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    def evaluate(self, X, y_true):\n",
    "        predictions = self.predict(X)\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "        accuracy = np.mean(predictions == y_true_labels) * 100\n",
    "        return accuracy\n",
    "\n",
    "# Create the improved model\n",
    "model = ImprovedNeuralNetwork(\n",
    "    input_size=64, \n",
    "    hidden_size=128, \n",
    "    output_size=10,\n",
    "    dropout_rate=0.3,\n",
    "    l2_lambda=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Step 3: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train(\n",
    "    X_train, y_train, \n",
    "    X_val, y_val,\n",
    "    epochs=200,\n",
    "    initial_lr=0.5,\n",
    "    patience=20,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Step 4: Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(model.train_loss_history, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0].plot(model.val_loss_history, 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate schedule\n",
    "axes[1].plot(model.lr_history, 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[1].set_title('Cosine Annealing LR Schedule', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Notice:\")\n",
    "print(\"   - Validation loss follows training loss (good generalization!)\")\n",
    "print(\"   - Learning rate smoothly decreases (cosine annealing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate Final Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on all sets\n",
    "train_accuracy = model.evaluate(X_train, y_train)\n",
    "val_accuracy = model.evaluate(X_val, y_val)\n",
    "test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"üìä Final Model Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training Accuracy:   {train_accuracy:.2f}%\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "print(f\"Test Accuracy:       {test_accuracy:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for overfitting\n",
    "overfit_gap = train_accuracy - test_accuracy\n",
    "print(f\"\\nüìà Overfitting gap: {overfit_gap:.2f}%\")\n",
    "\n",
    "if overfit_gap < 3:\n",
    "    print(\"‚úÖ Excellent generalization! Regularization working well.\")\n",
    "elif overfit_gap < 5:\n",
    "    print(\"‚úÖ Good generalization! Model is well-regularized.\")\n",
    "elif overfit_gap < 10:\n",
    "    print(\"‚ö†Ô∏è Slight overfitting. Consider stronger regularization.\")\n",
    "else:\n",
    "    print(\"‚ùå Significant overfitting. Increase dropout or L2 lambda.\")\n",
    "\n",
    "if test_accuracy > 95:\n",
    "    print(\"\\nüéâ Outstanding! Production-ready model!\")\n",
    "elif test_accuracy > 90:\n",
    "    print(\"\\n‚úÖ Great performance! Model is working well.\")\n",
    "else:\n",
    "    print(\"\\nüí° Good start! Try tuning hyperparameters for better results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Step 6: Compare with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a baseline model (no regularization)\n",
    "print(\"üî¨ Training baseline model (no regularization)...\\n\")\n",
    "\n",
    "baseline = ImprovedNeuralNetwork(\n",
    "    input_size=64,\n",
    "    hidden_size=128,\n",
    "    output_size=10,\n",
    "    dropout_rate=0.0,  # No dropout\n",
    "    l2_lambda=0.0       # No L2 regularization\n",
    ")\n",
    "\n",
    "baseline.train(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    epochs=100,\n",
    "    initial_lr=0.5,\n",
    "    patience=20,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "baseline_train_acc = baseline.evaluate(X_train, y_train)\n",
    "baseline_test_acc = baseline.evaluate(X_test, y_test)\n",
    "\n",
    "# Compare\n",
    "print(\"\\nüìä Comparison: Regularization vs No Regularization\")\n",
    "print(\"=\"*60)\n",
    "print(f\"                     Train Acc    Test Acc    Gap\")\n",
    "print(\"-\"*60)\n",
    "print(f\"No Regularization:   {baseline_train_acc:6.2f}%    {baseline_test_acc:6.2f}%    {baseline_train_acc - baseline_test_acc:5.2f}%\")\n",
    "print(f\"With Regularization: {train_accuracy:6.2f}%    {test_accuracy:6.2f}%    {train_accuracy - test_accuracy:5.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   - Regularization reduces overfitting (smaller gap)\")\n",
    "print(\"   - Better generalization ‚Üí better test accuracy\")\n",
    "print(\"   - Essential for production AI systems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## üéØ Why This Matters for Modern AI\n",
    "\n",
    "Every technique you just learned is ESSENTIAL in modern AI systems!\n",
    "\n",
    "### ü§ñ **ChatGPT & GPT-4**\n",
    "\n",
    "**Regularization in LLMs:**\n",
    "```python\n",
    "# GPT training (simplified)\n",
    "class GPTTraining:\n",
    "    def __init__(self):\n",
    "        self.dropout = 0.1           # ‚úÖ Dropout\n",
    "        self.weight_decay = 0.01     # ‚úÖ L2 regularization\n",
    "        self.layer_norm = True        # ‚úÖ Layer normalization (BatchNorm variant)\n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        # Forward with dropout\n",
    "        logits = self.model(batch, dropout=self.dropout)\n",
    "        \n",
    "        # Cross-entropy loss + L2 regularization\n",
    "        loss = cross_entropy(logits, labels) + weight_decay_penalty\n",
    "        \n",
    "        # Warmup + cosine decay learning rate\n",
    "        lr = self.lr_schedule(step)\n",
    "        \n",
    "        # Adam optimizer\n",
    "        self.optimizer.step(loss, lr)\n",
    "```\n",
    "\n",
    "**Without these techniques:**\n",
    "- GPT would memorize training data\n",
    "- Wouldn't generalize to new prompts\n",
    "- Unstable training with billions of parameters\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **RAG Systems**\n",
    "\n",
    "**Embedding models use:**\n",
    "- Dropout (0.1-0.2) in transformer layers\n",
    "- Layer normalization for stable training\n",
    "- Warmup + decay learning rate schedule\n",
    "- Weight decay to prevent overfitting\n",
    "\n",
    "**Result:**\n",
    "- Embeddings generalize to new documents\n",
    "- Robust retrieval in production\n",
    "- Better semantic understanding\n",
    "\n",
    "---\n",
    "\n",
    "### üé® **Multimodal AI (GPT-4V, Gemini)**\n",
    "\n",
    "**Vision Encoder:**\n",
    "- Batch Normalization in CNN layers\n",
    "- Dropout (0.3-0.5) to prevent overfitting on ImageNet\n",
    "- Learning rate warmup for stable training\n",
    "\n",
    "**Text Encoder:**\n",
    "- Layer Normalization in every transformer block\n",
    "- Dropout (0.1) in attention and feedforward layers\n",
    "- AdamW optimizer with weight decay\n",
    "\n",
    "**Joint Training:**\n",
    "- Regularization prevents mode collapse\n",
    "- Ensures vision and text align properly\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ù **Agentic AI**\n",
    "\n",
    "**Neural network components:**\n",
    "- Policy networks use dropout for exploration\n",
    "- Value networks use L2 regularization\n",
    "- Learning rate annealing for stable convergence\n",
    "\n",
    "**Without regularization:**\n",
    "- Agents overfit to training environments\n",
    "- Fail in novel situations\n",
    "- Unstable learning\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Production ML in 2024-2025**\n",
    "\n",
    "**Standard Recipe:**\n",
    "```python\n",
    "production_config = {\n",
    "    'optimizer': 'AdamW',           # Adam + weight decay\n",
    "    'learning_rate': 1e-4,\n",
    "    'lr_schedule': 'cosine',        # Cosine annealing\n",
    "    'warmup_steps': 1000,           # LR warmup\n",
    "    'dropout': 0.1,                 # Dropout\n",
    "    'weight_decay': 0.01,           # L2 regularization\n",
    "    'layer_norm': True,             # Layer/Batch normalization\n",
    "    'early_stopping_patience': 10,  # Early stopping\n",
    "    'gradient_clipping': 1.0,       # Prevent exploding gradients\n",
    "}\n",
    "```\n",
    "\n",
    "**This is the industry standard!** Used by:\n",
    "- OpenAI (GPT-4, ChatGPT)\n",
    "- Google (PaLM, Gemini)\n",
    "- Anthropic (Claude)\n",
    "- Meta (LLaMA)\n",
    "- All major AI companies!\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **Key Insight:**\n",
    "\n",
    "```\n",
    "Simple Model + No Regularization = Overfitting ‚ùå\n",
    "                ‚Üì\n",
    "Simple Model + Regularization = Good Performance ‚úÖ\n",
    "                ‚Üì\n",
    "Large Model + Regularization = State-of-the-Art üöÄ\n",
    "\n",
    "Regularization is the difference between\n",
    "research toy and production AI!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN: Interactive Exercise\n",
    "\n",
    "**Challenge:** Experiment with different regularization techniques!\n",
    "\n",
    "**Tasks:**\n",
    "1. Try different dropout rates (0.1, 0.3, 0.5, 0.7)\n",
    "2. Try different L2 lambda values (0.001, 0.01, 0.1)\n",
    "3. Compare different LR schedules (constant, step decay, cosine)\n",
    "4. Add a second hidden layer and see how it affects overfitting\n",
    "\n",
    "**Goal:** Find the best configuration for minimum overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "# Experiment with different configurations\n",
    "\n",
    "# Example experiments:\n",
    "# 1. High dropout\n",
    "# model_high_dropout = ImprovedNeuralNetwork(dropout_rate=0.5, l2_lambda=0.01)\n",
    "\n",
    "# 2. Strong L2 regularization\n",
    "# model_strong_l2 = ImprovedNeuralNetwork(dropout_rate=0.3, l2_lambda=0.1)\n",
    "\n",
    "# 3. No regularization (for comparison)\n",
    "# model_no_reg = ImprovedNeuralNetwork(dropout_rate=0.0, l2_lambda=0.0)\n",
    "\n",
    "print(\"üß™ Experiment with different configurations!\")\n",
    "print(\"\\nSuggestions:\")\n",
    "print(\"1. Vary dropout_rate: 0.0, 0.1, 0.3, 0.5, 0.7\")\n",
    "print(\"2. Vary l2_lambda: 0.0, 0.001, 0.01, 0.1\")\n",
    "print(\"3. Compare train/test accuracy gaps\")\n",
    "print(\"4. Plot loss curves to see overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution: Comprehensive Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Compare different regularization strengths\n",
    "\n",
    "print(\"üß™ Running comprehensive regularization experiment...\\n\")\n",
    "\n",
    "configs = [\n",
    "    {'name': 'No Regularization', 'dropout': 0.0, 'l2': 0.0},\n",
    "    {'name': 'Light Regularization', 'dropout': 0.2, 'l2': 0.001},\n",
    "    {'name': 'Medium Regularization', 'dropout': 0.3, 'l2': 0.01},\n",
    "    {'name': 'Heavy Regularization', 'dropout': 0.5, 'l2': 0.1},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"Training: {config['name']}...\")\n",
    "    \n",
    "    model_exp = ImprovedNeuralNetwork(\n",
    "        input_size=64,\n",
    "        hidden_size=128,\n",
    "        output_size=10,\n",
    "        dropout_rate=config['dropout'],\n",
    "        l2_lambda=config['l2']\n",
    "    )\n",
    "    \n",
    "    model_exp.train(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=100,\n",
    "        initial_lr=0.5,\n",
    "        patience=15,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    train_acc = model_exp.evaluate(X_train, y_train)\n",
    "    test_acc = model_exp.evaluate(X_test, y_test)\n",
    "    gap = train_acc - test_acc\n",
    "    \n",
    "    results.append({\n",
    "        'name': config['name'],\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'gap': gap\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä REGULARIZATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Configuration':<25} {'Train Acc':>12} {'Test Acc':>12} {'Overfit Gap':>12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['name']:<25} {r['train_acc']:>11.2f}% {r['test_acc']:>11.2f}% {r['gap']:>11.2f}%\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best configuration\n",
    "best = min(results, key=lambda x: x['gap'])\n",
    "print(f\"\\nüèÜ Best configuration: {best['name']}\")\n",
    "print(f\"   Smallest overfitting gap: {best['gap']:.2f}%\")\n",
    "print(f\"   Test accuracy: {best['test_acc']:.2f}%\")\n",
    "\n",
    "print(\"\\nüí° Insights:\")\n",
    "print(\"   - Too little regularization ‚Üí overfitting\")\n",
    "print(\"   - Too much regularization ‚Üí underfitting\")\n",
    "print(\"   - Sweet spot balances fit and generalization\")\n",
    "print(\"   - Same principle used in GPT-4 and all LLMs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just mastered:**\n",
    "- ‚úÖ Overfitting and generalization\n",
    "- ‚úÖ L1 and L2 regularization (weight decay)\n",
    "- ‚úÖ Dropout - the revolutionary technique\n",
    "- ‚úÖ Batch Normalization for stable training\n",
    "- ‚úÖ Learning rate scheduling (cosine annealing, warmup)\n",
    "- ‚úÖ Early stopping to prevent overfitting\n",
    "- ‚úÖ Built a production-ready neural network!\n",
    "- ‚úÖ How these techniques power ALL modern AI systems\n",
    "\n",
    "### üéØ Key Takeaways:\n",
    "\n",
    "1. **Regularization prevents overfitting**\n",
    "   - L2 regularization: Keep weights small\n",
    "   - Dropout: Random neuron dropout\n",
    "   - Early stopping: Stop when validation plateaus\n",
    "   - Essential for production AI!\n",
    "\n",
    "2. **Batch Normalization stabilizes training**\n",
    "   - Normalizes layer inputs\n",
    "   - Faster convergence\n",
    "   - Higher learning rates possible\n",
    "   - Used in almost all modern architectures\n",
    "\n",
    "3. **Learning rate scheduling improves results**\n",
    "   - Start high for fast progress\n",
    "   - Decrease for fine-tuning\n",
    "   - Warmup prevents early instability\n",
    "   - Standard in LLM training\n",
    "\n",
    "4. **Production AI = Base Model + Regularization**\n",
    "   - Simple models overfit\n",
    "   - Regularization enables generalization\n",
    "   - Same techniques in GPT-4, Claude, all LLMs\n",
    "   - The difference between toy and production!\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Final Project Challenge:**\n",
    "\n",
    "Build a neural network with:\n",
    "- 3 hidden layers (128, 64, 32 neurons)\n",
    "- Dropout after each hidden layer\n",
    "- L2 regularization\n",
    "- Batch normalization\n",
    "- Cosine annealing LR schedule\n",
    "- Early stopping\n",
    "\n",
    "**Goal:** Achieve >97% test accuracy with <3% overfitting gap!\n",
    "\n",
    "---\n",
    "\n",
    "**üìö What's Next?**\n",
    "\n",
    "**Week 12: Convolutional Neural Networks (CNNs)**\n",
    "- Architecture for computer vision\n",
    "- Convolution and pooling layers\n",
    "- Building image classifiers\n",
    "- Transfer learning with pre-trained models\n",
    "\n",
    "**Week 13: Recurrent Neural Networks (RNNs)**\n",
    "- Sequence modeling\n",
    "- LSTMs and GRUs\n",
    "- Applications in NLP\n",
    "\n",
    "**Week 14: Transformers**\n",
    "- The architecture behind GPT, BERT, ChatGPT!\n",
    "- Self-attention mechanism\n",
    "- Building your own transformer\n",
    "\n",
    "---\n",
    "\n",
    "**üí¨ Remember:**\n",
    "\n",
    "*\"You now understand the COMPLETE neural network training pipeline used in production AI! From GPT-4 to Claude to Gemini - they all use the techniques you just learned: L2 regularization, dropout, layer normalization, cosine LR schedules, and early stopping. You're ready to build real AI systems!\"* üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**üîó Connections to Modern AI:**\n",
    "\n",
    "| Technique | Used In | Purpose |\n",
    "|-----------|---------|----------|\n",
    "| L2 Regularization | GPT-4, BERT, all LLMs | Prevent overfitting |\n",
    "| Dropout | Transformers, CNNs | Ensemble learning |\n",
    "| Batch/Layer Norm | All modern networks | Stable training |\n",
    "| Cosine Annealing | GPT-3, GPT-4 | Smooth LR decay |\n",
    "| Warmup | All LLMs | Prevent early instability |\n",
    "| Early Stopping | Fine-tuning | Save compute |\n",
    "| AdamW | Industry standard | Efficient optimization |\n",
    "\n",
    "**You just learned the production AI toolkit! üéä**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
