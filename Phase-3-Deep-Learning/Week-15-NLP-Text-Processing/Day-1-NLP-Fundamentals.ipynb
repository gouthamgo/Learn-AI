{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Day 1: NLP Fundamentals\n",
    "\n",
    "**üéØ Goal:** Master text preprocessing, word embeddings, and build a spam detector\n",
    "\n",
    "**‚è±Ô∏è Time:** 90-120 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Text preprocessing is the FIRST step for ChatGPT, Claude, and all LLMs\n",
    "- Word embeddings power RAG systems, semantic search, and document retrieval\n",
    "- Understanding embeddings helps you build better chatbots, search engines, and AI assistants\n",
    "- GPT-4, BERT, and all modern NLP models use embeddings as their foundation\n",
    "- RAG systems rely on embedding similarity to find relevant documents!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† What is Natural Language Processing (NLP)?\n",
    "\n",
    "**Natural Language Processing** is teaching computers to understand and generate human language!\n",
    "\n",
    "### üéØ Real-World NLP Applications (2024-2025):\n",
    "\n",
    "#### ü§ñ **Large Language Models (LLMs)**\n",
    "- **ChatGPT, Claude, GPT-4**: Generate human-like text\n",
    "- **Gemini, Llama 3**: Open-source and multimodal models\n",
    "- **Applications**: Writing assistants, code generation, tutoring\n",
    "\n",
    "#### üîç **RAG (Retrieval-Augmented Generation)**\n",
    "- **Process**: Embed documents ‚Üí Find similar ‚Üí Generate answer\n",
    "- **Applications**: Customer support bots, knowledge bases, Q&A systems\n",
    "- **Why**: Reduces hallucinations, provides sources, keeps data current\n",
    "\n",
    "#### üí¨ **Chatbots & Virtual Assistants**\n",
    "- **Intent Classification**: Understanding what users want\n",
    "- **Entity Recognition**: Extracting names, dates, locations\n",
    "- **Applications**: Customer service, booking systems, AI agents\n",
    "\n",
    "#### üåê **Semantic Search**\n",
    "- **Traditional Search**: Keyword matching (\"apple\" only finds \"apple\")\n",
    "- **Semantic Search**: Meaning matching (\"fruit\" finds \"apple\", \"orange\")\n",
    "- **Applications**: Google, document search, recommendation systems\n",
    "\n",
    "### üîë The Key Challenge:\n",
    "\n",
    "**Computers only understand numbers, not words!**\n",
    "\n",
    "```\n",
    "Human:    \"I love AI\" ‚ùå Computer can't process this\n",
    "Computer: [0.2, 0.8, 0.5] ‚úÖ Computer understands vectors!\n",
    "```\n",
    "\n",
    "**Our Mission Today:**\n",
    "1. **Clean text**: Remove noise, normalize words\n",
    "2. **Convert to numbers**: Turn words into vectors (embeddings)\n",
    "3. **Capture meaning**: Similar words ‚Üí Similar vectors\n",
    "4. **Build real AI**: Spam detector using embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk scikit-learn gensim matplotlib numpy pandas seaborn --quiet\n",
    "\n",
    "print(\"‚úÖ Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK (Natural Language Toolkit)\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "print(\"üìö Libraries loaded!\")\n",
    "print(\"‚úÖ NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Part 1: Text Preprocessing Pipeline\n",
    "\n",
    "Before feeding text to AI models, we need to CLEAN and PREPARE it!\n",
    "\n",
    "### üìã Standard NLP Pipeline:\n",
    "\n",
    "```\n",
    "Raw Text\n",
    "   ‚Üì\n",
    "1Ô∏è‚É£ Tokenization     (Split into words/sentences)\n",
    "   ‚Üì\n",
    "2Ô∏è‚É£ Lowercasing      (\"Hello\" ‚Üí \"hello\")\n",
    "   ‚Üì\n",
    "3Ô∏è‚É£ Remove Stopwords (Remove \"the\", \"is\", \"a\")\n",
    "   ‚Üì\n",
    "4Ô∏è‚É£ Stemming/Lemmatization (\"running\" ‚Üí \"run\")\n",
    "   ‚Üì\n",
    "Clean Tokens\n",
    "   ‚Üì\n",
    "5Ô∏è‚É£ Vectorization    (Convert to numbers)\n",
    "   ‚Üì\n",
    "Ready for ML/DL!\n",
    "```\n",
    "\n",
    "### üéØ Why Each Step Matters:\n",
    "\n",
    "**1Ô∏è‚É£ Tokenization**\n",
    "- Splits text into units (words, subwords, characters)\n",
    "- **Example**: \"I love NLP!\" ‚Üí [\"I\", \"love\", \"NLP\", \"!\"]\n",
    "- **Used in**: All NLP models (BERT, GPT, etc.)\n",
    "\n",
    "**2Ô∏è‚É£ Lowercasing**\n",
    "- Treats \"Hello\" and \"hello\" as the same word\n",
    "- **Example**: \"Python\" ‚Üí \"python\"\n",
    "- **Trade-off**: Loses information (\"Apple\" company vs \"apple\" fruit)\n",
    "\n",
    "**3Ô∏è‚É£ Stopword Removal**\n",
    "- Removes common words with little meaning\n",
    "- **Example**: \"the\", \"is\", \"a\", \"an\", \"in\"\n",
    "- **Why**: Reduces noise, speeds up processing\n",
    "- **Caution**: Modern LLMs often keep stopwords!\n",
    "\n",
    "**4Ô∏è‚É£ Stemming vs Lemmatization**\n",
    "- **Stemming**: Crude chopping (\"running\" ‚Üí \"run\", \"better\" ‚Üí \"better\")\n",
    "- **Lemmatization**: Smart reduction (\"running\" ‚Üí \"run\", \"better\" ‚Üí \"good\")\n",
    "- **Example**: \"studies\", \"studying\", \"studied\" ‚Üí \"study\"\n",
    "\n",
    "**5Ô∏è‚É£ Vectorization**\n",
    "- Converts words to numbers\n",
    "- **Methods**: Bag of Words, TF-IDF, Word2Vec, GloVe, BERT embeddings\n",
    "- **This is where the magic happens!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî§ Step 1: Tokenization\n",
    "\n",
    "**Tokenization** = Breaking text into smaller units (tokens)\n",
    "\n",
    "### üìä Types of Tokenization:\n",
    "\n",
    "1. **Word Tokenization**: Split by words\n",
    "2. **Sentence Tokenization**: Split by sentences\n",
    "3. **Subword Tokenization**: Split into parts (used by BERT, GPT)\n",
    "\n",
    "### üéØ Why Tokenization Matters:\n",
    "- **ChatGPT**: Uses BPE (Byte-Pair Encoding) tokenization\n",
    "- **BERT**: Uses WordPiece tokenization\n",
    "- **Token Limits**: GPT-4 has 8K/32K/128K token limits\n",
    "- **Pricing**: Many APIs charge per token!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text about AI and RAG\n",
    "text = \"\"\"\n",
    "Natural Language Processing is revolutionizing AI in 2024-2025! \n",
    "Large Language Models like ChatGPT and Claude can understand context. \n",
    "RAG systems combine retrieval with generation for better accuracy.\n",
    "Word embeddings help computers understand semantic similarity.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Original Text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"üìÑ Sentence Tokenization:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent.strip()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"üî§ Word Tokenization:\")\n",
    "print(words[:20])  # First 20 tokens\n",
    "print(f\"\\nTotal tokens: {len(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 2: Text Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True, lowercase=True, remove_punct=True):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        remove_stopwords: Whether to remove stopwords\n",
    "        lowercase: Whether to convert to lowercase\n",
    "        remove_punct: Whether to remove punctuation\n",
    "    \n",
    "    Returns:\n",
    "        List of cleaned tokens\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercase\n",
    "    if lowercase:\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    if remove_punct:\n",
    "        tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test the preprocessing\n",
    "sample_text = \"I love learning about AI and Machine Learning in 2024-2025!\"\n",
    "\n",
    "print(\"üìù Original:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"üî§ After tokenization:\")\n",
    "print(word_tokenize(sample_text))\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"üßπ After full preprocessing:\")\n",
    "cleaned = preprocess_text(sample_text)\n",
    "print(cleaned)\n",
    "print(f\"\\nOriginal tokens: {len(word_tokenize(sample_text))}\")\n",
    "print(f\"Cleaned tokens: {len(cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Step 3: Stemming vs Lemmatization\n",
    "\n",
    "### üî™ Stemming (Fast but Crude)\n",
    "- Chops word endings\n",
    "- **Pro**: Very fast\n",
    "- **Con**: Sometimes creates non-words\n",
    "- **Example**: \"running\" ‚Üí \"run\", \"studies\" ‚Üí \"studi\"\n",
    "\n",
    "### üéØ Lemmatization (Slow but Accurate)\n",
    "- Uses dictionary and grammar\n",
    "- **Pro**: Returns real words\n",
    "- **Con**: Slower\n",
    "- **Example**: \"running\" ‚Üí \"run\", \"better\" ‚Üí \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Test words\n",
    "test_words = [\n",
    "    'running', 'runs', 'ran',\n",
    "    'better', 'good', 'best',\n",
    "    'studies', 'studying', 'studied',\n",
    "    'computing', 'computers', 'computed'\n",
    "]\n",
    "\n",
    "print(\"üî¨ Stemming vs Lemmatization Comparison:\\n\")\n",
    "print(f\"{'Original':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for word in test_words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word, pos='v')  # pos='v' for verb\n",
    "    print(f\"{word:<15} {stemmed:<15} {lemmatized:<15}\")\n",
    "\n",
    "print(\"\\nüí° Notice:\")\n",
    "print(\"   - Stemming sometimes creates non-words ('studi')\")\n",
    "print(\"   - Lemmatization preserves real words\")\n",
    "print(\"   - Both reduce vocabulary size!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéí Part 2: Bag of Words (BoW)\n",
    "\n",
    "**Bag of Words** = Count how many times each word appears\n",
    "\n",
    "### üìä How it Works:\n",
    "\n",
    "```\n",
    "Documents:\n",
    "D1: \"I love AI\"\n",
    "D2: \"I love machine learning\"\n",
    "D3: \"AI and machine learning\"\n",
    "\n",
    "Vocabulary: [I, love, AI, machine, learning, and]\n",
    "\n",
    "Vectors:\n",
    "D1: [1, 1, 1, 0, 0, 0]  ‚Üí counts for each word\n",
    "D2: [1, 1, 0, 1, 1, 0]\n",
    "D3: [0, 0, 1, 1, 1, 1]\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Limitations:\n",
    "- **No word order**: \"AI loves me\" = \"me loves AI\"\n",
    "- **No semantics**: \"good\" ‚â† \"great\" (different vectors)\n",
    "- **High dimensionality**: One column per unique word\n",
    "\n",
    "### ‚úÖ Strengths:\n",
    "- **Simple and fast**\n",
    "- **Works well for classification**\n",
    "- **Baseline for NLP tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about AI\n",
    "documents = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"I love machine learning\",\n",
    "    \"Natural language processing uses machine learning\",\n",
    "    \"Deep learning powers ChatGPT and Claude\"\n",
    "]\n",
    "\n",
    "print(\"üìö Documents:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"üéí Bag of Words Representation:\")\n",
    "print(f\"\\nVocabulary: {list(feature_names)}\")\n",
    "print(f\"Vocabulary size: {len(feature_names)}\")\n",
    "\n",
    "# Display as DataFrame\n",
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    columns=feature_names,\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Word Counts per Document:\")\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Bag of Words\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(bow_df, annot=True, fmt='d', cmap='YlOrRd', cbar_kws={'label': 'Count'})\n",
    "plt.title('Bag of Words Heatmap', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Words', fontsize=12)\n",
    "plt.ylabel('Documents', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üé® Each cell shows how many times a word appears in a document\")\n",
    "print(\"üìå Notice: Most cells are 0 (sparse matrix!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 3: TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "**TF-IDF** = Smart weighting that highlights important words\n",
    "\n",
    "### üéØ The Problem with Bag of Words:\n",
    "- Common words like \"the\", \"is\" get high counts\n",
    "- But they don't carry much meaning!\n",
    "- **Solution**: TF-IDF downweights common words\n",
    "\n",
    "### üìê How TF-IDF Works:\n",
    "\n",
    "**TF (Term Frequency):**\n",
    "```\n",
    "TF(word) = (Count of word in document) / (Total words in document)\n",
    "```\n",
    "\n",
    "**IDF (Inverse Document Frequency):**\n",
    "```\n",
    "IDF(word) = log(Total documents / Documents containing word)\n",
    "```\n",
    "\n",
    "**TF-IDF Score:**\n",
    "```\n",
    "TF-IDF = TF √ó IDF\n",
    "```\n",
    "\n",
    "### üí° Intuition:\n",
    "- **High TF-IDF**: Word is frequent in THIS document, rare in others ‚Üí Important!\n",
    "- **Low TF-IDF**: Word is common everywhere ‚Üí Not distinctive\n",
    "\n",
    "### üéØ Used In:\n",
    "- **Search Engines**: Ranking relevant documents\n",
    "- **Document Similarity**: Finding similar texts\n",
    "- **Keyword Extraction**: Identifying important terms\n",
    "- **RAG Systems**: Document retrieval before generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectors\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display as DataFrame\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_features,\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(\"üìä TF-IDF Representation:\")\n",
    "print(tfidf_df.round(3))\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - Higher values = More important to that document\")\n",
    "print(\"   - Values range from 0 to 1\")\n",
    "print(\"   - Common words get lower scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TF-IDF\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(tfidf_df, annot=True, fmt='.2f', cmap='Blues', cbar_kws={'label': 'TF-IDF Score'})\n",
    "plt.title('TF-IDF Heatmap', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Words', fontsize=12)\n",
    "plt.ylabel('Documents', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Notice how TF-IDF highlights distinctive words!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Part 4: Word Embeddings - Word2Vec\n",
    "\n",
    "**Word Embeddings** = Dense vectors that capture word meaning!\n",
    "\n",
    "### üéØ The Revolution:\n",
    "\n",
    "**Old Way (BoW/TF-IDF):**\n",
    "- Sparse vectors (mostly zeros)\n",
    "- No semantic meaning\n",
    "- \"king\" and \"queen\" are completely different\n",
    "\n",
    "**New Way (Embeddings):**\n",
    "- Dense vectors (all values meaningful)\n",
    "- **Captures semantics**: Similar words ‚Üí Similar vectors\n",
    "- **Math with meaning**: king - man + woman ‚âà queen\n",
    "\n",
    "### üìê Word2Vec Properties:\n",
    "\n",
    "```\n",
    "\"king\"   ‚Üí [0.2, 0.8, 0.5, ...] (300 dimensions)\n",
    "\"queen\"  ‚Üí [0.1, 0.7, 0.6, ...]\n",
    "\"man\"    ‚Üí [0.3, 0.2, 0.1, ...]\n",
    "\"woman\"  ‚Üí [0.2, 0.1, 0.2, ...]\n",
    "\n",
    "Magic:\n",
    "vector(\"king\") - vector(\"man\") + vector(\"woman\") ‚âà vector(\"queen\")\n",
    "```\n",
    "\n",
    "### üéØ Why This Matters for AI:\n",
    "\n",
    "**üîç RAG Systems:**\n",
    "- Embed documents and queries\n",
    "- Find similar documents using cosine similarity\n",
    "- Retrieve relevant context for LLM\n",
    "\n",
    "**üí¨ Chatbots:**\n",
    "- Understand user intent through embeddings\n",
    "- Match to similar training examples\n",
    "- Generate contextual responses\n",
    "\n",
    "**üåê Semantic Search:**\n",
    "- Search by meaning, not keywords\n",
    "- \"How to fix a bug\" finds \"debugging tutorial\"\n",
    "\n",
    "**ü§ñ Modern LLMs:**\n",
    "- BERT, GPT, Claude use advanced embeddings\n",
    "- Contextual embeddings (same word, different meanings)\n",
    "- Foundation of all transformer models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import Gensim for Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus about AI and ML\n",
    "sentences = [\n",
    "    ['machine', 'learning', 'is', 'subset', 'of', 'artificial', 'intelligence'],\n",
    "    ['deep', 'learning', 'uses', 'neural', 'networks'],\n",
    "    ['natural', 'language', 'processing', 'helps', 'computers', 'understand', 'text'],\n",
    "    ['chatgpt', 'and', 'claude', 'are', 'large', 'language', 'models'],\n",
    "    ['rag', 'systems', 'combine', 'retrieval', 'and', 'generation'],\n",
    "    ['word', 'embeddings', 'capture', 'semantic', 'meaning'],\n",
    "    ['transformers', 'revolutionized', 'natural', 'language', 'processing'],\n",
    "    ['bert', 'and', 'gpt', 'use', 'attention', 'mechanisms'],\n",
    "    ['vector', 'databases', 'store', 'embeddings', 'for', 'similarity', 'search'],\n",
    "    ['semantic', 'search', 'finds', 'documents', 'by', 'meaning'],\n",
    "]\n",
    "\n",
    "print(\"üî§ Training Word2Vec on AI/ML corpus...\\n\")\n",
    "\n",
    "# Train Word2Vec model\n",
    "# vector_size: dimension of embeddings\n",
    "# window: context window size\n",
    "# min_count: ignore words appearing less than this\n",
    "# sg: 1 for skip-gram, 0 for CBOW\n",
    "model_w2v = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=50,  # smaller for demo\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,  # Skip-gram\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Word2Vec model trained!\")\n",
    "print(f\"\\nVocabulary size: {len(model_w2v.wv)}\")\n",
    "print(f\"Embedding dimension: {model_w2v.wv.vector_size}\")\n",
    "\n",
    "# Get embedding for a word\n",
    "word = 'learning'\n",
    "embedding = model_w2v.wv[word]\n",
    "\n",
    "print(f\"\\nüî¢ Embedding for '{word}':\")\n",
    "print(f\"Shape: {embedding.shape}\")\n",
    "print(f\"First 10 dimensions: {embedding[:10].round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words\n",
    "test_words = ['learning', 'language', 'embeddings', 'chatgpt']\n",
    "\n",
    "print(\"üîç Finding Similar Words:\\n\")\n",
    "\n",
    "for word in test_words:\n",
    "    if word in model_w2v.wv:\n",
    "        similar = model_w2v.wv.most_similar(word, topn=3)\n",
    "        print(f\"üìå Words similar to '{word}':\")\n",
    "        for similar_word, score in similar:\n",
    "            print(f\"   - {similar_word}: {score:.3f}\")\n",
    "        print()\n",
    "\n",
    "print(\"üí° Higher score = More similar!\")\n",
    "print(\"üìä Similarity measured using cosine similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Part 5: GloVe Embeddings (Pre-trained)\n",
    "\n",
    "**GloVe** (Global Vectors for Word Representation) is another popular embedding method!\n",
    "\n",
    "### üéØ GloVe vs Word2Vec:\n",
    "\n",
    "**Word2Vec:**\n",
    "- Predicts context from words (or vice versa)\n",
    "- Local context window\n",
    "- Faster training\n",
    "\n",
    "**GloVe:**\n",
    "- Uses global word co-occurrence statistics\n",
    "- Matrix factorization approach\n",
    "- Better captures global semantic relationships\n",
    "\n",
    "### üì¶ Pre-trained Embeddings:\n",
    "\n",
    "Instead of training from scratch, use pre-trained embeddings trained on massive corpora!\n",
    "\n",
    "**Popular Pre-trained Embeddings:**\n",
    "- **GloVe**: Trained on Wikipedia + Gigaword (6B tokens)\n",
    "- **Word2Vec**: Trained on Google News (100B words)\n",
    "- **FastText**: Facebook's embeddings with subword information\n",
    "\n",
    "**In production, you'd use:**\n",
    "- Download pre-trained GloVe vectors\n",
    "- Load them into your application\n",
    "- Use immediately without training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating pre-trained embeddings (in production, you'd download GloVe)\n",
    "# For demonstration, we'll use our Word2Vec model\n",
    "\n",
    "def get_word_vector(word, model):\n",
    "    \"\"\"\n",
    "    Get word vector from model\n",
    "    \"\"\"\n",
    "    if word in model.wv:\n",
    "        return model.wv[word]\n",
    "    else:\n",
    "        # Return zero vector for unknown words\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "\n",
    "def document_vector(doc, model):\n",
    "    \"\"\"\n",
    "    Create document vector by averaging word vectors\n",
    "    This is used in many NLP applications!\n",
    "    \"\"\"\n",
    "    # Tokenize and preprocess\n",
    "    tokens = preprocess_text(doc)\n",
    "    \n",
    "    # Get word vectors\n",
    "    word_vectors = [get_word_vector(word, model) for word in tokens]\n",
    "    \n",
    "    # Average word vectors\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "\n",
    "# Test document vectorization\n",
    "test_doc = \"Machine learning and deep learning are powerful AI techniques\"\n",
    "\n",
    "doc_vec = document_vector(test_doc, model_w2v)\n",
    "\n",
    "print(\"üìÑ Document Vectorization:\")\n",
    "print(f\"Document: '{test_doc}'\")\n",
    "print(f\"\\nDocument vector shape: {doc_vec.shape}\")\n",
    "print(f\"First 10 values: {doc_vec[:10].round(3)}\")\n",
    "print(\"\\nüí° This vector represents the entire document!\")\n",
    "print(\"   Used in RAG systems for document retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Visualizing Word Embeddings\n",
    "\n",
    "Word embeddings are high-dimensional (50-300 dimensions). Let's use **t-SNE** to visualize them in 2D!\n",
    "\n",
    "**t-SNE** = Dimensionality reduction that preserves similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Get all words and their embeddings\n",
    "words = list(model_w2v.wv.index_to_key)\n",
    "word_vectors = np.array([model_w2v.wv[word] for word in words])\n",
    "\n",
    "# Reduce to 2D using t-SNE\n",
    "print(\"üé® Reducing embeddings to 2D using t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "embeddings_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "print(\"‚úÖ Dimensionality reduction complete!\\n\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100, alpha=0.6, c='steelblue')\n",
    "\n",
    "# Add word labels\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, \n",
    "                xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontsize=11,\n",
    "                fontweight='bold')\n",
    "\n",
    "plt.title('Word Embeddings Visualization (t-SNE)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Dimension 1', fontsize=12)\n",
    "plt.ylabel('Dimension 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Notice how similar words cluster together!\")\n",
    "print(\"üí° 'learning', 'language', 'processing' should be close\")\n",
    "print(\"üéØ This is how RAG systems find similar documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Real AI Application: Building a Spam Detector\n",
    "\n",
    "**Scenario**: Build an email spam detector using text embeddings!\n",
    "\n",
    "**Task**: Classify emails as SPAM or HAM (not spam)\n",
    "\n",
    "**Architecture**:\n",
    "1. Preprocess emails (clean text)\n",
    "2. Convert to TF-IDF vectors\n",
    "3. Train classifier (Naive Bayes)\n",
    "4. Evaluate on test set\n",
    "5. Test with real examples!\n",
    "\n",
    "**This is used in:**\n",
    "- Email providers (Gmail, Outlook)\n",
    "- Spam detection systems\n",
    "- Content moderation\n",
    "- Phishing detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic spam/ham dataset\n",
    "emails = [\n",
    "    # SPAM emails\n",
    "    \"Congratulations! You won a million dollars! Click here to claim your prize now!\",\n",
    "    \"Urgent! Your account will be closed. Verify your password immediately.\",\n",
    "    \"Get rich quick! Make money from home. Limited time offer!\",\n",
    "    \"Free iPhone! Click now to get your free phone today!\",\n",
    "    \"Weight loss miracle! Lose 20 pounds in 2 weeks guaranteed!\",\n",
    "    \"You have inherited 5 million dollars from a distant relative in Nigeria\",\n",
    "    \"Cheap medications! No prescription needed! Order now!\",\n",
    "    \"Work from home and earn $5000 per week! No experience required!\",\n",
    "    \"Congratulations! You are the lucky winner of our lottery!\",\n",
    "    \"Click here for free credit card with unlimited limit!\",\n",
    "    \n",
    "    # HAM emails (legitimate)\n",
    "    \"Hi, let's schedule a meeting tomorrow to discuss the project progress.\",\n",
    "    \"Your Amazon order has been shipped and will arrive on Friday.\",\n",
    "    \"Reminder: Team standup meeting at 10 AM tomorrow.\",\n",
    "    \"Thank you for your purchase. Your receipt is attached.\",\n",
    "    \"Hi Mom, I'll be visiting next weekend. Looking forward to seeing you!\",\n",
    "    \"Your flight booking confirmation for New York on Dec 25th.\",\n",
    "    \"The quarterly report is ready for review. Please let me know your feedback.\",\n",
    "    \"Happy birthday! Hope you have a wonderful day!\",\n",
    "    \"Reminder: Your dentist appointment is scheduled for next Tuesday at 2 PM.\",\n",
    "    \"The new software update includes several bug fixes and improvements.\",\n",
    "]\n",
    "\n",
    "# Labels (0 = HAM, 1 = SPAM)\n",
    "labels = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  # First 10 are SPAM\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  # Last 10 are HAM\n",
    "\n",
    "print(\"üìß Email Dataset:\")\n",
    "print(f\"Total emails: {len(emails)}\")\n",
    "print(f\"SPAM emails: {sum(labels)}\")\n",
    "print(f\"HAM emails: {len(labels) - sum(labels)}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'email': emails,\n",
    "    'label': labels,\n",
    "    'category': ['SPAM' if l == 1 else 'HAM' for l in labels]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Sample emails:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['email'], df['label'], test_size=0.3, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "print(\"üîÄ Data Split:\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(f\"  SPAM: {sum(y_train)}\")\n",
    "print(f\"  HAM: {len(y_train) - sum(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize emails using TF-IDF\n",
    "tfidf_vec = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit on training data\n",
    "X_train_tfidf = tfidf_vec.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vec.transform(X_test)\n",
    "\n",
    "print(\"üìä TF-IDF Vectorization:\")\n",
    "print(f\"Training matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Test matrix shape: {X_test_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vec.get_feature_names_out())}\")\n",
    "print(f\"\\nTop 20 features:\")\n",
    "print(list(tfidf_vec.get_feature_names_out()[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"ü§ñ Training Spam Classifier...\\n\")\n",
    "print(\"‚úÖ Model trained!\\n\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"üìä Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\\n\")\n",
    "\n",
    "print(\"üìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['HAM', 'SPAM']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['HAM', 'SPAM'], \n",
    "            yticklabels=['HAM', 'SPAM'])\n",
    "plt.title('Confusion Matrix - Spam Detector', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Confusion Matrix Interpretation:\")\n",
    "print(\"   - Diagonal cells = Correct predictions\")\n",
    "print(\"   - Off-diagonal = Misclassifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with new emails\n",
    "new_emails = [\n",
    "    \"Congratulations! You won a free vacation! Click here now!\",\n",
    "    \"Hi, are we still meeting for lunch tomorrow?\",\n",
    "    \"Urgent! Your package could not be delivered. Verify your address.\",\n",
    "    \"The Python programming course starts next Monday. See you there!\",\n",
    "]\n",
    "\n",
    "print(\"üîÆ Testing Spam Detector on New Emails:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Vectorize and predict\n",
    "new_emails_tfidf = tfidf_vec.transform(new_emails)\n",
    "predictions = clf.predict(new_emails_tfidf)\n",
    "probabilities = clf.predict_proba(new_emails_tfidf)\n",
    "\n",
    "for i, email in enumerate(new_emails):\n",
    "    prediction = \"SPAM\" if predictions[i] == 1 else \"HAM\"\n",
    "    spam_prob = probabilities[i][1]\n",
    "    ham_prob = probabilities[i][0]\n",
    "    \n",
    "    print(f\"\\nüìß Email: \\\"{email}\\\"\")\n",
    "    print(f\"üéØ Prediction: {prediction}\")\n",
    "    print(f\"üìä Confidence: HAM {ham_prob:.2%} | SPAM {spam_prob:.2%}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ Spam detector working!\")\n",
    "print(\"üí° This is how Gmail and Outlook filter your emails!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Why This Matters for Modern AI\n",
    "\n",
    "### üîç **RAG Systems (2024-2025 Trend)**\n",
    "\n",
    "What you learned today is the **foundation of RAG**!\n",
    "\n",
    "**Full RAG Pipeline:**\n",
    "```\n",
    "User Query: \"What is machine learning?\"\n",
    "     ‚Üì\n",
    "1. EMBED query ‚Üí vector (using Word2Vec, BERT, etc.)\n",
    "     ‚Üì\n",
    "2. RETRIEVE similar docs (cosine similarity with TF-IDF/embeddings)\n",
    "     ‚Üì\n",
    "3. GENERATE answer using LLM + retrieved docs\n",
    "     ‚Üì\n",
    "Answer: \"Based on our documentation, machine learning is...\"\n",
    "```\n",
    "\n",
    "### üéØ **Real-World Applications:**\n",
    "\n",
    "**1. Customer Support Bots**\n",
    "- Embed all support docs\n",
    "- User asks question ‚Üí retrieve relevant docs\n",
    "- LLM generates answer with sources\n",
    "\n",
    "**2. Spam & Content Moderation**\n",
    "- Email providers (Gmail, Outlook)\n",
    "- Social media platforms (Twitter, Facebook)\n",
    "- Comment filtering systems\n",
    "\n",
    "**3. Semantic Search**\n",
    "- Google, Bing use embeddings\n",
    "- Understand query intent\n",
    "- Find pages by meaning\n",
    "\n",
    "**4. Recommendation Systems**\n",
    "- Netflix, YouTube recommendations\n",
    "- E-commerce product suggestions\n",
    "- Content discovery\n",
    "\n",
    "### ü§ñ **Modern Embedding Models:**\n",
    "\n",
    "In production, you'd use advanced embeddings:\n",
    "- **OpenAI Embeddings**: `text-embedding-3-large`\n",
    "- **Sentence Transformers**: `all-MiniLM-L6-v2`\n",
    "- **Cohere Embeddings**: Multilingual support\n",
    "- **Voyage AI**: Optimized for RAG\n",
    "\n",
    "These are 100x better than Word2Vec for RAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercise\n",
    "\n",
    "**Challenge**: Build a product review sentiment analyzer!\n",
    "\n",
    "**Scenario**: Classify product reviews as POSITIVE or NEGATIVE\n",
    "\n",
    "**Tasks**:\n",
    "1. Create a dataset of product reviews (at least 10 positive, 10 negative)\n",
    "2. Preprocess the reviews\n",
    "3. Vectorize using TF-IDF\n",
    "4. Train a classifier\n",
    "5. Test on new reviews\n",
    "\n",
    "**Bonus**: Visualize the most important words for each class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "\n",
    "# TODO 1: Create product review dataset\n",
    "reviews = [\n",
    "    # Add positive reviews\n",
    "    # Add negative reviews\n",
    "]\n",
    "\n",
    "# TODO 2: Create labels (1 = positive, 0 = negative)\n",
    "review_labels = []\n",
    "\n",
    "# TODO 3: Split, vectorize, train, evaluate\n",
    "\n",
    "# TODO 4: Test on new reviews\n",
    "\n",
    "print(\"Complete the TODOs above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution (Try on your own first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# Product reviews dataset\n",
    "reviews = [\n",
    "    # POSITIVE reviews\n",
    "    \"This product is amazing! Best purchase ever!\",\n",
    "    \"Excellent quality, highly recommend to everyone!\",\n",
    "    \"Love it! Works perfectly and arrived quickly.\",\n",
    "    \"Outstanding product, exceeded my expectations!\",\n",
    "    \"Great value for money, very satisfied with purchase.\",\n",
    "    \"Fantastic! Exactly what I needed.\",\n",
    "    \"Best product in its category. 5 stars!\",\n",
    "    \"Impressed with the quality and fast shipping.\",\n",
    "    \"Wonderful experience, will buy again!\",\n",
    "    \"Absolutely love this product, works great!\",\n",
    "    \n",
    "    # NEGATIVE reviews\n",
    "    \"Terrible product, waste of money!\",\n",
    "    \"Poor quality, broke after one week.\",\n",
    "    \"Disappointed, does not work as advertised.\",\n",
    "    \"Awful experience, would not recommend.\",\n",
    "    \"Complete waste of time and money.\",\n",
    "    \"Low quality, returned immediately.\",\n",
    "    \"Horrible product, do not buy!\",\n",
    "    \"Very disappointed with this purchase.\",\n",
    "    \"Worst product ever, complete failure.\",\n",
    "    \"Useless, does not work at all.\",\n",
    "]\n",
    "\n",
    "# Labels (1 = positive, 0 = negative)\n",
    "review_labels = [1]*10 + [0]*10\n",
    "\n",
    "# Split data\n",
    "X_train_rev, X_test_rev, y_train_rev, y_test_rev = train_test_split(\n",
    "    reviews, review_labels, test_size=0.3, random_state=42, stratify=review_labels\n",
    ")\n",
    "\n",
    "# Vectorize\n",
    "tfidf_rev = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "X_train_rev_tfidf = tfidf_rev.fit_transform(X_train_rev)\n",
    "X_test_rev_tfidf = tfidf_rev.transform(X_test_rev)\n",
    "\n",
    "# Train classifier\n",
    "clf_rev = MultinomialNB()\n",
    "clf_rev.fit(X_train_rev_tfidf, y_train_rev)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_rev = clf_rev.predict(X_test_rev_tfidf)\n",
    "accuracy_rev = accuracy_score(y_test_rev, y_pred_rev)\n",
    "\n",
    "print(\"üéØ Product Review Sentiment Analyzer\\n\")\n",
    "print(f\"Accuracy: {accuracy_rev:.2%}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_rev, y_pred_rev, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Test on new reviews\n",
    "new_reviews = [\n",
    "    \"This is the best product I have ever bought!\",\n",
    "    \"Complete disaster, do not waste your money.\",\n",
    "    \"Okay product, nothing special.\",\n",
    "]\n",
    "\n",
    "print(\"\\nüîÆ Testing on new reviews:\\n\")\n",
    "new_reviews_tfidf = tfidf_rev.transform(new_reviews)\n",
    "predictions_rev = clf_rev.predict(new_reviews_tfidf)\n",
    "\n",
    "for review, pred in zip(new_reviews, predictions_rev):\n",
    "    sentiment = \"POSITIVE\" if pred == 1 else \"NEGATIVE\"\n",
    "    print(f\"Review: \\\"{review}\\\"\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")\n",
    "\n",
    "print(\"‚úÖ Sentiment analyzer built!\")\n",
    "print(\"üí° This is used by Amazon, Yelp, and all review platforms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- ‚úÖ Text preprocessing pipeline (tokenization, stemming, lemmatization)\n",
    "- ‚úÖ Bag of Words and TF-IDF vectorization\n",
    "- ‚úÖ Word embeddings (Word2Vec) and semantic similarity\n",
    "- ‚úÖ Understanding of GloVe embeddings\n",
    "- ‚úÖ Built a real spam detector using text classification\n",
    "- ‚úÖ How embeddings power modern AI (RAG, search, chatbots)\n",
    "\n",
    "### üéØ Key Takeaways:\n",
    "\n",
    "1. **Text preprocessing is essential**\n",
    "   - Cleaning and normalization improve model performance\n",
    "   - Tokenization is the first step for all NLP tasks\n",
    "   - Modern LLMs use sophisticated tokenizers (BPE, WordPiece)\n",
    "\n",
    "2. **Embeddings capture semantic meaning**\n",
    "   - Similar words ‚Üí Similar vectors\n",
    "   - Enable semantic search and similarity\n",
    "   - Foundation of modern NLP\n",
    "\n",
    "3. **TF-IDF highlights important words**\n",
    "   - Simple yet powerful for classification\n",
    "   - Used in search engines and document ranking\n",
    "   - Still relevant in production systems\n",
    "\n",
    "4. **Text classification has countless applications**\n",
    "   - Spam detection, sentiment analysis, intent classification\n",
    "   - Powers email filters, review systems, chatbots\n",
    "   - Critical for content moderation\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Practice Exercise (Before Day 2):**\n",
    "\n",
    "Build a news article categorizer:\n",
    "1. Create articles from 3-4 categories (tech, sports, politics, entertainment)\n",
    "2. Preprocess and vectorize using TF-IDF\n",
    "3. Train a multi-class classifier\n",
    "4. Evaluate and test on new articles\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Next Lesson:** Day 2 - Advanced NLP Techniques\n",
    "- Named Entity Recognition (NER)\n",
    "- Part-of-Speech (POS) tagging\n",
    "- Text classification with deep learning\n",
    "- News article categorization\n",
    "- Customer review analysis\n",
    "\n",
    "---\n",
    "\n",
    "**üí¨ Remember:**\n",
    "\n",
    "*\"Every AI system that processes text - from ChatGPT to Gmail's spam filter to Google Search - starts with the fundamentals you learned today. Text preprocessing, embeddings, and classification are the building blocks of modern NLP. You now understand the core concepts that power billions of AI-driven interactions every day!\"* üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**üîó Connections to Modern AI:**\n",
    "- **RAG**: Embeddings + TF-IDF for document retrieval\n",
    "- **LLMs**: Advanced tokenization and embeddings\n",
    "- **Chatbots**: Text classification for intent recognition\n",
    "- **Search**: TF-IDF and semantic similarity\n",
    "- **Content Moderation**: Text classification at scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
