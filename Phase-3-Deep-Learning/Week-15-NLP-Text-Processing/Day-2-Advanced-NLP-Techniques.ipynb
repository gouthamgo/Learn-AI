{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Day 2: Advanced NLP Techniques\n",
    "\n",
    "**üéØ Goal:** Master NER, POS tagging, and deep learning for text classification\n",
    "\n",
    "**‚è±Ô∏è Time:** 90-120 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Named Entity Recognition powers chatbots, search engines, and information extraction\n",
    "- POS tagging helps LLMs understand grammar and sentence structure\n",
    "- Deep learning text classification is used in content moderation, spam detection, and intent recognition\n",
    "- These techniques are building blocks for ChatGPT, Claude, and modern AI systems\n",
    "- Real-world applications: news categorization, sentiment analysis, customer support automation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† What are Advanced NLP Techniques?\n",
    "\n",
    "Beyond simple text preprocessing, we need to **understand** text structure and meaning!\n",
    "\n",
    "### üéØ Today's Focus:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Named Entity Recognition (NER)**\n",
    "- Extract names, locations, organizations, dates\n",
    "- **Example**: \"Apple CEO Tim Cook visited Paris\" ‚Üí [Apple: ORG, Tim Cook: PERSON, Paris: LOC]\n",
    "- **Used in**: Chatbots, search engines, knowledge graphs\n",
    "\n",
    "#### 2Ô∏è‚É£ **Part-of-Speech (POS) Tagging**\n",
    "- Identify grammatical roles (noun, verb, adjective)\n",
    "- **Example**: \"I love AI\" ‚Üí [I: PRONOUN, love: VERB, AI: NOUN]\n",
    "- **Used in**: Grammar checking, text-to-speech, LLM training\n",
    "\n",
    "#### 3Ô∏è‚É£ **Deep Learning Text Classification**\n",
    "- Use neural networks for better accuracy\n",
    "- **Methods**: Embedding layers, LSTMs, CNNs for text\n",
    "- **Used in**: Content moderation, intent classification, sentiment analysis\n",
    "\n",
    "### üîë Why This Matters for 2024-2025 AI:\n",
    "\n",
    "**ü§ñ ChatGPT & Claude:**\n",
    "- Use NER to extract entities from conversations\n",
    "- Understand sentence structure through POS patterns\n",
    "- Classify user intents for appropriate responses\n",
    "\n",
    "**üîç RAG Systems:**\n",
    "- Extract entities from documents for better indexing\n",
    "- Classify document types for retrieval\n",
    "- Understand query intent\n",
    "\n",
    "**üì∞ Real Applications:**\n",
    "- News aggregation and categorization\n",
    "- Customer review sentiment analysis\n",
    "- Social media content moderation\n",
    "- Email routing and prioritization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk spacy scikit-learn tensorflow numpy pandas matplotlib seaborn --quiet\n",
    "# Download spaCy model\n",
    "!{sys.executable} -m spacy download en_core_web_sm --quiet\n",
    "\n",
    "print(\"‚úÖ Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "from nltk import pos_tag, ne_chunk, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# spaCy (modern NLP library)\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# TensorFlow/Keras for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(\"üìö Libraries loaded!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"spaCy version: {spacy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Part 1: Named Entity Recognition (NER)\n",
    "\n",
    "**Named Entity Recognition** = Identifying and classifying named entities in text\n",
    "\n",
    "### üéØ Common Entity Types:\n",
    "\n",
    "- **PERSON**: Names of people (Tim Cook, Elon Musk)\n",
    "- **ORG**: Organizations (Apple, Google, Microsoft)\n",
    "- **GPE**: Geo-political entities (New York, USA, Paris)\n",
    "- **DATE**: Dates and times (January 2024, next week)\n",
    "- **MONEY**: Monetary values ($100, ‚Ç¨50)\n",
    "- **PRODUCT**: Products and objects (iPhone, ChatGPT)\n",
    "\n",
    "### üéØ Why NER Matters:\n",
    "\n",
    "**üîç Information Extraction:**\n",
    "- Extract structured data from unstructured text\n",
    "- Build knowledge graphs (Google Knowledge Graph)\n",
    "- Populate databases automatically\n",
    "\n",
    "**üí¨ Chatbots & Virtual Assistants:**\n",
    "- \"Book a flight to Paris\" ‚Üí Extract: Paris (GPE)\n",
    "- \"Set meeting with John tomorrow\" ‚Üí Extract: John (PERSON), tomorrow (DATE)\n",
    "\n",
    "**üì∞ News & Content Analysis:**\n",
    "- Track mentions of companies, people, locations\n",
    "- Generate article summaries\n",
    "- Create topic clusters\n",
    "\n",
    "**üîí Privacy & Compliance:**\n",
    "- Identify and mask PII (personally identifiable information)\n",
    "- GDPR compliance\n",
    "- Data anonymization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with NLTK (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text about tech companies\n",
    "text_ner = \"\"\"\n",
    "Apple CEO Tim Cook announced the new iPhone 15 will be released in September 2024.\n",
    "The company, based in Cupertino, California, expects to sell 50 million units.\n",
    "Meanwhile, Google and Microsoft are competing in the AI race with ChatGPT and Gemini.\n",
    "Elon Musk's xAI raised $6 billion in funding from investors in Silicon Valley.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Original Text:\")\n",
    "print(text_ner)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Tokenize and tag\n",
    "tokens = word_tokenize(text_ner)\n",
    "pos_tags = pos_tag(tokens)\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "print(\"üè∑Ô∏è Named Entities (NLTK):\")\n",
    "for entity in named_entities:\n",
    "    if hasattr(entity, 'label'):\n",
    "        print(f\"{entity.label()}: {' '.join([word for word, tag in entity])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with spaCy (Modern & Better!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text with spaCy\n",
    "doc = nlp(text_ner)\n",
    "\n",
    "print(\"üè∑Ô∏è Named Entities (spaCy):\\n\")\n",
    "print(f\"{'Entity':<20} {'Type':<15} {'Description':<30}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<20} {ent.label_:<15} {spacy.explain(ent.label_):<30}\")\n",
    "\n",
    "print(\"\\nüí° spaCy provides more accurate and detailed entity recognition!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entities\n",
    "from spacy import displacy\n",
    "\n",
    "print(\"üé® Visual NER (Entity Highlighting):\\n\")\n",
    "\n",
    "# Display in notebook (for Jupyter)\n",
    "colors = {'PERSON': '#aa9cfc', 'ORG': '#7aecec', 'GPE': '#feca74', 'DATE': '#ff9561', 'MONEY': '#9cc9cc'}\n",
    "options = {'ents': ['PERSON', 'ORG', 'GPE', 'DATE', 'MONEY', 'PRODUCT'], 'colors': colors}\n",
    "\n",
    "html = displacy.render(doc, style='ent', options=options, jupyter=False)\n",
    "print(\"Entity highlighting available in Jupyter notebooks!\")\n",
    "\n",
    "# Extract entities by type\n",
    "entities_by_type = {}\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ not in entities_by_type:\n",
    "        entities_by_type[ent.label_] = []\n",
    "    entities_by_type[ent.label_].append(ent.text)\n",
    "\n",
    "print(\"\\nüìä Entities Grouped by Type:\\n\")\n",
    "for entity_type, entities in entities_by_type.items():\n",
    "    print(f\"{entity_type}: {', '.join(set(entities))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî§ Part 2: Part-of-Speech (POS) Tagging\n",
    "\n",
    "**POS Tagging** = Labeling each word with its grammatical role\n",
    "\n",
    "### üéØ Common POS Tags:\n",
    "\n",
    "- **NN**: Noun (dog, computer, AI)\n",
    "- **VB**: Verb (run, learn, process)\n",
    "- **JJ**: Adjective (beautiful, fast, intelligent)\n",
    "- **RB**: Adverb (quickly, very, extremely)\n",
    "- **PRP**: Pronoun (I, you, he, she)\n",
    "- **DT**: Determiner (the, a, an)\n",
    "- **IN**: Preposition (in, on, at, with)\n",
    "\n",
    "### üéØ Why POS Tagging Matters:\n",
    "\n",
    "**üìù Text Understanding:**\n",
    "- Understand sentence structure\n",
    "- Disambiguate word meanings\n",
    "- Example: \"I fish\" (verb) vs \"a fish\" (noun)\n",
    "\n",
    "**üîç Information Extraction:**\n",
    "- Extract noun phrases (products, features)\n",
    "- Find adjectives (sentiment indicators)\n",
    "- Identify actions (verbs)\n",
    "\n",
    "**ü§ñ LLM Training:**\n",
    "- Pre-training data includes POS information\n",
    "- Helps models learn grammar\n",
    "- Improves generation quality\n",
    "\n",
    "**üí¨ Grammar Checking:**\n",
    "- Grammarly, Microsoft Word\n",
    "- Detect grammatical errors\n",
    "- Suggest corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"ChatGPT revolutionized natural language processing in 2024.\",\n",
    "    \"I love building AI applications with Python.\"\n",
    "]\n",
    "\n",
    "print(\"üî§ Part-of-Speech Tagging:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f\"\\nüìù Sentence: {sentence}\\n\")\n",
    "    \n",
    "    # NLTK POS tagging\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    print(f\"{'Word':<20} {'POS Tag':<10} {'Description':<30}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Process with spaCy for better descriptions\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:<20} {token.pos_:<10} {token.tag_} ({spacy.explain(token.tag_) or 'N/A'})\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific POS patterns\n",
    "text_pos = \"The amazing new iPhone 15 features incredible AI-powered camera capabilities that revolutionize mobile photography.\"\n",
    "\n",
    "doc = nlp(text_pos)\n",
    "\n",
    "# Extract nouns\n",
    "nouns = [token.text for token in doc if token.pos_ == 'NOUN']\n",
    "# Extract adjectives\n",
    "adjectives = [token.text for token in doc if token.pos_ == 'ADJ']\n",
    "# Extract verbs\n",
    "verbs = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "\n",
    "print(\"üìä Extracting Words by POS:\\n\")\n",
    "print(f\"Original: {text_pos}\\n\")\n",
    "print(f\"Nouns (NOUN): {', '.join(nouns)}\")\n",
    "print(f\"Adjectives (ADJ): {', '.join(adjectives)}\")\n",
    "print(f\"Verbs (VERB): {', '.join(verbs)}\")\n",
    "\n",
    "print(\"\\nüí° Use case: Extract product features (nouns) and sentiment indicators (adjectives)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Part 3: Deep Learning for Text Classification\n",
    "\n",
    "**Deep Learning** takes text classification to the next level!\n",
    "\n",
    "### üéØ Why Deep Learning for NLP?\n",
    "\n",
    "**Traditional ML (Day 1):**\n",
    "- TF-IDF + Naive Bayes\n",
    "- Simple, fast, interpretable\n",
    "- Limited accuracy on complex tasks\n",
    "\n",
    "**Deep Learning:**\n",
    "- Learns representations automatically\n",
    "- Captures context and semantics\n",
    "- Higher accuracy on complex tasks\n",
    "- Powers ChatGPT, Claude, BERT\n",
    "\n",
    "### üèóÔ∏è Text Classification Architecture:\n",
    "\n",
    "```\n",
    "Input Text: \"I love this product!\"\n",
    "     ‚Üì\n",
    "1. Tokenization ‚Üí [I, love, this, product]\n",
    "     ‚Üì\n",
    "2. Integer Encoding ‚Üí [34, 892, 12, 456]\n",
    "     ‚Üì\n",
    "3. Embedding Layer ‚Üí Dense vectors\n",
    "     ‚Üì\n",
    "4. LSTM/CNN Layer ‚Üí Learn patterns\n",
    "     ‚Üì\n",
    "5. Dense Layer ‚Üí Classification\n",
    "     ‚Üì\n",
    "Output: POSITIVE (98% confidence)\n",
    "```\n",
    "\n",
    "### üéØ Key Components:\n",
    "\n",
    "**1Ô∏è‚É£ Embedding Layer**\n",
    "- Converts words to dense vectors\n",
    "- Learned during training\n",
    "- Similar to Word2Vec but task-specific\n",
    "\n",
    "**2Ô∏è‚É£ LSTM Layer**\n",
    "- Processes sequences (remembers context)\n",
    "- Handles variable-length text\n",
    "- Captures long-range dependencies\n",
    "\n",
    "**3Ô∏è‚É£ Dense Layer**\n",
    "- Final classification\n",
    "- Outputs probabilities for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∞ Real AI Application #1: News Article Categorization\n",
    "\n",
    "**Scenario**: Build a system to automatically categorize news articles!\n",
    "\n",
    "**Categories**:\n",
    "- Technology\n",
    "- Sports\n",
    "- Politics\n",
    "- Entertainment\n",
    "\n",
    "**Why this matters**:\n",
    "- News aggregators (Google News, Apple News)\n",
    "- Content recommendation systems\n",
    "- Automated content organization\n",
    "- RAG systems for domain-specific retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create news article dataset\n",
    "news_articles = [\n",
    "    # Technology\n",
    "    \"Apple unveils new iPhone 15 with advanced AI features and improved camera system\",\n",
    "    \"Google releases Gemini AI model competing with ChatGPT and Claude\",\n",
    "    \"Microsoft Azure expands cloud computing services for enterprise customers\",\n",
    "    \"Tesla introduces new autonomous driving features powered by neural networks\",\n",
    "    \"OpenAI announces GPT-4 Turbo with improved performance and lower pricing\",\n",
    "    \"Meta launches new VR headset with enhanced graphics and processing power\",\n",
    "    \"Amazon Web Services introduces new machine learning tools for developers\",\n",
    "    \"NVIDIA releases powerful new GPU designed for AI training and inference\",\n",
    "    \n",
    "    # Sports\n",
    "    \"Lakers defeat Warriors in overtime thriller at Staples Center\",\n",
    "    \"Lionel Messi scores hat-trick in Champions League final victory\",\n",
    "    \"Serena Williams wins her 24th Grand Slam title at Wimbledon\",\n",
    "    \"Tom Brady announces retirement after legendary NFL career\",\n",
    "    \"Manchester United signs promising young striker from Barcelona\",\n",
    "    \"Usain Bolt breaks 100m world record at Olympic Games\",\n",
    "    \"Tiger Woods makes comeback at Masters tournament after injury\",\n",
    "    \"Cristiano Ronaldo transfers to Al-Nassr for record contract\",\n",
    "    \n",
    "    # Politics\n",
    "    \"President announces new economic policy to combat inflation\",\n",
    "    \"Senate passes bipartisan infrastructure bill after months of debate\",\n",
    "    \"Supreme Court rules on major constitutional amendment case\",\n",
    "    \"United Nations summit addresses global climate change initiatives\",\n",
    "    \"Governor signs executive order on healthcare reform legislation\",\n",
    "    \"Congressional committee investigates cybersecurity threats\",\n",
    "    \"White House announces diplomatic mission to strengthen international relations\",\n",
    "    \"Parliament debates new taxation and fiscal responsibility measures\",\n",
    "    \n",
    "    # Entertainment\n",
    "    \"New Marvel movie breaks box office records on opening weekend\",\n",
    "    \"Taylor Swift releases surprise album to critical acclaim\",\n",
    "    \"Netflix announces original series starring Hollywood A-listers\",\n",
    "    \"Academy Awards ceremony honors best films of the year\",\n",
    "    \"Beyonce embarks on world tour with sold-out stadium shows\",\n",
    "    \"Disney announces new theme park attractions and experiences\",\n",
    "    \"Grammy Awards celebrate best music and artists of the year\",\n",
    "    \"Christopher Nolan's latest film receives rave reviews from critics\",\n",
    "]\n",
    "\n",
    "# Labels (0: Tech, 1: Sports, 2: Politics, 3: Entertainment)\n",
    "news_labels = [0]*8 + [1]*8 + [2]*8 + [3]*8\n",
    "category_names = ['Technology', 'Sports', 'Politics', 'Entertainment']\n",
    "\n",
    "# Create DataFrame\n",
    "df_news = pd.DataFrame({\n",
    "    'article': news_articles,\n",
    "    'label': news_labels,\n",
    "    'category': [category_names[label] for label in news_labels]\n",
    "})\n",
    "\n",
    "print(\"üì∞ News Article Dataset:\")\n",
    "print(f\"Total articles: {len(df_news)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df_news['category'].value_counts())\n",
    "print(\"\\nüìä Sample articles:\")\n",
    "print(df_news.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for deep learning\n",
    "X_news = df_news['article'].values\n",
    "y_news = df_news['label'].values\n",
    "\n",
    "# Split data\n",
    "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(\n",
    "    X_news, y_news, test_size=0.25, random_state=42, stratify=y_news\n",
    ")\n",
    "\n",
    "print(\"üîÄ Data Split:\")\n",
    "print(f\"Training samples: {len(X_train_news)}\")\n",
    "print(f\"Test samples: {len(X_test_news)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and padding\n",
    "max_words = 1000  # Vocabulary size\n",
    "max_len = 20      # Maximum sequence length\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer_news = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer_news.fit_on_texts(X_train_news)\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_seq = tokenizer_news.texts_to_sequences(X_train_news)\n",
    "X_test_seq = tokenizer_news.texts_to_sequences(X_test_news)\n",
    "\n",
    "# Pad sequences to same length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "print(\"üìä Text Preprocessing for Deep Learning:\")\n",
    "print(f\"Vocabulary size: {len(tokenizer_news.word_index)}\")\n",
    "print(f\"Training data shape: {X_train_pad.shape}\")\n",
    "print(f\"Test data shape: {X_test_pad.shape}\")\n",
    "print(f\"\\nExample conversion:\")\n",
    "print(f\"Original: {X_train_news[0]}\")\n",
    "print(f\"Sequence: {X_train_seq[0]}\")\n",
    "print(f\"Padded: {X_train_pad[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model for news classification\n",
    "embedding_dim = 32\n",
    "num_classes = 4\n",
    "\n",
    "model_news = Sequential([\n",
    "    # Embedding layer - converts word indices to dense vectors\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "    \n",
    "    # LSTM layer - processes sequences\n",
    "    LSTM(64, return_sequences=False),\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Dense layer for classification\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer - 4 classes\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_news.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"üß† News Classification Model Architecture:\\n\")\n",
    "model_news.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"üöÄ Training News Classifier...\\n\")\n",
    "\n",
    "history_news = model_news.fit(\n",
    "    X_train_pad, y_train_news,\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training complete!\\n\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model_news.evaluate(X_test_pad, y_test_news, verbose=0)\n",
    "print(f\"üìä Test Accuracy: {test_acc:.2%}\")\n",
    "print(f\"üìâ Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(history_news.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "ax1.plot(history_news.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(history_news.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax2.plot(history_news.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Training progressed smoothly without overfitting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_news = model_news.predict(X_test_pad, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred_news, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"üìã Classification Report:\\n\")\n",
    "print(classification_report(y_test_news, y_pred_classes, target_names=category_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_news, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=category_names,\n",
    "            yticklabels=category_names)\n",
    "plt.title('Confusion Matrix - News Categorization', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Category', fontsize=12)\n",
    "plt.xlabel('Predicted Category', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new articles\n",
    "new_articles = [\n",
    "    \"Amazon introduces breakthrough quantum computing chips for data centers\",\n",
    "    \"LeBron James leads team to championship victory in playoff final\",\n",
    "    \"Congress debates new legislation on renewable energy standards\",\n",
    "    \"Streaming platform announces exclusive series with top Hollywood actors\",\n",
    "]\n",
    "\n",
    "print(\"üîÆ Testing News Categorizer on New Articles:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Preprocess and predict\n",
    "new_seq = tokenizer_news.texts_to_sequences(new_articles)\n",
    "new_pad = pad_sequences(new_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "predictions = model_news.predict(new_pad, verbose=0)\n",
    "\n",
    "for i, article in enumerate(new_articles):\n",
    "    pred_class = np.argmax(predictions[i])\n",
    "    confidence = predictions[i][pred_class]\n",
    "    \n",
    "    print(f\"\\nüì∞ Article: \\\"{article}\\\"\")\n",
    "    print(f\"üéØ Category: {category_names[pred_class]}\")\n",
    "    print(f\"üìä Confidence: {confidence:.2%}\")\n",
    "    print(f\"\\n   Probabilities:\")\n",
    "    for j, cat in enumerate(category_names):\n",
    "        print(f\"      {cat}: {predictions[i][j]:.2%}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ News categorizer working perfectly!\")\n",
    "print(\"üí° This is how Google News, Apple News organize articles!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚≠ê Real AI Application #2: Customer Review Sentiment Analysis\n",
    "\n",
    "**Scenario**: Analyze customer reviews to determine sentiment (Positive/Negative/Neutral)\n",
    "\n",
    "**Why this matters**:\n",
    "- E-commerce platforms (Amazon, eBay)\n",
    "- Product feedback analysis\n",
    "- Customer satisfaction monitoring\n",
    "- Brand reputation management\n",
    "- Automated support ticket routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer review dataset\n",
    "reviews = [\n",
    "    # Positive reviews\n",
    "    \"This product exceeded all my expectations! Absolutely love it!\",\n",
    "    \"Outstanding quality and fast shipping. Highly recommend!\",\n",
    "    \"Best purchase I've made this year. 5 stars!\",\n",
    "    \"Amazing product, works perfectly and looks great!\",\n",
    "    \"Excellent value for money. Very satisfied with this purchase.\",\n",
    "    \"Fantastic quality, exactly as described. Will buy again!\",\n",
    "    \"Love this! Great features and easy to use.\",\n",
    "    \"Superb product! Better than I expected.\",\n",
    "    \"Very impressed with the quality and performance!\",\n",
    "    \"Wonderful product, absolutely worth the price!\",\n",
    "    \n",
    "    # Negative reviews\n",
    "    \"Terrible quality, broke after one week. Very disappointed.\",\n",
    "    \"Worst purchase ever. Complete waste of money!\",\n",
    "    \"Poor quality and doesn't work as advertised. Avoid!\",\n",
    "    \"Awful product, returned immediately. Don't buy!\",\n",
    "    \"Very disappointed. Low quality and overpriced.\",\n",
    "    \"Horrible experience. Product failed on first use.\",\n",
    "    \"Not as described. Cheap materials and poor construction.\",\n",
    "    \"Terrible customer service and defective product.\",\n",
    "    \"Complete disaster. Nothing works properly.\",\n",
    "    \"Waste of money. Returned for refund immediately.\",\n",
    "    \n",
    "    # Neutral reviews\n",
    "    \"It's okay. Does what it's supposed to do.\",\n",
    "    \"Average product. Nothing special but works fine.\",\n",
    "    \"Decent quality for the price. Could be better.\",\n",
    "    \"It's alright. Met my basic expectations.\",\n",
    "    \"Product is okay. Some good features, some not so good.\",\n",
    "    \"Acceptable quality. Not great, not terrible.\",\n",
    "    \"Fair product for the price. Average performance.\",\n",
    "    \"It works. Nothing to complain about, nothing to praise.\",\n",
    "    \"Meets basic requirements. Could use improvements.\",\n",
    "    \"Standard product. Does the job adequately.\",\n",
    "]\n",
    "\n",
    "# Labels (0: Negative, 1: Neutral, 2: Positive)\n",
    "sentiment_labels = [2]*10 + [0]*10 + [1]*10\n",
    "sentiment_names = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# Create DataFrame\n",
    "df_reviews = pd.DataFrame({\n",
    "    'review': reviews,\n",
    "    'label': sentiment_labels,\n",
    "    'sentiment': [sentiment_names[label] for label in sentiment_labels]\n",
    "})\n",
    "\n",
    "print(\"‚≠ê Customer Review Dataset:\")\n",
    "print(f\"Total reviews: {len(df_reviews)}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df_reviews['sentiment'].value_counts())\n",
    "print(\"\\nüìä Sample reviews:\")\n",
    "print(df_reviews.sample(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_reviews = df_reviews['review'].values\n",
    "y_reviews = df_reviews['label'].values\n",
    "\n",
    "# Split data\n",
    "X_train_rev, X_test_rev, y_train_rev, y_test_rev = train_test_split(\n",
    "    X_reviews, y_reviews, test_size=0.25, random_state=42, stratify=y_reviews\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "max_words_rev = 500\n",
    "max_len_rev = 15\n",
    "\n",
    "tokenizer_rev = Tokenizer(num_words=max_words_rev, oov_token='<OOV>')\n",
    "tokenizer_rev.fit_on_texts(X_train_rev)\n",
    "\n",
    "X_train_rev_seq = tokenizer_rev.texts_to_sequences(X_train_rev)\n",
    "X_test_rev_seq = tokenizer_rev.texts_to_sequences(X_test_rev)\n",
    "\n",
    "X_train_rev_pad = pad_sequences(X_train_rev_seq, maxlen=max_len_rev, padding='post')\n",
    "X_test_rev_pad = pad_sequences(X_test_rev_seq, maxlen=max_len_rev, padding='post')\n",
    "\n",
    "print(\"üìä Review Data Prepared:\")\n",
    "print(f\"Training samples: {len(X_train_rev)}\")\n",
    "print(f\"Test samples: {len(X_test_rev)}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer_rev.word_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sentiment analysis model\n",
    "model_sentiment = Sequential([\n",
    "    Embedding(input_dim=max_words_rev, output_dim=32, input_length=max_len_rev),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 3 classes: Negative, Neutral, Positive\n",
    "])\n",
    "\n",
    "model_sentiment.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"üß† Sentiment Analysis Model:\\n\")\n",
    "model_sentiment.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sentiment model\n",
    "print(\"üöÄ Training Sentiment Analyzer...\\n\")\n",
    "\n",
    "history_sentiment = model_sentiment.fit(\n",
    "    X_train_rev_pad, y_train_rev,\n",
    "    epochs=25,\n",
    "    batch_size=4,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training complete!\\n\")\n",
    "\n",
    "# Evaluate\n",
    "test_loss_sent, test_acc_sent = model_sentiment.evaluate(X_test_rev_pad, y_test_rev, verbose=0)\n",
    "print(f\"üìä Test Accuracy: {test_acc_sent:.2%}\")\n",
    "print(f\"üìâ Test Loss: {test_loss_sent:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history_sentiment.history['accuracy'], label='Training', linewidth=2)\n",
    "ax1.plot(history_sentiment.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "ax1.set_title('Sentiment Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history_sentiment.history['loss'], label='Training', linewidth=2)\n",
    "ax2.plot(history_sentiment.history['val_loss'], label='Validation', linewidth=2)\n",
    "ax2.set_title('Sentiment Model Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new reviews\n",
    "new_reviews = [\n",
    "    \"This is absolutely amazing! Best product ever!\",\n",
    "    \"Terrible quality, completely useless and broken.\",\n",
    "    \"It's fine. Does the job, nothing more.\",\n",
    "    \"Love this so much! Exceeded expectations!\",\n",
    "    \"Worst purchase I've ever made. Total waste.\",\n",
    "    \"Decent product. Works as expected.\",\n",
    "]\n",
    "\n",
    "print(\"üîÆ Testing Sentiment Analyzer on New Reviews:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Predict\n",
    "new_rev_seq = tokenizer_rev.texts_to_sequences(new_reviews)\n",
    "new_rev_pad = pad_sequences(new_rev_seq, maxlen=max_len_rev, padding='post')\n",
    "predictions_sent = model_sentiment.predict(new_rev_pad, verbose=0)\n",
    "\n",
    "for i, review in enumerate(new_reviews):\n",
    "    pred_class = np.argmax(predictions_sent[i])\n",
    "    confidence = predictions_sent[i][pred_class]\n",
    "    sentiment = sentiment_names[pred_class]\n",
    "    \n",
    "    # Choose emoji based on sentiment\n",
    "    emoji = \"üòä\" if pred_class == 2 else \"üòû\" if pred_class == 0 else \"üòê\"\n",
    "    \n",
    "    print(f\"\\n‚≠ê Review: \\\"{review}\\\"\")\n",
    "    print(f\"üéØ Sentiment: {sentiment} {emoji}\")\n",
    "    print(f\"üìä Confidence: {confidence:.2%}\")\n",
    "    print(f\"\\n   Probabilities:\")\n",
    "    for j, sent in enumerate(sentiment_names):\n",
    "        print(f\"      {sent}: {predictions_sent[i][j]:.2%}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ Sentiment analyzer working excellently!\")\n",
    "print(\"üí° Used by Amazon, Yelp, TripAdvisor for review analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Why This Matters for Modern AI\n",
    "\n",
    "### üîç **NER & POS in Production AI**\n",
    "\n",
    "**ü§ñ ChatGPT & Claude:**\n",
    "- Extract entities from user messages (names, dates, places)\n",
    "- Understand sentence structure for better responses\n",
    "- Generate grammatically correct text using POS patterns\n",
    "\n",
    "**üîç RAG Systems:**\n",
    "- Index documents by extracted entities\n",
    "- Improve search relevance using NER\n",
    "- Filter results by entity types\n",
    "\n",
    "**üìä Analytics & Business Intelligence:**\n",
    "- Track brand mentions across social media\n",
    "- Extract product features from reviews\n",
    "- Monitor competitor activity\n",
    "\n",
    "### üß† **Deep Learning Text Classification**\n",
    "\n",
    "**üì∞ Content Platforms:**\n",
    "- Google News, Apple News categorization\n",
    "- Reddit post classification\n",
    "- YouTube content categorization\n",
    "\n",
    "**üí¨ Customer Support:**\n",
    "- Automatic ticket routing\n",
    "- Priority classification\n",
    "- Sentiment monitoring\n",
    "\n",
    "**üîí Content Moderation:**\n",
    "- Detect toxic content\n",
    "- Spam filtering\n",
    "- Policy violation detection\n",
    "\n",
    "### üéØ **Evolution to Transformers:**\n",
    "\n",
    "What you learned today (LSTMs for text) evolved into:\n",
    "- **BERT**: Better context understanding\n",
    "- **GPT**: Better text generation\n",
    "- **Transformers**: Replace LSTMs with attention\n",
    "\n",
    "**We'll explore this in Day 3!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercise\n",
    "\n",
    "**Challenge**: Build an email intent classifier!\n",
    "\n",
    "**Scenario**: Classify customer support emails into categories\n",
    "\n",
    "**Categories**:\n",
    "- Billing Question\n",
    "- Technical Support\n",
    "- Product Inquiry\n",
    "- Complaint\n",
    "\n",
    "**Tasks**:\n",
    "1. Create a dataset of customer emails (at least 20 total)\n",
    "2. Preprocess and tokenize\n",
    "3. Build an LSTM classifier\n",
    "4. Train and evaluate\n",
    "5. Test on new emails\n",
    "\n",
    "**Bonus**: Extract entities (product names, amounts) using NER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "\n",
    "# TODO 1: Create customer email dataset\n",
    "customer_emails = [\n",
    "    # Add emails for each category\n",
    "]\n",
    "\n",
    "# TODO 2: Create labels\n",
    "email_labels = []\n",
    "\n",
    "# TODO 3: Build LSTM model\n",
    "\n",
    "# TODO 4: Train and evaluate\n",
    "\n",
    "# TODO 5: Test on new emails\n",
    "\n",
    "print(\"Complete the TODOs above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution (Try on your own first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Email Intent Classifier\n",
    "\n",
    "customer_emails = [\n",
    "    # Billing Question (0)\n",
    "    \"Why was I charged twice for my subscription this month?\",\n",
    "    \"Can you explain the charges on my latest invoice?\",\n",
    "    \"I need a refund for the duplicate payment.\",\n",
    "    \"What is the billing cycle for my account?\",\n",
    "    \"How do I update my payment method?\",\n",
    "    \n",
    "    # Technical Support (1)\n",
    "    \"The application keeps crashing when I try to login.\",\n",
    "    \"I cannot access my account, please help.\",\n",
    "    \"The software is not working properly on my computer.\",\n",
    "    \"Error message appears when I try to upload files.\",\n",
    "    \"Having trouble connecting to the server.\",\n",
    "    \n",
    "    # Product Inquiry (2)\n",
    "    \"What features are included in the premium plan?\",\n",
    "    \"Do you offer enterprise pricing for large teams?\",\n",
    "    \"Can you tell me more about the new product release?\",\n",
    "    \"Is there a mobile app available for iOS?\",\n",
    "    \"What are the system requirements for this software?\",\n",
    "    \n",
    "    # Complaint (3)\n",
    "    \"Very disappointed with the customer service quality.\",\n",
    "    \"This product does not work as advertised.\",\n",
    "    \"I want to cancel my subscription immediately.\",\n",
    "    \"Poor quality and terrible user experience.\",\n",
    "    \"Your support team never responds to my emails.\",\n",
    "]\n",
    "\n",
    "email_labels = [0]*5 + [1]*5 + [2]*5 + [3]*5\n",
    "intent_names = ['Billing', 'Technical Support', 'Product Inquiry', 'Complaint']\n",
    "\n",
    "# Split and prepare\n",
    "X_train_em, X_test_em, y_train_em, y_test_em = train_test_split(\n",
    "    customer_emails, email_labels, test_size=0.25, random_state=42, stratify=email_labels\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "tokenizer_em = Tokenizer(num_words=300, oov_token='<OOV>')\n",
    "tokenizer_em.fit_on_texts(X_train_em)\n",
    "\n",
    "X_train_em_seq = tokenizer_em.texts_to_sequences(X_train_em)\n",
    "X_test_em_seq = tokenizer_em.texts_to_sequences(X_test_em)\n",
    "\n",
    "X_train_em_pad = pad_sequences(X_train_em_seq, maxlen=15, padding='post')\n",
    "X_test_em_pad = pad_sequences(X_test_em_seq, maxlen=15, padding='post')\n",
    "\n",
    "# Build model\n",
    "model_email = Sequential([\n",
    "    Embedding(300, 32, input_length=15),\n",
    "    LSTM(32),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model_email.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "print(\"üöÄ Training Email Intent Classifier...\\n\")\n",
    "model_email.fit(X_train_em_pad, y_train_em, epochs=30, batch_size=2, verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "_, acc_em = model_email.evaluate(X_test_em_pad, y_test_em, verbose=0)\n",
    "print(f\"‚úÖ Test Accuracy: {acc_em:.2%}\\n\")\n",
    "\n",
    "# Test on new emails\n",
    "new_emails = [\n",
    "    \"My credit card was charged but I didn't receive the invoice.\",\n",
    "    \"The app crashes every time I try to open it.\",\n",
    "    \"Do you have a student discount available?\",\n",
    "    \"Very unhappy with the service, considering switching to competitor.\",\n",
    "]\n",
    "\n",
    "print(\"üîÆ Testing Email Intent Classifier:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "new_em_seq = tokenizer_em.texts_to_sequences(new_emails)\n",
    "new_em_pad = pad_sequences(new_em_seq, maxlen=15, padding='post')\n",
    "predictions_em = model_email.predict(new_em_pad, verbose=0)\n",
    "\n",
    "for i, email in enumerate(new_emails):\n",
    "    pred_class = np.argmax(predictions_em[i])\n",
    "    intent = intent_names[pred_class]\n",
    "    confidence = predictions_em[i][pred_class]\n",
    "    \n",
    "    print(f\"\\nüìß Email: \\\"{email}\\\"\")\n",
    "    print(f\"üéØ Intent: {intent}\")\n",
    "    print(f\"üìä Confidence: {confidence:.2%}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ Email intent classifier built!\")\n",
    "print(\"üí° Used for automatic ticket routing in customer support systems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- ‚úÖ Named Entity Recognition (NER) for extracting entities from text\n",
    "- ‚úÖ Part-of-Speech (POS) tagging for understanding grammar\n",
    "- ‚úÖ Deep learning text classification with LSTMs\n",
    "- ‚úÖ Built a news article categorization system\n",
    "- ‚úÖ Built a customer review sentiment analyzer\n",
    "- ‚úÖ Understanding of how these power modern AI systems\n",
    "\n",
    "### üéØ Key Takeaways:\n",
    "\n",
    "1. **NER extracts structured information**\n",
    "   - Identifies entities (people, places, organizations)\n",
    "   - Powers chatbots, search engines, knowledge graphs\n",
    "   - Essential for information extraction\n",
    "\n",
    "2. **POS tagging understands grammar**\n",
    "   - Labels words by grammatical role\n",
    "   - Helps LLMs generate correct text\n",
    "   - Enables better text analysis\n",
    "\n",
    "3. **Deep learning improves text classification**\n",
    "   - LSTMs capture context and sequences\n",
    "   - Embedding layers learn semantic representations\n",
    "   - Higher accuracy than traditional methods\n",
    "\n",
    "4. **Real-world applications are everywhere**\n",
    "   - News categorization, sentiment analysis\n",
    "   - Content moderation, spam detection\n",
    "   - Customer support automation\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Practice Exercise (Before Day 3):**\n",
    "\n",
    "Build a multi-label text classifier:\n",
    "1. Create documents that can belong to multiple categories\n",
    "2. Use sigmoid activation instead of softmax\n",
    "3. Train LSTM model with binary cross-entropy loss\n",
    "4. Evaluate with multi-label metrics\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Next Lesson:** Day 3 - Modern NLP with Transformers\n",
    "- Introduction to Transformer architecture\n",
    "- BERT and GPT foundations\n",
    "- Using HuggingFace transformers library\n",
    "- Sentiment analysis with pre-trained BERT\n",
    "- Text summarization with T5\n",
    "- Fine-tuning for custom tasks\n",
    "\n",
    "---\n",
    "\n",
    "**üí¨ Remember:**\n",
    "\n",
    "*\"The techniques you learned today - NER, POS tagging, and LSTM classification - are the building blocks that led to modern Transformers. Understanding these fundamentals helps you grasp how ChatGPT, Claude, and BERT work under the hood. Every time you interact with an AI assistant, these concepts are at play!\"* üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**üîó Connections to Modern AI:**\n",
    "- **ChatGPT/Claude**: Use advanced NER and classification\n",
    "- **RAG Systems**: Leverage NER for entity-based retrieval\n",
    "- **Content Moderation**: Classification at massive scale\n",
    "- **Search Engines**: Entity extraction for knowledge graphs\n",
    "- **Transformers**: Evolution from LSTM-based models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
