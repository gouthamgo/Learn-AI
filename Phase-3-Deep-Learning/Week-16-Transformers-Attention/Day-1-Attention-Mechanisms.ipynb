{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 1: Attention Mechanisms\n",
    "\n",
    "**üéØ Goal:** Master attention mechanisms - the foundation of modern AI (ChatGPT, Claude, GPT-4)\n",
    "\n",
    "**‚è±Ô∏è Time:** 90-120 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Attention powers ALL modern language models (GPT-4, Claude, Gemini, ChatGPT)\n",
    "- Foundation of Transformers - the architecture behind LLMs\n",
    "- Enables models to \"focus\" on relevant information (like humans do)\n",
    "- Critical for RAG systems, multi-modal AI, and Agentic AI\n",
    "- Powers Google Translate, Copilot, Midjourney, and every AI you use daily\n",
    "- Understanding attention = Understanding how ChatGPT \"thinks\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention-intro",
   "metadata": {},
   "source": [
    "## ü§î What is Attention?\n",
    "\n",
    "**Attention = Focusing on what's important**\n",
    "\n",
    "**Human Analogy:**\n",
    "When you read this sentence, you don't give equal attention to every word. You focus on KEY words that carry meaning.\n",
    "\n",
    "**Example:**\n",
    "- Sentence: \"The **cat** sat on the **mat**\"\n",
    "- Your brain focuses on: \"cat\" and \"mat\" (nouns carrying the main meaning)\n",
    "- Less attention to: \"the\", \"sat\", \"on\" (structural words)\n",
    "\n",
    "**In AI:**\n",
    "- **Problem:** RNNs treat all words equally ‚Üí loses important context\n",
    "- **Solution:** Attention learns to focus on relevant words\n",
    "- **Result:** Better understanding, better predictions!\n",
    "\n",
    "### üéØ Real-World Applications (2024-2025)\n",
    "\n",
    "**Where attention is used:**\n",
    "1. **ChatGPT/Claude:** Attention determines which previous words matter for next prediction\n",
    "2. **Machine Translation:** Aligns source and target words (\"cat\" ‚Üí \"gato\")\n",
    "3. **RAG Systems:** Attention helps retrieve and focus on relevant document chunks\n",
    "4. **Question Answering:** Focuses on the part of context that answers the question\n",
    "5. **Multimodal AI:** Attention between image regions and text descriptions\n",
    "\n",
    "**The Revolution:**\n",
    "- **2017:** \"Attention Is All You Need\" paper introduced Transformers\n",
    "- **2018:** BERT revolutionized NLP\n",
    "- **2020:** GPT-3 showed massive scaling power\n",
    "- **2022:** ChatGPT launched (175B parameters, all using attention!)\n",
    "- **2024-2025:** GPT-4, Claude, Gemini - all built on attention mechanisms\n",
    "\n",
    "Let's build attention from scratch! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Make plots beautiful\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"Let's build attention mechanisms from scratch! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem-without-attention",
   "metadata": {},
   "source": [
    "## ‚ùå The Problem Without Attention\n",
    "\n",
    "**Scenario:** Machine Translation\n",
    "\n",
    "**Old Approach (RNN/LSTM without attention):**\n",
    "```\n",
    "English: \"The cat sat on the mat\"\n",
    "         ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì (RNN processes sequentially)\n",
    "         üß† (Single fixed-size vector - bottleneck!)\n",
    "         ‚Üì\n",
    "Spanish: \"El gato se sent√≥ en la alfombra\"\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "1. **Information Bottleneck:** Entire sentence compressed into one vector\n",
    "2. **Long-Range Dependencies:** Forgets early words in long sentences\n",
    "3. **Equal Weight:** All words treated equally (even \"the\", \"on\")\n",
    "\n",
    "**With Attention:**\n",
    "```\n",
    "English: \"The cat sat on the mat\"\n",
    "         ‚Üì   ‚Üì   ‚Üì   ‚Üì   ‚Üì   ‚Üì\n",
    "         [All word representations preserved]\n",
    "         ‚Üì\n",
    "When translating \"gato\": Focus 90% attention on \"cat\", 10% on others\n",
    "When translating \"alfombra\": Focus 90% attention on \"mat\", 10% on others\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ No information bottleneck\n",
    "- ‚úÖ Can attend to any word (no distance limit)\n",
    "- ‚úÖ Learns what to focus on automatically\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention-math",
   "metadata": {},
   "source": [
    "## üßÆ Attention Mathematics (Simplified)\n",
    "\n",
    "**Core Idea:** Calculate \"similarity\" between words, then focus on similar ones\n",
    "\n",
    "### Step-by-Step Process:\n",
    "\n",
    "**1. Query, Key, Value (QKV)**\n",
    "- **Query (Q):** \"What am I looking for?\"\n",
    "- **Key (K):** \"What do I have to offer?\"\n",
    "- **Value (V):** \"What information do I carry?\"\n",
    "\n",
    "**Analogy:** YouTube Search\n",
    "- **Query:** Your search term (\"how to cook pasta\")\n",
    "- **Keys:** Video titles/tags\n",
    "- **Values:** Actual video content\n",
    "- **Attention:** Match query to keys ‚Üí retrieve values\n",
    "\n",
    "**2. Attention Formula:**\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q ¬∑ K^T / ‚àöd_k) ¬∑ V\n",
    "```\n",
    "\n",
    "**Breaking it down:**\n",
    "1. **Q ¬∑ K^T:** Calculate similarity between query and all keys (dot product)\n",
    "2. **/ ‚àöd_k:** Scale down (prevents large values that make softmax too sharp)\n",
    "3. **softmax(...):** Convert to probabilities (attention weights sum to 1)\n",
    "4. **¬∑ V:** Weighted sum of values (focus on relevant info)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Sentence: \"The cat sat\"\n",
    "Question: What did the cat do?\n",
    "\n",
    "Query = \"cat\"\n",
    "Keys = [\"The\", \"cat\", \"sat\"]\n",
    "Similarities = [0.1, 0.8, 0.7]  (cat is similar to itself and \"sat\")\n",
    "Attention weights after softmax = [0.05, 0.45, 0.50]\n",
    "Output = 0.05*V[The] + 0.45*V[cat] + 0.50*V[sat]\n",
    "       = Focus mostly on \"cat\" and \"sat\"\n",
    "```\n",
    "\n",
    "Let's implement this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Attention Mechanism from Scratch\n",
    "\n",
    "def simple_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute attention scores and output\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (1, d_k)\n",
    "        K: Key matrix (n, d_k) where n = number of words\n",
    "        V: Value matrix (n, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention-weighted values (1, d_v)\n",
    "        attention_weights: Attention scores (1, n)\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate similarity scores (Q ¬∑ K^T)\n",
    "    d_k = K.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Scaled dot product\n",
    "    \n",
    "    # Step 2: Apply softmax to get attention weights\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Step 3: Weighted sum of values\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Example: Simple sentence\n",
    "# Sentence: \"The cat sat on the mat\"\n",
    "# Let's use 4-dimensional embeddings for simplicity\n",
    "\n",
    "# Word embeddings (simplified - in reality, these are learned)\n",
    "words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "embeddings = np.array([\n",
    "    [0.1, 0.2, 0.1, 0.0],  # The\n",
    "    [0.8, 0.3, 0.9, 0.7],  # cat (noun - rich features)\n",
    "    [0.3, 0.9, 0.4, 0.6],  # sat (verb - different features)\n",
    "    [0.1, 0.1, 0.2, 0.1],  # on (preposition)\n",
    "    [0.1, 0.2, 0.1, 0.0],  # the\n",
    "    [0.7, 0.4, 0.8, 0.6],  # mat (noun - similar to cat)\n",
    "])\n",
    "\n",
    "# For simplicity: Q = K = V = embeddings\n",
    "# In practice, these are learned linear transformations\n",
    "\n",
    "# Question: What should we focus on when understanding \"cat\"?\n",
    "query = embeddings[1:2]  # \"cat\" as query (shape: 1, 4)\n",
    "keys = embeddings        # All words as keys (shape: 6, 4)\n",
    "values = embeddings      # All words as values (shape: 6, 4)\n",
    "\n",
    "# Compute attention\n",
    "output, attention_weights = simple_attention(query, keys, values)\n",
    "\n",
    "print(\"üéØ Attention Analysis: Focus on 'cat'\\n\" + \"=\"*50)\n",
    "print(\"\\nAttention Weights (where does 'cat' look?):\")\n",
    "for word, weight in zip(words, attention_weights[0]):\n",
    "    bar = \"‚ñà\" * int(weight * 50)\n",
    "    print(f\"  {word:6s}: {weight:.3f} {bar}\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "max_attention_idx = np.argmax(attention_weights[0])\n",
    "print(f\"  'cat' pays most attention to: '{words[max_attention_idx]}'\")\n",
    "print(f\"  This makes sense - a word attends strongly to itself!\")\n",
    "print(f\"  Secondary attention to 'sat' (verb describing cat) and 'mat' (similar noun)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Attention Heatmap\n",
    "\n",
    "def visualize_attention(words, attention_matrix, title=\"Attention Heatmap\"):\n",
    "    \"\"\"\n",
    "    Create attention heatmap visualization\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention_matrix, \n",
    "                xticklabels=words, \n",
    "                yticklabels=words,\n",
    "                annot=True, \n",
    "                fmt='.2f',\n",
    "                cmap='YlOrRd',\n",
    "                cbar_kws={'label': 'Attention Weight'},\n",
    "                linewidths=0.5)\n",
    "    plt.title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Keys (Attending TO)', fontsize=12)\n",
    "    plt.ylabel('Queries (Attending FROM)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute attention for all words\n",
    "attention_matrix = np.zeros((len(words), len(words)))\n",
    "\n",
    "for i in range(len(words)):\n",
    "    query = embeddings[i:i+1]\n",
    "    _, weights = simple_attention(query, keys, values)\n",
    "    attention_matrix[i] = weights[0]\n",
    "\n",
    "visualize_attention(words, attention_matrix, \n",
    "                   \"üéØ Self-Attention: 'The cat sat on the mat'\")\n",
    "\n",
    "print(\"\\nüìä How to Read This Heatmap:\")\n",
    "print(\"  - Each row = one word's query\")\n",
    "print(\"  - Each column = what that query attends to\")\n",
    "print(\"  - Diagonal = words attend to themselves (usually high)\")\n",
    "print(\"  - Bright cells = strong attention (important relationships)\")\n",
    "print(\"  - Dark cells = weak attention (less relevant)\")\n",
    "print(\"\\nüí° Notice: 'cat' and 'mat' attend to each other (similar nouns!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "self-attention",
   "metadata": {},
   "source": [
    "## üîÑ Self-Attention Explained\n",
    "\n",
    "**What is Self-Attention?**\n",
    "- Attention mechanism where queries, keys, and values all come from the SAME sequence\n",
    "- Each word attends to all other words (including itself)\n",
    "- Learns relationships between words in a sentence\n",
    "\n",
    "**Why \"Self\"?**\n",
    "- Regular attention: Source ‚Üí Target (e.g., English ‚Üí Spanish)\n",
    "- Self-attention: Sequence ‚Üí Itself (e.g., sentence attends to sentence)\n",
    "\n",
    "### üéØ Real-World Example: Understanding Context\n",
    "\n",
    "**Sentence:** \"The animal didn't cross the street because it was too tired.\"\n",
    "\n",
    "**Question:** What does \"it\" refer to?\n",
    "\n",
    "**Without attention:** \n",
    "- RNN might guess \"street\" (closer to \"it\")\n",
    "\n",
    "**With self-attention:**\n",
    "- \"it\" attends strongly to \"animal\" (semantically correct!)\n",
    "- Why? Because \"tired\" is an animal property\n",
    "- Self-attention learns these relationships!\n",
    "\n",
    "### The Process:\n",
    "\n",
    "1. **Input:** Word embeddings for entire sentence\n",
    "2. **Transform:** Create Q, K, V using learned weight matrices\n",
    "   - Q = X ¬∑ W_Q\n",
    "   - K = X ¬∑ W_K\n",
    "   - V = X ¬∑ W_V\n",
    "3. **Compute Attention:** Each word attends to all words\n",
    "4. **Output:** Context-aware representations\n",
    "\n",
    "**Key Insight:**\n",
    "- Before attention: \"bank\" has same embedding in \"river bank\" and \"money bank\"\n",
    "- After attention: \"bank\" has different representations based on context!\n",
    "- Self-attention creates **contextualized embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self-attention-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention Layer Implementation (PyTorch)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Self-Attention Layer\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Embedding dimension (e.g., 512)\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Learned transformation matrices\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)  # Query\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)  # Key\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)  # Value\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input embeddings (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output (batch_size, seq_len, embed_dim)\n",
    "            attention_weights: (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # Step 1: Create Q, K, V\n",
    "        Q = self.W_q(x)  # (batch, seq_len, embed_dim)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Step 2: Calculate attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq_len, seq_len)\n",
    "        scores = scores / np.sqrt(self.embed_dim)  # Scale\n",
    "        \n",
    "        # Step 3: Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 4: Weighted sum of values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test the self-attention layer\n",
    "embed_dim = 64\n",
    "seq_len = 6  # \"The cat sat on the mat\"\n",
    "batch_size = 1\n",
    "\n",
    "# Create self-attention layer\n",
    "self_attn = SelfAttention(embed_dim)\n",
    "\n",
    "# Random input embeddings (in practice, these come from embedding layer)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = self_attn(x)\n",
    "\n",
    "print(\"‚úÖ Self-Attention Layer Test\")\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nüí° Notice: Output has same shape as input!\")\n",
    "print(f\"   But now each word embedding is context-aware!\")\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attn_weights[0].detach().numpy(), \n",
    "            annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=words,\n",
    "            yticklabels=words)\n",
    "plt.title('üéØ Self-Attention Weights (Learned)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Key (Attending TO)')\n",
    "plt.ylabel('Query (Attending FROM)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-head-attention",
   "metadata": {},
   "source": [
    "## üé≠ Multi-Head Attention\n",
    "\n",
    "**Problem with Single Attention:**\n",
    "- One attention mechanism can only learn ONE type of relationship\n",
    "- Example: Subject-verb OR adjective-noun, but not both simultaneously\n",
    "\n",
    "**Solution: Multi-Head Attention**\n",
    "- Run multiple attention mechanisms in parallel (\"heads\")\n",
    "- Each head learns DIFFERENT relationships!\n",
    "- Combine outputs for richer representation\n",
    "\n",
    "### üéØ Analogy: Multiple Perspectives\n",
    "\n",
    "**Sentence:** \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "**Different attention heads might learn:**\n",
    "- **Head 1:** Syntax - \"fox\" ‚Üí \"jumps\" (subject-verb)\n",
    "- **Head 2:** Attributes - \"fox\" ‚Üí \"quick, brown\" (adjective-noun)\n",
    "- **Head 3:** Objects - \"jumps\" ‚Üí \"dog\" (verb-object)\n",
    "- **Head 4:** Spatial - \"jumps\" ‚Üí \"over\" (action-preposition)\n",
    "\n",
    "**Each head provides different insight!**\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "```\n",
    "Input Embeddings\n",
    "    ‚Üì\n",
    "    ‚îú‚îÄ‚Üí Head 1 (Q‚ÇÅ, K‚ÇÅ, V‚ÇÅ) ‚Üí Attention Output 1\n",
    "    ‚îú‚îÄ‚Üí Head 2 (Q‚ÇÇ, K‚ÇÇ, V‚ÇÇ) ‚Üí Attention Output 2\n",
    "    ‚îú‚îÄ‚Üí Head 3 (Q‚ÇÉ, K‚ÇÉ, V‚ÇÉ) ‚Üí Attention Output 3\n",
    "    ‚îî‚îÄ‚Üí Head 4 (Q‚ÇÑ, K‚ÇÑ, V‚ÇÑ) ‚Üí Attention Output 4\n",
    "    ‚Üì\n",
    "Concatenate all heads\n",
    "    ‚Üì\n",
    "Linear transformation\n",
    "    ‚Üì\n",
    "Final Output\n",
    "```\n",
    "\n",
    "### üåü In Modern LLMs:\n",
    "\n",
    "**GPT-3:**\n",
    "- 96 attention heads per layer!\n",
    "- 96 layers total\n",
    "- = 9,216 total attention heads\n",
    "\n",
    "**GPT-4 (estimated):**\n",
    "- 128+ heads per layer\n",
    "- 120+ layers\n",
    "- = 15,000+ total attention heads\n",
    "\n",
    "**Why so many?**\n",
    "- Each head specializes in different linguistic patterns\n",
    "- More heads = richer understanding\n",
    "- Enables complex reasoning and context understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-head-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention Implementation\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention Layer\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Total embedding dimension (must be divisible by num_heads)\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads  # Dimension per head\n",
    "        \n",
    "        # Q, K, V projections for all heads (combined)\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split embedding into multiple heads\"\"\"\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        # Reshape: (batch, seq_len, num_heads, head_dim)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        # Transpose: (batch, num_heads, seq_len, head_dim)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine multiple heads back\"\"\"\n",
    "        batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "        # Transpose: (batch, seq_len, num_heads, head_dim)\n",
    "        x = x.transpose(1, 2)\n",
    "        # Reshape: (batch, seq_len, embed_dim)\n",
    "        return x.contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, embed_dim)\n",
    "            attention_weights: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        # Step 1: Linear projections\n",
    "        Q = self.W_q(x)  # (batch, seq_len, embed_dim)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Step 2: Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch, num_heads, seq_len, head_dim)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Step 3: Scaled dot-product attention (for each head)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Step 4: Combine heads\n",
    "        attention_output = self.combine_heads(attention_output)\n",
    "        \n",
    "        # Step 5: Final linear projection\n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "embed_dim = 64\n",
    "num_heads = 8  # Like GPT-2 small\n",
    "seq_len = 6\n",
    "batch_size = 1\n",
    "\n",
    "# Create multi-head attention\n",
    "mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "\n",
    "# Random input\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = mha(x)\n",
    "\n",
    "print(\"‚úÖ Multi-Head Attention Test\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Embedding dimension: {embed_dim}\")\n",
    "print(f\"  Number of heads: {num_heads}\")\n",
    "print(f\"  Dimension per head: {embed_dim // num_heads}\")\n",
    "print(f\"\\nShapes:\")\n",
    "print(f\"  Input: {x.shape}\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "print(f\"  Attention weights: {attn_weights.shape}\")\n",
    "print(f\"\\nüí° Each of the {num_heads} heads learns different patterns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-multi-head",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Different Attention Heads\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('üé≠ Multi-Head Attention: Different Heads Learn Different Patterns', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx // 4, head_idx % 4]\n",
    "    \n",
    "    # Get attention weights for this head\n",
    "    head_weights = attn_weights[0, head_idx].detach().numpy()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(head_weights, \n",
    "                annot=True, fmt='.2f', \n",
    "                cmap='viridis',\n",
    "                xticklabels=words,\n",
    "                yticklabels=words,\n",
    "                ax=ax,\n",
    "                cbar=False,\n",
    "                square=True)\n",
    "    \n",
    "    ax.set_title(f'Head {head_idx + 1}', fontweight='bold')\n",
    "    if head_idx // 4 == 1:\n",
    "        ax.set_xlabel('Key', fontsize=10)\n",
    "    if head_idx % 4 == 0:\n",
    "        ax.set_ylabel('Query', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  - Each head shows DIFFERENT attention patterns\")\n",
    "print(\"  - Some heads focus on nearby words (local patterns)\")\n",
    "print(\"  - Other heads attend to distant words (long-range dependencies)\")\n",
    "print(\"  - In trained models (like GPT-4), heads specialize in:\")\n",
    "print(\"    ‚Ä¢ Syntax (grammar rules)\")\n",
    "print(\"    ‚Ä¢ Semantics (meaning relationships)\")\n",
    "print(\"    ‚Ä¢ Positional patterns (word order)\")\n",
    "print(\"    ‚Ä¢ Coreference (pronoun resolution)\")\n",
    "print(\"\\nüåü This diversity is why multi-head attention is so powerful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positional-encoding",
   "metadata": {},
   "source": [
    "## üìç Positional Encoding\n",
    "\n",
    "**Problem: Attention Has No Sense of Order!**\n",
    "\n",
    "**These sentences have SAME attention scores:**\n",
    "- \"The cat chased the dog\"\n",
    "- \"The dog chased the cat\"\n",
    "\n",
    "**Why?** Attention only looks at word relationships, not positions!\n",
    "\n",
    "**But word order matters!**\n",
    "- \"Dog bites man\" ‚â† \"Man bites dog\"\n",
    "- \"I will not go\" ‚â† \"Will I not go?\"\n",
    "\n",
    "### Solution: Positional Encoding\n",
    "\n",
    "**Add position information to embeddings:**\n",
    "```\n",
    "Final_Embedding = Word_Embedding + Position_Encoding\n",
    "```\n",
    "\n",
    "**Two Approaches:**\n",
    "\n",
    "**1. Learned Positional Embeddings (GPT, BERT)**\n",
    "- Train position embeddings like word embeddings\n",
    "- Position 0, 1, 2, ... each has learnable vector\n",
    "- Pro: Flexible, learns optimal positions\n",
    "- Con: Fixed maximum length\n",
    "\n",
    "**2. Sinusoidal Positional Encoding (Original Transformer)**\n",
    "- Use sine/cosine functions of different frequencies\n",
    "- Mathematical formula (don't worry about details):\n",
    "  ```\n",
    "  PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "  PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "  ```\n",
    "- Pro: Works for any sequence length\n",
    "- Con: Not learned (fixed pattern)\n",
    "\n",
    "### üéØ Why It Works:\n",
    "\n",
    "**Positional encoding creates unique \"fingerprint\" for each position:**\n",
    "- Position 0: [0.00, 1.00, 0.00, 1.00, ...]\n",
    "- Position 1: [0.84, 0.54, 0.01, 1.00, ...]\n",
    "- Position 2: [0.91, -0.42, 0.02, 1.00, ...]\n",
    "\n",
    "**Now the model knows:**\n",
    "- Which words come first/last\n",
    "- Distance between words\n",
    "- Relative positions\n",
    "\n",
    "Let's implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positional-encoding-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding Implementation\n",
    "\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Generate sinusoidal positional encodings\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        d_model: Embedding dimension\n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding: (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    position = np.arange(seq_len)[:, np.newaxis]  # (seq_len, 1)\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    pos_encoding = np.zeros((seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = np.sin(position * div_term)  # Even indices\n",
    "    pos_encoding[:, 1::2] = np.cos(position * div_term)  # Odd indices\n",
    "    \n",
    "    return pos_encoding\n",
    "\n",
    "# Generate positional encodings\n",
    "seq_len = 50  # Up to 50 words\n",
    "d_model = 128\n",
    "\n",
    "pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "\n",
    "print(\"‚úÖ Positional Encoding Generated\")\n",
    "print(f\"\\nShape: {pos_encoding.shape}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Embedding dimension: {d_model}\")\n",
    "\n",
    "# Visualize positional encodings\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Heatmap of positional encodings\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(pos_encoding, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Embedding Dimension', fontsize=12)\n",
    "plt.ylabel('Position in Sequence', fontsize=12)\n",
    "plt.title('üìç Sinusoidal Positional Encoding Heatmap', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Plot 2: Encoding values for specific positions\n",
    "plt.subplot(1, 2, 2)\n",
    "for pos in [0, 10, 20, 30, 40]:\n",
    "    plt.plot(pos_encoding[pos, :50], label=f'Position {pos}', alpha=0.7)\n",
    "plt.xlabel('Dimension Index', fontsize=12)\n",
    "plt.ylabel('Encoding Value', fontsize=12)\n",
    "plt.title('üìä Positional Encoding Patterns', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"  - Each position has a UNIQUE pattern (fingerprint)\")\n",
    "print(\"  - Different frequencies capture both local and global positions\")\n",
    "print(\"  - Low-frequency components: track position in long sequences\")\n",
    "print(\"  - High-frequency components: distinguish nearby positions\")\n",
    "print(\"\\nüåü This allows transformers to understand word order!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positional-encoding-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Position Similarity\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compute similarity between all position pairs\n",
    "similarity_matrix = np.zeros((seq_len, seq_len))\n",
    "\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        similarity_matrix[i, j] = cosine_similarity(pos_encoding[i], pos_encoding[j])\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix, cmap='coolwarm', center=0,\n",
    "            xticklabels=10, yticklabels=10)\n",
    "plt.xlabel('Position', fontsize=12)\n",
    "plt.ylabel('Position', fontsize=12)\n",
    "plt.title('üìç Positional Encoding Similarity Matrix\\n(How similar are different positions?)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Analysis:\")\n",
    "print(\"  - Diagonal (same position) = 1.0 (perfect similarity)\")\n",
    "print(\"  - Nearby positions = high similarity (but not identical!)\")\n",
    "print(\"  - Distant positions = lower similarity\")\n",
    "print(\"  - Pattern helps model learn: 'Position 5 is close to 6, far from 40'\")\n",
    "print(\"\\nüéØ This relative position information is crucial for understanding!\")\n",
    "\n",
    "# Example: How different are position 5 and position 6?\n",
    "pos_5 = pos_encoding[5]\n",
    "pos_6 = pos_encoding[6]\n",
    "pos_30 = pos_encoding[30]\n",
    "\n",
    "sim_5_6 = cosine_similarity(pos_5, pos_6)\n",
    "sim_5_30 = cosine_similarity(pos_5, pos_30)\n",
    "\n",
    "print(f\"\\nConcrete Example:\")\n",
    "print(f\"  Similarity(pos 5, pos 6): {sim_5_6:.4f} (neighbors ‚Üí high)\")\n",
    "print(f\"  Similarity(pos 5, pos 30): {sim_5_30:.4f} (distant ‚Üí lower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-ai-example",
   "metadata": {},
   "source": [
    "## üåü Real AI Example: Building Attention from Scratch for Sentiment Analysis\n",
    "\n",
    "**Task:** Classify movie reviews as positive or negative\n",
    "\n",
    "**Why Attention Helps:**\n",
    "- Some words are MORE important: \"amazing\", \"terrible\", \"love\", \"hate\"\n",
    "- Attention learns to focus on sentiment-bearing words\n",
    "- Ignores filler words: \"the\", \"a\", \"is\"\n",
    "\n",
    "**Pipeline:**\n",
    "1. Convert words to embeddings\n",
    "2. Add positional encoding\n",
    "3. Apply self-attention (focus on important words)\n",
    "4. Aggregate with attention weights\n",
    "5. Classify sentiment\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment-attention-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Attention-Based Sentiment Classifier\n",
    "\n",
    "class AttentionSentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_classes=2):\n",
    "        \"\"\"\n",
    "        Sentiment classifier using attention\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            embed_dim: Embedding dimension\n",
    "            num_heads: Number of attention heads\n",
    "            num_classes: Number of output classes (2 for binary)\n",
    "        \"\"\"\n",
    "        super(AttentionSentimentClassifier, self).__init__()\n",
    "        \n",
    "        # Word embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def add_positional_encoding(self, x):\n",
    "        \"\"\"Add positional encoding to embeddings\"\"\"\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        # Generate positional encoding\n",
    "        pos_enc = torch.FloatTensor(\n",
    "            get_positional_encoding(seq_len, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Add to embeddings\n",
    "        return x + pos_enc.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token IDs (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch_size, num_classes)\n",
    "            attention_weights: For visualization\n",
    "        \"\"\"\n",
    "        # 1. Word embeddings\n",
    "        embeddings = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # 2. Add positional encoding\n",
    "        embeddings = self.add_positional_encoding(embeddings)\n",
    "        \n",
    "        # 3. Self-attention\n",
    "        attn_output, attn_weights = self.attention(embeddings)\n",
    "        \n",
    "        # 4. Aggregate (mean pooling)\n",
    "        pooled = attn_output.mean(dim=1)  # (batch, embed_dim)\n",
    "        \n",
    "        # 5. Classification\n",
    "        logits = self.fc(pooled)  # (batch, num_classes)\n",
    "        \n",
    "        return logits, attn_weights\n",
    "\n",
    "print(\"‚úÖ Attention-based Sentiment Classifier built!\")\n",
    "print(\"\\nüéØ This is a simplified version of how BERT classifies text!\")\n",
    "print(\"\\nKey Components:\")\n",
    "print(\"  1. Word Embeddings (like Word2Vec/GloVe)\")\n",
    "print(\"  2. Positional Encoding (adds order information)\")\n",
    "print(\"  3. Multi-Head Self-Attention (focuses on important words)\")\n",
    "print(\"  4. Classification Head (final sentiment prediction)\")\n",
    "\n",
    "# Create model\n",
    "vocab_size = 1000  # Small vocabulary for demo\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "model = AttentionSentimentClassifier(vocab_size, embed_dim, num_heads)\n",
    "\n",
    "# Example input (batch of 2 sentences, each 10 tokens)\n",
    "example_input = torch.randint(0, vocab_size, (2, 10))\n",
    "\n",
    "# Forward pass\n",
    "logits, attn_weights = model(example_input)\n",
    "\n",
    "print(f\"\\nModel Test:\")\n",
    "print(f\"  Input shape: {example_input.shape}\")\n",
    "print(f\"  Output logits shape: {logits.shape}\")\n",
    "print(f\"  Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\n‚úÖ Model works! Ready to train on real data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercises\n",
    "\n",
    "Test your understanding of attention mechanisms!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement Scaled Dot-Product Attention\n",
    "\n",
    "**Task:** Complete the function to compute attention scores\n",
    "\n",
    "**Steps:**\n",
    "1. Compute Q ¬∑ K^T\n",
    "2. Scale by ‚àöd_k\n",
    "3. Apply softmax\n",
    "4. Multiply by V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Implement scaled dot-product attention\n",
    "    \n",
    "    Args:\n",
    "        Q: Query (batch, seq_len, d_k)\n",
    "        K: Key (batch, seq_len, d_k)\n",
    "        V: Value (batch, seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output\n",
    "        attention_weights: Attention scores\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Step 1: Compute scores = Q ¬∑ K^T\n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    # Step 3: Apply softmax\n",
    "    # Step 4: Multiply by V\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "Q = torch.randn(1, 5, 8)\n",
    "K = torch.randn(1, 5, 8)\n",
    "V = torch.randn(1, 5, 8)\n",
    "\n",
    "# output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "# print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # Step 2: Scale\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 4: Weighted sum\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### Exercise 2: Calculate Attention Weights Manually\n",
    "\n",
    "**Given:**\n",
    "- Query: \"cat\" = [0.5, 0.8]\n",
    "- Keys: {\"cat\": [0.5, 0.8], \"dog\": [0.4, 0.7], \"mat\": [0.3, 0.2]}\n",
    "\n",
    "**Task:** Calculate attention weights (by hand or code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "query = np.array([0.5, 0.8])\n",
    "keys = np.array([\n",
    "    [0.5, 0.8],  # cat\n",
    "    [0.4, 0.7],  # dog\n",
    "    [0.3, 0.2]   # mat\n",
    "])\n",
    "\n",
    "# Calculate attention weights\n",
    "# Step 1: Dot products\n",
    "# Step 2: Softmax\n",
    "# Which word gets highest attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Compute dot products (similarities)\n",
    "scores = np.dot(query, keys.T)\n",
    "print(f\"Scores: {scores}\")  # [0.89, 0.76, 0.31]\n",
    "\n",
    "# Step 2: Apply softmax\n",
    "attention_weights = np.exp(scores) / np.sum(np.exp(scores))\n",
    "print(f\"Attention weights: {attention_weights}\")\n",
    "\n",
    "# Result: [0.44, 0.38, 0.18]\n",
    "# Highest attention on \"cat\" itself (0.44)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3",
   "metadata": {},
   "source": [
    "### Exercise 3: Why Multi-Head Attention?\n",
    "\n",
    "**Question:** Explain in your own words why we use multiple attention heads instead of just one.\n",
    "\n",
    "**Think about:**\n",
    "- What different relationships exist in language?\n",
    "- Can one attention head capture all patterns?\n",
    "- How does this relate to ChatGPT/GPT-4?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for answer</summary>\n",
    "\n",
    "**Why Multi-Head Attention:**\n",
    "\n",
    "1. **Different Linguistic Patterns:**\n",
    "   - Head 1 might learn syntax (subject-verb agreement)\n",
    "   - Head 2 might learn semantics (word meanings)\n",
    "   - Head 3 might learn coreference (pronouns)\n",
    "   - Head 4 might learn long-range dependencies\n",
    "\n",
    "2. **Richer Representations:**\n",
    "   - Single head = one perspective\n",
    "   - Multiple heads = diverse perspectives ‚Üí better understanding\n",
    "\n",
    "3. **Real Examples:**\n",
    "   - Sentence: \"The cat, which was sleeping, woke up\"\n",
    "   - Head A: \"cat\" ‚Üí \"was sleeping\" (descriptive clause)\n",
    "   - Head B: \"cat\" ‚Üí \"woke up\" (main action)\n",
    "   - Head C: \"which\" ‚Üí \"cat\" (pronoun reference)\n",
    "\n",
    "4. **In GPT-4:**\n",
    "   - 128+ heads per layer √ó 120 layers = 15,000+ specialized attention heads!\n",
    "   - Each learns unique patterns\n",
    "   - Combined = deep understanding of language\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**You just learned:**\n",
    "\n",
    "### 1. **What is Attention?**\n",
    "   - ‚úÖ Mechanism for focusing on important information\n",
    "   - ‚úÖ Solves information bottleneck in RNNs\n",
    "   - ‚úÖ Enables long-range dependencies\n",
    "   - **Use when:** Processing sequences (text, time-series, DNA)\n",
    "\n",
    "### 2. **Self-Attention**\n",
    "   - ‚úÖ Each word attends to all words (including itself)\n",
    "   - ‚úÖ Creates context-aware embeddings\n",
    "   - ‚úÖ Formula: Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V\n",
    "   - **Powers:** All modern LLMs (GPT, BERT, Claude)\n",
    "\n",
    "### 3. **Multi-Head Attention**\n",
    "   - ‚úÖ Multiple attention mechanisms in parallel\n",
    "   - ‚úÖ Each head learns different patterns\n",
    "   - ‚úÖ Richer representations\n",
    "   - **Used in:** GPT-4 (128 heads), Claude, Gemini\n",
    "\n",
    "### 4. **Positional Encoding**\n",
    "   - ‚úÖ Adds word order information\n",
    "   - ‚úÖ Sinusoidal or learned embeddings\n",
    "   - ‚úÖ Critical for understanding sequences\n",
    "   - **Without it:** \"dog bites man\" = \"man bites dog\"\n",
    "\n",
    "### üåü Real-World Impact (2024-2025):\n",
    "\n",
    "**What You Can Build:**\n",
    "- ü§ñ **Chatbots** using attention-based models\n",
    "- üåê **Translation Systems** (Google Translate uses this!)\n",
    "- üìù **Text Summarization** for RAG systems\n",
    "- üéØ **Question Answering** (like ChatGPT)\n",
    "- üìä **Sentiment Analysis** with attention weights\n",
    "\n",
    "**Modern Applications:**\n",
    "- **ChatGPT/GPT-4:** 100% attention-based\n",
    "- **Claude:** Anthropic's AI using attention\n",
    "- **Gemini:** Google's multimodal AI\n",
    "- **GitHub Copilot:** Code generation with attention\n",
    "- **Midjourney/DALL-E:** Cross-attention between text and images\n",
    "\n",
    "### üìä Attention vs RNNs:\n",
    "\n",
    "| Feature | RNN/LSTM | Attention/Transformer |\n",
    "|---------|----------|----------------------|\n",
    "| Speed | ‚ùå Sequential (slow) | ‚úÖ Parallel (fast) |\n",
    "| Long sequences | ‚ùå Forgets early words | ‚úÖ Access all words |\n",
    "| Interpretability | ‚ùå Black box | ‚úÖ See attention weights |\n",
    "| Scalability | ‚ùå Limited | ‚úÖ Scales to billions of parameters |\n",
    "| Used in 2024 | ‚ùå Mostly replaced | ‚úÖ State-of-the-art |\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now understand the core mechanism behind:\n",
    "- ChatGPT\n",
    "- Claude\n",
    "- GPT-4\n",
    "- BERT\n",
    "- Every modern LLM!\n",
    "\n",
    "**Next:** We'll build the full Transformer architecture! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "**Practice Exercises:**\n",
    "1. Modify the number of attention heads (2, 4, 8, 16) - what changes?\n",
    "2. Experiment with different positional encoding strategies\n",
    "3. Visualize attention weights on real sentences\n",
    "4. Implement masking for padding tokens\n",
    "\n",
    "**Coming Next:**\n",
    "- **Day 2:** Complete Transformer Architecture (Encoder, Decoder, the full \"Attention Is All You Need\" paper)\n",
    "- **Day 3:** Modern LLMs (BERT, GPT, T5) and Fine-tuning with HuggingFace\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Deep Dive Resources:**\n",
    "- \"Attention Is All You Need\" paper (Vaswani et al., 2017)\n",
    "- The Illustrated Transformer (Jay Alammar)\n",
    "- HuggingFace Transformers course\n",
    "- Fast.ai course on NLP\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: Attention is all you need! This simple mechanism powers every AI breakthrough from 2017 to 2025.* üåü\n",
    "\n",
    "**üéØ You now understand how ChatGPT \"pays attention\" to your prompts!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
