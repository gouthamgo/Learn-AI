{
 "cells": [
  {
   "id": "intro",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Day 2: The Transformer Architecture\n",
    "\n",
    "**ğŸ¯ Goal:** Master the complete Transformer architecture - the foundation of ALL modern LLMs\n",
    "\n",
    "**â±ï¸ Time:** 120-150 minutes\n",
    "\n",
    "**ğŸŒŸ Why This Matters for AI:**\n",
    "- Transformers power EVERY modern AI: GPT-4, Claude, Gemini, ChatGPT, BERT, T5\n",
    "- The \"Attention Is All You Need\" (2017) paper revolutionized AI\n",
    "- Understanding Transformers = Understanding how ALL modern LLMs work\n",
    "- From language to images: ViT, DALL-E, Stable Diffusion use transformers\n",
    "- From coding to science: GitHub Copilot, AlphaFold use transformers\n",
    "- The architecture that started the AI revolution (2017-2025)\n",
    "\n",
    "**What You'll Build Today:**\n",
    "- Complete Transformer encoder-decoder from scratch\n",
    "- Understand GPT (decoder-only) vs BERT (encoder-only) vs T5 (encoder-decoder)\n",
    "- Mini-transformer for sequence-to-sequence tasks\n",
    "- See how ChatGPT architecture differs from BERT\n",
    "\n",
    "---"
   ]
  },
  {
   "id": "transformer-revolution",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ The Transformer Revolution\n",
    "\n",
    "**Before Transformers (Pre-2017):**\n",
    "- **RNNs/LSTMs**: Slow, sequential, forget long contexts\n",
    "- **CNNs**: Only local patterns, limited for sequences\n",
    "- **Maximum performance**: Limited by architecture\n",
    "\n",
    "**After Transformers (2017-2025):**\n",
    "```\n",
    "2017: \"Attention Is All You Need\" paper published\n",
    "2018: BERT revolutionized NLP (bidirectional understanding)\n",
    "2019: GPT-2 showed impressive text generation\n",
    "2020: GPT-3 (175B params) - massive scale breakthrough\n",
    "2022: ChatGPT launched - transformers meet RLHF\n",
    "2023: GPT-4, Claude, Gemini - multimodal transformers\n",
    "2024-2025: Every AI breakthrough uses transformers!\n",
    "```\n",
    "\n",
    "### ğŸ¯ Why Transformers Won:\n",
    "\n",
    "**1. Parallelization**\n",
    "- RNN: Process words one-by-one â†’ SLOW âŒ\n",
    "- Transformer: Process ALL words simultaneously â†’ FAST âœ…\n",
    "- Result: Can train on billions of tokens efficiently\n",
    "\n",
    "**2. Long-Range Dependencies**\n",
    "- RNN: Forgets early context in long sequences âŒ\n",
    "- Transformer: Self-attention connects ALL positions âœ…\n",
    "- Result: Understands context from thousands of tokens away\n",
    "\n",
    "**3. Scalability**\n",
    "- RNN: Doesn't improve much with more parameters âŒ\n",
    "- Transformer: Scales beautifully to billions of parameters âœ…\n",
    "- Result: GPT-3 (175B), GPT-4 (1.7T+ estimated), Claude, Gemini\n",
    "\n",
    "**4. Transfer Learning**\n",
    "- Pre-train once on massive data\n",
    "- Fine-tune for ANY task\n",
    "- Result: BERT, GPT revolutionized NLP transfer learning\n",
    "\n",
    "### ğŸ“Š Transformer Timeline:\n",
    "\n",
    "| Year | Model | Type | Parameters | Impact |\n",
    "|------|-------|------|------------|--------|\n",
    "| 2017 | Transformer | Encoder-Decoder | 65M | Started revolution |\n",
    "| 2018 | BERT | Encoder-only | 340M | NLP understanding |\n",
    "| 2019 | GPT-2 | Decoder-only | 1.5B | Text generation |\n",
    "| 2020 | GPT-3 | Decoder-only | 175B | Few-shot learning |\n",
    "| 2020 | T5 | Encoder-Decoder | 11B | Text-to-text |\n",
    "| 2022 | ChatGPT | Decoder-only | 175B+ | Conversational AI |\n",
    "| 2023 | GPT-4 | Decoder-only | 1.7T+ | Multimodal AI |\n",
    "| 2023 | Claude | Decoder-only | Unknown | Constitutional AI |\n",
    "| 2024 | Gemini | Decoder-only | 1.5T+ | Native multimodal |\n",
    "\n",
    "Let's build this revolutionary architecture!"
   ]
  },
  {
   "id": "imports",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from IPython.display import Image, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Make plots beautiful\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"Let's build the Transformer architecture from scratch! ğŸš€\")"
   ]
  },
  {
   "id": "architecture-overview",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Transformer Architecture Overview\n",
    "\n",
    "**The Complete Picture:**\n",
    "\n",
    "```\n",
    "INPUT SEQUENCE (e.g., \"The cat sat\")\n",
    "        â†“\n",
    "    EMBEDDINGS + POSITIONAL ENCODING\n",
    "        â†“\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚         ENCODER STACK           â”‚\n",
    "  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "  â”‚  â”‚  Multi-Head Attention    â”‚  â”‚ â† Self-attention\n",
    "  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "  â”‚           â†“                     â”‚\n",
    "  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "  â”‚  â”‚  Feed Forward Network    â”‚  â”‚\n",
    "  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "  â”‚    (Repeat N=6 times)          â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "  ENCODER OUTPUT (contextual representations)\n",
    "        â†“\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚         DECODER STACK           â”‚\n",
    "  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "  â”‚  â”‚ Masked Multi-Head Attn   â”‚  â”‚ â† Looks only at previous words\n",
    "  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "  â”‚           â†“                     â”‚\n",
    "  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "  â”‚  â”‚ Cross-Attention          â”‚  â”‚ â† Attends to encoder output\n",
    "  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "  â”‚           â†“                     â”‚\n",
    "  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "  â”‚  â”‚ Feed Forward Network     â”‚  â”‚\n",
    "  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "  â”‚    (Repeat N=6 times)          â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "    LINEAR + SOFTMAX\n",
    "        â†“\n",
    "OUTPUT SEQUENCE (e.g., \"Le chat Ã©tait assis\")\n",
    "```\n",
    "\n",
    "### ğŸ¯ Key Components:\n",
    "\n",
    "**1. Embeddings + Positional Encoding**\n",
    "- Convert words to vectors\n",
    "- Add position information\n",
    "- *(Learned this in Day 1!)*\n",
    "\n",
    "**2. Encoder Block** (Repeated N times)\n",
    "- **Multi-Head Self-Attention**: Each word attends to all words\n",
    "- **Feed-Forward Network**: Process each position independently\n",
    "- **Layer Normalization**: Stabilize training\n",
    "- **Residual Connections**: Skip connections for gradient flow\n",
    "\n",
    "**3. Decoder Block** (Repeated N times)\n",
    "- **Masked Self-Attention**: Can't peek at future words\n",
    "- **Cross-Attention**: Attend to encoder outputs\n",
    "- **Feed-Forward Network**: Same as encoder\n",
    "- **Layer Norm + Residuals**: Same as encoder\n",
    "\n",
    "**4. Output Layer**\n",
    "- Linear projection to vocabulary size\n",
    "- Softmax for probability distribution\n",
    "- Predict next token\n",
    "\n",
    "### ğŸ”‘ Critical Design Choices:\n",
    "\n",
    "**Why Add & Norm?**\n",
    "```\n",
    "Output = LayerNorm(Input + Sublayer(Input))\n",
    "```\n",
    "- **Residual (+)**: Helps gradients flow through deep networks\n",
    "- **LayerNorm**: Stabilizes training, enables deeper networks\n",
    "\n",
    "**Why Feed-Forward Network?**\n",
    "- Attention mixes information\n",
    "- FFN processes each position independently\n",
    "- Adds non-linearity and capacity\n",
    "\n",
    "**Why Stack Multiple Layers?**\n",
    "- Layer 1: Learn basic patterns (\"cat\" relates to \"sat\")\n",
    "- Layer 6: Learn complex reasoning (coreference, inference)\n",
    "- Deeper = More sophisticated understanding\n",
    "\n",
    "Let's implement each component!"
   ]
  },
  {
   "id": "positional-encoding",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding (from Day 1, but let's implement it again)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Positional Encoding Layer\n",
    "        \n",
    "        Args:\n",
    "            d_model: Embedding dimension\n",
    "            max_len: Maximum sequence length\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but should be saved/loaded)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input embeddings (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            x with positional encoding added\n",
    "        \"\"\"\n",
    "        # Add positional encoding\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Test positional encoding\n",
    "d_model = 512\n",
    "seq_len = 20\n",
    "batch_size = 2\n",
    "\n",
    "pos_enc = PositionalEncoding(d_model)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = pos_enc(x)\n",
    "\n",
    "print(\"âœ… Positional Encoding Test\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nğŸ’¡ Positional encoding adds position information to embeddings!\")"
   ]
  },
  {
   "id": "multi-head-attention-review",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention (improved version from Day 1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention Layer\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model dimension (must be divisible by num_heads)\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k)\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Inverse of split_heads\"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.shape\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch_size, seq_len_q, d_model)\n",
    "            key: (batch_size, seq_len_k, d_model)\n",
    "            value: (batch_size, seq_len_v, d_model)\n",
    "            mask: Optional mask (batch_size, 1, seq_len_q, seq_len_k)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch_size, seq_len_q, d_model)\n",
    "            attention_weights: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)  # (batch, seq_len_q, d_model)\n",
    "        K = self.W_k(key)    # (batch, seq_len_k, d_model)\n",
    "        V = self.W_v(value)  # (batch, seq_len_v, d_model)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch, num_heads, seq_len_q, d_k)\n",
    "        K = self.split_heads(K)  # (batch, num_heads, seq_len_k, d_k)\n",
    "        V = self.split_heads(V)  # (batch, num_heads, seq_len_v, d_k)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Combine heads\n",
    "        attention_output = self.combine_heads(attention_output)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(\"âœ… Multi-Head Attention Test\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nğŸ’¡ {num_heads} heads learn different attention patterns!\")"
   ]
  },
  {
   "id": "feed-forward",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Feed-Forward Network\n",
    "\n",
    "**Purpose:** Process each position independently with non-linear transformations\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (d_model=512)\n",
    "    â†“\n",
    "Linear (512 â†’ 2048)\n",
    "    â†“\n",
    "ReLU Activation\n",
    "    â†“\n",
    "Linear (2048 â†’ 512)\n",
    "    â†“\n",
    "Dropout\n",
    "    â†“\n",
    "Output (d_model=512)\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Expands dimension by 4x (512 â†’ 2048)\n",
    "- Applies non-linearity (ReLU)\n",
    "- Projects back to original dimension\n",
    "- Applied to EACH position independently\n",
    "\n",
    "**Why It Matters:**\n",
    "- **Attention**: Mixes information across positions\n",
    "- **FFN**: Adds depth and capacity to each position\n",
    "- Together: Rich, deep representations!"
   ]
  },
  {
   "id": "feed-forward-impl",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed-Forward Network\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Position-wise Feed-Forward Network\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            d_ff: Hidden dimension (typically 4 * d_model)\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Output (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Expand: d_model â†’ d_ff\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Project back: d_ff â†’ d_model\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test feed-forward network\n",
    "d_model = 512\n",
    "d_ff = 2048  # 4x expansion\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "ffn = PositionWiseFeedForward(d_model, d_ff)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = ffn(x)\n",
    "\n",
    "print(\"âœ… Feed-Forward Network Test\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Hidden dimension: {d_ff}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nğŸ’¡ FFN processes each position independently!\")\n",
    "print(f\"   Expands to {d_ff} dims, then back to {d_model}\")"
   ]
  },
  {
   "id": "encoder-block",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Encoder Block\n",
    "\n",
    "**The Encoder Layer combines everything:**\n",
    "\n",
    "```\n",
    "Input\n",
    "  â†“\n",
    "  â”œâ”€â”€â†’ Multi-Head Self-Attention\n",
    "  â”‚           â†“\n",
    "  â””â”€â”€â”€â”€â”€â†’ Add & Norm\n",
    "  â†“\n",
    "  â”œâ”€â”€â†’ Feed-Forward Network\n",
    "  â”‚           â†“\n",
    "  â””â”€â”€â”€â”€â”€â†’ Add & Norm\n",
    "  â†“\n",
    "Output (to next encoder layer or decoder)\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "1. **Multi-Head Self-Attention**: Learn relationships between all words\n",
    "2. **Residual Connection**: `output = input + sublayer(input)`\n",
    "3. **Layer Normalization**: Stabilize training\n",
    "4. **Feed-Forward Network**: Add depth and capacity\n",
    "5. **Another Residual + LayerNorm**: More stable gradients\n",
    "\n",
    "**Why Residuals?**\n",
    "- Deep networks (6-96 layers) need gradient flow\n",
    "- Residuals create shortcut paths\n",
    "- Enables training very deep transformers (GPT-3: 96 layers!)\n",
    "\n",
    "**Why LayerNorm?**\n",
    "- Normalizes across feature dimension\n",
    "- Stabilizes training\n",
    "- Enables deeper, more stable networks"
   ]
  },
  {
   "id": "encoder-block-impl",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Block\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Transformer Encoder Block\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            Output (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # Add & Norm\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))  # Add & Norm\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test encoder block\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "encoder_block = EncoderBlock(d_model, num_heads, d_ff)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = encoder_block(x)\n",
    "\n",
    "print(\"âœ… Encoder Block Test\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nğŸ—ï¸ Complete encoder block working!\")\n",
    "print(f\"   - Multi-head self-attention âœ…\")\n",
    "print(f\"   - Residual connections âœ…\")\n",
    "print(f\"   - Layer normalization âœ…\")\n",
    "print(f\"   - Feed-forward network âœ…\")"
   ]
  },
  {
   "id": "decoder-block",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Decoder Block\n",
    "\n",
    "**The Decoder is more complex:**\n",
    "\n",
    "```\n",
    "Input (target sequence)\n",
    "  â†“\n",
    "  â”œâ”€â”€â†’ Masked Multi-Head Self-Attention (can't see future)\n",
    "  â”‚           â†“\n",
    "  â””â”€â”€â”€â”€â”€â†’ Add & Norm\n",
    "  â†“\n",
    "  â”œâ”€â”€â†’ Cross-Attention (attend to encoder output)\n",
    "  â”‚           â†“\n",
    "  â””â”€â”€â”€â”€â”€â†’ Add & Norm\n",
    "  â†“\n",
    "  â”œâ”€â”€â†’ Feed-Forward Network\n",
    "  â”‚           â†“\n",
    "  â””â”€â”€â”€â”€â”€â†’ Add & Norm\n",
    "  â†“\n",
    "Output (to next decoder layer or final prediction)\n",
    "```\n",
    "\n",
    "### ğŸ­ Key Differences from Encoder:\n",
    "\n",
    "**1. Masked Self-Attention**\n",
    "- **Why?** During training, we can't let decoder see future words!\n",
    "- **Example:** Translating \"The cat\" â†’ \"Le chat\"\n",
    "  - When predicting \"Le\", can't see \"chat\" (future)\n",
    "  - Mask prevents attention to future positions\n",
    "\n",
    "**Masking Example:**\n",
    "```\n",
    "Sentence: [Le, chat, dort]\n",
    "\n",
    "Attention matrix (before masking):\n",
    "       Le  chat  dort\n",
    "Le   [ 1    1     1  ]  â† Can see all (BAD!)\n",
    "chat [ 1    1     1  ]  â† Can see all (BAD!)\n",
    "dort [ 1    1     1  ]  â† Can see all (BAD!)\n",
    "\n",
    "Attention matrix (after masking):\n",
    "       Le  chat  dort\n",
    "Le   [ 1    0     0  ]  â† Only sees itself âœ…\n",
    "chat [ 1    1     0  ]  â† Sees Le, chat âœ…\n",
    "dort [ 1    1     1  ]  â† Sees all previous âœ…\n",
    "```\n",
    "\n",
    "**2. Cross-Attention**\n",
    "- **Query**: From decoder (what I'm generating)\n",
    "- **Key & Value**: From encoder (source sequence)\n",
    "- **Purpose**: Align target with source\n",
    "- **Example:** When generating \"chat\", attend strongly to \"cat\"\n",
    "\n",
    "**3. Same Feed-Forward as Encoder**\n",
    "- Identical architecture\n",
    "- Separate parameters\n",
    "\n",
    "### ğŸ¯ This is How Translation Works!\n",
    "\n",
    "**English â†’ French:**\n",
    "1. **Encoder**: Understands \"The cat sat on the mat\"\n",
    "2. **Decoder**: Generates \"Le chat Ã©tait assis sur le tapis\"\n",
    "3. **Cross-Attention**: Links \"cat\" â†” \"chat\", \"mat\" â†” \"tapis\""
   ]
  },
  {
   "id": "decoder-block-impl",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Decoder Block\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Transformer Decoder Block\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        # Masked multi-head self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Cross-attention (attend to encoder output)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Decoder input (batch_size, tgt_seq_len, d_model)\n",
    "            encoder_output: Encoder output (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Source mask for encoder-decoder attention\n",
    "            tgt_mask: Target mask for masked self-attention\n",
    "        \n",
    "        Returns:\n",
    "            Output (batch_size, tgt_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 1. Masked self-attention (can't see future)\n",
    "        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        \n",
    "        # 2. Cross-attention to encoder output\n",
    "        cross_attn_output, _ = self.cross_attn(\n",
    "            query=x,\n",
    "            key=encoder_output,\n",
    "            value=encoder_output,\n",
    "            mask=src_mask\n",
    "        )\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        \n",
    "        # 3. Feed-forward network\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test decoder block\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "src_seq_len = 12  # Encoder output length\n",
    "tgt_seq_len = 8   # Decoder input length\n",
    "batch_size = 2\n",
    "\n",
    "decoder_block = DecoderBlock(d_model, num_heads, d_ff)\n",
    "decoder_input = torch.randn(batch_size, tgt_seq_len, d_model)\n",
    "encoder_output = torch.randn(batch_size, src_seq_len, d_model)\n",
    "\n",
    "output = decoder_block(decoder_input, encoder_output)\n",
    "\n",
    "print(\"âœ… Decoder Block Test\")\n",
    "print(f\"Decoder input shape: {decoder_input.shape}\")\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "print(f\"Decoder output shape: {output.shape}\")\n",
    "print(f\"\\nğŸ¯ Complete decoder block working!\")\n",
    "print(f\"   - Masked self-attention âœ…\")\n",
    "print(f\"   - Cross-attention to encoder âœ…\")\n",
    "print(f\"   - Feed-forward network âœ…\")"
   ]
  },
  {
   "id": "complete-transformer",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Complete Transformer Architecture\n",
    "\n",
    "**Now let's put it all together!**\n",
    "\n",
    "### ğŸ“ Full Transformer:\n",
    "\n",
    "```\n",
    "SOURCE SEQUENCE          TARGET SEQUENCE\n",
    "     â†“                         â†“\n",
    " Embeddings              Embeddings\n",
    "     â†“                         â†“\n",
    " Pos Encoding            Pos Encoding\n",
    "     â†“                         â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Encoder â”‚             â”‚ Decoder â”‚\n",
    "â”‚  Block  â”‚             â”‚  Block  â”‚ â† Cross-attn to encoder\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â†“                         â†“\n",
    "     â‹®                         â‹®\n",
    "(Repeat N times)        (Repeat N times)\n",
    "     â†“                         â†“\n",
    "Encoder Output          Linear + Softmax\n",
    "                              â†“\n",
    "                        Output Probabilities\n",
    "```\n",
    "\n",
    "### ğŸ¯ Typical Hyperparameters:\n",
    "\n",
    "**Original Transformer (2017):**\n",
    "- `d_model = 512`\n",
    "- `num_heads = 8`\n",
    "- `d_ff = 2048`\n",
    "- `num_layers = 6`\n",
    "- `vocab_size = 37,000`\n",
    "\n",
    "**GPT-3 (2020):**\n",
    "- `d_model = 12,288`\n",
    "- `num_heads = 96`\n",
    "- `d_ff = 49,152`\n",
    "- `num_layers = 96`\n",
    "- `vocab_size = 50,257`\n",
    "- **Total: 175 billion parameters!**"
   ]
  },
  {
   "id": "complete-transformer-impl",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Transformer Model\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=6,\n",
    "        d_ff=2048,\n",
    "        max_seq_len=5000,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Complete Transformer Model\n",
    "        \n",
    "        Args:\n",
    "            src_vocab_size: Source vocabulary size\n",
    "            tgt_vocab_size: Target vocabulary size\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            num_encoder_layers: Number of encoder blocks\n",
    "            num_decoder_layers: Number of decoder blocks\n",
    "            d_ff: Feed-forward dimension\n",
    "            max_seq_len: Maximum sequence length\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Encoder stack\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder stack\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def encode(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        Encode source sequence\n",
    "        \n",
    "        Args:\n",
    "            src: Source token IDs (batch_size, src_seq_len)\n",
    "            src_mask: Source mask\n",
    "        \n",
    "        Returns:\n",
    "            Encoder output (batch_size, src_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Embed and scale\n",
    "        x = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Decode target sequence\n",
    "        \n",
    "        Args:\n",
    "            tgt: Target token IDs (batch_size, tgt_seq_len)\n",
    "            encoder_output: Encoder output (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Source mask\n",
    "            tgt_mask: Target mask (for masking future positions)\n",
    "        \n",
    "        Returns:\n",
    "            Decoder output (batch_size, tgt_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Embed and scale\n",
    "        x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            src: Source tokens (batch_size, src_seq_len)\n",
    "            tgt: Target tokens (batch_size, tgt_seq_len)\n",
    "            src_mask: Source mask\n",
    "            tgt_mask: Target mask\n",
    "        \n",
    "        Returns:\n",
    "            Output logits (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        # Encode source\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        \n",
    "        # Decode target\n",
    "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.fc_out(decoder_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test complete transformer\n",
    "src_vocab_size = 10000\n",
    "tgt_vocab_size = 10000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "\n",
    "transformer = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_encoder_layers=num_layers,\n",
    "    num_decoder_layers=num_layers,\n",
    "    d_ff=d_ff\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in transformer.parameters())\n",
    "\n",
    "# Test with random data\n",
    "batch_size = 2\n",
    "src_seq_len = 10\n",
    "tgt_seq_len = 8\n",
    "\n",
    "src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len))\n",
    "tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "output = transformer(src, tgt)\n",
    "\n",
    "print(\"ğŸ‰ COMPLETE TRANSFORMER BUILT!\\n\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Model Configuration:\")\n",
    "print(f\"   - Model dimension (d_model): {d_model}\")\n",
    "print(f\"   - Number of attention heads: {num_heads}\")\n",
    "print(f\"   - Number of encoder layers: {num_layers}\")\n",
    "print(f\"   - Number of decoder layers: {num_layers}\")\n",
    "print(f\"   - Feed-forward dimension: {d_ff}\")\n",
    "print(f\"   - Source vocabulary: {src_vocab_size:,}\")\n",
    "print(f\"   - Target vocabulary: {tgt_vocab_size:,}\")\n",
    "print(f\"\\nğŸ’ª Total Parameters: {num_params:,}\")\n",
    "print(f\"\\nâœ… Test Forward Pass:\")\n",
    "print(f\"   - Source shape: {src.shape}\")\n",
    "print(f\"   - Target shape: {tgt.shape}\")\n",
    "print(f\"   - Output shape: {output.shape}\")\n",
    "print(f\"\\nğŸš€ This is the SAME architecture as the original 'Attention Is All You Need' paper!\")\n",
    "print(f\"   You just built the foundation of modern AI! ğŸ‰\")"
   ]
  },
  {
   "id": "gpt-bert-t5",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ­ GPT vs BERT vs T5: How They Differ\n",
    "\n",
    "**All use transformers, but with different architectures!**\n",
    "\n",
    "### ğŸ¤– GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "**Architecture:** Decoder-Only\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on\"\n",
    "   â†“\n",
    "Embeddings + Positional Encoding\n",
    "   â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Masked Self-Attention   â”‚ â† Can only see previous words\n",
    "â”‚  Feed-Forward Network    â”‚\n",
    "â”‚  (Repeat 96 layers)      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â†“\n",
    "Output: \"the\" (next word)\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- âŒ **No encoder** (decoder-only)\n",
    "- âœ… **Masked attention** (autoregressive)\n",
    "- âœ… **Left-to-right** generation\n",
    "- ğŸ¯ **Objective:** Predict next token\n",
    "\n",
    "**Models:**\n",
    "- GPT-2 (1.5B parameters)\n",
    "- GPT-3 (175B parameters)\n",
    "- ChatGPT (GPT-3.5/4 with RLHF)\n",
    "- GPT-4 (1.7T+ parameters, multimodal)\n",
    "\n",
    "**Best For:**\n",
    "- Text generation\n",
    "- Completion tasks\n",
    "- Conversational AI\n",
    "- Creative writing\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "**Architecture:** Encoder-Only\n",
    "\n",
    "```\n",
    "Input: \"The [MASK] sat on the mat\"\n",
    "   â†“\n",
    "Embeddings + Positional Encoding\n",
    "   â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Self-Attention          â”‚ â† Sees ALL words (bidirectional)\n",
    "â”‚  Feed-Forward Network    â”‚\n",
    "â”‚  (Repeat 12-24 layers)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â†“\n",
    "Output: \"cat\" (fill in the mask)\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- âœ… **Encoder-only** architecture\n",
    "- âœ… **Bidirectional** (sees past AND future)\n",
    "- âŒ **No masked attention** (not autoregressive)\n",
    "- ğŸ¯ **Objective:** Masked Language Modeling (MLM)\n",
    "\n",
    "**Models:**\n",
    "- BERT-Base (110M parameters)\n",
    "- BERT-Large (340M parameters)\n",
    "- RoBERTa (355M, improved BERT)\n",
    "- DistilBERT (66M, faster)\n",
    "\n",
    "**Best For:**\n",
    "- Text classification\n",
    "- Named entity recognition\n",
    "- Question answering\n",
    "- Understanding tasks\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ T5 (Text-to-Text Transfer Transformer)\n",
    "\n",
    "**Architecture:** Encoder-Decoder (Full Transformer)\n",
    "\n",
    "```\n",
    "Input: \"translate English to French: The cat\"\n",
    "   â†“\n",
    "ENCODER (bidirectional)\n",
    "   â†“\n",
    "DECODER (autoregressive)\n",
    "   â†“\n",
    "Output: \"Le chat\"\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- âœ… **Both encoder AND decoder**\n",
    "- âœ… **Everything is text-to-text**\n",
    "- âœ… **Unified framework** for all tasks\n",
    "- ğŸ¯ **Objective:** Span corruption + supervised tasks\n",
    "\n",
    "**Models:**\n",
    "- T5-Small (60M)\n",
    "- T5-Base (220M)\n",
    "- T5-Large (770M)\n",
    "- T5-11B (11 billion)\n",
    "\n",
    "**Best For:**\n",
    "- Translation\n",
    "- Summarization\n",
    "- Question answering\n",
    "- Any text-to-text task\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Quick Comparison:\n",
    "\n",
    "| Feature | GPT | BERT | T5 |\n",
    "|---------|-----|------|----|\n",
    "| **Architecture** | Decoder-only | Encoder-only | Encoder-Decoder |\n",
    "| **Attention** | Masked (left-to-right) | Bidirectional | Both |\n",
    "| **Objective** | Next token prediction | Masked LM | Text-to-text |\n",
    "| **Best For** | Generation | Understanding | Both |\n",
    "| **Example** | ChatGPT | Search ranking | Translation |\n",
    "| **Can generate?** | âœ… Yes | âŒ No | âœ… Yes |\n",
    "| **Can understand?** | âœ… Yes | âœ… Yes (better) | âœ… Yes |\n",
    "\n",
    "### ğŸŒŸ Modern Trends (2024-2025):\n",
    "\n",
    "**Decoder-only models (GPT-style) are WINNING:**\n",
    "- GPT-4, Claude, Gemini all use decoder-only\n",
    "- Llama 3, Mistral, Phi-3 all decoder-only\n",
    "- **Why?** Scales better, simpler architecture, strong at both generation AND understanding\n",
    "\n",
    "**But BERT-style still used for:**\n",
    "- Embeddings (sentence-transformers)\n",
    "- Search and ranking\n",
    "- Classification tasks\n",
    "\n",
    "**T5-style for specific tasks:**\n",
    "- Translation\n",
    "- Summarization\n",
    "- Structured outputs"
   ]
  },
  {
   "id": "mini-transformer-example",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Real AI Example: Building a Mini-Transformer\n",
    "\n",
    "**Task:** Train a small transformer for sequence copying\n",
    "\n",
    "**Goal:** Given `[1, 2, 3, 4]`, output `[1, 2, 3, 4]`\n",
    "\n",
    "**Why This Matters:**\n",
    "- Validates our transformer implementation\n",
    "- Shows how transformers learn patterns\n",
    "- Simpler than full translation, but same principles\n",
    "- This is how ChatGPT training starts (next token prediction)\n",
    "\n",
    "Let's build and train it!"
   ]
  },
  {
   "id": "mini-transformer-training",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Transformer for sequence copying\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generate mask for decoder (prevents looking at future tokens)\"\"\"\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask\n",
    "\n",
    "# Create a small transformer\n",
    "vocab_size = 20  # Small vocabulary for demo\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "d_ff = 512\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_encoder_layers=num_layers,\n",
    "    num_decoder_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Generate training data (sequence copying)\n",
    "def generate_data(batch_size, seq_len, vocab_size):\n",
    "    \"\"\"Generate random sequences for copying task\"\"\"\n",
    "    # Source and target are the same (copying task)\n",
    "    src = torch.randint(1, vocab_size, (batch_size, seq_len))\n",
    "    tgt = src.clone()\n",
    "    return src, tgt\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop (just a few iterations for demo)\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "seq_len = 8\n",
    "\n",
    "print(\"ğŸ“ Training Mini-Transformer on Sequence Copying Task...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Generate batch\n",
    "    src, tgt = generate_data(batch_size, seq_len, vocab_size)\n",
    "    \n",
    "    # Create target input (shift right by 1)\n",
    "    tgt_input = tgt[:, :-1]  # All but last token\n",
    "    tgt_output = tgt[:, 1:]  # All but first token\n",
    "    \n",
    "    # Create target mask (prevent looking at future)\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_input.size(1))\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(src, tgt_input, tgt_mask=tgt_mask)\n",
    "    \n",
    "    # Calculate loss\n",
    "    output = output.reshape(-1, vocab_size)\n",
    "    tgt_output = tgt_output.reshape(-1)\n",
    "    loss = criterion(output, tgt_output)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Transformer Training Loss (Sequence Copying)', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Loss decreasing = Transformer is learning!\")"
   ]
  },
  {
   "id": "test-transformer",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained transformer\n",
    "\n",
    "def test_transformer(model, test_seq):\n",
    "    \"\"\"\n",
    "    Test transformer on a sequence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Prepare input\n",
    "        src = test_seq.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Encode\n",
    "        encoder_output = model.encode(src)\n",
    "        \n",
    "        # Start with start token (0)\n",
    "        tgt_tokens = [0]\n",
    "        \n",
    "        # Generate sequence autoregressively\n",
    "        for _ in range(test_seq.size(0)):\n",
    "            tgt = torch.LongTensor([tgt_tokens]).to(src.device)\n",
    "            tgt_mask = generate_square_subsequent_mask(tgt.size(1))\n",
    "            \n",
    "            # Decode\n",
    "            output = model.decode(tgt, encoder_output, tgt_mask=tgt_mask)\n",
    "            output = model.fc_out(output)\n",
    "            \n",
    "            # Get next token (greedy)\n",
    "            next_token = output[0, -1, :].argmax().item()\n",
    "            tgt_tokens.append(next_token)\n",
    "        \n",
    "        return tgt_tokens[1:]  # Remove start token\n",
    "\n",
    "# Test with several sequences\n",
    "print(\"ğŸ§ª Testing Trained Transformer:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(5):\n",
    "    # Generate random test sequence\n",
    "    test_seq = torch.randint(1, vocab_size, (seq_len,))\n",
    "    predicted = test_transformer(model, test_seq)\n",
    "    \n",
    "    print(f\"\\nTest {i+1}:\")\n",
    "    print(f\"  Input:     {test_seq.tolist()}\")\n",
    "    print(f\"  Predicted: {predicted}\")\n",
    "    \n",
    "    # Check accuracy\n",
    "    correct = sum([a == b for a, b in zip(test_seq.tolist(), predicted)])\n",
    "    accuracy = correct / len(test_seq) * 100\n",
    "    print(f\"  Accuracy:  {accuracy:.1f}% ({correct}/{len(test_seq)} tokens)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nğŸ’¡ The transformer learned to copy sequences!\")\n",
    "print(\"   This demonstrates the same principle as ChatGPT:\")\n",
    "print(\"   - Encoder understands the input\")\n",
    "print(\"   - Decoder generates output token by token\")\n",
    "print(\"   - Attention connects relevant parts\")\n",
    "print(\"\\nğŸš€ You just built and trained a working Transformer!\")"
   ]
  },
  {
   "id": "exercises",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Interactive Exercises\n",
    "\n",
    "Test your understanding of the Transformer architecture!"
   ]
  },
  {
   "id": "exercise-1",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Architecture Design\n",
    "\n",
    "**Question:** You want to build a model for text classification (sentiment analysis). Which architecture should you use?\n",
    "\n",
    "A) Full Transformer (Encoder + Decoder)  \n",
    "B) Encoder-only (BERT-style)  \n",
    "C) Decoder-only (GPT-style)  \n",
    "\n",
    "**Explain your reasoning.**"
   ]
  },
  {
   "id": "solution-1",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ“– Click for solution</summary>\n",
    "\n",
    "**Answer: B) Encoder-only (BERT-style)**\n",
    "\n",
    "**Reasoning:**\n",
    "\n",
    "1. **Text Classification Needs:**\n",
    "   - Understand the ENTIRE input\n",
    "   - No need to generate output sequences\n",
    "   - Single prediction (positive/negative)\n",
    "\n",
    "2. **Why Encoder-only?**\n",
    "   - âœ… Bidirectional attention sees full context\n",
    "   - âœ… Better understanding of meaning\n",
    "   - âœ… No decoder overhead (faster)\n",
    "   - âœ… BERT, RoBERTa designed for this!\n",
    "\n",
    "3. **Why NOT the others?**\n",
    "   - **Full Transformer**: Overkill, decoder not needed\n",
    "   - **Decoder-only**: Can work but less efficient for classification\n",
    "\n",
    "**In Practice:**\n",
    "- Sentiment analysis: BERT âœ…\n",
    "- Text generation: GPT âœ…\n",
    "- Translation: T5 (full transformer) âœ…\n",
    "</details>"
   ]
  },
  {
   "id": "exercise-2",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Masking Understanding\n",
    "\n",
    "**Question:** Why does the decoder use masked self-attention but the encoder doesn't?\n",
    "\n",
    "Draw or explain the attention patterns for:\n",
    "1. Encoder self-attention\n",
    "2. Decoder masked self-attention"
   ]
  },
  {
   "id": "solution-2",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ“– Click for solution</summary>\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Encoder (No Masking):**\n",
    "```\n",
    "Input: \"The cat sat\"\n",
    "\n",
    "Attention Matrix (all positions can see all positions):\n",
    "       The  cat  sat\n",
    "The  [ âœ…   âœ…   âœ… ]  â† Can see all words\n",
    "cat  [ âœ…   âœ…   âœ… ]  â† Can see all words\n",
    "sat  [ âœ…   âœ…   âœ… ]  â† Can see all words\n",
    "\n",
    "Why? Encoder just understands input, not generating.\n",
    "```\n",
    "\n",
    "**Decoder (With Masking):**\n",
    "```\n",
    "Output: \"Le chat dort\"\n",
    "\n",
    "Attention Matrix (can only see previous positions):\n",
    "       Le  chat dort\n",
    "Le   [ âœ…   âŒ   âŒ ]  â† Only sees itself\n",
    "chat [ âœ…   âœ…   âŒ ]  â† Sees Le, chat\n",
    "dort [ âœ…   âœ…   âœ… ]  â† Sees all previous\n",
    "\n",
    "Why? During training, we know full output.\n",
    "Must prevent cheating by seeing future!\n",
    "```\n",
    "\n",
    "**Key Insight:**\n",
    "- **Encoder**: Understands complete input â†’ no masking needed\n",
    "- **Decoder**: Generates word-by-word â†’ must mask future to prevent cheating\n",
    "\n",
    "**During Inference:**\n",
    "- Masking naturally enforced (we don't have future tokens yet!)\n",
    "- Generate one token at a time\n",
    "- This is how ChatGPT generates responses!\n",
    "</details>"
   ]
  },
  {
   "id": "exercise-3",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Scaling Analysis\n",
    "\n",
    "**Challenge:** Calculate the number of parameters in a transformer\n",
    "\n",
    "**Given:**\n",
    "- `d_model = 512`\n",
    "- `num_heads = 8`\n",
    "- `d_ff = 2048`\n",
    "- `vocab_size = 50,000`\n",
    "- `num_encoder_layers = 6`\n",
    "- `num_decoder_layers = 6`\n",
    "\n",
    "**Calculate (approximately):**\n",
    "1. Parameters in one multi-head attention layer\n",
    "2. Parameters in one feed-forward network\n",
    "3. Total model parameters\n",
    "\n",
    "**Hint:** \n",
    "- Multi-head attention: 4 linear layers (Q, K, V, O) of size `d_model Ã— d_model`\n",
    "- FFN: 2 linear layers (`d_model Ã— d_ff` and `d_ff Ã— d_model`)\n",
    "- Don't forget embeddings!"
   ]
  },
  {
   "id": "solution-3",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Parameter Calculation\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "vocab_size = 50000\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "print(\"ğŸ“Š Transformer Parameter Calculation\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Multi-Head Attention Parameters\n",
    "# Q, K, V, O projections: 4 Ã— (d_model Ã— d_model)\n",
    "mha_params = 4 * (d_model * d_model)\n",
    "print(f\"\\n1ï¸âƒ£ Multi-Head Attention (one layer):\")\n",
    "print(f\"   4 linear layers Ã— ({d_model} Ã— {d_model}) = {mha_params:,} params\")\n",
    "\n",
    "# 2. Feed-Forward Network Parameters\n",
    "# Two linear layers: (d_model Ã— d_ff) + (d_ff Ã— d_model)\n",
    "ffn_params = (d_model * d_ff) + (d_ff * d_model)\n",
    "print(f\"\\n2ï¸âƒ£ Feed-Forward Network (one layer):\")\n",
    "print(f\"   ({d_model} Ã— {d_ff}) + ({d_ff} Ã— {d_model}) = {ffn_params:,} params\")\n",
    "\n",
    "# 3. One Encoder Block\n",
    "encoder_block_params = mha_params + ffn_params\n",
    "print(f\"\\n3ï¸âƒ£ One Encoder Block:\")\n",
    "print(f\"   MHA + FFN = {encoder_block_params:,} params\")\n",
    "\n",
    "# 4. One Decoder Block (has 2 attention layers!)\n",
    "decoder_block_params = 2 * mha_params + ffn_params  # Masked + cross attention\n",
    "print(f\"\\n4ï¸âƒ£ One Decoder Block:\")\n",
    "print(f\"   2Ã—MHA + FFN = {decoder_block_params:,} params\")\n",
    "\n",
    "# 5. All Encoder Layers\n",
    "total_encoder_params = num_encoder_layers * encoder_block_params\n",
    "print(f\"\\n5ï¸âƒ£ All Encoder Layers ({num_encoder_layers} layers):\")\n",
    "print(f\"   {num_encoder_layers} Ã— {encoder_block_params:,} = {total_encoder_params:,} params\")\n",
    "\n",
    "# 6. All Decoder Layers\n",
    "total_decoder_params = num_decoder_layers * decoder_block_params\n",
    "print(f\"\\n6ï¸âƒ£ All Decoder Layers ({num_decoder_layers} layers):\")\n",
    "print(f\"   {num_decoder_layers} Ã— {decoder_block_params:,} = {total_decoder_params:,} params\")\n",
    "\n",
    "# 7. Embeddings (source + target)\n",
    "embedding_params = 2 * (vocab_size * d_model)\n",
    "print(f\"\\n7ï¸âƒ£ Embeddings (source + target):\")\n",
    "print(f\"   2 Ã— ({vocab_size:,} Ã— {d_model}) = {embedding_params:,} params\")\n",
    "\n",
    "# 8. Output Layer\n",
    "output_params = d_model * vocab_size\n",
    "print(f\"\\n8ï¸âƒ£ Output Layer:\")\n",
    "print(f\"   {d_model} Ã— {vocab_size:,} = {output_params:,} params\")\n",
    "\n",
    "# Total (approximate, ignoring layer norm and biases)\n",
    "total_params = (\n",
    "    total_encoder_params + \n",
    "    total_decoder_params + \n",
    "    embedding_params + \n",
    "    output_params\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"\\nğŸ¯ TOTAL PARAMETERS (approximate): {total_params:,}\")\n",
    "print(f\"   That's about {total_params / 1e6:.1f} Million parameters!\")\n",
    "\n",
    "print(\"\\nğŸ“Š Parameter Breakdown:\")\n",
    "print(f\"   Embeddings:    {embedding_params:,} ({embedding_params/total_params*100:.1f}%)\")\n",
    "print(f\"   Encoder:       {total_encoder_params:,} ({total_encoder_params/total_params*100:.1f}%)\")\n",
    "print(f\"   Decoder:       {total_decoder_params:,} ({total_decoder_params/total_params*100:.1f}%)\")\n",
    "print(f\"   Output:        {output_params:,} ({output_params/total_params*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Insights:\")\n",
    "print(\"   - Most parameters in embeddings + output (vocab size!)\")\n",
    "print(\"   - GPT-3 uses similar architecture, but MUCH larger\")\n",
    "print(f\"   - GPT-3: 175 BILLION params ({175000 / (total_params/1e6):.0f}x larger!)\")\n",
    "print(\"\\nğŸš€ Larger models = More capacity = Better performance!\")"
   ]
  },
  {
   "id": "key-takeaways",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Key Takeaways\n",
    "\n",
    "**You just mastered the Transformer architecture!**\n",
    "\n",
    "### 1ï¸âƒ£ **Transformer Components**\n",
    "   - âœ… Multi-Head Self-Attention (parallel attention patterns)\n",
    "   - âœ… Positional Encoding (adds order information)\n",
    "   - âœ… Feed-Forward Networks (processes each position)\n",
    "   - âœ… Residual Connections + Layer Norm (stable training)\n",
    "\n",
    "### 2ï¸âƒ£ **Encoder-Decoder Architecture**\n",
    "   - âœ… Encoder: Bidirectional understanding of input\n",
    "   - âœ… Decoder: Autoregressive generation of output\n",
    "   - âœ… Cross-Attention: Links source and target\n",
    "   - âœ… Masking: Prevents cheating during training\n",
    "\n",
    "### 3ï¸âƒ£ **Modern Variants**\n",
    "   - âœ… **GPT (Decoder-only)**: Text generation, ChatGPT\n",
    "   - âœ… **BERT (Encoder-only)**: Text understanding, classification\n",
    "   - âœ… **T5 (Full Transformer)**: Text-to-text, translation\n",
    "\n",
    "### 4ï¸âƒ£ **Why Transformers Won**\n",
    "   - âœ… Parallelizable (faster training)\n",
    "   - âœ… Long-range dependencies (no forgetting)\n",
    "   - âœ… Scalable (works with billions of parameters)\n",
    "   - âœ… Transfer learning (pre-train once, use everywhere)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ Real-World Impact (2024-2025)\n",
    "\n",
    "**Every AI you use is built on Transformers:**\n",
    "\n",
    "### ğŸ¤– **Language Models**\n",
    "- **ChatGPT**: GPT architecture (decoder-only)\n",
    "- **Claude**: Constitutional AI transformer\n",
    "- **Gemini**: Google's multimodal transformer\n",
    "- **GPT-4**: 1.7T+ parameters, decoder-only\n",
    "\n",
    "### ğŸ¨ **Vision Models**\n",
    "- **ViT (Vision Transformer)**: Images as token sequences\n",
    "- **DALL-E**: Text â†’ Image using transformers\n",
    "- **Stable Diffusion**: Transformer-based diffusion\n",
    "\n",
    "### ğŸ’» **Code Models**\n",
    "- **GitHub Copilot**: Code generation transformer\n",
    "- **CodeLlama**: Specialized for programming\n",
    "- **AlphaCode**: Competitive programming AI\n",
    "\n",
    "### ğŸ”¬ **Science**\n",
    "- **AlphaFold**: Protein structure prediction\n",
    "- **ESM**: Protein language model\n",
    "- **MolFormer**: Molecular property prediction\n",
    "\n",
    "### ğŸµ **Audio**\n",
    "- **Whisper**: Speech recognition transformer\n",
    "- **MusicGen**: Music generation\n",
    "- **AudioLM**: Audio synthesis\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Scaling Laws\n",
    "\n",
    "**Bigger Transformers = Better Performance:**\n",
    "\n",
    "| Model | Year | Params | Architecture | What Changed |\n",
    "|-------|------|--------|--------------|-------------|\n",
    "| Transformer | 2017 | 65M | Enc-Dec | Invented attention |\n",
    "| BERT | 2018 | 340M | Enc-only | Bidirectional understanding |\n",
    "| GPT-2 | 2019 | 1.5B | Dec-only | Impressive generation |\n",
    "| GPT-3 | 2020 | 175B | Dec-only | Few-shot learning |\n",
    "| ChatGPT | 2022 | 175B+ | Dec-only | + RLHF |\n",
    "| GPT-4 | 2023 | 1.7T+ | Dec-only | Multimodal |\n",
    "\n",
    "**Key Insight:** Same architecture, just BIGGER!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ What You Can Build Now\n",
    "\n",
    "**With this knowledge:**\n",
    "1. âœ… Understand how ChatGPT works internally\n",
    "2. âœ… Fine-tune BERT for classification tasks\n",
    "3. âœ… Use GPT-2/3 for text generation\n",
    "4. âœ… Build custom transformers for specific tasks\n",
    "5. âœ… Read and understand transformer research papers\n",
    "\n",
    "**Next:** Day 3 - Working with LLMs (fine-tuning, APIs, RAG)!\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¬ Remember:**\n",
    "\n",
    "*\"Attention Is All You Need\" wasn't just a paper title - it was a prediction. In 2017, nobody expected transformers to completely replace RNNs, CNNs, and every other architecture. But here we are in 2025, and transformers power EVERYTHING. You now understand the architecture behind this revolution!\"* ğŸš€"
   ]
  },
  {
   "id": "next-steps",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Next Steps\n",
    "\n",
    "**Before Day 3, explore:**\n",
    "1. Modify the transformer architecture (change layers, heads)\n",
    "2. Visualize attention weights from the trained model\n",
    "3. Try different sequence tasks (reversal, sorting)\n",
    "4. Read the original \"Attention Is All You Need\" paper\n",
    "\n",
    "**Coming Next:**\n",
    "- **Day 3:** Working with Modern LLMs\n",
    "  - Using OpenAI API and HuggingFace\n",
    "  - Fine-tuning GPT-2 for custom text generation\n",
    "  - Building a RAG system with embeddings\n",
    "  - Prompt engineering for better results\n",
    "  - Deploying LLM applications\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“š Deep Dive Resources:**\n",
    "- \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
    "- The Illustrated Transformer (Jay Alammar)\n",
    "- The Annotated Transformer (Harvard NLP)\n",
    "- HuggingFace Transformers Documentation\n",
    "\n",
    "---\n",
    "\n",
    "*You just built the architecture that powers ChatGPT, Claude, and every modern AI! ğŸ‰*\n",
    "\n",
    "**See you on Day 3 for hands-on LLM applications!** ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
