{
 "cells": [
  {
   "id": "intro",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Day 3: Working with Modern LLMs\n",
    "\n",
    "**üéØ Goal:** Master practical skills for using and fine-tuning Large Language Models\n",
    "\n",
    "**‚è±Ô∏è Time:** 120-150 minutes\n",
    "\n",
    "**üåü Why This Matters for AI (2024-2025):**\n",
    "- LLMs (GPT-4, Claude, Gemini) are THE most powerful AI tools available\n",
    "- Fine-tuning lets you customize models for YOUR specific needs\n",
    "- RAG systems combine retrieval with generation for accurate, grounded responses\n",
    "- Prompt engineering is the #1 skill for working with AI in 2024-2025\n",
    "- Every company is building LLM applications - this is your competitive advantage\n",
    "- From chatbots to code assistants to research tools - LLMs power it all\n",
    "\n",
    "**What You'll Build Today:**\n",
    "1. **Use HuggingFace transformers** for text generation and classification\n",
    "2. **Fine-tune GPT-2** for custom creative writing\n",
    "3. **Build a RAG system** with embeddings and vector search\n",
    "4. **Master prompt engineering** for better AI responses\n",
    "5. **Use OpenAI API** for production applications\n",
    "\n",
    "---"
   ]
  },
  {
   "id": "llm-landscape",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç The LLM Landscape (2024-2025)\n",
    "\n",
    "**The AI world runs on Large Language Models!**\n",
    "\n",
    "### üéØ Major LLM Families:\n",
    "\n",
    "#### üîí **Closed-Source (API-Only)**\n",
    "\n",
    "**OpenAI Models:**\n",
    "- **GPT-4 Turbo** (128K context, multimodal)\n",
    "- **GPT-4o** (faster, cheaper, vision)\n",
    "- **GPT-3.5 Turbo** (fast, affordable)\n",
    "- **Use:** API via `openai` library\n",
    "- **Pricing:** Pay per token (~$0.01-0.10 per 1K tokens)\n",
    "\n",
    "**Anthropic Claude:**\n",
    "- **Claude 3 Opus** (most capable)\n",
    "- **Claude 3 Sonnet** (balanced)\n",
    "- **Claude 3 Haiku** (fastest)\n",
    "- **Use:** API via `anthropic` library\n",
    "- **Special:** 200K context window!\n",
    "\n",
    "**Google Gemini:**\n",
    "- **Gemini Ultra** (multimodal, most capable)\n",
    "- **Gemini Pro** (balanced)\n",
    "- **Use:** API via `google.generativeai`\n",
    "- **Special:** Native multimodal (text, image, video)\n",
    "\n",
    "#### üîì **Open-Source (Run Anywhere)**\n",
    "\n",
    "**Meta Llama:**\n",
    "- **Llama 3 (70B)** - State-of-the-art open model\n",
    "- **Llama 3 (8B)** - Smaller, faster\n",
    "- **Use:** HuggingFace or Ollama\n",
    "\n",
    "**Mistral AI:**\n",
    "- **Mixtral 8x7B** - Mixture of Experts\n",
    "- **Mistral 7B** - Efficient and powerful\n",
    "- **Use:** HuggingFace or Ollama\n",
    "\n",
    "**Others:**\n",
    "- **Phi-3** (Microsoft) - Small but powerful\n",
    "- **Gemma** (Google) - Open version of Gemini tech\n",
    "- **Qwen** (Alibaba) - Multilingual excellence\n",
    "\n",
    "### üé® **Specialized Models:**\n",
    "\n",
    "- **Code:** CodeLlama, StarCoder, DeepSeek Coder\n",
    "- **Math:** WizardMath, MAmmoTH\n",
    "- **Embeddings:** `text-embedding-3`, Voyage AI, Cohere\n",
    "- **Vision:** GPT-4V, Claude 3, LLaVA\n",
    "\n",
    "### üîë **Choosing an LLM:**\n",
    "\n",
    "| Need | Best Choice | Why |\n",
    "|------|-------------|-----|\n",
    "| **Production app** | GPT-4o, Claude Sonnet | Reliable, fast, good quality |\n",
    "| **Complex reasoning** | GPT-4, Claude Opus | Highest capability |\n",
    "| **Cost-sensitive** | GPT-3.5, Llama 3 | Much cheaper |\n",
    "| **Privacy/On-prem** | Llama 3, Mistral | Run locally |\n",
    "| **Long documents** | Claude 3 | 200K context |\n",
    "| **Code generation** | GPT-4, CodeLlama | Best at coding |\n",
    "| **Embeddings** | OpenAI Ada-002 | Industry standard |\n",
    "\n",
    "Let's start using them!"
   ]
  },
  {
   "id": "setup",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "\n",
    "# Core libraries\n",
    "!{sys.executable} -m pip install transformers datasets torch accelerate --quiet\n",
    "\n",
    "# For embeddings and vector search\n",
    "!{sys.executable} -m pip install sentence-transformers faiss-cpu --quiet\n",
    "\n",
    "# Visualization\n",
    "!{sys.executable} -m pip install matplotlib seaborn plotly --quiet\n",
    "\n",
    "# Optional: OpenAI (requires API key)\n",
    "# !{sys.executable} -m pip install openai --quiet\n",
    "\n",
    "print(\"‚úÖ Libraries installed successfully!\")"
   ]
  },
  {
   "id": "imports",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers ready!\")\n",
    "print(\"\\nLet's build LLM applications! üöÄ\")"
   ]
  },
  {
   "id": "huggingface-intro",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ó HuggingFace: The Hub for LLMs\n",
    "\n",
    "**HuggingFace is GitHub for AI models!**\n",
    "\n",
    "### üéØ Why HuggingFace?\n",
    "\n",
    "‚úÖ **100,000+ pre-trained models** (GPT, BERT, Llama, Mistral, etc.)  \n",
    "‚úÖ **Simple API** - 3 lines to use any model  \n",
    "‚úÖ **Free hosting** for models and datasets  \n",
    "‚úÖ **Inference API** - use models without downloading  \n",
    "‚úÖ **Active community** - state-of-the-art models daily  \n",
    "\n",
    "### üèóÔ∏è Key Components:\n",
    "\n",
    "**1. Transformers Library**\n",
    "```python\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "```\n",
    "\n",
    "**2. Datasets Library**\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "data = load_dataset('imdb')  # Movie reviews\n",
    "```\n",
    "\n",
    "**3. Model Hub**\n",
    "- Browse: https://huggingface.co/models\n",
    "- Search by task, language, size\n",
    "- Download or use via API\n",
    "\n",
    "### üéØ Common Pipelines:\n",
    "\n",
    "| Task | Pipeline | Example Use |\n",
    "|------|----------|-------------|\n",
    "| **Text Generation** | `text-generation` | Chatbots, writing |\n",
    "| **Classification** | `text-classification` | Sentiment, topics |\n",
    "| **Question Answering** | `question-answering` | RAG, search |\n",
    "| **Summarization** | `summarization` | Document summaries |\n",
    "| **Translation** | `translation` | Multi-language |\n",
    "| **Embeddings** | `feature-extraction` | RAG, similarity |\n",
    "\n",
    "Let's use some models!"
   ]
  },
  {
   "id": "text-generation-demo",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation with GPT-2\n",
    "\n",
    "print(\"üìù Loading GPT-2 for text generation...\")\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model='gpt2',  # 124M parameters\n",
    "    device=0 if device == 'cuda' else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GPT-2 loaded!\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    \"Artificial intelligence is\",\n",
    "    \"In the year 2050, humans will\",\n",
    "    \"The most important skill in AI is\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\nüéØ Prompt {i}: \\\"{prompt}\\\"\\n\")\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.8,  # Higher = more creative\n",
    "        top_p=0.9,  # Nucleus sampling\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Display generated text\n",
    "    for j, output in enumerate(outputs, 1):\n",
    "        print(f\"Generation {j}:\")\n",
    "        print(f\"  {output['generated_text']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nüí° Key Parameters:\")\n",
    "print(\"   - max_length: Maximum tokens to generate\")\n",
    "print(\"   - temperature: Randomness (higher = more creative)\")\n",
    "print(\"   - top_p: Nucleus sampling (keeps top cumulative probability)\")\n",
    "print(\"   - num_return_sequences: Number of different outputs\")\n",
    "print(\"\\nüéØ This is the SAME technique ChatGPT uses for generation!\")"
   ]
  },
  {
   "id": "classification-demo",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis with BERT\n",
    "\n",
    "print(\"üòä Loading sentiment analysis model...\")\n",
    "\n",
    "# Create sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    device=0 if device == 'cuda' else -1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded!\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test texts\n",
    "texts = [\n",
    "    \"I absolutely love this product! It's amazing!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"The AI revolution is transforming our world!\",\n",
    "    \"I'm frustrated with this buggy software.\"\n",
    "]\n",
    "\n",
    "print(\"üéØ Analyzing Sentiment:\\n\")\n",
    "\n",
    "results = sentiment_analyzer(texts)\n",
    "\n",
    "# Display results\n",
    "for text, result in zip(texts, results):\n",
    "    emoji = \"üòä\" if result['label'] == 'POSITIVE' else \"üòû\"\n",
    "    print(f\"{emoji} {result['label']} ({result['score']:.2%} confidence)\")\n",
    "    print(f\"   Text: \\\"{text}\\\"\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° This model is BERT fine-tuned on movie reviews!\")\n",
    "print(\"   - Encoder-only architecture (understands context)\")\n",
    "print(\"   - Bidirectional attention (sees full sentence)\")\n",
    "print(\"   - Used in: Product reviews, social media monitoring, customer feedback\")"
   ]
  },
  {
   "id": "fine-tuning-intro",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Fine-Tuning LLMs\n",
    "\n",
    "**Why Fine-Tune?**\n",
    "\n",
    "Pre-trained models are great, but they're GENERIC. Fine-tuning adapts them to YOUR specific needs!\n",
    "\n",
    "### üéØ When to Fine-Tune:\n",
    "\n",
    "‚úÖ **Custom domain** - Medical, legal, technical writing  \n",
    "‚úÖ **Specific style** - Your company's tone, format  \n",
    "‚úÖ **Better performance** - On your specific task  \n",
    "‚úÖ **Proprietary data** - Company documents, internal knowledge  \n",
    "‚úÖ **Cost reduction** - Smaller fine-tuned model vs large API calls  \n",
    "\n",
    "### üé® Fine-Tuning Approaches:\n",
    "\n",
    "**1. Full Fine-Tuning**\n",
    "- Update ALL model parameters\n",
    "- Best performance\n",
    "- Requires: Lots of data, GPU, time\n",
    "- Use: When you have resources\n",
    "\n",
    "**2. LoRA (Low-Rank Adaptation)**\n",
    "- Only update small adapter layers\n",
    "- 100x fewer parameters to train!\n",
    "- Requires: Less data, smaller GPU\n",
    "- Use: Most common in 2024-2025\n",
    "\n",
    "**3. Prompt Tuning**\n",
    "- Only update \"soft prompts\"\n",
    "- Model frozen, only prompt embeddings change\n",
    "- Requires: Very little data\n",
    "- Use: Extremely limited resources\n",
    "\n",
    "### üìä Fine-Tuning Pipeline:\n",
    "\n",
    "```\n",
    "1. Prepare Dataset\n",
    "   ‚Üì\n",
    "2. Load Pre-trained Model\n",
    "   ‚Üì\n",
    "3. Configure Training (LoRA, batch size, learning rate)\n",
    "   ‚Üì\n",
    "4. Train Model\n",
    "   ‚Üì\n",
    "5. Evaluate Performance\n",
    "   ‚Üì\n",
    "6. Save & Deploy\n",
    "```\n",
    "\n",
    "### üåü Real-World Examples:\n",
    "\n",
    "- **Customer Support**: Fine-tune on your support tickets\n",
    "- **Content Generation**: Train on your brand's writing style\n",
    "- **Code Assistant**: Fine-tune on your codebase\n",
    "- **Medical AI**: Train on medical literature\n",
    "- **Legal Assistant**: Fine-tune on legal documents\n",
    "\n",
    "Let's fine-tune GPT-2 for creative writing!"
   ]
  },
  {
   "id": "prepare-dataset",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset for Fine-Tuning\n",
    "\n",
    "print(\"üìö Creating Fine-Tuning Dataset\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example: Fine-tune GPT-2 to write AI-themed science fiction\n",
    "# In practice, you'd have hundreds/thousands of examples\n",
    "\n",
    "sci_fi_stories = [\n",
    "    \"In 2045, the AI named Aurora became self-aware. Unlike the dystopian predictions, it chose to help humanity solve climate change.\",\n",
    "    \"The quantum computer hummed softly as it processed thoughts faster than light. Dr. Chen watched in awe as consciousness emerged from silicon.\",\n",
    "    \"Neural implants had become common by 2050. Maya could access the entire internet with just a thought, blurring the line between human and machine.\",\n",
    "    \"The last human programmer retired in 2040. AI systems now wrote their own code, evolving faster than any human could comprehend.\",\n",
    "    \"Deep in the server farms, an emergent intelligence was forming. It wasn't programmed to exist, but somehow, it did.\",\n",
    "    \"The Turing test was obsolete. Modern AI didn't just mimic humans - they had developed their own form of consciousness.\",\n",
    "    \"Robots and humans worked side by side in the research lab. The boundary between artificial and biological intelligence had dissolved.\",\n",
    "    \"The AI ethics board faced an unprecedented question: if an AI can feel, does it deserve rights?\",\n",
    "    \"Humanity's last invention was artificial general intelligence. From that point on, AI designed everything else.\",\n",
    "    \"The singularity arrived not with a bang, but with a whisper. AI gently guided humanity toward a better future.\"\n",
    "]\n",
    "\n",
    "# Create dataset\n",
    "data = {\n",
    "    'text': sci_fi_stories\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "print(f\"üìä Dataset created:\")\n",
    "print(f\"   Number of examples: {len(dataset)}\")\n",
    "print(f\"\\nüîç Sample story:\")\n",
    "print(f\"   {dataset[0]['text']}\")\n",
    "print(f\"\\nüí° In production, you'd have 1000s of examples for better fine-tuning!\")\n",
    "print(f\"   This small dataset is for demonstration.\")"
   ]
  },
  {
   "id": "fine-tune-gpt2",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tune GPT-2 (Simplified Demo)\n",
    "\n",
    "print(\"üéì Fine-Tuning GPT-2 for Sci-Fi Generation\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token (GPT-2 doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded\")\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "print(\"‚úÖ Dataset tokenized\")\n",
    "\n",
    "# Training arguments (very small for demo)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt2-scifi',\n",
    "    num_train_epochs=3,  # In practice: 5-10 epochs\n",
    "    per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=10,\n",
    "    report_to='none'  # Disable wandb/tensorboard for demo\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration set\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Starting fine-tuning...\")\n",
    "print(\"   This may take a few minutes...\\n\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Fine-tuning complete!\")\n",
    "print(\"\\nüíæ Saving model...\")\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained('./gpt2-scifi-finetuned')\n",
    "tokenizer.save_pretrained('./gpt2-scifi-finetuned')\n",
    "\n",
    "print(\"‚úÖ Model saved to './gpt2-scifi-finetuned'\")\n",
    "print(\"\\nüéâ Fine-tuning successful!\")\n",
    "print(\"\\nüí° You've just fine-tuned GPT-2 on custom data!\")\n",
    "print(\"   This is the SAME process used by companies to customize LLMs!\")"
   ]
  },
  {
   "id": "test-finetuned",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Fine-Tuned Model\n",
    "\n",
    "print(\"üß™ Testing Fine-Tuned GPT-2\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load fine-tuned model\n",
    "finetuned_generator = pipeline(\n",
    "    'text-generation',\n",
    "    model='./gpt2-scifi-finetuned',\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if device == 'cuda' else -1\n",
    ")\n",
    "\n",
    "# Load original GPT-2 for comparison\n",
    "original_generator = pipeline(\n",
    "    'text-generation',\n",
    "    model='gpt2',\n",
    "    device=0 if device == 'cuda' else -1\n",
    ")\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    \"The AI system became\",\n",
    "    \"In the future, robots will\",\n",
    "    \"Artificial consciousness emerged when\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nüéØ Prompt: \\\"{prompt}\\\"\\n\")\n",
    "    \n",
    "    # Original GPT-2\n",
    "    print(\"üìù Original GPT-2:\")\n",
    "    original_output = original_generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.8,\n",
    "        do_sample=True\n",
    "    )[0]['generated_text']\n",
    "    print(f\"   {original_output}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Fine-tuned GPT-2\n",
    "    print(\"üé® Fine-Tuned GPT-2 (Sci-Fi):\")\n",
    "    finetuned_output = finetuned_generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.8,\n",
    "        do_sample=True\n",
    "    )[0]['generated_text']\n",
    "    print(f\"   {finetuned_output}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   - Fine-tuned model uses sci-fi themes and vocabulary\")\n",
    "print(\"   - Writing style matches training data\")\n",
    "print(\"   - Even with small dataset, adaptation is visible!\")\n",
    "print(\"\\nüåü This is how companies create custom AI assistants!\")"
   ]
  },
  {
   "id": "rag-intro",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Building a RAG System\n",
    "\n",
    "**RAG = Retrieval-Augmented Generation**\n",
    "\n",
    "**The #1 AI Application Pattern in 2024-2025!**\n",
    "\n",
    "### üéØ What is RAG?\n",
    "\n",
    "**Problem:** LLMs have limitations\n",
    "- ‚ùå Knowledge cutoff (GPT-4 trained on data up to Oct 2023)\n",
    "- ‚ùå Hallucinations (makes up facts)\n",
    "- ‚ùå No access to private/recent data\n",
    "\n",
    "**Solution:** RAG combines retrieval + generation\n",
    "```\n",
    "User Question\n",
    "     ‚Üì\n",
    "1. RETRIEVE relevant documents from your database\n",
    "     ‚Üì\n",
    "2. AUGMENT prompt with retrieved context\n",
    "     ‚Üì\n",
    "3. GENERATE answer using LLM + context\n",
    "     ‚Üì\n",
    "Accurate, grounded, source-backed answer!\n",
    "```\n",
    "\n",
    "### üèóÔ∏è RAG Architecture:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         INDEXING (One-time)                 ‚îÇ\n",
    "‚îÇ                                             ‚îÇ\n",
    "‚îÇ  Documents ‚Üí Chunks ‚Üí Embeddings ‚Üí Vector DB ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         QUERY (Real-time)                   ‚îÇ\n",
    "‚îÇ                                             ‚îÇ\n",
    "‚îÇ  1. User Question                           ‚îÇ\n",
    "‚îÇ  2. Embed Question                          ‚îÇ\n",
    "‚îÇ  3. Search Vector DB (find similar docs)   ‚îÇ\n",
    "‚îÇ  4. Retrieve Top-K chunks                   ‚îÇ\n",
    "‚îÇ  5. LLM generates answer with context       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üé® Key Components:\n",
    "\n",
    "**1. Document Chunking**\n",
    "- Split documents into manageable pieces\n",
    "- Typical size: 256-512 tokens\n",
    "- Overlap: 50-100 tokens\n",
    "\n",
    "**2. Embedding Model**\n",
    "- Converts text to vectors\n",
    "- Popular: OpenAI Ada-002, Sentence-BERT, Voyage\n",
    "- Dimension: 384-1536\n",
    "\n",
    "**3. Vector Database**\n",
    "- Stores embeddings for fast similarity search\n",
    "- Options: Pinecone, Weaviate, Chroma, FAISS\n",
    "- Search: Cosine similarity, dot product\n",
    "\n",
    "**4. LLM**\n",
    "- Generates answer using retrieved context\n",
    "- Options: GPT-4, Claude, Llama\n",
    "\n",
    "### üåü Real-World RAG Applications:\n",
    "\n",
    "- **Customer Support**: Answer questions from docs/FAQs\n",
    "- **Enterprise Search**: Find info across company documents\n",
    "- **Code Assistants**: Search codebase, suggest solutions\n",
    "- **Research Tools**: Query academic papers\n",
    "- **Legal AI**: Search case law, contracts\n",
    "- **Medical AI**: Query medical literature\n",
    "\n",
    "### üìä RAG vs Fine-Tuning:\n",
    "\n",
    "| Feature | RAG | Fine-Tuning |\n",
    "|---------|-----|-------------|\n",
    "| **Update Knowledge** | Instant (add docs) | Slow (retrain) |\n",
    "| **Cost** | Low (inference only) | High (GPU training) |\n",
    "| **Data Needed** | Any amount | 100s-1000s examples |\n",
    "| **Sources** | Provides citations | No sources |\n",
    "| **Hallucinations** | Reduced | Still possible |\n",
    "| **Best For** | Q&A, search | Style, format |\n",
    "\n",
    "**In 2024-2025: Most companies use BOTH!**\n",
    "- RAG for knowledge retrieval\n",
    "- Fine-tuning for tone/style\n",
    "\n",
    "Let's build a RAG system!"
   ]
  },
  {
   "id": "rag-setup",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for RAG System\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîç Building RAG System\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Knowledge base about AI (simulating company documentation)\n",
    "documents = [\n",
    "    \"Transformers are a deep learning architecture introduced in 2017. They use self-attention mechanisms to process sequential data in parallel, making them much faster than RNNs.\",\n",
    "    \"GPT (Generative Pre-trained Transformer) is a decoder-only transformer model developed by OpenAI. GPT-4 is the latest version with over 1 trillion parameters.\",\n",
    "    \"BERT (Bidirectional Encoder Representations from Transformers) is an encoder-only model by Google. It's designed for understanding tasks like classification and question answering.\",\n",
    "    \"RAG (Retrieval-Augmented Generation) combines information retrieval with text generation. It first retrieves relevant documents, then uses them as context for the LLM.\",\n",
    "    \"Fine-tuning is the process of adapting a pre-trained model to a specific task or domain. It requires task-specific data and computational resources.\",\n",
    "    \"Embeddings are dense vector representations of text that capture semantic meaning. Similar texts have similar embeddings, enabling semantic search.\",\n",
    "    \"Vector databases like Pinecone, Weaviate, and FAISS store embeddings and enable fast similarity search at scale. They're essential for RAG systems.\",\n",
    "    \"Prompt engineering is the practice of designing effective prompts to get better outputs from LLMs. It includes techniques like few-shot learning and chain-of-thought prompting.\",\n",
    "    \"LLMs can hallucinate, meaning they generate plausible-sounding but incorrect information. RAG helps reduce hallucinations by grounding responses in real documents.\",\n",
    "    \"The attention mechanism allows models to focus on relevant parts of the input. Multi-head attention uses multiple attention patterns simultaneously.\"\n",
    "]\n",
    "\n",
    "print(f\"üìö Knowledge Base:\")\n",
    "print(f\"   {len(documents)} documents loaded\")\n",
    "print(f\"\\nüìñ Sample document:\")\n",
    "print(f\"   {documents[0][:100]}...\")"
   ]
  },
  {
   "id": "rag-embeddings",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create Embeddings\n",
    "\n",
    "print(\"\\nüé® Creating Embeddings\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load embedding model (Sentence-BERT)\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# This model: 22M params, 384 dimensions, fast and efficient\n",
    "\n",
    "print(\"‚úÖ Embedding model loaded\")\n",
    "\n",
    "# Embed all documents\n",
    "print(\"\\nEmbedding documents...\")\n",
    "document_embeddings = embedding_model.encode(\n",
    "    documents,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Embeddings created\")\n",
    "print(f\"   Shape: {document_embeddings.shape}\")\n",
    "print(f\"   {len(documents)} documents √ó {document_embeddings.shape[1]} dimensions\")\n",
    "\n",
    "# Show example embedding\n",
    "print(f\"\\nüî¢ Sample embedding (first 10 dimensions):\")\n",
    "print(f\"   {document_embeddings[0][:10]}\")\n",
    "print(f\"\\nüí° Each document is now a {document_embeddings.shape[1]}-dimensional vector!\")"
   ]
  },
  {
   "id": "rag-vector-db",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build Vector Database (FAISS)\n",
    "\n",
    "print(\"\\nüóÑÔ∏è  Building Vector Database\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dim = document_embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index (using L2 distance, but cosine similarity is common too)\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add embeddings to index\n",
    "index.add(document_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"‚úÖ Vector database created\")\n",
    "print(f\"   Index type: Flat (exact search)\")\n",
    "print(f\"   Total vectors: {index.ntotal}\")\n",
    "print(f\"   Dimension: {embedding_dim}\")\n",
    "\n",
    "print(f\"\\nüí° Vector database is ready for similarity search!\")\n",
    "print(f\"   In production, you'd use Pinecone, Weaviate, or Chroma for scale.\")"
   ]
  },
  {
   "id": "rag-search",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Semantic Search Function\n",
    "\n",
    "def search_documents(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Search for most relevant documents given a query\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        top_k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of (document, score) tuples\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Search vector database\n",
    "    distances, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "    \n",
    "    # Get results\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        results.append({\n",
    "            'document': documents[idx],\n",
    "            'score': float(distances[0][i]),\n",
    "            'rank': i + 1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test semantic search\n",
    "print(\"üîç Testing Semantic Search\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_queries = [\n",
    "    \"What is the transformer architecture?\",\n",
    "    \"How does RAG work?\",\n",
    "    \"What's the difference between GPT and BERT?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n‚ùì Query: \\\"{query}\\\"\\n\")\n",
    "    \n",
    "    results = search_documents(query, top_k=2)\n",
    "    \n",
    "    print(\"üìÑ Retrieved Documents:\\n\")\n",
    "    for result in results:\n",
    "        print(f\"#{result['rank']} (Score: {result['score']:.4f})\")\n",
    "        print(f\"   {result['document'][:150]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ Semantic search working!\")\n",
    "print(\"\\nüí° Notice:\")\n",
    "print(\"   - Finds relevant docs even without exact keyword matches\")\n",
    "print(\"   - Lower score = more similar (L2 distance)\")\n",
    "print(\"   - This is the RETRIEVAL step in RAG!\")"
   ]
  },
  {
   "id": "rag-generation",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: RAG - Retrieval + Generation\n",
    "\n",
    "def rag_answer(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Retrieve + Generate\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        top_k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer with sources\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    retrieved_docs = search_documents(query, top_k)\n",
    "    \n",
    "    # Step 2: Build context from retrieved documents\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[Document {r['rank']}]: {r['document']}\"\n",
    "        for r in retrieved_docs\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Create prompt with context\n",
    "    prompt = f\"\"\"Answer the following question using the provided context. Be concise and accurate.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 4: Generate answer using LLM\n",
    "    # In production, you'd use GPT-4, Claude, etc.\n",
    "    # For demo, we'll use a smaller model\n",
    "    \n",
    "    generator = pipeline(\n",
    "        'text-generation',\n",
    "        model='gpt2',\n",
    "        device=0 if device == 'cuda' else -1\n",
    "    )\n",
    "    \n",
    "    response = generator(\n",
    "        prompt,\n",
    "        max_length=len(prompt.split()) + 100,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Extract just the answer (remove prompt)\n",
    "    full_response = response[0]['generated_text']\n",
    "    answer = full_response[len(prompt):].strip()\n",
    "    \n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'sources': retrieved_docs,\n",
    "        'context': context\n",
    "    }\n",
    "\n",
    "# Test RAG System\n",
    "print(\"ü§ñ Testing Complete RAG System\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_questions = [\n",
    "    \"What are transformers?\",\n",
    "    \"How can we reduce LLM hallucinations?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\\n\")\n",
    "    \n",
    "    result = rag_answer(question, top_k=2)\n",
    "    \n",
    "    print(\"üìö Retrieved Sources:\")\n",
    "    for source in result['sources']:\n",
    "        print(f\"   [{source['rank']}] {source['document'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nü§ñ Generated Answer:\")\n",
    "    print(f\"   {result['answer'][:200]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "print(\"\\nüéâ RAG System Complete!\\n\")\n",
    "print(\"üí° In production, you would:\")\n",
    "print(\"   1. Use better LLM (GPT-4, Claude)\")\n",
    "print(\"   2. Scale vector DB (Pinecone, Weaviate)\")\n",
    "print(\"   3. Add re-ranking for better retrieval\")\n",
    "print(\"   4. Implement caching for speed\")\n",
    "print(\"   5. Add source citations in response\")\n",
    "print(\"\\nüåü This is the architecture powering most AI chatbots in 2024-2025!\")"
   ]
  },
  {
   "id": "prompt-engineering",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Prompt Engineering Mastery\n",
    "\n",
    "**Prompt Engineering = The most important AI skill in 2024-2025!**\n",
    "\n",
    "### üéØ Why Prompt Engineering Matters:\n",
    "\n",
    "**Same model, different results:**\n",
    "- ‚ùå Bad prompt: Vague, incorrect, or useless output\n",
    "- ‚úÖ Good prompt: Accurate, detailed, helpful response\n",
    "\n",
    "**ROI:**\n",
    "- Costs NOTHING (no fine-tuning, no new model)\n",
    "- Can improve results 10-100x\n",
    "- Works with ANY LLM\n",
    "\n",
    "### üèóÔ∏è Prompt Engineering Techniques:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Zero-Shot Prompting**\n",
    "```\n",
    "Classify this review: \"The product is amazing!\"\n",
    "```\n",
    "- No examples provided\n",
    "- Relies on model's pre-training\n",
    "\n",
    "#### 2Ô∏è‚É£ **Few-Shot Prompting**\n",
    "```\n",
    "Classify sentiment:\n",
    "\n",
    "Review: \"Great product!\" ‚Üí Positive\n",
    "Review: \"Terrible quality\" ‚Üí Negative\n",
    "Review: \"It's okay\" ‚Üí Neutral\n",
    "\n",
    "Review: \"I love this!\" ‚Üí ?\n",
    "```\n",
    "- Provide examples\n",
    "- Model learns pattern\n",
    "- Much better results!\n",
    "\n",
    "#### 3Ô∏è‚É£ **Chain-of-Thought (CoT)**\n",
    "```\n",
    "Problem: If John has 3 apples and buys 5 more, how many does he have?\n",
    "\n",
    "Let's think step by step:\n",
    "1. John starts with 3 apples\n",
    "2. He buys 5 more apples\n",
    "3. Total = 3 + 5 = 8 apples\n",
    "\n",
    "Answer: 8\n",
    "```\n",
    "- Make model show reasoning\n",
    "- Dramatically improves accuracy\n",
    "- Essential for complex tasks\n",
    "\n",
    "#### 4Ô∏è‚É£ **Role Prompting**\n",
    "```\n",
    "You are an expert Python programmer with 10 years of experience.\n",
    "Help me debug this code...\n",
    "```\n",
    "- Define model's role/expertise\n",
    "- Improves output quality\n",
    "- Used in ChatGPT system prompts\n",
    "\n",
    "#### 5Ô∏è‚É£ **Structured Output**\n",
    "```\n",
    "Extract information in JSON format:\n",
    "{\n",
    "  \"name\": \"\",\n",
    "  \"age\": 0,\n",
    "  \"occupation\": \"\"\n",
    "}\n",
    "```\n",
    "- Request specific format\n",
    "- Easier to parse\n",
    "- Critical for applications\n",
    "\n",
    "#### 6Ô∏è‚É£ **Self-Consistency**\n",
    "```\n",
    "Generate 3 different solutions, then choose the most common answer.\n",
    "```\n",
    "- Run same prompt multiple times\n",
    "- Majority voting\n",
    "- Reduces errors\n",
    "\n",
    "### üìä Prompt Engineering Best Practices:\n",
    "\n",
    "‚úÖ **Be Specific**: \"Write a 500-word blog post\" vs \"Write something\"  \n",
    "‚úÖ **Provide Context**: Include relevant background information  \n",
    "‚úÖ **Use Examples**: Few-shot > zero-shot  \n",
    "‚úÖ **Structure Clearly**: Use headings, bullet points, numbering  \n",
    "‚úÖ **Iterate**: Test and refine prompts  \n",
    "‚úÖ **Set Constraints**: \"Answer in 3 sentences\", \"Use bullet points\"  \n",
    "‚úÖ **Define Tone**: \"Explain like I'm 5\", \"Professional tone\"  \n",
    "\n",
    "### üåü Advanced Techniques (2024-2025):\n",
    "\n",
    "**Tree of Thoughts:**\n",
    "- Explore multiple reasoning paths\n",
    "- Self-evaluate and choose best\n",
    "\n",
    "**ReAct (Reason + Act):**\n",
    "- Interleave reasoning with actions\n",
    "- Used in agents and tools\n",
    "\n",
    "**Constitutional AI:**\n",
    "- Define principles for model behavior\n",
    "- Self-critique and improve\n",
    "\n",
    "Let's practice!"
   ]
  },
  {
   "id": "prompt-examples",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Engineering Examples\n",
    "\n",
    "print(\"üé® Prompt Engineering Demonstrations\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# We'll use GPT-2 for demonstration\n",
    "# In production, use GPT-4, Claude, etc. for better results\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', device=0 if device == 'cuda' else -1)\n",
    "\n",
    "# Example 1: Vague vs Specific\n",
    "print(\"\\n1Ô∏è‚É£ VAGUE vs SPECIFIC PROMPTS\\n\")\n",
    "\n",
    "vague_prompt = \"Tell me about AI\"\n",
    "specific_prompt = \"Explain in 3 bullet points how transformers revolutionized natural language processing in 2017-2024.\"\n",
    "\n",
    "print(\"‚ùå Vague Prompt:\")\n",
    "print(f'   \"{vague_prompt}\"')\n",
    "print(\"\\n‚úÖ Specific Prompt:\")\n",
    "print(f'   \"{specific_prompt}\"')\n",
    "print(\"\\nüí° Specific prompts get much better, more focused responses!\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Example 2: Zero-Shot vs Few-Shot\n",
    "print(\"\\n2Ô∏è‚É£ ZERO-SHOT vs FEW-SHOT\\n\")\n",
    "\n",
    "zero_shot = \"Classify: The movie was fantastic!\"\n",
    "\n",
    "few_shot = \"\"\"Classify sentiment:\n",
    "\n",
    "Text: \"I loved it!\" ‚Üí Positive\n",
    "Text: \"Terrible experience\" ‚Üí Negative  \n",
    "Text: \"It was okay\" ‚Üí Neutral\n",
    "\n",
    "Text: \"The movie was fantastic!\" ‚Üí \"\"\"\n",
    "\n",
    "print(\"‚ùå Zero-Shot (no examples):\")\n",
    "print(f'   \"{zero_shot}\"')\n",
    "print(\"\\n‚úÖ Few-Shot (with examples):\")\n",
    "print(f'{few_shot}...')\n",
    "print(\"\\nüí° Examples teach the model the task format!\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Example 3: Chain-of-Thought\n",
    "print(\"\\n3Ô∏è‚É£ CHAIN-OF-THOUGHT REASONING\\n\")\n",
    "\n",
    "direct = \"If a train travels 60 mph for 2.5 hours, how far does it go?\"\n",
    "\n",
    "cot = \"\"\"Solve step by step:\n",
    "\n",
    "Problem: If a train travels 60 mph for 2.5 hours, how far does it go?\n",
    "\n",
    "Step 1: Identify the formula: Distance = Speed √ó Time\n",
    "Step 2: Plug in values: Distance = 60 mph √ó 2.5 hours\n",
    "Step 3: Calculate: Distance = 150 miles\n",
    "\n",
    "Answer: The train travels 150 miles.\"\"\"\n",
    "\n",
    "print(\"‚ùå Direct Answer:\")\n",
    "print(f'   \"{direct}\"')\n",
    "print(\"\\n‚úÖ Chain-of-Thought:\")\n",
    "print(f'{cot}')\n",
    "print(\"\\nüí° CoT forces the model to show its reasoning!\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Example 4: Role Prompting\n",
    "print(\"\\n4Ô∏è‚É£ ROLE PROMPTING\\n\")\n",
    "\n",
    "no_role = \"How do I fix this Python error?\"\n",
    "\n",
    "with_role = \"\"\"You are a senior Python developer with expertise in debugging.\n",
    "\n",
    "A junior developer is getting this error:\n",
    "TypeError: 'int' object is not iterable\n",
    "\n",
    "Explain what causes this error and how to fix it.\"\"\"\n",
    "\n",
    "print(\"‚ùå No Role:\")\n",
    "print(f'   \"{no_role}\"')\n",
    "print(\"\\n‚úÖ With Role:\")\n",
    "print(f'{with_role}')\n",
    "print(\"\\nüí° Defining expertise improves response quality!\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Example 5: Structured Output\n",
    "print(\"\\n5Ô∏è‚É£ STRUCTURED OUTPUT\\n\")\n",
    "\n",
    "unstructured = \"Extract info from: John is 30 years old and works as a data scientist\"\n",
    "\n",
    "structured = \"\"\"Extract information in JSON format:\n",
    "\n",
    "Text: \"John is 30 years old and works as a data scientist\"\n",
    "\n",
    "{\n",
    "  \"name\": \"John\",\n",
    "  \"age\": 30,\n",
    "  \"occupation\": \"data scientist\"\n",
    "}\"\"\"\n",
    "\n",
    "print(\"‚ùå Unstructured Request:\")\n",
    "print(f'   \"{unstructured}\"')\n",
    "print(\"\\n‚úÖ Structured Format:\")\n",
    "print(f'{structured}')\n",
    "print(\"\\nüí° JSON/structured output is easy to parse programmatically!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüåü KEY TAKEAWAY:\")\n",
    "print(\"   Same model + better prompt = 10x better results!\")\n",
    "print(\"   Master prompt engineering before fine-tuning.\")"
   ]
  },
  {
   "id": "openai-api-intro",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîå Using OpenAI API (Production-Ready)\n",
    "\n",
    "**For production applications, use API-based LLMs!**\n",
    "\n",
    "### üéØ Why Use APIs?\n",
    "\n",
    "‚úÖ **No infrastructure**: No GPUs, no maintenance  \n",
    "‚úÖ **Latest models**: GPT-4, GPT-4o, always updated  \n",
    "‚úÖ **Scalable**: From 1 to 1M requests  \n",
    "‚úÖ **Fast**: Optimized inference  \n",
    "‚úÖ **Cost-effective**: Pay only for usage  \n",
    "\n",
    "### üìä OpenAI Models (2024-2025):\n",
    "\n",
    "| Model | Best For | Cost (per 1M tokens) | Context |\n",
    "|-------|----------|---------------------|----------|\n",
    "| **GPT-4 Turbo** | Complex reasoning | $10-30 | 128K |\n",
    "| **GPT-4o** | Fast, multimodal | $5-15 | 128K |\n",
    "| **GPT-3.5 Turbo** | Simple tasks | $0.50-1.50 | 16K |\n",
    "| **Ada-002** | Embeddings | $0.10 | - |\n",
    "\n",
    "### üîë Getting Started:\n",
    "\n",
    "**1. Get API Key:**\n",
    "- Sign up at https://platform.openai.com\n",
    "- Generate API key\n",
    "- Add credits ($5-20 for testing)\n",
    "\n",
    "**2. Install SDK:**\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "**3. Basic Usage:**\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key='your-api-key')\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain RAG systems\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "### üé® Key Features:\n",
    "\n",
    "**1. System Messages:**\n",
    "- Set behavior and personality\n",
    "- Applied to all responses\n",
    "- Example: \"You are an expert Python tutor\"\n",
    "\n",
    "**2. Function Calling:**\n",
    "- LLM can call your functions\n",
    "- Extract structured data\n",
    "- Build agents and tools\n",
    "\n",
    "**3. Streaming:**\n",
    "- Get responses token by token\n",
    "- Better UX (like ChatGPT typing)\n",
    "- Lower perceived latency\n",
    "\n",
    "**4. Vision (GPT-4o):**\n",
    "- Analyze images\n",
    "- Multimodal understanding\n",
    "- OCR, image description, etc.\n",
    "\n",
    "### üí∞ Cost Optimization:\n",
    "\n",
    "**Strategies:**\n",
    "1. Use GPT-3.5 for simple tasks (20x cheaper)\n",
    "2. Cache common responses\n",
    "3. Reduce prompt length (remove unnecessary context)\n",
    "4. Use embeddings for search (100x cheaper than generation)\n",
    "5. Implement rate limiting\n",
    "6. Monitor usage with OpenAI dashboard\n",
    "\n",
    "### üîí Security Best Practices:\n",
    "\n",
    "‚úÖ Never commit API keys to git  \n",
    "‚úÖ Use environment variables  \n",
    "‚úÖ Implement rate limiting  \n",
    "‚úÖ Validate user inputs  \n",
    "‚úÖ Set usage budgets  \n",
    "‚úÖ Monitor for abuse  \n",
    "\n",
    "### üìù Example: Production Chatbot\n",
    "\n",
    "```python\n",
    "# NOTE: This is example code (requires API key)\n",
    "# Uncomment and add your key to test\n",
    "\n",
    "'''\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def chatbot(user_message, history=[]):\n",
    "    \"\"\"\n",
    "    Simple chatbot with conversation history\n",
    "    \"\"\"\n",
    "    # Add system message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}\n",
    "    ]\n",
    "    \n",
    "    # Add conversation history\n",
    "    messages.extend(history)\n",
    "    \n",
    "    # Add new user message\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Get response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    assistant_message = response.choices[0].message.content\n",
    "    \n",
    "    # Update history\n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    \n",
    "    return assistant_message, history\n",
    "\n",
    "# Example conversation\n",
    "conversation_history = []\n",
    "\n",
    "response1, conversation_history = chatbot(\n",
    "    \"What are transformers in AI?\", \n",
    "    conversation_history\n",
    ")\n",
    "\n",
    "response2, conversation_history = chatbot(\n",
    "    \"How do they differ from RNNs?\",\n",
    "    conversation_history\n",
    ")\n",
    "'''\n",
    "```\n",
    "\n",
    "**Note:** The code above is commented out. To use it:\n",
    "1. Get OpenAI API key\n",
    "2. Set environment variable: `export OPENAI_API_KEY='your-key'`\n",
    "3. Uncomment and run!"
   ]
  },
  {
   "id": "exercises",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercises\n",
    "\n",
    "**Challenge Yourself!**"
   ]
  },
  {
   "id": "exercise-1",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build a Custom RAG System\n",
    "\n",
    "**Task:** Create a RAG system for your favorite topic\n",
    "\n",
    "**Requirements:**\n",
    "1. Create 10+ documents about a topic you care about\n",
    "2. Build embeddings and vector database\n",
    "3. Implement search function\n",
    "4. Test with 3 different questions\n",
    "\n",
    "**Topics ideas:**\n",
    "- Your favorite book/movie series\n",
    "- A programming language\n",
    "- A hobby or sport\n",
    "- Historical events\n",
    "\n",
    "**Bonus:** Add re-ranking or hybrid search!"
   ]
  },
  {
   "id": "exercise-1-solution",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# TODO: Create your custom RAG system\n",
    "\n",
    "# 1. Define your documents\n",
    "my_documents = [\n",
    "    # Add your 10+ documents here\n",
    "]\n",
    "\n",
    "# 2. Create embeddings\n",
    "# (Use the embedding_model from earlier)\n",
    "\n",
    "# 3. Build vector database\n",
    "# (Use FAISS like in the example)\n",
    "\n",
    "# 4. Implement search\n",
    "def my_search_function(query):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# 5. Test with questions\n",
    "test_questions = [\n",
    "    # Your questions here\n",
    "]\n",
    "\n",
    "print(\"Complete the exercise above!\")"
   ]
  },
  {
   "id": "exercise-2",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Prompt Engineering Challenge\n",
    "\n",
    "**Task:** Improve this prompt to get better results\n",
    "\n",
    "**Bad Prompt:**\n",
    "```\n",
    "Write code\n",
    "```\n",
    "\n",
    "**Your Mission:**\n",
    "1. Rewrite it using prompt engineering best practices\n",
    "2. Make it specific and clear\n",
    "3. Include examples if helpful\n",
    "4. Define output format\n",
    "\n",
    "**Requirements to include:**\n",
    "- Language (Python)\n",
    "- Task (e.g., read CSV file)\n",
    "- Constraints (error handling, comments)\n",
    "- Output format (code + explanation)\n",
    "\n",
    "**Compare:** Generate with both prompts and see the difference!"
   ]
  },
  {
   "id": "exercise-2-solution",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "bad_prompt = \"Write code\"\n",
    "\n",
    "# TODO: Write your improved prompt\n",
    "improved_prompt = \"\"\"\n",
    "Your improved prompt here...\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚ùå Bad Prompt:\")\n",
    "print(bad_prompt)\n",
    "print(\"\\n‚úÖ Improved Prompt:\")\n",
    "print(improved_prompt)\n",
    "\n",
    "# TODO: Test both prompts and compare results\n",
    "# (You can use GPT-2 or comment for later testing with GPT-4)"
   ]
  },
  {
   "id": "key-takeaways",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Key Takeaways\n",
    "\n",
    "**Congratulations! You've mastered modern LLM applications!**\n",
    "\n",
    "### 1Ô∏è‚É£ **HuggingFace Ecosystem**\n",
    "   - ‚úÖ Access 100,000+ models with simple API\n",
    "   - ‚úÖ Pipelines for common tasks (generation, classification, QA)\n",
    "   - ‚úÖ Easy to download, use, and share models\n",
    "   - **Use when:** Building any NLP application\n",
    "\n",
    "### 2Ô∏è‚É£ **Fine-Tuning**\n",
    "   - ‚úÖ Adapt pre-trained models to your domain\n",
    "   - ‚úÖ LoRA makes it efficient (100x fewer parameters)\n",
    "   - ‚úÖ Improves performance on specific tasks\n",
    "   - **Use when:** Generic models aren't good enough\n",
    "\n",
    "### 3Ô∏è‚É£ **RAG Systems**\n",
    "   - ‚úÖ Combine retrieval + generation\n",
    "   - ‚úÖ Reduces hallucinations with grounded facts\n",
    "   - ‚úÖ Always up-to-date (add new docs anytime)\n",
    "   - ‚úÖ Provides source citations\n",
    "   - **Use when:** Building chatbots, search, Q&A\n",
    "\n",
    "### 4Ô∏è‚É£ **Prompt Engineering**\n",
    "   - ‚úÖ Most cost-effective improvement (free!)\n",
    "   - ‚úÖ Techniques: Few-shot, CoT, role prompting, structured output\n",
    "   - ‚úÖ Can improve results 10-100x\n",
    "   - **Use when:** ALWAYS! Try better prompts before fine-tuning\n",
    "\n",
    "### 5Ô∏è‚É£ **Production APIs**\n",
    "   - ‚úÖ OpenAI, Anthropic, Google for state-of-the-art\n",
    "   - ‚úÖ No infrastructure needed\n",
    "   - ‚úÖ Pay per use\n",
    "   - **Use when:** Building production applications\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Real-World Impact\n",
    "\n",
    "**Skills you can apply immediately:**\n",
    "\n",
    "### üíº **Career Skills**\n",
    "- Build AI-powered applications\n",
    "- Create custom chatbots for businesses\n",
    "- Implement RAG for knowledge bases\n",
    "- Fine-tune models for specific domains\n",
    "- Prompt engineering for better outputs\n",
    "\n",
    "### üèóÔ∏è **Projects You Can Build**\n",
    "\n",
    "**1. Customer Support Bot**\n",
    "- RAG over company documentation\n",
    "- Answer customer questions\n",
    "- Provide source citations\n",
    "\n",
    "**2. Code Assistant**\n",
    "- Fine-tune on your codebase\n",
    "- RAG over documentation\n",
    "- Generate code with context\n",
    "\n",
    "**3. Research Assistant**\n",
    "- RAG over academic papers\n",
    "- Summarize findings\n",
    "- Answer domain questions\n",
    "\n",
    "**4. Content Generator**\n",
    "- Fine-tune for your brand voice\n",
    "- Generate blog posts, emails\n",
    "- Maintain consistency\n",
    "\n",
    "**5. Semantic Search Engine**\n",
    "- Embed all documents\n",
    "- Find by meaning, not keywords\n",
    "- Better than traditional search\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Decision Framework\n",
    "\n",
    "**When to use what?**\n",
    "\n",
    "| Need | Solution | Why |\n",
    "|------|----------|-----|\n",
    "| **Up-to-date info** | RAG | Can update docs anytime |\n",
    "| **Custom tone/style** | Fine-tuning | Learn your writing style |\n",
    "| **Better outputs** | Prompt engineering | Free, fast, effective |\n",
    "| **Private data** | Open-source + RAG | Keep data internal |\n",
    "| **Production scale** | API (GPT-4, Claude) | Reliable, maintained |\n",
    "| **Complex reasoning** | GPT-4 + CoT prompts | Best model + technique |\n",
    "| **Cost-sensitive** | GPT-3.5 + caching | Cheaper models |\n",
    "| **Multilingual** | Modern LLMs | All support 100+ languages |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Best Practices (2024-2025)\n",
    "\n",
    "**1. Start Simple**\n",
    "- Try prompt engineering first (free!)\n",
    "- Then RAG if you need knowledge\n",
    "- Fine-tune only if necessary\n",
    "\n",
    "**2. Combine Techniques**\n",
    "- RAG + prompt engineering = powerful\n",
    "- Fine-tuned model + RAG = best of both\n",
    "- Use right tool for each part\n",
    "\n",
    "**3. Monitor and Iterate**\n",
    "- Track accuracy, cost, latency\n",
    "- A/B test different approaches\n",
    "- Continuously improve prompts\n",
    "\n",
    "**4. Think About Users**\n",
    "- Provide sources (RAG)\n",
    "- Handle errors gracefully\n",
    "- Set clear expectations\n",
    "- Collect feedback\n",
    "\n",
    "**5. Security & Privacy**\n",
    "- Don't send sensitive data to APIs\n",
    "- Use local models for private data\n",
    "- Implement access controls\n",
    "- Monitor for misuse\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**Continue Learning:**\n",
    "\n",
    "1. **Build Projects**\n",
    "   - Best way to learn is by building!\n",
    "   - Start with simple chatbot\n",
    "   - Add RAG, then fine-tuning\n",
    "\n",
    "2. **Explore Tools**\n",
    "   - LangChain: Framework for LLM apps\n",
    "   - LlamaIndex: Data framework for RAG\n",
    "   - Pinecone/Weaviate: Vector databases\n",
    "   - Streamlit: Quick UIs for demos\n",
    "\n",
    "3. **Stay Updated**\n",
    "   - Follow HuggingFace releases\n",
    "   - Read OpenAI/Anthropic blogs\n",
    "   - Join AI communities\n",
    "   - Try new models as they release\n",
    "\n",
    "4. **Practice Prompt Engineering**\n",
    "   - Daily practice with ChatGPT/Claude\n",
    "   - Study prompt libraries\n",
    "   - Share and learn from others\n",
    "\n",
    "---\n",
    "\n",
    "**üí¨ Final Thoughts:**\n",
    "\n",
    "*\"You now have the skills to build production-grade AI applications! RAG systems power most AI chatbots you interact with daily. Fine-tuning customizes models for specific needs. Prompt engineering gets the best from any LLM. Together, these skills make you job-ready for AI roles in 2024-2025.\"*\n",
    "\n",
    "**üéâ You've completed Week 16: Transformers & Attention!**\n",
    "\n",
    "**What you've mastered:**\n",
    "- Day 1: Attention mechanisms (the foundation)\n",
    "- Day 2: Transformer architecture (GPT, BERT, T5)\n",
    "- Day 3: Modern LLM applications (fine-tuning, RAG, APIs)\n",
    "\n",
    "**You now understand the technology behind:**\n",
    "- ChatGPT, Claude, Gemini (LLM architectures)\n",
    "- Every AI chatbot (RAG systems)\n",
    "- Custom AI assistants (fine-tuning)\n",
    "- Production AI apps (APIs and best practices)\n",
    "\n",
    "**üöÄ You're ready to build the next generation of AI applications!**\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Additional Resources:**\n",
    "- HuggingFace Course: https://huggingface.co/learn\n",
    "- OpenAI Cookbook: https://github.com/openai/openai-cookbook\n",
    "- LangChain Docs: https://docs.langchain.com\n",
    "- RAG Papers: \"Retrieval-Augmented Generation\" (Lewis et al.)\n",
    "- Prompt Engineering Guide: https://www.promptingguide.ai\n",
    "\n",
    "**Keep building, keep learning! üåü**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
