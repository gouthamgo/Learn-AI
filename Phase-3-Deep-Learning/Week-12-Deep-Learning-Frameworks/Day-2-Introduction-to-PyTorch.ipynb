{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Day 2: Introduction to PyTorch\n",
    "\n",
    "**üéØ Goal:** Master PyTorch for building deep learning models\n",
    "\n",
    "**‚è±Ô∏è Time:** 60-90 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- PyTorch powers cutting-edge AI research: GPT, DALL-E, Meta's LLaMA, Tesla Autopilot\n",
    "- Preferred by researchers and AI labs (OpenAI, Meta, Tesla, DeepMind)\n",
    "- More Pythonic and intuitive than TensorFlow\n",
    "- Dynamic computation graphs = easier debugging and experimentation\n",
    "\n",
    "**üî• 2024-2025 AI Trends:**\n",
    "- Most transformer models (GPT, BERT, LLaMA) are built with PyTorch\n",
    "- PyTorch dominates research papers and new AI innovations\n",
    "- HuggingFace (largest AI model hub) uses PyTorch\n",
    "- Essential for fine-tuning LLMs and building RAG systems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ What is PyTorch?\n",
    "\n",
    "**PyTorch** = Meta's (Facebook's) deep learning framework\n",
    "\n",
    "**Why PyTorch is Special:**\n",
    "- üêç **Pythonic**: Feels like natural Python code\n",
    "- üîß **Dynamic**: Change model structure on the fly\n",
    "- üîç **Debuggable**: Use standard Python debuggers\n",
    "- üöÄ **Research-friendly**: Easy to experiment with new ideas\n",
    "\n",
    "**Real-World Uses:**\n",
    "- ChatGPT and GPT-4 (OpenAI)\n",
    "- LLaMA and Llama 2 (Meta)\n",
    "- Tesla Autopilot (computer vision)\n",
    "- Midjourney (image generation)\n",
    "\n",
    "**TensorFlow vs PyTorch:**\n",
    "- TensorFlow: Better for production, mobile, web deployment\n",
    "- PyTorch: Better for research, experimentation, prototyping\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (run this once)\n",
    "!pip install torch torchvision torchaudio numpy matplotlib scikit-learn\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"‚úÖ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"üíª Using CPU (GPU not available)\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nüéØ Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Section 1: PyTorch Tensors\n",
    "\n",
    "**Tensors** = The fundamental data structure in PyTorch (like NumPy arrays but GPU-enabled)\n",
    "\n",
    "**Key Difference from NumPy:**\n",
    "- PyTorch tensors can run on GPU\n",
    "- Automatic differentiation (autograd) for backpropagation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors\n",
    "\n",
    "# From Python list\n",
    "tensor_from_list = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(f\"From list: {tensor_from_list}\")\n",
    "print(f\"Data type: {tensor_from_list.dtype}\\n\")\n",
    "\n",
    "# 2D tensor (matrix)\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "print(f\"Matrix:\\n{matrix}\")\n",
    "print(f\"Shape: {matrix.shape}\\n\")\n",
    "\n",
    "# From NumPy array\n",
    "numpy_array = np.array([1, 2, 3])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(f\"From NumPy: {tensor_from_numpy}\\n\")\n",
    "\n",
    "# Special tensors\n",
    "zeros = torch.zeros(2, 3)  # 2x3 tensor of zeros\n",
    "ones = torch.ones(2, 3)    # 2x3 tensor of ones\n",
    "random = torch.randn(2, 3) # 2x3 tensor with random values\n",
    "\n",
    "print(f\"Zeros:\\n{zeros}\\n\")\n",
    "print(f\"Ones:\\n{ones}\\n\")\n",
    "print(f\"Random:\\n{random}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¢ Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic operations\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Addition\n",
    "print(f\"Addition: {a + b}\")\n",
    "\n",
    "# Multiplication (element-wise)\n",
    "print(f\"Multiplication: {a * b}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "mat1 = torch.tensor([[1, 2], [3, 4]])\n",
    "mat2 = torch.tensor([[5, 6], [7, 8]])\n",
    "print(f\"\\nMatrix multiplication:\\n{torch.matmul(mat1, mat2)}\")\n",
    "# Or use @ operator\n",
    "print(f\"\\nUsing @ operator:\\n{mat1 @ mat2}\")\n",
    "\n",
    "# Reshaping\n",
    "x = torch.randn(6)\n",
    "print(f\"\\nOriginal shape: {x.shape}\")\n",
    "x_reshaped = x.view(2, 3)  # Reshape to 2x3\n",
    "print(f\"Reshaped:\\n{x_reshaped}\")\n",
    "print(f\"New shape: {x_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° GPU Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensors to GPU (if available)\n",
    "cpu_tensor = torch.randn(3, 3)\n",
    "print(f\"CPU Tensor:\\n{cpu_tensor}\")\n",
    "print(f\"Device: {cpu_tensor.device}\\n\")\n",
    "\n",
    "# Move to GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tensor = cpu_tensor.to('cuda')\n",
    "    print(f\"GPU Tensor:\\n{gpu_tensor}\")\n",
    "    print(f\"Device: {gpu_tensor.device}\")\n",
    "else:\n",
    "    print(\"GPU not available, staying on CPU\")\n",
    "\n",
    "# Recommended: Use device variable\n",
    "tensor = torch.randn(3, 3).to(device)\n",
    "print(f\"\\nTensor on {device}:\\n{tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Section 2: Autograd - Automatic Differentiation\n",
    "\n",
    "**Autograd** = PyTorch's automatic differentiation engine\n",
    "\n",
    "**Why This Matters:**\n",
    "- Automatically computes gradients for backpropagation\n",
    "- No need to manually calculate derivatives\n",
    "- Core of deep learning training\n",
    "\n",
    "**How it works:**\n",
    "1. Set `requires_grad=True` on tensors you want to track\n",
    "2. Perform operations\n",
    "3. Call `.backward()` to compute gradients\n",
    "4. Access gradients with `.grad`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Computing gradients\n",
    "\n",
    "# Create a tensor and enable gradient tracking\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "\n",
    "# Define a function: y = x^2 + 2x + 1\n",
    "y = x**2 + 2*x + 1\n",
    "print(f\"y = x^2 + 2x + 1 = {y}\")\n",
    "\n",
    "# Compute gradients (dy/dx)\n",
    "y.backward()\n",
    "\n",
    "# Access gradient: dy/dx = 2x + 2 = 2(2) + 2 = 6\n",
    "print(f\"\\nGradient dy/dx at x=2: {x.grad}\")\n",
    "print(f\"Expected: 2x + 2 = 2(2) + 2 = 6 ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Neural network gradient\n",
    "\n",
    "# Weights and biases\n",
    "w = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Input\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Forward pass: y = w¬∑x + b\n",
    "y = torch.dot(w, x) + b\n",
    "print(f\"Output y = {y}\")\n",
    "\n",
    "# Backward pass\n",
    "y.backward()\n",
    "\n",
    "# Gradients\n",
    "print(f\"\\nGradient dw: {w.grad}\")\n",
    "print(f\"Gradient db: {b.grad}\")\n",
    "print(\"\\nüß† These gradients are used to update weights during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Section 3: Building Models with nn.Module\n",
    "\n",
    "**nn.Module** = Base class for all neural network modules in PyTorch\n",
    "\n",
    "**Structure:**\n",
    "1. `__init__()`: Define layers\n",
    "2. `forward()`: Define forward pass (how data flows)\n",
    "\n",
    "**PyTorch automatically handles:**\n",
    "- Backward pass (gradient computation)\n",
    "- Parameter management\n",
    "- GPU/CPU transfers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple Neural Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) # Second hidden layer\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size) # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        x = F.relu(self.fc1(x))  # Hidden layer 1 + ReLU\n",
    "        x = F.relu(self.fc2(x))  # Hidden layer 2 + ReLU\n",
    "        x = self.fc3(x)          # Output layer (no activation)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleNN(input_size=10, hidden_size=64, output_size=1)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with random input\n",
    "sample_input = torch.randn(5, 10)  # Batch of 5 samples, 10 features each\n",
    "output = model(sample_input)\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nOutput:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñºÔ∏è Example 2: CNN for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 1 input channel, 32 output\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 32 input, 64 output\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # 64 input, 128 output\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 128)  # After 3 pooling: 28->14->7->3\n",
    "        self.fc2 = nn.Linear(128, 10)           # 10 classes\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Conv block 3\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 128*3*3)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = CNN().to(device)\n",
    "print(cnn_model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in cnn_model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Section 4: Training Loop in PyTorch\n",
    "\n",
    "**PyTorch Training Loop** = You write the loop explicitly (unlike Keras's `.fit()`)\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Full control over training process\n",
    "- ‚úÖ Easy to customize\n",
    "- ‚úÖ Better for research and experimentation\n",
    "\n",
    "**Standard Training Loop:**\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # 1. Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 2. Backward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update weights\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: Train on synthetic data\n",
    "\n",
    "# Generate synthetic data\n",
    "X_train = torch.randn(1000, 10).to(device)\n",
    "y_train = torch.randint(0, 2, (1000,)).to(device)  # Binary classification\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create model, loss, optimizer\n",
    "model = SimpleNN(10, 64, 2).to(device)  # 2 output classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # 1. Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # 2. Backward pass\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update weights\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    # Print epoch stats\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Section 5: REAL AI EXAMPLE - Text Classifier\n",
    "\n",
    "**Project:** Build a sentiment classifier for movie reviews\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Social media sentiment analysis\n",
    "- Customer review classification\n",
    "- Brand monitoring\n",
    "- Market sentiment prediction\n",
    "\n",
    "**Dataset:** IMDB Movie Reviews (25,000 reviews)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll simulate text data with embeddings\n",
    "# In real applications, you'd use libraries like torchtext or HuggingFace\n",
    "\n",
    "# Simulated text embeddings (pretend we already embedded text)\n",
    "# Real dimensions: (num_samples, sequence_length, embedding_dim)\n",
    "train_samples = 10000\n",
    "test_samples = 2000\n",
    "sequence_length = 200  # Max words in review\n",
    "embedding_dim = 100    # Word embedding dimension\n",
    "\n",
    "# Generate synthetic data (in reality, these would be real text embeddings)\n",
    "X_train_text = torch.randn(train_samples, sequence_length, embedding_dim)\n",
    "y_train_text = torch.randint(0, 2, (train_samples,))  # 0=negative, 1=positive\n",
    "\n",
    "X_test_text = torch.randn(test_samples, sequence_length, embedding_dim)\n",
    "y_test_text = torch.randint(0, 2, (test_samples,))\n",
    "\n",
    "print(f\"Training data shape: {X_train_text.shape}\")\n",
    "print(f\"Training labels shape: {y_train_text.shape}\")\n",
    "print(f\"\\nEach review: {sequence_length} words, each word: {embedding_dim}-dim embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Text Classifier with LSTM\n",
    "class TextClassifierLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_layers=2, dropout=0.5):\n",
    "        super(TextClassifierLSTM, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # LSTM output\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Use the last hidden state\n",
    "        hidden = hidden[-1, :, :]  # Shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        # Dropout and fully connected\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc(hidden)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create model\n",
    "text_model = TextClassifierLSTM(\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    output_dim=2,  # 2 classes: positive/negative\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(\"üìù Text Classifier Architecture:\")\n",
    "print(text_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in text_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "train_dataset = TensorDataset(X_train_text, y_train_text)\n",
    "test_dataset = TensorDataset(X_test_text, y_test_text)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(text_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"‚úÖ Data loaders ready!\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with validation\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=5):\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += batch_y.size(0)\n",
    "            train_correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():  # No gradient computation during validation\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += batch_y.size(0)\n",
    "                test_correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    return train_losses, test_accuracies\n",
    "\n",
    "# Train the model\n",
    "print(\"üöÄ Training Text Classifier...\\n\")\n",
    "losses, accuracies = train_model(text_model, train_loader, test_loader, criterion, optimizer, num_epochs=5)\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, marker='o')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies, marker='o', color='green')\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Section 6: TensorFlow vs PyTorch Comparison\n",
    "\n",
    "Let's implement the SAME model in both frameworks!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TensorFlow vs PyTorch: Simple CNN Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PyTorch Version\n",
    "print(\"\\nüî• PyTorch Version:\")\n",
    "print(\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64*5*5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop (you write it!)\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\"\"\")\n",
    "\n",
    "# TensorFlow Version\n",
    "print(\"\\nüîµ TensorFlow Version:\")\n",
    "print(\"\"\"\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training (one line!)\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Key Differences:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "PyTorch:\n",
    "‚úÖ More control over training loop\n",
    "‚úÖ Pythonic and intuitive\n",
    "‚úÖ Better for research and experimentation\n",
    "‚úÖ Dynamic computation graphs\n",
    "‚úÖ Easier debugging (use Python debugger)\n",
    "\n",
    "TensorFlow/Keras:\n",
    "‚úÖ Simpler API (model.fit())\n",
    "‚úÖ Better for production deployment\n",
    "‚úÖ TensorFlow Lite for mobile\n",
    "‚úÖ TensorFlow.js for web\n",
    "‚úÖ Better documentation and tutorials\n",
    "\n",
    "üéØ Recommendation:\n",
    "- Research & Prototyping ‚Üí PyTorch\n",
    "- Production & Deployment ‚Üí TensorFlow\n",
    "- Learn BOTH for maximum flexibility!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercise 1: Build Your Own PyTorch Model\n",
    "\n",
    "**Challenge:** Build a CNN to classify MNIST digits using PyTorch\n",
    "\n",
    "**Requirements:**\n",
    "1. Use torchvision to load MNIST\n",
    "2. Build a CNN with 2 conv layers\n",
    "3. Train for 3 epochs\n",
    "4. Report test accuracy\n",
    "\n",
    "**Starter Code Below** üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: MNIST Classifier in PyTorch\n",
    "\n",
    "# Step 1: Load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Step 2: Build your CNN\n",
    "# TODO: Define your CNN class here\n",
    "\n",
    "# Step 3: Create model, loss, optimizer\n",
    "# TODO: Your code here\n",
    "\n",
    "# Step 4: Train the model\n",
    "# TODO: Your code here\n",
    "\n",
    "# Step 5: Evaluate\n",
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution to Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: MNIST Classifier\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "mnist_model = MNISTNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mnist_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "print(\"üöÄ Training MNIST model...\\n\")\n",
    "for epoch in range(3):\n",
    "    mnist_model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = mnist_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/3], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "mnist_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mnist_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"\\nüìä Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercise 2: Compare TensorFlow vs PyTorch\n",
    "\n",
    "**Challenge:** Implement the SAME model in both frameworks and compare:\n",
    "1. Code simplicity\n",
    "2. Training speed\n",
    "3. Final accuracy\n",
    "\n",
    "**Task:** Choose one and explain why you prefer it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your comparison notes\n",
    "print(\"\"\"\n",
    "My Framework Comparison:\n",
    "========================\n",
    "\n",
    "TensorFlow:\n",
    "- Pros: ___________\n",
    "- Cons: ___________\n",
    "- Use when: ___________\n",
    "\n",
    "PyTorch:\n",
    "- Pros: ___________\n",
    "- Cons: ___________\n",
    "- Use when: ___________\n",
    "\n",
    "My Preference: ___________\n",
    "Reason: ___________\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Saving and Loading Models in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Save entire model\n",
    "torch.save(mnist_model, 'mnist_model.pth')\n",
    "print(\"‚úÖ Model saved as 'mnist_model.pth'\")\n",
    "\n",
    "# Method 2: Save only state dict (recommended)\n",
    "torch.save(mnist_model.state_dict(), 'mnist_model_state.pth')\n",
    "print(\"‚úÖ State dict saved as 'mnist_model_state.pth'\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = MNISTNet().to(device)\n",
    "loaded_model.load_state_dict(torch.load('mnist_model_state.pth'))\n",
    "loaded_model.eval()\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- ‚úÖ PyTorch tensors and operations\n",
    "- ‚úÖ Autograd for automatic differentiation\n",
    "- ‚úÖ Building models with nn.Module\n",
    "- ‚úÖ Writing custom training loops\n",
    "- ‚úÖ Building CNN and LSTM models\n",
    "- ‚úÖ Text classification with PyTorch\n",
    "- ‚úÖ TensorFlow vs PyTorch comparison\n",
    "\n",
    "**üî• Real-World Skills:**\n",
    "- Build production-ready PyTorch models\n",
    "- Implement custom architectures for research\n",
    "- Fine-tune pre-trained models (like GPT, BERT)\n",
    "- Deploy models with TorchServe\n",
    "\n",
    "**üéØ Practice Challenges:**\n",
    "1. Build a ResNet-style model with skip connections\n",
    "2. Implement a Transformer encoder block\n",
    "3. Create a custom loss function\n",
    "4. Add learning rate scheduling\n",
    "5. Implement early stopping\n",
    "\n",
    "**üî• 2024-2025 Trends:**\n",
    "- Fine-tuning LLMs with PyTorch\n",
    "- Building RAG systems with PyTorch embeddings\n",
    "- Multimodal AI with CLIP (PyTorch)\n",
    "- Deploying models with HuggingFace\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Next Lesson:** Day 3 - Deep Learning Project (End-to-end Fashion-MNIST classifier!)\n",
    "\n",
    "**üí¨ Questions?** Experiment with different architectures, optimizers, and hyperparameters!\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: PyTorch powers the latest AI breakthroughs - GPT, LLaMA, Stable Diffusion, and more!* üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
