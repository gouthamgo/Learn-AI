{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Day 1: Introduction to Recurrent Neural Networks (RNNs)\n",
    "\n",
    "**ğŸ¯ Goal:** Understand RNNs and build a stock price predictor\n",
    "\n",
    "**â±ï¸ Time:** 60-75 minutes\n",
    "\n",
    "**ğŸŒŸ Why This Matters for AI:**\n",
    "- RNNs are the foundation of sequence modeling (ChatGPT's predecessor!)\n",
    "- Understanding RNNs helps you grasp how Transformers work (GPT, Claude, BERT)\n",
    "- Used in time series forecasting, speech recognition, and language models\n",
    "- Agentic AI uses sequence models to understand conversation history and context\n",
    "- RAG systems use sequence models to understand document context and relationships\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ğŸ§  What are Recurrent Neural Networks?\n",
    "\n",
    "**Recurrent Neural Networks (RNNs)** are neural networks designed to process **sequential data** - data where order matters!\n",
    "\n",
    "### ğŸ”„ The Key Difference:\n",
    "\n",
    "**Regular Neural Networks:**\n",
    "- Process each input independently\n",
    "- No memory of previous inputs\n",
    "- Example: Image classification (one image at a time)\n",
    "\n",
    "**Recurrent Neural Networks:**\n",
    "- Process sequences of inputs\n",
    "- Maintain **hidden state** (memory!) across time steps\n",
    "- Example: Language translation, stock prediction, speech recognition\n",
    "\n",
    "```\n",
    "FEEDFORWARD NN:           RECURRENT NN:\n",
    "==================        ==================\n",
    "Input â†’ Output            Inputâ‚ â†’ Outputâ‚\n",
    "                               â†“  (memory)\n",
    "                          Inputâ‚‚ â†’ Outputâ‚‚\n",
    "                               â†“  (memory)\n",
    "                          Inputâ‚ƒ â†’ Outputâ‚ƒ\n",
    "\n",
    "No memory                 Has memory!\n",
    "```\n",
    "\n",
    "### ğŸ¯ Real-World RNN Applications (2024-2025):\n",
    "\n",
    "1. **Language Models (Pre-Transformer Era)**\n",
    "   - Early versions of GPT used RNN-like architectures\n",
    "   - Text generation, translation, summarization\n",
    "   \n",
    "2. **Speech Recognition**\n",
    "   - Siri, Alexa, Google Assistant use RNN/LSTM variants\n",
    "   - Process audio sequences over time\n",
    "   \n",
    "3. **Time Series Forecasting**\n",
    "   - Stock prices, weather, energy consumption\n",
    "   - Sales forecasting for businesses\n",
    "   \n",
    "4. **Agentic AI Context**\n",
    "   - Agents use RNN concepts to remember conversation history\n",
    "   - Understand multi-turn dialogues\n",
    "   \n",
    "5. **RAG Systems**\n",
    "   - Sequential processing of document chunks\n",
    "   - Understanding context across paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## ğŸ“Š Sequential Data & Time Series\n",
    "\n",
    "### ğŸ¯ What is Sequential Data?\n",
    "\n",
    "**Sequential data** is data where the **order matters**!\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "1. **Text/Language** ğŸ“\n",
    "   - \"I love AI\" â‰  \"AI love I\" (order matters!)\n",
    "   - Each word depends on previous words\n",
    "   - Used in: ChatGPT, Claude, translation\n",
    "\n",
    "2. **Time Series** â°\n",
    "   - Stock prices over time\n",
    "   - Weather measurements (temperature, rainfall)\n",
    "   - Sensor data from IoT devices\n",
    "\n",
    "3. **Audio/Speech** ğŸµ\n",
    "   - Sound waves over time\n",
    "   - Music generation\n",
    "   - Voice assistants\n",
    "\n",
    "4. **Video** ğŸ¥\n",
    "   - Sequence of frames\n",
    "   - Action recognition\n",
    "   - Multimodal AI (GPT-4V)\n",
    "\n",
    "5. **User Behavior** ğŸ–±ï¸\n",
    "   - Clickstream data\n",
    "   - Purchase history\n",
    "   - Recommendation systems\n",
    "\n",
    "### ğŸ“ˆ Time Series Characteristics:\n",
    "\n",
    "```\n",
    "Time: tâ‚  tâ‚‚  tâ‚ƒ  tâ‚„  tâ‚…  tâ‚†  tâ‚‡\n",
    "Data: 10  12  15  14  16  18  20\n",
    "      â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n",
    "      Sequential relationship!\n",
    "\n",
    "Key Properties:\n",
    "- Trend: Long-term increase/decrease\n",
    "- Seasonality: Repeating patterns\n",
    "- Noise: Random fluctuations\n",
    "- Autocorrelation: Value depends on past values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ RNN Architecture & Hidden States\n",
    "\n",
    "### ğŸ”§ Core Components:\n",
    "\n",
    "#### 1ï¸âƒ£ **Hidden State (Memory)**\n",
    "The hidden state is the \"memory\" that gets passed from one time step to the next!\n",
    "\n",
    "```\n",
    "hâ‚€ = initial state (usually zeros)\n",
    "hâ‚ = f(xâ‚, hâ‚€)  â† Uses current input + previous state\n",
    "hâ‚‚ = f(xâ‚‚, hâ‚)  â† Uses current input + previous state\n",
    "hâ‚ƒ = f(xâ‚ƒ, hâ‚‚)  â† Pattern continues...\n",
    "```\n",
    "\n",
    "#### 2ï¸âƒ£ **RNN Cell Architecture:**\n",
    "\n",
    "```\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "h_{t-1} â”‚         â”‚ h_t\n",
    "â”€â”€â”€â”€â”€â”€â”€>â”‚  RNN    â”‚â”€â”€â”€â”€â”€â”€â”€â”€>\n",
    "        â”‚  Cell   â”‚\n",
    "x_t â”€â”€â”€>â”‚         â”‚â”€â”€â”€> y_t\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Mathematical Formula:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "h_t = tanh(W_hh Â· h_{t-1} + W_xh Â· x_t + b_h)\n",
    "y_t = W_hy Â· h_t + b_y\n",
    "\n",
    "Where:\n",
    "- x_t = input at time t\n",
    "- h_t = hidden state at time t\n",
    "- y_t = output at time t\n",
    "- W = weight matrices\n",
    "- b = bias vectors\n",
    "```\n",
    "\n",
    "#### 3ï¸âƒ£ **Unfolded RNN (Over Time):**\n",
    "\n",
    "```\n",
    "t=1         t=2         t=3         t=4\n",
    "â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”\n",
    "â”‚RNNâ”‚      â”‚RNNâ”‚      â”‚RNNâ”‚      â”‚RNNâ”‚\n",
    "â””â”€â”€â”€â”˜      â””â”€â”€â”€â”˜      â””â”€â”€â”€â”˜      â””â”€â”€â”€â”˜\n",
    "  â†“          â†“          â†“          â†“\n",
    " yâ‚         yâ‚‚         yâ‚ƒ         yâ‚„\n",
    "  â†‘          â†‘          â†‘          â†‘\n",
    " xâ‚ â”€â”€â”€â†’ hâ‚ â†’ xâ‚‚ â”€â”€â”€â†’ hâ‚‚ â†’ xâ‚ƒ â”€â”€â”€â†’ hâ‚ƒ â†’ xâ‚„\n",
    "\n",
    "Same weights shared across all time steps!\n",
    "```\n",
    "\n",
    "### ğŸ¯ Key Insight:\n",
    "**RNNs share the same weights across all time steps!** This is called **parameter sharing** - it allows RNNs to generalize to sequences of any length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## âš ï¸ The Vanishing Gradient Problem\n",
    "\n",
    "### ğŸ› The Problem:\n",
    "\n",
    "When training RNNs with backpropagation through time (BPTT), gradients can become **extremely small** as they propagate backwards through many time steps.\n",
    "\n",
    "```\n",
    "FORWARD PASS:\n",
    "xâ‚ â†’ hâ‚ â†’ hâ‚‚ â†’ hâ‚ƒ â†’ hâ‚„ â†’ hâ‚… â†’ yâ‚…\n",
    "\n",
    "BACKWARD PASS (Gradient Flow):\n",
    "âˆ‚L/âˆ‚yâ‚… â†’ âˆ‚L/âˆ‚hâ‚… â†’ âˆ‚L/âˆ‚hâ‚„ â†’ âˆ‚L/âˆ‚hâ‚ƒ â†’ âˆ‚L/âˆ‚hâ‚‚ â†’ âˆ‚L/âˆ‚hâ‚\n",
    "           â†“         â†“         â†“         â†“\n",
    "        Small     Smaller   Tiny     Vanishes!\n",
    "\n",
    "Problem: Gradients get multiplied by small numbers repeatedly\n",
    "Result: Network can't learn long-term dependencies!\n",
    "```\n",
    "\n",
    "### ğŸ“‰ Why It Happens:\n",
    "\n",
    "1. **Repeated multiplication by weights < 1**\n",
    "   - If weight = 0.9, after 10 steps: 0.9Â¹â° â‰ˆ 0.35\n",
    "   - After 50 steps: 0.9âµâ° â‰ˆ 0.005 (tiny!)\n",
    "\n",
    "2. **Activation function derivatives (tanh, sigmoid)**\n",
    "   - tanh'(x) < 1 for all x\n",
    "   - Gradients shrink with each time step\n",
    "\n",
    "### ğŸ¯ Consequences:\n",
    "\n",
    "- **Can't learn long-term dependencies**\n",
    "  - Example: \"The cat, which was very hungry and had been wandering the streets all day, **ate**\"\n",
    "  - RNN struggles to connect \"cat\" (far back) with \"ate\" (current word)\n",
    "\n",
    "- **Training becomes unstable**\n",
    "  - Gradients â†’ 0 (vanishing) or â†’ âˆ (exploding)\n",
    "\n",
    "### âœ… Solutions (Coming in Day 2!):\n",
    "\n",
    "1. **LSTM (Long Short-Term Memory)**\n",
    "   - Gating mechanisms prevent vanishing gradients\n",
    "   - Can remember information for 100+ time steps\n",
    "\n",
    "2. **GRU (Gated Recurrent Unit)**\n",
    "   - Simplified version of LSTM\n",
    "   - Fewer parameters, faster training\n",
    "\n",
    "3. **Gradient Clipping**\n",
    "   - Limit maximum gradient value\n",
    "   - Prevents exploding gradients\n",
    "\n",
    "**This is why modern language models use Transformers instead of RNNs!**\n",
    "- Transformers don't have vanishing gradient problem\n",
    "- Can process sequences in parallel (RNNs are sequential)\n",
    "- But understanding RNNs is crucial for understanding Transformers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## ğŸš€ Real AI Example: Stock Price Prediction with Simple RNN\n",
    "\n",
    "**Project:** Build an RNN to predict stock prices based on historical data\n",
    "\n",
    "**Why this example?**\n",
    "- Demonstrates time series forecasting\n",
    "- Shows how RNNs use past information to predict future\n",
    "- Same principles used in financial AI systems\n",
    "\n",
    "**Architecture:**\n",
    "- Input: Sequence of past prices (e.g., last 10 days)\n",
    "- Hidden: RNN layer with hidden state\n",
    "- Output: Predicted next price\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy matplotlib pandas scikit-learn --quiet\n",
    "\n",
    "print(\"âœ… Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ğŸ“š Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Step 1: Generate Synthetic Stock Price Data\n",
    "\n",
    "We'll create realistic stock price data with:\n",
    "- Upward trend\n",
    "- Random fluctuations\n",
    "- Seasonal patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stock_data(n_days=200, start_price=100):\n",
    "    \"\"\"\n",
    "    Generate synthetic stock price data\n",
    "    \n",
    "    Args:\n",
    "        n_days: Number of trading days\n",
    "        start_price: Initial stock price\n",
    "    \n",
    "    Returns:\n",
    "        Array of stock prices\n",
    "    \"\"\"\n",
    "    time = np.arange(n_days)\n",
    "    \n",
    "    # Trend component (slight upward trend)\n",
    "    trend = 0.05 * time\n",
    "    \n",
    "    # Seasonal component (weekly patterns)\n",
    "    seasonal = 5 * np.sin(2 * np.pi * time / 7)\n",
    "    \n",
    "    # Random walk component\n",
    "    random_walk = np.cumsum(np.random.randn(n_days) * 2)\n",
    "    \n",
    "    # Combine components\n",
    "    prices = start_price + trend + seasonal + random_walk\n",
    "    \n",
    "    # Ensure prices stay positive\n",
    "    prices = np.maximum(prices, 50)\n",
    "    \n",
    "    return prices\n",
    "\n",
    "# Generate data\n",
    "stock_prices = generate_stock_data(200, start_price=100)\n",
    "\n",
    "print(\"ğŸ“Š Stock price data generated!\")\n",
    "print(f\"Shape: {stock_prices.shape}\")\n",
    "print(f\"First 10 prices: {stock_prices[:10]}\")\n",
    "print(f\"Min price: ${stock_prices.min():.2f}\")\n",
    "print(f\"Max price: ${stock_prices.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stock prices\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(stock_prices, linewidth=2, color='blue', alpha=0.7)\n",
    "plt.title('Synthetic Stock Price Data', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Day', fontsize=12)\n",
    "plt.ylabel('Price ($)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“ˆ Stock prices show realistic patterns:\")\n",
    "print(\"   - Upward trend (market growth)\")\n",
    "print(\"   - Weekly seasonality (trading patterns)\")\n",
    "print(\"   - Random fluctuations (market volatility)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Sequences for RNN\n",
    "\n",
    "RNNs need sequences as input. We'll create:\n",
    "- **Input:** Last N days of prices\n",
    "- **Output:** Next day's price\n",
    "\n",
    "Example with window=3:\n",
    "```\n",
    "Input: [Day 1, Day 2, Day 3] â†’ Output: Day 4\n",
    "Input: [Day 2, Day 3, Day 4] â†’ Output: Day 5\n",
    "Input: [Day 3, Day 4, Day 5] â†’ Output: Day 6\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window_size=10):\n",
    "    \"\"\"\n",
    "    Create sequences for RNN training\n",
    "    \n",
    "    Args:\n",
    "        data: Time series data\n",
    "        window_size: Number of past time steps to use\n",
    "    \n",
    "    Returns:\n",
    "        X: Input sequences (n_samples, window_size, 1)\n",
    "        y: Target values (n_samples, 1)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - window_size):\n",
    "        # Input: window_size past values\n",
    "        X.append(data[i:i+window_size])\n",
    "        # Output: next value\n",
    "        y.append(data[i+window_size])\n",
    "    \n",
    "    return np.array(X).reshape(-1, window_size, 1), np.array(y).reshape(-1, 1)\n",
    "\n",
    "# Normalize data to [0, 1] range (helps training!)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "prices_normalized = scaler.fit_transform(stock_prices.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create sequences\n",
    "window_size = 10\n",
    "X_seq, y_seq = create_sequences(prices_normalized, window_size)\n",
    "\n",
    "# Split into train and test\n",
    "split_idx = int(0.8 * len(X_seq))\n",
    "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "print(\"ğŸ”¢ Sequences created!\")\n",
    "print(f\"Window size: {window_size} days\")\n",
    "print(f\"X_train shape: {X_train.shape} (samples, time_steps, features)\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Input sequence (10 days): {X_train[0].flatten()[:5]}... (showing first 5)\")\n",
    "print(f\"Target (next day): {y_train[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Step 3: Build Simple RNN from Scratch\n",
    "\n",
    "Let's implement a basic RNN cell using NumPy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    Simple RNN for time series prediction\n",
    "    \n",
    "    Architecture:\n",
    "    - RNN layer with hidden_size neurons\n",
    "    - Output layer (1 neuron for price prediction)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden_size=32, output_size=1, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize RNN weights\n",
    "        \"\"\"\n",
    "        # RNN weights\n",
    "        self.Wxh = np.random.randn(input_size, hidden_size) * 0.01  # Input to hidden\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
    "        self.bh = np.zeros((1, hidden_size))  # Hidden bias\n",
    "        \n",
    "        # Output weights\n",
    "        self.Why = np.random.randn(hidden_size, output_size) * 0.01  # Hidden to output\n",
    "        self.by = np.zeros((1, output_size))  # Output bias\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        print(\"ğŸ§  Simple RNN initialized!\")\n",
    "        print(f\"   Input size: {input_size}\")\n",
    "        print(f\"   Hidden size: {hidden_size}\")\n",
    "        print(f\"   Output size: {output_size}\")\n",
    "        print(f\"   Learning rate: {learning_rate}\")\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through RNN\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequence (batch_size, time_steps, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Output (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        batch_size, time_steps, _ = X.shape\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        # Process each time step\n",
    "        for t in range(time_steps):\n",
    "            x_t = X[:, t, :]  # Input at time t\n",
    "            \n",
    "            # RNN cell: h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)\n",
    "            h = np.tanh(np.dot(x_t, self.Wxh) + np.dot(h, self.Whh) + self.bh)\n",
    "        \n",
    "        # Output from final hidden state\n",
    "        y = np.dot(h, self.Why) + self.by\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "# Create RNN\n",
    "rnn = SimpleRNN(input_size=1, hidden_size=32, output_size=1, learning_rate=0.001)\n",
    "\n",
    "print(\"\\nğŸ“Š RNN Architecture:\")\n",
    "print(f\"   Wxh shape: {rnn.Wxh.shape}\")\n",
    "print(f\"   Whh shape: {rnn.Whh.shape}\")\n",
    "print(f\"   Why shape: {rnn.Why.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Step 4: Test the RNN (Before Training)\n",
    "\n",
    "Let's see what predictions the untrained RNN makes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with untrained model\n",
    "predictions_untrained = rnn.predict(X_test)\n",
    "\n",
    "# Denormalize predictions\n",
    "predictions_untrained_denorm = scaler.inverse_transform(predictions_untrained)\n",
    "y_test_denorm = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculate error\n",
    "mse = np.mean((predictions_untrained - y_test)**2)\n",
    "mae = np.mean(np.abs(predictions_untrained - y_test))\n",
    "\n",
    "print(\"ğŸ”® Predictions BEFORE training:\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"\\nFirst 5 predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"   Predicted: ${predictions_untrained_denorm[i][0]:.2f}, Actual: ${y_test_denorm[i][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_test_denorm[:50], label='Actual Prices', linewidth=2, color='blue')\n",
    "plt.plot(predictions_untrained_denorm[:50], label='Predicted Prices (Untrained)', \n",
    "         linewidth=2, color='red', alpha=0.7, linestyle='--')\n",
    "plt.title('Stock Price Predictions - BEFORE Training', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Day', fontsize=12)\n",
    "plt.ylabel('Price ($)', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âš ï¸ Random predictions! The RNN needs training to learn patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## ğŸ¯ Why This Matters for Modern AI\n",
    "\n",
    "The RNN concepts you just learned are **fundamental to understanding modern AI**!\n",
    "\n",
    "### ğŸ¤– **ChatGPT & Claude Evolution**\n",
    "\n",
    "**RNN Era (2013-2017):**\n",
    "- Early language models used LSTMs/GRUs\n",
    "- Could generate text, translate languages\n",
    "- Limited to ~100 tokens of context\n",
    "\n",
    "**Transformer Era (2018-2025):**\n",
    "- GPT, BERT, Claude use Transformers (not RNNs)\n",
    "- BUT: Transformers borrowed key ideas from RNNs!\n",
    "  - Sequential processing concept\n",
    "  - Hidden states â†’ Attention mechanisms\n",
    "  - Positional information\n",
    "\n",
    "**Why Transformers replaced RNNs:**\n",
    "- âœ… No vanishing gradient problem\n",
    "- âœ… Can process sequences in parallel (faster!)\n",
    "- âœ… Better long-range dependencies\n",
    "- âœ… Scales to billions of parameters\n",
    "\n",
    "### ğŸ” **RAG Systems & Sequential Understanding**\n",
    "\n",
    "**RAG Pipeline:**\n",
    "1. **Document Encoding** (uses RNN concepts)\n",
    "   - Process document chunks sequentially\n",
    "   - Build context from previous paragraphs\n",
    "   - Modern systems use Transformers, but RNN principles apply\n",
    "\n",
    "2. **Query Understanding**\n",
    "   - Sequential processing of user question\n",
    "   - Context from conversation history (like RNN hidden state!)\n",
    "\n",
    "3. **Response Generation**\n",
    "   - Generate answer token by token (sequential!)\n",
    "   - Each token depends on previous tokens\n",
    "\n",
    "### ğŸ¤ **Agentic AI & Decision Sequences**\n",
    "\n",
    "**Agentic AI uses sequence models for:**\n",
    "- **Multi-turn conversations:** Remember what user said 10 messages ago\n",
    "- **Action planning:** Sequence of steps to complete a task\n",
    "- **Tool usage:** Decide which tool to call based on conversation history\n",
    "\n",
    "Example:\n",
    "```\n",
    "User: \"Find me cheap flights to Paris\"\n",
    "Agent: [Search tool] â†’ [Filter results] â†’ [Present options]\n",
    "User: \"Book the morning flight\"\n",
    "Agent: Remembers context from previous message!\n",
    "```\n",
    "\n",
    "### ğŸ“Š **Time Series in Production AI**\n",
    "\n",
    "**Real applications:**\n",
    "- **Anomaly detection:** Fraud detection in banking\n",
    "- **Predictive maintenance:** Predict equipment failures\n",
    "- **Demand forecasting:** Inventory optimization for e-commerce\n",
    "- **Energy optimization:** Smart grid management\n",
    "\n",
    "### ğŸ“ **Key Takeaway:**\n",
    "**Understanding RNNs is essential for:**\n",
    "- Grasping how Transformers work\n",
    "- Building time series models\n",
    "- Understanding sequential decision-making in AI agents\n",
    "- Appreciating why modern architectures evolved the way they did"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## ğŸ¯ Interactive Exercise 1: Text Character Prediction\n",
    "\n",
    "**Challenge:** Build an RNN to predict the next character in a sequence!\n",
    "\n",
    "**Task:**\n",
    "Given the text \"HELLO\", train an RNN to predict:\n",
    "- Input: \"H\" â†’ Output: \"E\"\n",
    "- Input: \"E\" â†’ Output: \"L\"\n",
    "- Input: \"L\" â†’ Output: \"L\"\n",
    "- Input: \"L\" â†’ Output: \"O\"\n",
    "\n",
    "This is exactly how early language models worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "\n",
    "# Step 1: Create character-to-index mapping\n",
    "text = \"HELLO\"\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Characters: {chars}\")\n",
    "print(f\"Vocabulary size: {len(chars)}\")\n",
    "print(f\"Char to index: {char_to_idx}\")\n",
    "\n",
    "# Step 2: Create training sequences\n",
    "# TODO: Convert characters to indices\n",
    "# TODO: Create input-output pairs\n",
    "# TODO: One-hot encode the inputs\n",
    "\n",
    "# Step 3: Build and test RNN\n",
    "# TODO: Create RNN with appropriate input/output sizes\n",
    "# TODO: Test predictions\n",
    "\n",
    "print(\"\\nComplete the TODOs above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### âœ… Solution (Try on your own first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"Convert index to one-hot vector\"\"\"\n",
    "    vec = np.zeros(vocab_size)\n",
    "    vec[idx] = 1\n",
    "    return vec\n",
    "\n",
    "# Create training data\n",
    "text = \"HELLO\"\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create sequences\n",
    "X_char = []\n",
    "y_char = []\n",
    "\n",
    "for i in range(len(text) - 1):\n",
    "    input_char = text[i]\n",
    "    output_char = text[i + 1]\n",
    "    \n",
    "    X_char.append(one_hot_encode(char_to_idx[input_char], vocab_size))\n",
    "    y_char.append(char_to_idx[output_char])\n",
    "\n",
    "X_char = np.array(X_char).reshape(-1, 1, vocab_size)\n",
    "y_char = np.array(y_char)\n",
    "\n",
    "print(\"ğŸ“Š Character-level RNN data:\")\n",
    "print(f\"Vocabulary: {chars}\")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"X shape: {X_char.shape} (samples, time_steps, features)\")\n",
    "print(f\"y shape: {y_char.shape}\")\n",
    "\n",
    "print(\"\\nğŸ”¤ Training pairs:\")\n",
    "for i in range(len(X_char)):\n",
    "    input_char = idx_to_char[np.argmax(X_char[i])]\n",
    "    output_char = idx_to_char[y_char[i]]\n",
    "    print(f\"   '{input_char}' â†’ '{output_char}'\")\n",
    "\n",
    "print(\"\\nâœ… This is the foundation of GPT, Claude, and all language models!\")\n",
    "print(\"   Modern LLMs predict next token (word piece) instead of character.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## ğŸ¯ Interactive Exercise 2: Sine Wave Prediction\n",
    "\n",
    "**Challenge:** Predict future values of a sine wave!\n",
    "\n",
    "**Task:**\n",
    "1. Generate sine wave data\n",
    "2. Create sequences (use past 20 points to predict next point)\n",
    "3. Build and visualize RNN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "\n",
    "# TODO: Generate sine wave data\n",
    "t = np.linspace(0, 4*np.pi, 200)\n",
    "sine_wave = np.sin(t)\n",
    "\n",
    "# TODO: Create sequences (window_size=20)\n",
    "# TODO: Build RNN\n",
    "# TODO: Make predictions and visualize\n",
    "\n",
    "print(\"Complete the exercise above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### âœ… Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# Generate sine wave\n",
    "t = np.linspace(0, 4*np.pi, 200)\n",
    "sine_wave = np.sin(t)\n",
    "\n",
    "# Normalize\n",
    "sine_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "sine_norm = sine_scaler.fit_transform(sine_wave.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create sequences\n",
    "X_sine, y_sine = create_sequences(sine_norm, window_size=20)\n",
    "\n",
    "# Create and test RNN\n",
    "sine_rnn = SimpleRNN(input_size=1, hidden_size=16, output_size=1)\n",
    "predictions_sine = sine_rnn.predict(X_sine)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_sine[:100], label='Actual', linewidth=2, color='blue')\n",
    "plt.plot(predictions_sine[:100], label='Predicted (Untrained)', \n",
    "         linewidth=2, color='red', alpha=0.7, linestyle='--')\n",
    "plt.title('Sine Wave Prediction - BEFORE Training', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Time Step', fontsize=12)\n",
    "plt.ylabel('Value (Normalized)', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Sine wave RNN created!\")\n",
    "print(\"ğŸ“Œ After training (Day 2), this will predict the sine wave accurately!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## ğŸ“Š Visualizing RNN Hidden States\n",
    "\n",
    "Let's visualize how the hidden state changes over time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify SimpleRNN to return hidden states\n",
    "class SimpleRNN_WithStates(SimpleRNN):\n",
    "    def forward_with_states(self, X):\n",
    "        \"\"\"Forward pass that returns all hidden states\"\"\"\n",
    "        batch_size, time_steps, _ = X.shape\n",
    "        \n",
    "        # Store hidden states\n",
    "        h = np.zeros((batch_size, self.hidden_size))\n",
    "        hidden_states = []\n",
    "        \n",
    "        for t in range(time_steps):\n",
    "            x_t = X[:, t, :]\n",
    "            h = np.tanh(np.dot(x_t, self.Wxh) + np.dot(h, self.Whh) + self.bh)\n",
    "            hidden_states.append(h.copy())\n",
    "        \n",
    "        y = np.dot(h, self.Why) + self.by\n",
    "        \n",
    "        return y, np.array(hidden_states)\n",
    "\n",
    "# Create RNN and get hidden states\n",
    "rnn_states = SimpleRNN_WithStates(input_size=1, hidden_size=8, output_size=1)\n",
    "\n",
    "# Get hidden states for one sample\n",
    "sample_input = X_test[0:1]  # First test sample\n",
    "_, hidden_states = rnn_states.forward_with_states(sample_input)\n",
    "\n",
    "# Visualize hidden states\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(hidden_states.squeeze().T, aspect='auto', cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar(label='Activation Value')\n",
    "plt.title('RNN Hidden States Over Time', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Time Step', fontsize=12)\n",
    "plt.ylabel('Hidden Neuron', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Hidden State Visualization:\")\n",
    "print(\"   - Each row = one hidden neuron\")\n",
    "print(\"   - Each column = one time step\")\n",
    "print(\"   - Color = activation value\")\n",
    "print(\"   - Notice how states evolve over time!\")\n",
    "print(\"\\nğŸ’¡ This is the 'memory' of the RNN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- âœ… What RNNs are and how they differ from feedforward networks\n",
    "- âœ… Sequential data and time series characteristics\n",
    "- âœ… RNN architecture and hidden states (the memory!)\n",
    "- âœ… The vanishing gradient problem and why it matters\n",
    "- âœ… Built a stock price predictor from scratch!\n",
    "- âœ… How RNNs power language models, speech recognition, and time series\n",
    "- âœ… How these concepts evolved into modern Transformers (ChatGPT, Claude)\n",
    "\n",
    "### ğŸ¯ Key Takeaways:\n",
    "\n",
    "1. **RNNs process sequences with memory**\n",
    "   - Hidden state carries information from past time steps\n",
    "   - Same weights shared across all time steps\n",
    "   - Essential for understanding temporal dependencies\n",
    "\n",
    "2. **Vanishing gradients are a major challenge**\n",
    "   - Gradients shrink as they propagate through time\n",
    "   - Limits learning of long-term dependencies\n",
    "   - Solution: LSTM/GRU (Day 2!) or Transformers\n",
    "\n",
    "3. **RNNs are foundation for modern NLP**\n",
    "   - Understanding RNNs â†’ Understanding Transformers\n",
    "   - Same sequential processing concept\n",
    "   - Evolved into self-attention mechanisms\n",
    "\n",
    "4. **Time series forecasting has real applications**\n",
    "   - Finance, weather, sales, energy\n",
    "   - Critical for business decision-making\n",
    "   - Powers predictive AI systems\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ¯ Practice Before Day 2:**\n",
    "\n",
    "Build an RNN to predict:\n",
    "1. **Temperature forecasting:** Daily temperatures over a month\n",
    "2. **Sales prediction:** Predict next week's sales from past 4 weeks\n",
    "3. **Simple text generation:** Predict next character in your name!\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“š Next Lesson:** Day 2 - LSTMs and GRUs\n",
    "- Long Short-Term Memory networks\n",
    "- Gated Recurrent Units\n",
    "- Solving the vanishing gradient problem\n",
    "- Sentiment analysis on movie reviews\n",
    "- Bidirectional RNNs\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¬ Remember:**\n",
    "\n",
    "*\"RNNs were the breakthrough that made language modeling possible. While Transformers have largely replaced them in production, understanding RNNs is essential for understanding how modern AI processes sequences. Every ChatGPT conversation, every RAG query, every agent decision - they all build on the concepts you learned today!\"* ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ”— Connections to Modern AI:**\n",
    "- **Pre-Transformer LLMs:** Used LSTM/GRU architectures\n",
    "- **Transformers:** Replaced RNN sequential processing with parallel attention\n",
    "- **RAG Systems:** Sequential document processing borrows RNN concepts\n",
    "- **Agentic AI:** Conversation memory similar to RNN hidden states\n",
    "- **Multimodal:** Video understanding still uses RNN-like temporal modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
