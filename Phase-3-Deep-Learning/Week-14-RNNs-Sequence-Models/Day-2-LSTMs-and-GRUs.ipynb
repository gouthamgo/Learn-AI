{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Day 2: Long Short-Term Memory (LSTM) & Gated Recurrent Units (GRU)\n",
    "\n",
    "**ðŸŽ¯ Goal:** Master LSTM and GRU architectures for advanced sequence modeling\n",
    "\n",
    "**â±ï¸ Time:** 75-90 minutes\n",
    "\n",
    "**ðŸŒŸ Why This Matters for AI:**\n",
    "- LSTMs powered pre-Transformer language models (GPT-1's predecessor!)\n",
    "- Still used in production for time series, speech, and video\n",
    "- Understanding gates â†’ Understanding Transformers' attention mechanisms\n",
    "- Critical for hybrid models combining RNNs with Transformers\n",
    "- GRUs power many real-time AI systems (faster than LSTMs!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ðŸ§  The Problem: Vanishing Gradients in RNNs\n",
    "\n",
    "### ðŸ“‰ Recap from Day 1:\n",
    "\n",
    "**Simple RNNs struggle with long-term dependencies:**\n",
    "\n",
    "```\n",
    "The cat, which had been wandering the streets for hours and was getting\n",
    "very tired and hungry, finally _______ the food.\n",
    "\n",
    "Answer: \"ate\"\n",
    "\n",
    "Problem:\n",
    "- \"cat\" is 15 words before \"ate\"\n",
    "- Simple RNN gradient: 0.9^15 â‰ˆ 0.2 (weak!)\n",
    "- Can't learn the connection!\n",
    "```\n",
    "\n",
    "### âœ… The Solution: Gating Mechanisms!\n",
    "\n",
    "**Key Insight:**\n",
    "> Instead of forcing information through the same path (causing gradients to vanish),\n",
    "> create **highways** that let important information flow unchanged!\n",
    "\n",
    "**Think of it like:**\n",
    "- **Simple RNN:** One-lane road with many toll booths (gradients shrink!)\n",
    "- **LSTM/GRU:** Multi-lane highway with express lanes (information flows freely!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## ðŸšª LSTM: Long Short-Term Memory\n",
    "\n",
    "**Invented in 1997 by Hochreiter & Schmidhuber**\n",
    "- Revolutionized sequence modeling\n",
    "- Powered Google Translate (pre-2017)\n",
    "- Foundation of early speech recognition systems\n",
    "\n",
    "### ðŸ—ï¸ LSTM Architecture: The Cell State & Gates\n",
    "\n",
    "**LSTM has TWO states instead of one:**\n",
    "1. **Cell State (C_t):** Long-term memory highway\n",
    "2. **Hidden State (h_t):** Short-term working memory\n",
    "\n",
    "**Three Gates Control Information Flow:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           LSTM CELL                     â”‚\n",
    "â”‚                                         â”‚\n",
    "â”‚    C_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ C_t     â”‚  â† Cell State (Highway!)\n",
    "â”‚              â†“        â†“        â†“        â”‚\n",
    "â”‚          [Forget] [Input] [Output]     â”‚  â† Gates\n",
    "â”‚              â†“        â†“        â†“        â”‚\n",
    "â”‚    h_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ h_t    â”‚  â† Hidden State\n",
    "â”‚      â†‘                           â†‘      â”‚\n",
    "â”‚    x_t                          y_t     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ðŸšª The Three Gates:\n",
    "\n",
    "#### 1ï¸âƒ£ **Forget Gate (f_t)** - What to forget?\n",
    "```\n",
    "f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)\n",
    "\n",
    "Decides: \"Should I forget old information?\"\n",
    "Output: 0 (forget everything) to 1 (keep everything)\n",
    "\n",
    "Example:\n",
    "\"The cat was hungry. The dog was tired.\"\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â†‘\n",
    "      Forget cat     New subject!\n",
    "```\n",
    "\n",
    "#### 2ï¸âƒ£ **Input Gate (i_t)** - What new info to add?\n",
    "```\n",
    "i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)           â† How much to update\n",
    "CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)       â† Candidate values\n",
    "\n",
    "Decides: \"What new information should I store?\"\n",
    "\n",
    "Example:\n",
    "\"The dog was tired\"\n",
    "       â†‘\n",
    "  Store \"dog\" as new subject!\n",
    "```\n",
    "\n",
    "#### 3ï¸âƒ£ **Output Gate (o_t)** - What to output?\n",
    "```\n",
    "o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)\n",
    "h_t = o_t * tanh(C_t)\n",
    "\n",
    "Decides: \"What parts of memory to expose?\"\n",
    "```\n",
    "\n",
    "### ðŸ”„ Complete LSTM Forward Pass:\n",
    "\n",
    "```python\n",
    "# Step 1: Forget Gate - decide what to forget from cell state\n",
    "f_t = sigmoid(W_f @ [h_{t-1}, x_t] + b_f)\n",
    "\n",
    "# Step 2: Input Gate - decide what new information to store\n",
    "i_t = sigmoid(W_i @ [h_{t-1}, x_t] + b_i)\n",
    "CÌƒ_t = tanh(W_C @ [h_{t-1}, x_t] + b_C)\n",
    "\n",
    "# Step 3: Update Cell State\n",
    "C_t = f_t * C_{t-1} + i_t * CÌƒ_t\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "      Forget old     Add new\n",
    "\n",
    "# Step 4: Output Gate - decide what to output\n",
    "o_t = sigmoid(W_o @ [h_{t-1}, x_t] + b_o)\n",
    "h_t = o_t * tanh(C_t)\n",
    "```\n",
    "\n",
    "### ðŸŽ¯ Why LSTM Solves Vanishing Gradients:\n",
    "\n",
    "**Key: The Cell State Highway!**\n",
    "\n",
    "```\n",
    "Simple RNN Gradient Flow:\n",
    "âˆ‚L/âˆ‚h_1 â† tanh' Ã— W Ã— tanh' Ã— W Ã— tanh' Ã— W â† âˆ‚L/âˆ‚h_4\n",
    "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Shrinks! â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "LSTM Cell State Gradient Flow:\n",
    "âˆ‚L/âˆ‚C_1 â† f_2 Ã— f_3 Ã— f_4 â† âˆ‚L/âˆ‚C_4\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       No repeated multiplication!\n",
    "       Gradients flow through!\n",
    "```\n",
    "\n",
    "**Cell state acts like a conveyor belt** - information can flow unchanged for 100+ time steps!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## âš¡ GRU: Gated Recurrent Unit\n",
    "\n",
    "**Invented in 2014 by Cho et al.**\n",
    "- Simplified version of LSTM\n",
    "- Fewer parameters â†’ Faster training\n",
    "- Often performs as well as LSTM!\n",
    "\n",
    "### ðŸ—ï¸ GRU Architecture: Simpler but Powerful\n",
    "\n",
    "**Key Simplifications:**\n",
    "1. Only ONE state (no separate cell state)\n",
    "2. Only TWO gates (vs LSTM's three)\n",
    "3. Fewer parameters â†’ Faster!\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         GRU CELL                â”‚\n",
    "â”‚                                 â”‚\n",
    "â”‚   h_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ h_t    â”‚  â† Single state!\n",
    "â”‚        â†“      â†“         â†“       â”‚\n",
    "â”‚    [Reset] [Update]             â”‚  â† Only 2 gates!\n",
    "â”‚        â†“      â†“                 â”‚\n",
    "â”‚      x_t    y_t                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ðŸšª The Two Gates:\n",
    "\n",
    "#### 1ï¸âƒ£ **Update Gate (z_t)** - How much to update?\n",
    "```\n",
    "z_t = Ïƒ(W_z Â· [h_{t-1}, x_t] + b_z)\n",
    "\n",
    "Acts like LSTM's forget + input gates combined!\n",
    "- z_t = 0: Keep old state (ignore new input)\n",
    "- z_t = 1: Replace with new state\n",
    "```\n",
    "\n",
    "#### 2ï¸âƒ£ **Reset Gate (r_t)** - How much past to forget?\n",
    "```\n",
    "r_t = Ïƒ(W_r Â· [h_{t-1}, x_t] + b_r)\n",
    "\n",
    "Controls how much past information to use when computing new state\n",
    "```\n",
    "\n",
    "### ðŸ”„ Complete GRU Forward Pass:\n",
    "\n",
    "```python\n",
    "# Step 1: Reset Gate - decide what past info to use\n",
    "r_t = sigmoid(W_r @ [h_{t-1}, x_t] + b_r)\n",
    "\n",
    "# Step 2: Candidate state (using reset gate)\n",
    "hÌƒ_t = tanh(W_h @ [r_t * h_{t-1}, x_t] + b_h)\n",
    "\n",
    "# Step 3: Update Gate - interpolate between old and new\n",
    "z_t = sigmoid(W_z @ [h_{t-1}, x_t] + b_z)\n",
    "\n",
    "# Step 4: Final state (weighted average)\n",
    "h_t = (1 - z_t) * h_{t-1} + z_t * hÌƒ_t\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "      Keep old          Use new\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## âš–ï¸ LSTM vs GRU: When to Use Each?\n",
    "\n",
    "| Feature | LSTM | GRU |\n",
    "|---------|------|-----|\n",
    "| **Gates** | 3 (forget, input, output) | 2 (reset, update) |\n",
    "| **States** | 2 (cell + hidden) | 1 (hidden only) |\n",
    "| **Parameters** | More (~4x input size) | Fewer (~3x input size) |\n",
    "| **Training Speed** | Slower | **Faster** âœ… |\n",
    "| **Memory Usage** | Higher | **Lower** âœ… |\n",
    "| **Performance** | Slightly better on complex tasks | **Similar for most tasks** âœ… |\n",
    "| **Best For** | Very long sequences (>100 steps) | Most real-world tasks |\n",
    "\n",
    "### ðŸŽ¯ General Guidelines:\n",
    "\n",
    "**Use LSTM when:**\n",
    "- Very long sequences (100+ time steps)\n",
    "- Complex dependencies\n",
    "- Plenty of training data\n",
    "- Computational resources available\n",
    "- Examples: Long documents, music generation\n",
    "\n",
    "**Use GRU when:**\n",
    "- Medium-length sequences (10-100 steps)\n",
    "- Need faster training\n",
    "- Limited computational resources\n",
    "- Real-time applications\n",
    "- Examples: Chatbots, sentiment analysis, stock prediction\n",
    "\n",
    "**Pro Tip:** Start with GRU! It's simpler and often works just as well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## ðŸš€ Let's Build! Real AI Example 1: Sentiment Analysis with LSTM\n",
    "\n",
    "**Project:** Movie review sentiment classifier\n",
    "- Input: Text review (\"This movie was amazing!\")\n",
    "- Output: Positive (ðŸ˜Š) or Negative (ðŸ˜ž)\n",
    "\n",
    "**Why this matters:**\n",
    "- Same architecture used in early ChatGPT models\n",
    "- Foundation of understanding how LLMs process text\n",
    "- Used in production for social media monitoring, customer feedback\n",
    "\n",
    "Let's build it with TensorFlow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow numpy matplotlib scikit-learn --quiet\n",
    "\n",
    "print(\"âœ… Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"ðŸ“š Libraries loaded!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Step 1: Load IMDB Movie Review Dataset\n",
    "\n",
    "**Dataset:**\n",
    "- 50,000 movie reviews from IMDB\n",
    "- 25,000 for training, 25,000 for testing\n",
    "- Labels: 0 (negative) or 1 (positive)\n",
    "- Pre-tokenized (words â†’ integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset (top 10,000 most common words)\n",
    "vocab_size = 10000\n",
    "max_length = 200  # Truncate reviews to 200 words\n",
    "\n",
    "print(\"ðŸ“¥ Loading IMDB dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded!\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "print(f\"   Vocabulary size: {vocab_size}\")\n",
    "print(f\"\\n   Example review (first 10 words): {X_train[0][:10]}\")\n",
    "print(f\"   Label: {y_train[0]} ({'Positive' if y_train[0] == 1 else 'Negative'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode a review to see actual text\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n",
    "\n",
    "print(\"ðŸ“– Example review (decoded):\")\n",
    "print(f\"\\nReview: {decode_review(X_train[0][:50])}...\")\n",
    "print(f\"\\nSentiment: {'Positive ðŸ˜Š' if y_train[0] == 1 else 'Negative ðŸ˜ž'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Step 2: Preprocess Data\n",
    "\n",
    "**Why pad sequences?**\n",
    "- Reviews have different lengths\n",
    "- Neural networks need fixed-size inputs\n",
    "- Solution: Pad/truncate to max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to same length\n",
    "X_train_pad = pad_sequences(X_train, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "print(\"âœ… Sequences padded!\")\n",
    "print(f\"   X_train shape: {X_train_pad.shape}\")\n",
    "print(f\"   X_test shape: {X_test_pad.shape}\")\n",
    "print(f\"\\n   Example padded sequence (last 10 values): {X_train_pad[0][-10:]}\")\n",
    "print(\"   (zeros = padding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Step 3: Build LSTM Model\n",
    "\n",
    "**Architecture:**\n",
    "1. **Embedding Layer:** Convert word indices to dense vectors\n",
    "2. **LSTM Layer:** Process sequence with long-term memory\n",
    "3. **Dense Layer:** Classification (positive/negative)\n",
    "\n",
    "**This is similar to how early GPT models worked!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "def build_lstm_model(vocab_size, embedding_dim=128, lstm_units=64):\n",
    "    \"\"\"\n",
    "    Build LSTM sentiment classifier\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding: Convert words to vectors\n",
    "    - LSTM: Process sequence\n",
    "    - Dense: Binary classification\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Embedding layer (learn word representations)\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        \n",
    "        # LSTM layer (process sequence with memory)\n",
    "        layers.LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2),\n",
    "        \n",
    "        # Output layer (binary classification)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "lstm_model = build_lstm_model(vocab_size, embedding_dim=128, lstm_units=64)\n",
    "\n",
    "print(\"ðŸ§  LSTM Model created!\")\n",
    "print(\"\\nðŸ“Š Model Architecture:\")\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… Model compiled!\")\n",
    "print(\"   Optimizer: Adam\")\n",
    "print(\"   Loss: Binary Cross-Entropy\")\n",
    "print(\"   Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Step 4: Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"ðŸš€ Training LSTM model...\")\n",
    "print(\"   This will take 5-10 minutes depending on hardware\\n\")\n",
    "\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(history_lstm.history['accuracy'], label='Training', linewidth=2)\n",
    "ax1.plot(history_lstm.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "ax1.set_title('LSTM Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(history_lstm.history['loss'], label='Training', linewidth=2)\n",
    "ax2.plot(history_lstm.history['val_loss'], label='Validation', linewidth=2)\n",
    "ax2.set_title('LSTM Model Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"ðŸ”¬ Evaluating LSTM model...\\n\")\n",
    "test_loss, test_accuracy = lstm_model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f\"âœ… Test Results:\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nðŸŽ¯ LSTM correctly classifies {test_accuracy*100:.1f}% of movie reviews!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample reviews\n",
    "sample_indices = [0, 10, 50, 100, 500]\n",
    "\n",
    "print(\"ðŸŽ¬ Sample Predictions:\\n\")\n",
    "for idx in sample_indices:\n",
    "    review = decode_review(X_test[idx][:30])  # First 30 words\n",
    "    true_sentiment = \"Positive ðŸ˜Š\" if y_test[idx] == 1 else \"Negative ðŸ˜ž\"\n",
    "    \n",
    "    # Predict\n",
    "    pred = lstm_model.predict(X_test_pad[idx:idx+1], verbose=0)[0][0]\n",
    "    pred_sentiment = \"Positive ðŸ˜Š\" if pred > 0.5 else \"Negative ðŸ˜ž\"\n",
    "    confidence = pred if pred > 0.5 else 1 - pred\n",
    "    \n",
    "    print(f\"Review: {review}...\")\n",
    "    print(f\"True: {true_sentiment} | Predicted: {pred_sentiment} ({confidence*100:.1f}% confidence)\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Real AI Example 2: Character-Level Text Generation\n",
    "\n",
    "**Project:** Train LSTM to generate text character by character\n",
    "\n",
    "**How it works:**\n",
    "- Input: Sequence of characters (\"Hell\")\n",
    "- Output: Next character (\"o\")\n",
    "- Generate: Sample next character, add to sequence, repeat!\n",
    "\n",
    "**This is how early text generation worked!**\n",
    "- Shakespeare text generation (Andrej Karpathy's famous demo)\n",
    "- Music generation (character = note)\n",
    "- Code generation (character = code token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text (you can replace with any text!)\n",
    "text = \"\"\"In the age of artificial intelligence, machine learning has transformed how we\n",
    "interact with technology. From chatbots like ChatGPT to autonomous vehicles, AI is everywhere.\n",
    "Deep learning, particularly transformers and neural networks, powers modern AI systems.\n",
    "Understanding these foundations is crucial for building the next generation of AI applications.\"\"\"\n",
    "\n",
    "print(\"ðŸ“š Training text:\")\n",
    "print(text[:200] + \"...\")\n",
    "print(f\"\\n   Text length: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size_char = len(chars)\n",
    "\n",
    "print(f\"ðŸ“Š Character vocabulary:\")\n",
    "print(f\"   Unique characters: {vocab_size_char}\")\n",
    "print(f\"   Characters: {chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for character prediction\n",
    "seq_length = 40  # Use 40 characters to predict next one\n",
    "\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(len(text) - seq_length):\n",
    "    sequences.append(text[i:i+seq_length])\n",
    "    next_chars.append(text[i+seq_length])\n",
    "\n",
    "print(f\"ðŸ”¢ Created {len(sequences)} sequences\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"   Input: '{sequences[0]}'\")\n",
    "print(f\"   Target: '{next_chars[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "X_char = np.zeros((len(sequences), seq_length, vocab_size_char), dtype=bool)\n",
    "y_char = np.zeros((len(sequences), vocab_size_char), dtype=bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X_char[i, t, char_to_idx[char]] = 1\n",
    "    y_char[i, char_to_idx[next_chars[i]]] = 1\n",
    "\n",
    "print(f\"âœ… Data prepared!\")\n",
    "print(f\"   X shape: {X_char.shape} (samples, time_steps, features)\")\n",
    "print(f\"   y shape: {y_char.shape} (samples, classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build character-level LSTM\n",
    "char_model = keras.Sequential([\n",
    "    layers.LSTM(128, input_shape=(seq_length, vocab_size_char), return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(128),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(vocab_size_char, activation='softmax')\n",
    "])\n",
    "\n",
    "char_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"ðŸ§  Character-level LSTM Model:\")\n",
    "char_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train character model\n",
    "print(\"ðŸš€ Training character-level LSTM...\\n\")\n",
    "\n",
    "history_char = char_model.fit(\n",
    "    X_char, y_char,\n",
    "    batch_size=128,\n",
    "    epochs=30,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text!\n",
    "def generate_text(model, seed_text, length=200, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Generate text using trained LSTM\n",
    "    \n",
    "    Args:\n",
    "        seed_text: Starting text (must be >= seq_length)\n",
    "        length: Number of characters to generate\n",
    "        temperature: Randomness (lower = more conservative)\n",
    "    \"\"\"\n",
    "    generated = seed_text\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # Prepare input\n",
    "        x_pred = np.zeros((1, seq_length, vocab_size_char))\n",
    "        for t, char in enumerate(seed_text[-seq_length:]):\n",
    "            x_pred[0, t, char_to_idx[char]] = 1\n",
    "        \n",
    "        # Predict next character\n",
    "        predictions = model.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature\n",
    "        predictions = np.log(predictions + 1e-7) / temperature\n",
    "        predictions = np.exp(predictions) / np.sum(np.exp(predictions))\n",
    "        \n",
    "        # Sample next character\n",
    "        next_idx = np.random.choice(len(predictions), p=predictions)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "        \n",
    "        generated += next_char\n",
    "        seed_text += next_char\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Generate text samples\n",
    "seed = \"In the age of artificial intelligence\"\n",
    "\n",
    "print(\"ðŸŽ¨ Generated Text Samples:\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in [0.2, 0.5, 1.0]:\n",
    "    print(f\"\\nTemperature: {temp} {'(conservative)' if temp < 0.5 else '(creative)' if temp > 0.7 else '(balanced)'}\")\n",
    "    generated = generate_text(char_model, seed, length=150, temperature=temp)\n",
    "    print(generated)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Why This Matters for Modern AI (2024-2025)\n",
    "\n",
    "### ðŸ¤– **From LSTMs to Transformers**\n",
    "\n",
    "**The Evolution:**\n",
    "```\n",
    "2014: LSTM/GRU dominate NLP\n",
    "  â””â”€â”€ Seq2Seq models (translation)\n",
    "  â””â”€â”€ Early language models\n",
    "\n",
    "2017: Attention mechanism added to LSTMs\n",
    "  â””â”€â”€ \"Attention is All You Need\" paper\n",
    "  â””â”€â”€ Birth of Transformers!\n",
    "\n",
    "2018-2025: Transformers replace LSTMs\n",
    "  â”œâ”€â”€ GPT (2018): 117M parameters\n",
    "  â”œâ”€â”€ GPT-2 (2019): 1.5B parameters\n",
    "  â”œâ”€â”€ GPT-3 (2020): 175B parameters\n",
    "  â”œâ”€â”€ GPT-4 (2023): ~1.8T parameters (estimated)\n",
    "  â””â”€â”€ Claude 3.5 Sonnet (2024): Advanced reasoning\n",
    "```\n",
    "\n",
    "### ðŸ”‘ **LSTM Concepts That Survive in Transformers:**\n",
    "\n",
    "1. **Gates â†’ Attention**\n",
    "   - LSTM gates decide what to remember/forget\n",
    "   - Transformer attention decides what to focus on\n",
    "   - Same principle: **selective information flow**\n",
    "\n",
    "2. **Cell State â†’ Residual Connections**\n",
    "   - LSTM cell state: information highway\n",
    "   - Transformer residuals: skip connections\n",
    "   - Both solve vanishing gradients!\n",
    "\n",
    "3. **Sequential Processing â†’ Positional Encoding**\n",
    "   - LSTMs: inherent order from recurrence\n",
    "   - Transformers: explicit positional embeddings\n",
    "   - Both preserve sequence information\n",
    "\n",
    "### ðŸ“Š **Where LSTMs Still Shine (2024-2025):**\n",
    "\n",
    "**1. Time Series Forecasting:**\n",
    "- Stock prices, weather, energy demand\n",
    "- More efficient than Transformers for numerical sequences\n",
    "- Lower computational cost\n",
    "\n",
    "**2. Speech Recognition:**\n",
    "- Real-time audio processing\n",
    "- Streaming applications (can't wait for full sequence)\n",
    "- Used in Siri, Alexa, Google Assistant\n",
    "\n",
    "**3. Video Analysis:**\n",
    "- Temporal modeling across frames\n",
    "- Action recognition\n",
    "- Hybrid with CNNs (ConvLSTM)\n",
    "\n",
    "**4. IoT & Edge Devices:**\n",
    "- Smaller models (GRU especially)\n",
    "- Lower memory footprint\n",
    "- Real-time processing on mobile devices\n",
    "\n",
    "**5. Hybrid Architectures:**\n",
    "- Combining LSTMs with Transformers\n",
    "- Best of both worlds!\n",
    "\n",
    "### ðŸŽ¯ **Key Takeaway:**\n",
    "Understanding LSTMs/GRUs is essential because:\n",
    "- They teach you how to handle sequences\n",
    "- Gating mechanisms appear everywhere in modern AI\n",
    "- Many production systems still use them\n",
    "- They're the stepping stone to understanding Transformers!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Interactive Exercise 1: Build a GRU Model\n",
    "\n",
    "**Challenge:** Modify the sentiment analysis model to use GRU instead of LSTM!\n",
    "\n",
    "**Your Task:**\n",
    "1. Create a GRU model with similar architecture\n",
    "2. Train for 5 epochs\n",
    "3. Compare performance with LSTM\n",
    "4. Compare training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "\n",
    "# TODO: Build GRU model (replace LSTM with GRU layer)\n",
    "# TODO: Compile with same settings\n",
    "# TODO: Train and time the training\n",
    "# TODO: Evaluate and compare with LSTM\n",
    "\n",
    "print(\"Complete the exercise above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "### âœ… Solution (Try on your own first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "import time\n",
    "\n",
    "# Build GRU model\n",
    "gru_model = keras.Sequential([\n",
    "    layers.Embedding(vocab_size, 128, input_length=max_length),\n",
    "    layers.GRU(64, dropout=0.2, recurrent_dropout=0.2),  # GRU instead of LSTM!\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "gru_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"ðŸ§  GRU Model:\")\n",
    "gru_model.summary()\n",
    "\n",
    "# Train and time\n",
    "print(\"\\nðŸš€ Training GRU model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history_gru = gru_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gru_train_time = time.time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "gru_test_loss, gru_test_acc = gru_model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "# Compare\n",
    "print(\"\\nðŸ“Š LSTM vs GRU Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<25} {'LSTM':<15} {'GRU':<15}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Test Accuracy':<25} {test_accuracy:.4f}          {gru_test_acc:.4f}\")\n",
    "print(f\"{'Parameters':<25} {lstm_model.count_params():<15} {gru_model.count_params():<15}\")\n",
    "print(f\"{'Training Time':<25} {gru_train_time:.2f}s\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if gru_test_acc > test_accuracy:\n",
    "    print(\"\\nâœ… GRU slightly outperformed LSTM!\")\n",
    "elif abs(gru_test_acc - test_accuracy) < 0.01:\n",
    "    print(\"\\nâœ… GRU and LSTM performed similarly!\")\n",
    "    print(\"   ðŸ’¡ GRU is often preferred: fewer parameters, faster training\")\n",
    "else:\n",
    "    print(\"\\nâœ… LSTM slightly outperformed GRU!\")\n",
    "    \n",
    "print(f\"\\nðŸŽ¯ Parameter reduction: {(1 - gru_model.count_params()/lstm_model.count_params())*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Interactive Exercise 2: Bidirectional LSTM\n",
    "\n",
    "**Challenge:** Create a Bidirectional LSTM!\n",
    "\n",
    "**What is Bidirectional RNN?**\n",
    "- Processes sequence both forward AND backward\n",
    "- Sees future context too!\n",
    "- Better for understanding full context\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "\"The movie was ____\"\n",
    "Forward LSTM: Only sees \"The movie was\"\n",
    "Backward LSTM: Sees what comes after!\n",
    "\n",
    "Useful when you have full sequence (not streaming)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "\n",
    "# TODO: Build model with Bidirectional wrapper\n",
    "# Hint: layers.Bidirectional(layers.LSTM(64))\n",
    "\n",
    "print(\"Complete the exercise above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "### âœ… Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# Build Bidirectional LSTM\n",
    "bilstm_model = keras.Sequential([\n",
    "    layers.Embedding(vocab_size, 128, input_length=max_length),\n",
    "    layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"ðŸ§  Bidirectional LSTM Model:\")\n",
    "bilstm_model.summary()\n",
    "\n",
    "# Train\n",
    "print(\"\\nðŸš€ Training Bidirectional LSTM...\")\n",
    "history_bilstm = bilstm_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "bilstm_test_loss, bilstm_test_acc = bilstm_model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nâœ… Bidirectional LSTM Test Accuracy: {bilstm_test_acc:.4f}\")\n",
    "print(f\"\\nðŸ“Š Comparison:\")\n",
    "print(f\"   Regular LSTM: {test_accuracy:.4f}\")\n",
    "print(f\"   Bidirectional: {bilstm_test_acc:.4f}\")\n",
    "print(f\"\\nðŸ’¡ Bidirectional sees both past and future context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- âœ… LSTM architecture with 3 gates (forget, input, output)\n",
    "- âœ… GRU architecture (simplified LSTM with 2 gates)\n",
    "- âœ… How gates solve the vanishing gradient problem\n",
    "- âœ… When to use LSTM vs GRU\n",
    "- âœ… Built sentiment analysis with LSTM (like early ChatGPT!)\n",
    "- âœ… Character-level text generation (foundation of GPT)\n",
    "- âœ… Bidirectional RNNs for full context understanding\n",
    "- âœ… How LSTM concepts evolved into Transformers\n",
    "\n",
    "### ðŸŽ¯ Key Takeaways:\n",
    "\n",
    "1. **Gates enable long-term memory**\n",
    "   - Forget gate: Remove irrelevant information\n",
    "   - Input gate: Add new information\n",
    "   - Output gate: Control what to expose\n",
    "   - Cell state: Information highway!\n",
    "\n",
    "2. **GRU is often better in practice**\n",
    "   - Fewer parameters (25% fewer than LSTM)\n",
    "   - Faster training\n",
    "   - Similar performance\n",
    "   - Use GRU unless you need very long sequences\n",
    "\n",
    "3. **LSTM/GRU â†’ Transformers evolution**\n",
    "   - Gates â†’ Attention mechanisms\n",
    "   - Cell state â†’ Residual connections\n",
    "   - Sequential processing â†’ Parallel processing\n",
    "   - Understanding LSTMs helps you understand GPT!\n",
    "\n",
    "4. **Still relevant in 2024-2025**\n",
    "   - Time series forecasting\n",
    "   - Speech recognition (streaming)\n",
    "   - Video analysis\n",
    "   - Edge devices (smaller models)\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ“š Next Lesson:** Day 3 - Sequence-to-Sequence Models\n",
    "- Encoder-Decoder architecture\n",
    "- Attention mechanisms (foundation of Transformers!)\n",
    "- Machine translation\n",
    "- Building a chatbot\n",
    "- Name generation with RNNs\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¬ Remember:**\n",
    "\n",
    "*\"LSTMs and GRUs were the breakthrough that made modern NLP possible. While Transformers have replaced them for most language tasks, understanding these architectures is essential for grasping how attention mechanisms work. The gating concepts you learned today appear everywhere in modern AI - from GPT's attention to diffusion models' conditioning!\"* ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ”— Connections to Modern AI:**\n",
    "- **Pre-Transformer Era:** LSTMs dominated NLP (2014-2017)\n",
    "- **Attention Mechanisms:** Born from LSTM limitations\n",
    "- **Transformers:** Replaced sequential processing with parallel attention\n",
    "- **Hybrid Models:** Combining LSTMs with Transformers for best results\n",
    "- **Production Systems:** Many still use GRUs for efficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
