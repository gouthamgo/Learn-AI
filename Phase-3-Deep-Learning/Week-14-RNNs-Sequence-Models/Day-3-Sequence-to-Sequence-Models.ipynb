{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Day 3: Sequence-to-Sequence Models & Attention Mechanisms\n",
    "\n",
    "**ğŸ¯ Goal:** Master Seq2Seq architecture and build real AI applications\n",
    "\n",
    "**â±ï¸ Time:** 75-90 minutes\n",
    "\n",
    "**ğŸŒŸ Why This Matters for AI:**\n",
    "- Seq2Seq is the **direct predecessor to ChatGPT**!\n",
    "- Attention mechanism (2017) â†’ Transformers â†’ GPT/Claude/Gemini\n",
    "- Powers machine translation (Google Translate before 2023)\n",
    "- Foundation of chatbots, summarization, and question-answering\n",
    "- Understanding Seq2Seq â†’ Understanding how LLMs work!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ğŸ”„ What are Sequence-to-Sequence Models?\n",
    "\n",
    "**Sequence-to-Sequence (Seq2Seq)** models transform one sequence into another sequence!\n",
    "\n",
    "### ğŸ“Š The Problem:\n",
    "\n",
    "**Previous architectures:**\n",
    "- Simple RNN/LSTM: Fixed-size input â†’ Fixed-size output\n",
    "- Example: Sentiment analysis (sentence â†’ positive/negative)\n",
    "\n",
    "**But what about:**\n",
    "- **Translation:** English sentence â†’ French sentence (different lengths!)\n",
    "- **Summarization:** Long article â†’ Short summary\n",
    "- **Chatbot:** User message â†’ Bot response\n",
    "- **Question Answering:** Question + Context â†’ Answer\n",
    "\n",
    "**These need variable-length input AND output!**\n",
    "\n",
    "### ğŸ¯ Real-World Examples:\n",
    "\n",
    "```\n",
    "MACHINE TRANSLATION:\n",
    "Input:  \"Hello, how are you?\"\n",
    "Output: \"Bonjour, comment allez-vous?\"\n",
    "\n",
    "SUMMARIZATION:\n",
    "Input:  \"Artificial intelligence has transformed modern technology...\"\n",
    "        (500 words)\n",
    "Output: \"AI revolutionizes tech through machine learning.\"\n",
    "        (50 words)\n",
    "\n",
    "CHATBOT:\n",
    "Input:  \"What's the weather like today?\"\n",
    "Output: \"The weather is sunny with a high of 75Â°F.\"\n",
    "\n",
    "QUESTION ANSWERING:\n",
    "Input:  \"Who invented the transformer architecture?\"\n",
    "Output: \"Vaswani et al. in the 'Attention is All You Need' paper (2017)\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Seq2Seq Architecture: Encoder-Decoder\n",
    "\n",
    "**Key Idea:** Split the model into two parts!\n",
    "\n",
    "### ğŸ“¦ Two Components:\n",
    "\n",
    "#### 1ï¸âƒ£ **Encoder** - Understanding the Input\n",
    "```\n",
    "Input Sequence â†’ ENCODER â†’ Context Vector (fixed-size representation)\n",
    "\n",
    "Example (Translation):\n",
    "\"Hello how are you\" â†’ [LSTM] â†’ [context vector]\n",
    "                                      â†“\n",
    "                          Compressed meaning of input!\n",
    "```\n",
    "\n",
    "#### 2ï¸âƒ£ **Decoder** - Generating the Output\n",
    "```\n",
    "Context Vector â†’ DECODER â†’ Output Sequence (token by token)\n",
    "\n",
    "Example:\n",
    "[context vector] â†’ [LSTM] â†’ \"Bonjour\" â†’ \"comment\" â†’ \"allez-vous\"\n",
    "```\n",
    "\n",
    "### ğŸ”„ Complete Architecture:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  ENCODER-DECODER                        â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  INPUT SEQUENCE                                         â”‚\n",
    "â”‚  \"Hello how are you\"                                    â”‚\n",
    "â”‚      â†“    â†“   â†“   â†“                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”                           â”‚\n",
    "â”‚  â”‚LSTMâ”‚â†’â”‚LSTMâ”‚â†’â”‚LSTMâ”‚â†’â”‚LSTMâ”‚ â† ENCODER                 â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜                           â”‚\n",
    "â”‚                        â†“                                â”‚\n",
    "â”‚                 [CONTEXT VECTOR]                        â”‚\n",
    "â”‚                   (fixed size)                          â”‚\n",
    "â”‚                        â†“                                â”‚\n",
    "â”‚                   â”Œâ”€â”€â”€â”€â”                                â”‚\n",
    "â”‚  <START> â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚LSTMâ”‚â”€â”€â”€â”€â”€â†’ \"Bonjour\"               â”‚\n",
    "â”‚                   â””â”€â”€â”€â”€â”˜                                â”‚\n",
    "â”‚                      â†“                                  â”‚\n",
    "â”‚  \"Bonjour\" â”€â”€â”€â”€â”€â”€â”€â†’â”Œâ”€â”€â”€â”€â”                              â”‚\n",
    "â”‚                    â”‚LSTMâ”‚â”€â”€â”€â”€â”€â†’ \"comment\"              â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                      â†“                                  â”‚\n",
    "â”‚  \"comment\" â”€â”€â”€â”€â”€â”€â”€â†’â”Œâ”€â”€â”€â”€â”                              â”‚\n",
    "â”‚                    â”‚LSTMâ”‚â”€â”€â”€â”€â”€â†’ \"allez-vous\"           â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                      â†“         â†‘                        â”‚\n",
    "â”‚                    <END>    DECODER                     â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ”‘ Key Concepts:\n",
    "\n",
    "**1. Context Vector (Thought Vector):**\n",
    "- Fixed-size representation of entire input\n",
    "- Acts as \"memory\" of what encoder understood\n",
    "- Passed to decoder as initial state\n",
    "- **Bottleneck:** All information must fit in this vector!\n",
    "\n",
    "**2. Teacher Forcing (Training):**\n",
    "- During training: Use ground truth as input to decoder\n",
    "- During inference: Use model's own predictions\n",
    "```\n",
    "Training:\n",
    "<START> â†’ LSTM â†’ \"Bonjour\" (correct!)\n",
    "\"Bonjour\" â†’ LSTM â†’ \"comment\" (use ground truth)\n",
    "\n",
    "Inference:\n",
    "<START> â†’ LSTM â†’ \"Bonjour\"\n",
    "\"Bonjour\" â†’ LSTM â†’ \"comment\" (use prediction!)\n",
    "```\n",
    "\n",
    "**3. Special Tokens:**\n",
    "- `<START>`: Begin decoding\n",
    "- `<END>`: Stop generating\n",
    "- `<PAD>`: Padding for batching\n",
    "- `<UNK>`: Unknown words\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## âš ï¸ The Bottleneck Problem\n",
    "\n",
    "### ğŸ› Major Issue with Basic Seq2Seq:\n",
    "\n",
    "**Problem:** Context vector is fixed-size!\n",
    "\n",
    "```\n",
    "SHORT SENTENCE: \"Hi\" â†’ [context] â†’ Easy!\n",
    "LONG SENTENCE: \"In the age of artificial intelligence, machine\n",
    "                learning, deep learning, neural networks, and \n",
    "                transformers, we build amazing AI systems that\n",
    "                can understand and generate human language...\"\n",
    "                â†“\n",
    "            [context] â† Same size! Information loss!\n",
    "```\n",
    "\n",
    "**Consequences:**\n",
    "- âŒ Forgets information from long sequences\n",
    "- âŒ Beginning of sentence lost by the end\n",
    "- âŒ Poor performance on long inputs\n",
    "- âŒ Can't focus on relevant parts\n",
    "\n",
    "**Solution:** **ATTENTION MECHANISM!** âš¡\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## ğŸ‘ï¸ Attention Mechanism: The Game Changer!\n",
    "\n",
    "**Invented in 2014 (Bahdanau et al.) and perfected in 2017 (Vaswani et al.)**\n",
    "- Revolutionized sequence modeling\n",
    "- Led to Transformers (\"Attention is All You Need\")\n",
    "- Foundation of GPT, BERT, Claude, Gemini!\n",
    "\n",
    "### ğŸ’¡ The Core Idea:\n",
    "\n",
    "> Instead of compressing everything into one context vector,\n",
    "> let the decoder **attend to (focus on)** different parts of the input!\n",
    "\n",
    "**Human Analogy:**\n",
    "```\n",
    "Translating: \"The cat sat on the mat\"\n",
    "\n",
    "When generating \"chat\" (cat in French):\n",
    "â†’ Focus on \"cat\" in the input!\n",
    "\n",
    "When generating \"tapis\" (mat in French):\n",
    "â†’ Focus on \"mat\" in the input!\n",
    "\n",
    "Don't try to remember everything at once!\n",
    "```\n",
    "\n",
    "### ğŸ” How Attention Works:\n",
    "\n",
    "**Step-by-step:**\n",
    "\n",
    "```python\n",
    "# 1. Encoder creates hidden states for ALL input tokens\n",
    "encoder_outputs = [hâ‚, hâ‚‚, hâ‚ƒ, hâ‚„]  # For \"Hello how are you\"\n",
    "\n",
    "# 2. For each decoder step, compute attention scores\n",
    "# \"How relevant is each encoder output to current decoder state?\"\n",
    "scores = [score(decoder_state, hâ‚),\n",
    "          score(decoder_state, hâ‚‚),\n",
    "          score(decoder_state, hâ‚ƒ),\n",
    "          score(decoder_state, hâ‚„)]\n",
    "\n",
    "# 3. Softmax to get attention weights (sum to 1)\n",
    "attention_weights = softmax(scores)  # [0.1, 0.6, 0.2, 0.1]\n",
    "                                     #       â†‘\n",
    "                                  # Focusing on \"how\"!\n",
    "\n",
    "# 4. Weighted sum of encoder outputs\n",
    "context = 0.1*hâ‚ + 0.6*hâ‚‚ + 0.2*hâ‚ƒ + 0.1*hâ‚„\n",
    "\n",
    "# 5. Use this context to generate next word!\n",
    "output = decoder(context, decoder_state)\n",
    "```\n",
    "\n",
    "### ğŸ¯ Attention Visualization:\n",
    "\n",
    "```\n",
    "INPUT:  \"The  cat  sat  on  the  mat\"\n",
    "         â†“    â†“    â†“    â†“    â†“    â†“\n",
    "        [hâ‚] [hâ‚‚] [hâ‚ƒ] [hâ‚„] [hâ‚…] [hâ‚†] â† Encoder outputs\n",
    "         â†“    â†“    â†“    â†“    â†“    â†“\n",
    "ATTENTION WEIGHTS (when generating \"chat\"):\n",
    "        0.05  0.85  0.03  0.02  0.03  0.02\n",
    "              ^^^\n",
    "         Focusing on \"cat\"!\n",
    "\n",
    "OUTPUT: \"Le   chat  Ã©tait sur  le   tapis\"\n",
    "```\n",
    "\n",
    "### ğŸŒŸ Benefits of Attention:\n",
    "\n",
    "1. **No fixed bottleneck** - access all encoder outputs!\n",
    "2. **Better long sequences** - can focus on distant words\n",
    "3. **Interpretability** - see what model focuses on\n",
    "4. **Better performance** - state-of-the-art results\n",
    "\n",
    "### ğŸš€ From Attention to Transformers:\n",
    "\n",
    "```\n",
    "2014: Attention added to Seq2Seq (Bahdanau)\n",
    "  â†“\n",
    "2017: \"Attention is All You Need\" (Vaswani et al.)\n",
    "  â†“   Remove RNNs entirely!\n",
    "  â†“   Use ONLY attention (self-attention)\n",
    "  â†“\n",
    "TRANSFORMER architecture born!\n",
    "  â†“\n",
    "2018: GPT-1 (decoder-only Transformer)\n",
    "2018: BERT (encoder-only Transformer)\n",
    "2019: GPT-2\n",
    "2020: GPT-3\n",
    "2022: ChatGPT\n",
    "2023: GPT-4, Claude, Gemini\n",
    "2024-2025: Advanced reasoning models\n",
    "```\n",
    "\n",
    "**Understanding attention is THE KEY to understanding modern AI!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## ğŸš€ Real AI Example 1: Simple Chatbot with Seq2Seq\n",
    "\n",
    "**Project:** Build a simple chatbot using Seq2Seq!\n",
    "\n",
    "**Architecture:**\n",
    "- Encoder: Process user message\n",
    "- Decoder: Generate bot response\n",
    "- Training: Conversation pairs\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow numpy matplotlib --quiet\n",
    "\n",
    "print(\"âœ… Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"ğŸ“š Libraries loaded!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Step 1: Create Training Data\n",
    "\n",
    "We'll create simple question-answer pairs for our chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation pairs (input, target)\n",
    "conversations = [\n",
    "    # Greetings\n",
    "    (\"hi\", \"hello\"),\n",
    "    (\"hello\", \"hi there\"),\n",
    "    (\"hey\", \"hello\"),\n",
    "    (\"good morning\", \"good morning to you\"),\n",
    "    (\"good evening\", \"good evening\"),\n",
    "    \n",
    "    # How are you\n",
    "    (\"how are you\", \"i am doing great\"),\n",
    "    (\"how are you doing\", \"i am fine thanks\"),\n",
    "    (\"are you okay\", \"yes i am okay\"),\n",
    "    \n",
    "    # Questions about AI\n",
    "    (\"what is ai\", \"artificial intelligence is machine learning\"),\n",
    "    (\"what is machine learning\", \"machine learning is learning from data\"),\n",
    "    (\"what is deep learning\", \"deep learning uses neural networks\"),\n",
    "    (\"what are transformers\", \"transformers use attention mechanisms\"),\n",
    "    \n",
    "    # About the bot\n",
    "    (\"what is your name\", \"i am a chatbot\"),\n",
    "    (\"who are you\", \"i am an ai assistant\"),\n",
    "    (\"what can you do\", \"i can answer simple questions\"),\n",
    "    \n",
    "    # Thanks and goodbye\n",
    "    (\"thank you\", \"you are welcome\"),\n",
    "    (\"thanks\", \"happy to help\"),\n",
    "    (\"bye\", \"goodbye\"),\n",
    "    (\"goodbye\", \"see you later\"),\n",
    "    \n",
    "    # Fun questions\n",
    "    (\"tell me a joke\", \"why did the neural network cross the road\"),\n",
    "    (\"are you smart\", \"i am learning every day\"),\n",
    "    (\"do you like ai\", \"yes i love ai\"),\n",
    "]\n",
    "\n",
    "print(\"ğŸ’¬ Chatbot Training Data:\")\n",
    "print(f\"   Number of conversations: {len(conversations)}\")\n",
    "print(\"\\nğŸ“ Sample conversations:\")\n",
    "for i in range(5):\n",
    "    user, bot = conversations[i]\n",
    "    print(f\"   User: {user}\")\n",
    "    print(f\"   Bot:  {bot}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Step 2: Tokenize and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens and create vocabulary\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "PAD_TOKEN = '<PAD>'\n",
    "\n",
    "# Get all unique words\n",
    "all_words = set()\n",
    "for user_msg, bot_msg in conversations:\n",
    "    all_words.update(user_msg.split())\n",
    "    all_words.update(bot_msg.split())\n",
    "\n",
    "# Create vocabulary with special tokens\n",
    "vocab = [PAD_TOKEN, START_TOKEN, END_TOKEN] + sorted(list(all_words))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"ğŸ“š Vocabulary:\")\n",
    "print(f\"   Vocabulary size: {vocab_size}\")\n",
    "print(f\"   Sample words: {vocab[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to sequences of indices\n",
    "def sentence_to_sequence(sentence, word_to_idx):\n",
    "    \"\"\"Convert sentence to sequence of word indices\"\"\"\n",
    "    return [word_to_idx[word] for word in sentence.split()]\n",
    "\n",
    "def sequence_to_sentence(sequence, idx_to_word):\n",
    "    \"\"\"Convert sequence of indices back to sentence\"\"\"\n",
    "    return ' '.join([idx_to_word[idx] for idx in sequence if idx not in [0, 1, 2]])\n",
    "\n",
    "# Prepare encoder and decoder sequences\n",
    "encoder_inputs = []\n",
    "decoder_inputs = []\n",
    "decoder_targets = []\n",
    "\n",
    "for user_msg, bot_msg in conversations:\n",
    "    # Encoder input: user message\n",
    "    encoder_seq = sentence_to_sequence(user_msg, word_to_idx)\n",
    "    encoder_inputs.append(encoder_seq)\n",
    "    \n",
    "    # Decoder input: <START> + bot message\n",
    "    decoder_seq = [word_to_idx[START_TOKEN]] + sentence_to_sequence(bot_msg, word_to_idx)\n",
    "    decoder_inputs.append(decoder_seq)\n",
    "    \n",
    "    # Decoder target: bot message + <END>\n",
    "    target_seq = sentence_to_sequence(bot_msg, word_to_idx) + [word_to_idx[END_TOKEN]]\n",
    "    decoder_targets.append(target_seq)\n",
    "\n",
    "print(\"âœ… Sequences prepared!\")\n",
    "print(f\"\\nğŸ“ Example:\")\n",
    "print(f\"   User: {conversations[0][0]}\")\n",
    "print(f\"   Encoder input: {encoder_inputs[0]}\")\n",
    "print(f\"   Bot: {conversations[0][1]}\")\n",
    "print(f\"   Decoder input: {decoder_inputs[0]}\")\n",
    "print(f\"   Decoder target: {decoder_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_encoder_len = max(len(seq) for seq in encoder_inputs)\n",
    "max_decoder_len = max(len(seq) for seq in decoder_inputs)\n",
    "\n",
    "encoder_inputs_padded = pad_sequences(encoder_inputs, maxlen=max_encoder_len, padding='post')\n",
    "decoder_inputs_padded = pad_sequences(decoder_inputs, maxlen=max_decoder_len, padding='post')\n",
    "decoder_targets_padded = pad_sequences(decoder_targets, maxlen=max_decoder_len, padding='post')\n",
    "\n",
    "# Convert to one-hot for decoder targets\n",
    "decoder_targets_one_hot = tf.keras.utils.to_categorical(decoder_targets_padded, num_classes=vocab_size)\n",
    "\n",
    "print(\"âœ… Data padded and ready!\")\n",
    "print(f\"   Encoder input shape: {encoder_inputs_padded.shape}\")\n",
    "print(f\"   Decoder input shape: {decoder_inputs_padded.shape}\")\n",
    "print(f\"   Decoder target shape: {decoder_targets_one_hot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Step 3: Build Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "embedding_dim = 64\n",
    "hidden_units = 128\n",
    "\n",
    "# ENCODER\n",
    "encoder_inputs_layer = layers.Input(shape=(max_encoder_len,))\n",
    "encoder_embedding = layers.Embedding(vocab_size, embedding_dim)(encoder_inputs_layer)\n",
    "encoder_lstm = layers.LSTM(hidden_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]  # Save states for decoder\n",
    "\n",
    "# DECODER\n",
    "decoder_inputs_layer = layers.Input(shape=(max_decoder_len,))\n",
    "decoder_embedding = layers.Embedding(vocab_size, embedding_dim)(decoder_inputs_layer)\n",
    "decoder_lstm = layers.LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = layers.Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Complete model\n",
    "model = keras.Model([encoder_inputs_layer, decoder_inputs_layer], decoder_outputs)\n",
    "\n",
    "print(\"ğŸ§  Seq2Seq Model built!\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… Model compiled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Step 4: Train the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"ğŸš€ Training chatbot...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    [encoder_inputs_padded, decoder_inputs_padded],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=8,\n",
    "    epochs=200,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "print(f\"   Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"   Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "plt.title('Chatbot Model Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "plt.title('Chatbot Model Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Step 5: Build Inference Models for Chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder inference model\n",
    "encoder_model = keras.Model(encoder_inputs_layer, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = layers.Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = layers.Input(shape=(hidden_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_embedding_inf = layers.Embedding(vocab_size, embedding_dim)(decoder_inputs_layer)\n",
    "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(\n",
    "    decoder_embedding_inf, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states_inf = [state_h_inf, state_c_inf]\n",
    "decoder_outputs_inf = decoder_dense(decoder_outputs_inf)\n",
    "\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs_layer] + decoder_states_inputs,\n",
    "    [decoder_outputs_inf] + decoder_states_inf\n",
    ")\n",
    "\n",
    "print(\"âœ… Inference models created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat function\n",
    "def chat(user_input):\n",
    "    \"\"\"\n",
    "    Generate bot response to user input\n",
    "    \"\"\"\n",
    "    # Encode user input\n",
    "    input_seq = sentence_to_sequence(user_input.lower(), word_to_idx)\n",
    "    input_seq_padded = pad_sequences([input_seq], maxlen=max_encoder_len, padding='post')\n",
    "    \n",
    "    # Get encoder states\n",
    "    states_value = encoder_model.predict(input_seq_padded, verbose=0)\n",
    "    \n",
    "    # Start with START token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word_to_idx[START_TOKEN]\n",
    "    \n",
    "    # Generate response\n",
    "    decoded_sentence = []\n",
    "    max_length = 20\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Predict next word\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, verbose=0\n",
    "        )\n",
    "        \n",
    "        # Sample token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = idx_to_word[sampled_token_index]\n",
    "        \n",
    "        # Exit if END token\n",
    "        if sampled_word == END_TOKEN:\n",
    "            break\n",
    "        \n",
    "        decoded_sentence.append(sampled_word)\n",
    "        \n",
    "        # Update target sequence and states\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return ' '.join(decoded_sentence)\n",
    "\n",
    "print(\"âœ… Chat function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Step 6: Test the Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chatbot\n",
    "test_inputs = [\n",
    "    \"hi\",\n",
    "    \"how are you\",\n",
    "    \"what is ai\",\n",
    "    \"what is your name\",\n",
    "    \"thank you\",\n",
    "    \"goodbye\",\n",
    "    \"what is machine learning\",\n",
    "    \"tell me a joke\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ’¬ Chatbot Conversation:\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for user_msg in test_inputs:\n",
    "    bot_response = chat(user_msg)\n",
    "    print(f\"You: {user_msg}\")\n",
    "    print(f\"Bot: {bot_response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## ğŸ¨ Real AI Example 2: Name Generation with RNNs\n",
    "\n",
    "**Project:** Generate creative names using character-level RNN!\n",
    "\n",
    "**Applications:**\n",
    "- Brand name generation\n",
    "- Character names for games/stories\n",
    "- Product naming\n",
    "- Creative writing assistance\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample names dataset\n",
    "names = [\n",
    "    # Tech company names\n",
    "    \"Google\", \"Amazon\", \"Microsoft\", \"Apple\", \"Meta\", \"Tesla\", \"Netflix\",\n",
    "    \"Spotify\", \"Adobe\", \"Oracle\", \"Nvidia\", \"Intel\", \"AMD\", \"Zoom\",\n",
    "    \n",
    "    # AI company names\n",
    "    \"OpenAI\", \"Anthropic\", \"DeepMind\", \"Cohere\", \"Hugging\", \"Stability\",\n",
    "    \"Replicate\", \"Inflection\", \"Adept\", \"Character\",\n",
    "    \n",
    "    # Fantasy names\n",
    "    \"Aria\", \"Luna\", \"Nova\", \"Zara\", \"Kai\", \"Rex\", \"Max\", \"Leo\",\n",
    "    \"Maya\", \"Zoe\", \"Ivy\", \"Ruby\", \"Sky\", \"River\", \"Storm\",\n",
    "]\n",
    "\n",
    "# Convert to lowercase\n",
    "names = [name.lower() for name in names]\n",
    "\n",
    "print(\"ğŸ“ Name Dataset:\")\n",
    "print(f\"   Number of names: {len(names)}\")\n",
    "print(f\"   Sample names: {names[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character vocabulary\n",
    "all_chars = set(''.join(names))\n",
    "chars = ['<PAD>', '<START>', '<END>'] + sorted(list(all_chars))\n",
    "char_to_idx_name = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char_name = {idx: ch for idx, ch in enumerate(chars)}\n",
    "vocab_size_name = len(chars)\n",
    "\n",
    "print(f\"ğŸ“š Character vocabulary:\")\n",
    "print(f\"   Vocabulary size: {vocab_size_name}\")\n",
    "print(f\"   Characters: {chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences\n",
    "max_name_len = max(len(name) for name in names) + 2  # +2 for START and END tokens\n",
    "\n",
    "X_names = []\n",
    "y_names = []\n",
    "\n",
    "for name in names:\n",
    "    # Add START and END tokens\n",
    "    name_with_tokens = '<START>' + name + '<END>'\n",
    "    \n",
    "    # Create input-output pairs\n",
    "    for i in range(1, len(name_with_tokens)):\n",
    "        input_seq = name_with_tokens[:i]\n",
    "        target_char = name_with_tokens[i]\n",
    "        \n",
    "        # Convert to indices\n",
    "        input_indices = [char_to_idx_name[ch] for ch in input_seq]\n",
    "        target_index = char_to_idx_name[target_char]\n",
    "        \n",
    "        X_names.append(input_indices)\n",
    "        y_names.append(target_index)\n",
    "\n",
    "# Pad sequences\n",
    "X_names_padded = pad_sequences(X_names, maxlen=max_name_len, padding='post')\n",
    "y_names_cat = tf.keras.utils.to_categorical(y_names, num_classes=vocab_size_name)\n",
    "\n",
    "print(\"âœ… Name sequences prepared!\")\n",
    "print(f\"   X shape: {X_names_padded.shape}\")\n",
    "print(f\"   y shape: {y_names_cat.shape}\")\n",
    "print(f\"   Total training samples: {len(X_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build name generation model\n",
    "name_model = keras.Sequential([\n",
    "    layers.Embedding(vocab_size_name, 32, input_length=max_name_len),\n",
    "    layers.LSTM(64, return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(vocab_size_name, activation='softmax')\n",
    "])\n",
    "\n",
    "name_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"ğŸ§  Name Generation Model:\")\n",
    "name_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train name generator\n",
    "print(\"ğŸš€ Training name generator...\\n\")\n",
    "\n",
    "history_names = name_model.fit(\n",
    "    X_names_padded, y_names_cat,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "print(f\"   Final accuracy: {history_names.history['accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate names\n",
    "def generate_name(model, max_length=15, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Generate a name using the trained model\n",
    "    \"\"\"\n",
    "    # Start with START token\n",
    "    current_seq = [char_to_idx_name['<START>']]\n",
    "    generated_name = \"\"\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Pad and predict\n",
    "        padded = pad_sequences([current_seq], maxlen=max_name_len, padding='post')\n",
    "        predictions = model.predict(padded, verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature\n",
    "        predictions = np.log(predictions + 1e-7) / temperature\n",
    "        predictions = np.exp(predictions) / np.sum(np.exp(predictions))\n",
    "        \n",
    "        # Sample next character\n",
    "        next_idx = np.random.choice(len(predictions), p=predictions)\n",
    "        next_char = idx_to_char_name[next_idx]\n",
    "        \n",
    "        # Stop if END token\n",
    "        if next_char == '<END>':\n",
    "            break\n",
    "        \n",
    "        if next_char != '<PAD>' and next_char != '<START>':\n",
    "            generated_name += next_char\n",
    "        \n",
    "        current_seq.append(next_idx)\n",
    "    \n",
    "    return generated_name.capitalize()\n",
    "\n",
    "# Generate sample names\n",
    "print(\"ğŸ¨ Generated Names:\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in [0.3, 0.5, 0.8, 1.0]:\n",
    "    print(f\"\\nTemperature: {temp} {'(conservative)' if temp < 0.5 else '(creative)' if temp > 0.7 else '(balanced)'}\")\n",
    "    generated_names = [generate_name(name_model, temperature=temp) for _ in range(5)]\n",
    "    print(f\"Names: {', '.join(generated_names)}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## ğŸ¯ Why This Matters for Modern AI (2024-2025)\n",
    "\n",
    "### ğŸ¤– **Seq2Seq â†’ ChatGPT Evolution**\n",
    "\n",
    "**The Path to Modern LLMs:**\n",
    "\n",
    "```\n",
    "2014: Seq2Seq (Sutskever et al.)\n",
    "  â”œâ”€â”€ Encoder-Decoder architecture\n",
    "  â”œâ”€â”€ Used for machine translation\n",
    "  â””â”€â”€ Problem: Fixed context vector bottleneck\n",
    "\n",
    "2015: Attention Mechanism (Bahdanau et al.)\n",
    "  â”œâ”€â”€ Decoder can \"look back\" at encoder outputs\n",
    "  â”œâ”€â”€ Solves long-sequence problem\n",
    "  â””â”€â”€ Enables better translation\n",
    "\n",
    "2017: Transformer (\"Attention is All You Need\")\n",
    "  â”œâ”€â”€ Remove RNNs entirely!\n",
    "  â”œâ”€â”€ Use ONLY attention (self-attention)\n",
    "  â”œâ”€â”€ Parallel processing (faster!)\n",
    "  â””â”€â”€ Birth of modern NLP\n",
    "\n",
    "2018-2019: GPT-1 & GPT-2\n",
    "  â”œâ”€â”€ Decoder-only Transformer\n",
    "  â”œâ”€â”€ Pre-training on massive text\n",
    "  â””â”€â”€ Text generation becomes viable\n",
    "\n",
    "2020: GPT-3\n",
    "  â”œâ”€â”€ 175B parameters\n",
    "  â”œâ”€â”€ Few-shot learning\n",
    "  â””â”€â”€ Near-human text generation\n",
    "\n",
    "2022: ChatGPT\n",
    "  â”œâ”€â”€ GPT-3.5 + RLHF (reinforcement learning)\n",
    "  â”œâ”€â”€ Conversational AI\n",
    "  â””â”€â”€ Viral success (100M users in 2 months!)\n",
    "\n",
    "2023-2025: GPT-4, Claude, Gemini\n",
    "  â”œâ”€â”€ Multimodal (vision + text)\n",
    "  â”œâ”€â”€ Advanced reasoning\n",
    "  â”œâ”€â”€ Agentic capabilities\n",
    "  â””â”€â”€ Production-ready AI systems\n",
    "```\n",
    "\n",
    "### ğŸ”‘ **Key Concepts That Survived:**\n",
    "\n",
    "**1. Encoder-Decoder Concept:**\n",
    "- Seq2Seq: Encoder â†’ Decoder\n",
    "- Transformer: Encoder â†’ Decoder (BERT uses encoder, GPT uses decoder)\n",
    "- Same principle, better implementation!\n",
    "\n",
    "**2. Attention Mechanism:**\n",
    "- Seq2Seq Attention: Decoder attends to encoder outputs\n",
    "- Transformer Self-Attention: Tokens attend to all other tokens\n",
    "- Multi-Head Attention: Multiple attention patterns\n",
    "- **This is THE breakthrough that enabled ChatGPT!**\n",
    "\n",
    "**3. Autoregressive Generation:**\n",
    "- Seq2Seq: Generate one token at a time\n",
    "- GPT: Same! Generate one token at a time\n",
    "- Both use previous tokens to predict next\n",
    "\n",
    "### ğŸ“Š **Where Seq2Seq Still Shines:**\n",
    "\n",
    "**1. Specific Translation Tasks:**\n",
    "- Domain-specific translation (medical, legal)\n",
    "- Low-resource languages\n",
    "- Smaller models for edge devices\n",
    "\n",
    "**2. Time Series Forecasting:**\n",
    "- Encoder: Historical data\n",
    "- Decoder: Future predictions\n",
    "- More efficient than Transformers for numerical data\n",
    "\n",
    "**3. Video Captioning:**\n",
    "- Encoder: CNN for frames\n",
    "- Decoder: RNN for caption\n",
    "- Hybrid architectures\n",
    "\n",
    "**4. Speech Recognition:**\n",
    "- Encoder: Audio features\n",
    "- Decoder: Text transcription\n",
    "- Used in Siri, Alexa, Google Assistant\n",
    "\n",
    "### ğŸ“ **Understanding the Evolution:**\n",
    "\n",
    "```\n",
    "Seq2Seq taught us:\n",
    "  â”œâ”€â”€ Encoder-Decoder pattern\n",
    "  â”œâ”€â”€ Attention mechanisms\n",
    "  â”œâ”€â”€ Teacher forcing\n",
    "  â””â”€â”€ Autoregressive generation\n",
    "\n",
    "Transformers improved:\n",
    "  â”œâ”€â”€ Replaced RNNs with self-attention\n",
    "  â”œâ”€â”€ Parallel processing (much faster!)\n",
    "  â”œâ”€â”€ Better long-range dependencies\n",
    "  â””â”€â”€ Scalable to billions of parameters\n",
    "\n",
    "Modern LLMs added:\n",
    "  â”œâ”€â”€ Massive scale (trillions of parameters)\n",
    "  â”œâ”€â”€ Pre-training on internet-scale data\n",
    "  â”œâ”€â”€ Instruction tuning (RLHF)\n",
    "  â””â”€â”€ Multimodal capabilities\n",
    "```\n",
    "\n",
    "### ğŸ¯ **Key Takeaway:**\n",
    "\n",
    "**Every concept you learned today appears in ChatGPT!**\n",
    "- Encoder-Decoder â†’ Transformer architecture\n",
    "- Attention â†’ Self-attention, multi-head attention\n",
    "- Seq2Seq chatbot â†’ ChatGPT conversational AI\n",
    "- Character generation â†’ Token-by-token text generation\n",
    "\n",
    "**Understanding Seq2Seq = Understanding how LLMs work!** ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## ğŸ¯ Interactive Exercise: Improve the Chatbot\n",
    "\n",
    "**Challenge:** Enhance the chatbot with more training data!\n",
    "\n",
    "**Your Task:**\n",
    "1. Add 10 more conversation pairs to the training data\n",
    "2. Retrain the model\n",
    "3. Test with new questions\n",
    "4. Compare performance before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "\n",
    "# TODO: Add more conversation pairs\n",
    "# TODO: Recreate vocabulary and sequences\n",
    "# TODO: Retrain model\n",
    "# TODO: Test chatbot\n",
    "\n",
    "print(\"Complete the exercise above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- âœ… Sequence-to-Sequence architecture (Encoder-Decoder)\n",
    "- âœ… Context vector and the bottleneck problem\n",
    "- âœ… Attention mechanisms (foundation of Transformers!)\n",
    "- âœ… How attention solved the long-sequence problem\n",
    "- âœ… Built a working chatbot with Seq2Seq\n",
    "- âœ… Character-level name generation\n",
    "- âœ… The evolution from Seq2Seq â†’ Attention â†’ Transformers â†’ ChatGPT\n",
    "- âœ… How modern LLMs work under the hood!\n",
    "\n",
    "### ğŸ¯ Key Takeaways:\n",
    "\n",
    "1. **Encoder-Decoder separates understanding and generation**\n",
    "   - Encoder: Compress input to context vector\n",
    "   - Decoder: Generate output from context\n",
    "   - Both are LSTMs/GRUs in classic Seq2Seq\n",
    "\n",
    "2. **Attention is the game-changer**\n",
    "   - Solves fixed-size bottleneck\n",
    "   - Allows dynamic focus on relevant parts\n",
    "   - Led to Transformers (\"Attention is All You Need\")\n",
    "   - **Foundation of all modern LLMs!**\n",
    "\n",
    "3. **Seq2Seq â†’ ChatGPT evolution**\n",
    "   - Same core concepts (encoder-decoder, attention)\n",
    "   - Transformers replaced RNNs with self-attention\n",
    "   - Massive scale enabled by parallel processing\n",
    "   - Understanding Seq2Seq = Understanding LLMs!\n",
    "\n",
    "4. **Applications everywhere**\n",
    "   - Translation, summarization, chatbots\n",
    "   - Question answering, text generation\n",
    "   - Speech recognition, video captioning\n",
    "   - **Every conversational AI uses these concepts!**\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ¯ Week 14 Complete! You now understand:**\n",
    "1. **Day 1:** RNNs and sequential processing\n",
    "2. **Day 2:** LSTMs/GRUs and gating mechanisms\n",
    "3. **Day 3:** Seq2Seq and attention mechanisms\n",
    "\n",
    "**This is the foundation of modern AI!**\n",
    "- You understand how sequences are processed\n",
    "- You know how attention mechanisms work\n",
    "- You're ready to learn Transformers (Week 16)!\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“š Next Week:** Week 15 - NLP & Text Processing\n",
    "- Text preprocessing and tokenization\n",
    "- Word embeddings (Word2Vec, GloVe)\n",
    "- Contextual embeddings (ELMo, BERT)\n",
    "- Modern tokenization (BPE, WordPiece)\n",
    "- Preparing text for LLMs\n",
    "\n",
    "Then Week 16: **Transformers & Attention!**\n",
    "- Self-attention mechanism\n",
    "- Multi-head attention\n",
    "- Transformer architecture\n",
    "- How GPT, BERT, and Claude work\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¬ Remember:**\n",
    "\n",
    "*\"Sequence-to-Sequence models with attention mechanisms were the missing piece that made modern NLP possible. When researchers removed the RNN and kept only the attention, Transformers were born - and with them, GPT, BERT, ChatGPT, Claude, and the entire AI revolution of 2022-2025. You now understand the foundations that power every conversation with ChatGPT!\"* ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ”— The Complete Journey:**\n",
    "```\n",
    "Week 14: RNNs & Seq2Seq (You are here!)\n",
    "   â†“\n",
    "Week 15: NLP & Text Processing\n",
    "   â†“\n",
    "Week 16: Transformers & Attention\n",
    "   â†“\n",
    "You'll understand how ChatGPT, Claude, and all LLMs work! ğŸ‰\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
