{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# ğŸ“˜ Day 3: Computer Vision Applications\n",
        "\n",
        "**ğŸ¯ Goal:** Build real-world computer vision applications with CNNs\n",
        "\n",
        "**â±ï¸ Time:** 90-120 minutes\n",
        "\n",
        "**ğŸŒŸ Why This Matters for AI:**\n",
        "- Computer vision powers autonomous vehicles, medical imaging, and surveillance\n",
        "- Object detection enables self-driving cars and robotics\n",
        "- Face recognition used in security, authentication, and social media\n",
        "- Image segmentation critical for medical imaging and AR/VR\n",
        "- These techniques are building blocks for multimodal AI like GPT-4V\n",
        "\n",
        "**ğŸ”¥ 2024-2025 AI Trends You'll Learn:**\n",
        "- YOLO (You Only Look Once) for real-time object detection\n",
        "- Semantic segmentation for autonomous vehicles\n",
        "- Face recognition with deep learning\n",
        "- Emotion detection for human-AI interaction\n",
        "- Custom vision AI for specialized domains\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "## ğŸ¯ Computer Vision Task Taxonomy\n",
        "\n",
        "### ğŸ“Š Main Computer Vision Tasks:\n",
        "\n",
        "```\n",
        "COMPUTER VISION\n",
        "â”‚\n",
        "â”œâ”€â”€ 1. IMAGE CLASSIFICATION\n",
        "â”‚   â”œâ”€â”€ Input: Image\n",
        "â”‚   â”œâ”€â”€ Output: Single label (cat, dog, car)\n",
        "â”‚   â””â”€â”€ Use: Content categorization, tagging\n",
        "â”‚\n",
        "â”œâ”€â”€ 2. OBJECT DETECTION\n",
        "â”‚   â”œâ”€â”€ Input: Image\n",
        "â”‚   â”œâ”€â”€ Output: Bounding boxes + labels\n",
        "â”‚   â”œâ”€â”€ Where + What\n",
        "â”‚   â””â”€â”€ Use: Autonomous vehicles, surveillance\n",
        "â”‚\n",
        "â”œâ”€â”€ 3. SEMANTIC SEGMENTATION\n",
        "â”‚   â”œâ”€â”€ Input: Image\n",
        "â”‚   â”œâ”€â”€ Output: Pixel-level classification\n",
        "â”‚   â”œâ”€â”€ Every pixel labeled\n",
        "â”‚   â””â”€â”€ Use: Medical imaging, autonomous driving\n",
        "â”‚\n",
        "â”œâ”€â”€ 4. INSTANCE SEGMENTATION\n",
        "â”‚   â”œâ”€â”€ Input: Image\n",
        "â”‚   â”œâ”€â”€ Output: Separate each object instance\n",
        "â”‚   â”œâ”€â”€ Segment + identify individual objects\n",
        "â”‚   â””â”€â”€ Use: Robotics, AR/VR\n",
        "â”‚\n",
        "â”œâ”€â”€ 5. FACE RECOGNITION\n",
        "â”‚   â”œâ”€â”€ Input: Face image\n",
        "â”‚   â”œâ”€â”€ Output: Identity or embedding\n",
        "â”‚   â”œâ”€â”€ Face detection + identification\n",
        "â”‚   â””â”€â”€ Use: Security, authentication, social media\n",
        "â”‚\n",
        "â””â”€â”€ 6. POSE ESTIMATION\n",
        "    â”œâ”€â”€ Input: Image\n",
        "    â”œâ”€â”€ Output: Key points (joints, landmarks)\n",
        "    â”œâ”€â”€ Skeleton detection\n",
        "    â””â”€â”€ Use: Action recognition, fitness apps\n",
        "```\n",
        "\n",
        "### ğŸ¯ Difficulty & Requirements:\n",
        "\n",
        "| Task | Difficulty | Data Needs | Speed | Applications |\n",
        "|------|------------|------------|-------|-------------|\n",
        "| **Classification** | â­ Easy | Low | Fast | Tagging, filtering |\n",
        "| **Object Detection** | â­â­â­ Medium | Medium | Medium | Autonomous vehicles |\n",
        "| **Segmentation** | â­â­â­â­ Hard | High | Slow | Medical imaging |\n",
        "| **Instance Segmentation** | â­â­â­â­â­ Very Hard | Very High | Slow | Robotics, AR |\n",
        "| **Face Recognition** | â­â­â­ Medium | Medium | Fast | Security, social |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "## ğŸ“¦ Object Detection: YOLO Concepts\n",
        "\n",
        "**Object Detection** = Find WHERE objects are + WHAT they are\n",
        "\n",
        "### ğŸ¯ Evolution of Object Detection:\n",
        "\n",
        "```\n",
        "2014: R-CNN (Region-based CNN)\n",
        "      â”œâ”€â”€ Generate region proposals (slow!)\n",
        "      â”œâ”€â”€ Classify each region\n",
        "      â””â”€â”€ Speed: ~20 seconds per image\n",
        "\n",
        "2015: Fast R-CNN\n",
        "      â”œâ”€â”€ Share CNN computation\n",
        "      â””â”€â”€ Speed: ~2 seconds per image\n",
        "\n",
        "2015: Faster R-CNN\n",
        "      â”œâ”€â”€ Region Proposal Network (RPN)\n",
        "      â””â”€â”€ Speed: ~0.2 seconds per image\n",
        "\n",
        "2016: YOLO (You Only Look Once) âš¡\n",
        "      â”œâ”€â”€ Single-stage detector\n",
        "      â”œâ”€â”€ Entire image in one pass\n",
        "      â””â”€â”€ Speed: ~0.022 seconds (45 FPS!)\n",
        "\n",
        "2017: YOLOv2, YOLOv3\n",
        "      â”œâ”€â”€ Better accuracy + speed\n",
        "      â””â”€â”€ Multi-scale predictions\n",
        "\n",
        "2020: YOLOv4, YOLOv5\n",
        "      â”œâ”€â”€ State-of-the-art accuracy\n",
        "      â””â”€â”€ Easy to train and deploy\n",
        "\n",
        "2023: YOLOv8, YOLOv9\n",
        "      â”œâ”€â”€ Transformer-inspired improvements\n",
        "      â””â”€â”€ Best accuracy/speed trade-off\n",
        "```\n",
        "\n",
        "### ğŸ§  YOLO Architecture (Simplified):\n",
        "\n",
        "```\n",
        "INPUT IMAGE (416Ã—416)\n",
        "    â†“\n",
        "DARKNET BACKBONE (CNN)\n",
        "    â”œâ”€â”€ Extract features\n",
        "    â”œâ”€â”€ Multi-scale feature maps\n",
        "    â””â”€â”€ Similar to ResNet/VGG\n",
        "    â†“\n",
        "DETECTION HEAD\n",
        "    â”œâ”€â”€ Grid division (e.g., 13Ã—13)\n",
        "    â”œâ”€â”€ Each cell predicts:\n",
        "    â”‚   â”œâ”€â”€ Bounding box coordinates (x, y, w, h)\n",
        "    â”‚   â”œâ”€â”€ Confidence score\n",
        "    â”‚   â””â”€â”€ Class probabilities (80 classes in COCO)\n",
        "    â””â”€â”€ Anchor boxes (multiple per cell)\n",
        "    â†“\n",
        "POST-PROCESSING\n",
        "    â”œâ”€â”€ Non-Maximum Suppression (NMS)\n",
        "    â”‚   â””â”€â”€ Remove duplicate detections\n",
        "    â””â”€â”€ Threshold filtering\n",
        "    â†“\n",
        "FINAL DETECTIONS\n",
        "    â””â”€â”€ Bounding boxes + classes + scores\n",
        "```\n",
        "\n",
        "### ğŸ¯ Key YOLO Innovations:\n",
        "\n",
        "1. **Single-Stage Detection**\n",
        "   - One neural network pass\n",
        "   - No region proposals\n",
        "   - Extremely fast (real-time!)\n",
        "\n",
        "2. **Grid-Based Prediction**\n",
        "   - Divide image into grid (e.g., 13Ã—13)\n",
        "   - Each cell predicts objects\n",
        "   - Parallel processing\n",
        "\n",
        "3. **Anchor Boxes**\n",
        "   - Pre-defined box shapes\n",
        "   - Handle different object sizes\n",
        "   - Learned from training data\n",
        "\n",
        "4. **Multi-Scale Predictions**\n",
        "   - YOLOv3+: Predict at 3 scales\n",
        "   - Detect small, medium, large objects\n",
        "   - Feature Pyramid Network\n",
        "\n",
        "### ğŸ“Š YOLO Performance (COCO Dataset):\n",
        "\n",
        "| Model | mAP | Speed (FPS) | Parameters |\n",
        "|-------|-----|-------------|------------|\n",
        "| Faster R-CNN | 42.0% | 5 | 137M |\n",
        "| YOLOv3 | 55.0% | 30 | 62M |\n",
        "| YOLOv5s | 37.0% | 140 | 7.2M |\n",
        "| YOLOv8m | 50.2% | 80 | 25.9M |\n",
        "\n",
        "**Real-World Uses:**\n",
        "- Autonomous vehicles (Tesla, Waymo)\n",
        "- Surveillance cameras (real-time detection)\n",
        "- Retail (inventory management, theft detection)\n",
        "- Sports analytics (player tracking)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {},
      "source": [
        "## ğŸ–¼ï¸ Image Segmentation Introduction\n",
        "\n",
        "**Segmentation** = Classify EVERY pixel in the image\n",
        "\n",
        "### ğŸ¯ Types of Segmentation:\n",
        "\n",
        "**1. Semantic Segmentation**\n",
        "```\n",
        "Input: Street scene\n",
        "Output:\n",
        "    â”œâ”€â”€ Pixels 1-1000: Road\n",
        "    â”œâ”€â”€ Pixels 1001-2000: Car\n",
        "    â”œâ”€â”€ Pixels 2001-3000: Pedestrian\n",
        "    â””â”€â”€ Pixels 3001-4000: Building\n",
        "\n",
        "All cars labeled as \"car\" (no distinction between individual cars)\n",
        "```\n",
        "\n",
        "**2. Instance Segmentation**\n",
        "```\n",
        "Input: Street scene\n",
        "Output:\n",
        "    â”œâ”€â”€ Car #1: Pixels [100-500]\n",
        "    â”œâ”€â”€ Car #2: Pixels [600-900]\n",
        "    â”œâ”€â”€ Person #1: Pixels [1000-1200]\n",
        "    â””â”€â”€ Person #2: Pixels [1300-1500]\n",
        "\n",
        "Each instance separately identified\n",
        "```\n",
        "\n",
        "### ğŸ—ï¸ U-Net Architecture (Famous Segmentation Model):\n",
        "\n",
        "```\n",
        "INPUT IMAGE (256Ã—256Ã—3)\n",
        "    â†“\n",
        "ENCODER (Downsampling Path)\n",
        "    â”œâ”€â”€ Conv + Pool â†’ 128Ã—128Ã—64\n",
        "    â”œâ”€â”€ Conv + Pool â†’ 64Ã—64Ã—128\n",
        "    â”œâ”€â”€ Conv + Pool â†’ 32Ã—32Ã—256\n",
        "    â””â”€â”€ Conv + Pool â†’ 16Ã—16Ã—512 (bottleneck)\n",
        "    â†“\n",
        "DECODER (Upsampling Path)\n",
        "    â”œâ”€â”€ UpConv + Skip â†’ 32Ã—32Ã—256\n",
        "    â”œâ”€â”€ UpConv + Skip â†’ 64Ã—64Ã—128\n",
        "    â”œâ”€â”€ UpConv + Skip â†’ 128Ã—128Ã—64\n",
        "    â””â”€â”€ UpConv + Skip â†’ 256Ã—256Ã—num_classes\n",
        "    â†“\n",
        "OUTPUT SEGMENTATION MAP\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "- **Encoder-Decoder** structure\n",
        "- **Skip connections** preserve spatial details\n",
        "- **Symmetric** architecture (U-shape)\n",
        "- Used in medical imaging, satellite imagery\n",
        "\n",
        "### ğŸ¯ Applications:\n",
        "\n",
        "**Autonomous Vehicles:**\n",
        "```\n",
        "Segment road scene:\n",
        "â”œâ”€â”€ Drivable area (road)\n",
        "â”œâ”€â”€ Obstacles (cars, pedestrians)\n",
        "â”œâ”€â”€ Lane markings\n",
        "â””â”€â”€ Traffic signs\n",
        "```\n",
        "\n",
        "**Medical Imaging:**\n",
        "```\n",
        "Segment MRI/CT scan:\n",
        "â”œâ”€â”€ Tumors\n",
        "â”œâ”€â”€ Organs\n",
        "â”œâ”€â”€ Blood vessels\n",
        "â””â”€â”€ Healthy tissue\n",
        "```\n",
        "\n",
        "**Satellite Imagery:**\n",
        "```\n",
        "Land use classification:\n",
        "â”œâ”€â”€ Forests\n",
        "â”œâ”€â”€ Urban areas\n",
        "â”œâ”€â”€ Water bodies\n",
        "â””â”€â”€ Agricultural land\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "## ğŸ‘¤ Face Recognition Systems\n",
        "\n",
        "**Face Recognition** = Identify or verify a person from their face\n",
        "\n",
        "### ğŸ¯ Face Recognition Pipeline:\n",
        "\n",
        "```\n",
        "INPUT IMAGE\n",
        "    â†“\n",
        "1. FACE DETECTION\n",
        "    â”œâ”€â”€ Find faces in image\n",
        "    â”œâ”€â”€ Bounding box coordinates\n",
        "    â””â”€â”€ Methods: Haar Cascades, MTCNN, RetinaFace\n",
        "    â†“\n",
        "2. FACE ALIGNMENT\n",
        "    â”œâ”€â”€ Detect facial landmarks (eyes, nose, mouth)\n",
        "    â”œâ”€â”€ Align face to standard pose\n",
        "    â””â”€â”€ Normalize rotation, scale\n",
        "    â†“\n",
        "3. FACE EMBEDDING\n",
        "    â”œâ”€â”€ Deep CNN (FaceNet, VGGFace, ArcFace)\n",
        "    â”œâ”€â”€ Extract 128D or 512D vector\n",
        "    â”œâ”€â”€ Similar faces â†’ similar vectors\n",
        "    â””â”€â”€ Triplet loss training\n",
        "    â†“\n",
        "4. FACE MATCHING\n",
        "    â”œâ”€â”€ Compare embeddings\n",
        "    â”œâ”€â”€ Cosine similarity or Euclidean distance\n",
        "    â”œâ”€â”€ Threshold for match/no-match\n",
        "    â””â”€â”€ Output: Identity or \"Unknown\"\n",
        "```\n",
        "\n",
        "### ğŸ§  FaceNet Architecture:\n",
        "\n",
        "**Key Innovation: Triplet Loss**\n",
        "\n",
        "```\n",
        "Training with triplets:\n",
        "    â”œâ”€â”€ Anchor: Person A (face 1)\n",
        "    â”œâ”€â”€ Positive: Person A (face 2, different angle)\n",
        "    â””â”€â”€ Negative: Person B (face)\n",
        "\n",
        "Goal:\n",
        "    distance(Anchor, Positive) < distance(Anchor, Negative)\n",
        "\n",
        "Network learns:\n",
        "    âœ… Same person â†’ close embeddings\n",
        "    âœ… Different people â†’ far embeddings\n",
        "```\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "FACE IMAGE (160Ã—160Ã—3)\n",
        "    â†“\n",
        "CNN BACKBONE (Inception/ResNet)\n",
        "    â”œâ”€â”€ Convolutional layers\n",
        "    â”œâ”€â”€ Feature extraction\n",
        "    â””â”€â”€ Dimension reduction\n",
        "    â†“\n",
        "EMBEDDING LAYER\n",
        "    â”œâ”€â”€ 128D or 512D vector\n",
        "    â””â”€â”€ L2 normalized\n",
        "    â†“\n",
        "TRIPLET LOSS\n",
        "    â””â”€â”€ Train to cluster same identities\n",
        "```\n",
        "\n",
        "### ğŸ“Š Modern Face Recognition Models:\n",
        "\n",
        "| Model | Accuracy (LFW) | Embedding Size | Year |\n",
        "|-------|----------------|----------------|------|\n",
        "| DeepFace (Facebook) | 97.35% | 4096D | 2014 |\n",
        "| FaceNet (Google) | 99.63% | 128D | 2015 |\n",
        "| VGGFace | 98.95% | 2622D | 2015 |\n",
        "| ArcFace | 99.83% | 512D | 2019 |\n",
        "| AdaFace | 99.85% | 512D | 2022 |\n",
        "\n",
        "**Real-World Applications:**\n",
        "- **iPhone Face ID**: On-device face unlock\n",
        "- **Facebook**: Tag suggestions\n",
        "- **Airport Security**: Identity verification\n",
        "- **Payment Systems**: Face-based authentication\n",
        "- **Attendance Systems**: Automated check-in\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "import sys\n",
        "!{sys.executable} -m pip install tensorflow numpy matplotlib scikit-learn opencv-python pillow --quiet\n",
        "\n",
        "print(\"âœ… Libraries installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"ğŸ“š Libraries loaded!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"OpenCV version: {cv2.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {},
      "source": [
        "## ğŸ±ğŸ¶ REAL AI EXAMPLE 1: Custom Image Classifier (Cats vs Dogs)\n",
        "\n",
        "**Project:** Build a production-quality binary image classifier!\n",
        "\n",
        "**Real-World Applications:**\n",
        "- E-commerce: Product categorization\n",
        "- Medical: Disease vs healthy tissue\n",
        "- Agriculture: Crop disease detection\n",
        "- Quality Control: Defect detection\n",
        "\n",
        "**Techniques we'll use:**\n",
        "- Transfer learning (MobileNetV2)\n",
        "- Data augmentation\n",
        "- Fine-tuning\n",
        "- Model evaluation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic dataset (simulating cats vs dogs)\n",
        "print(\"ğŸ¨ Creating dataset for Cats vs Dogs classification...\\n\")\n",
        "\n",
        "# Generate synthetic images (in real projects, you'd load actual images)\n",
        "num_samples_per_class = 500\n",
        "img_size = 224\n",
        "\n",
        "# Simulate images\n",
        "np.random.seed(42)\n",
        "cats = np.random.rand(num_samples_per_class, img_size, img_size, 3).astype('float32')\n",
        "dogs = np.random.rand(num_samples_per_class, img_size, img_size, 3).astype('float32')\n",
        "\n",
        "# Combine and create labels\n",
        "X = np.concatenate([cats, dogs], axis=0)\n",
        "y = np.concatenate([np.zeros(num_samples_per_class), np.ones(num_samples_per_class)])\n",
        "\n",
        "# Shuffle\n",
        "indices = np.random.permutation(len(X))\n",
        "X = X[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Split into train/validation/test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"âœ… Dataset created:\")\n",
        "print(f\"   Training samples: {len(X_train)}\")\n",
        "print(f\"   Validation samples: {len(X_val)}\")\n",
        "print(f\"   Test samples: {len(X_test)}\")\n",
        "print(f\"   Image shape: {X_train.shape[1:]}\")\n",
        "print(f\"   Classes: Cats (0) vs Dogs (1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize samples\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(10):\n",
        "    axes[i].imshow(X_train[i])\n",
        "    label = 'Cat' if y_train[i] == 0 else 'Dog'\n",
        "    axes[i].set_title(f'{label}', fontsize=12)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Sample Images from Dataset', fontsize=14, fontweight='bold', y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ Goal: Train a model to distinguish cats from dogs!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {},
      "source": [
        "### ğŸ—ï¸ Building the Model with Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build model with MobileNetV2 (efficient for deployment)\n",
        "print(\"ğŸ—ï¸ Building Cats vs Dogs classifier...\\n\")\n",
        "\n",
        "# Load pre-trained MobileNetV2\n",
        "base_model = MobileNetV2(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "\n",
        "# Freeze base model initially\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build complete model\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "], name='CatsVsDogs_Classifier')\n",
        "\n",
        "print(\"ğŸ§  Model Architecture:\")\n",
        "model.summary()\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Model compiled with Adam optimizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator()  # No augmentation for validation\n",
        "\n",
        "# Create generators\n",
        "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
        "val_generator = val_datagen.flow(X_val, y_val, batch_size=32)\n",
        "\n",
        "print(\"âœ… Data augmentation configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"ğŸš€ Training Cats vs Dogs classifier...\\n\")\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Initial training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tuning: Unfreeze some layers\n",
        "print(\"ğŸ”“ Fine-tuning model...\\n\")\n",
        "\n",
        "# Unfreeze top layers of MobileNetV2\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# Continue training\n",
        "history_finetune = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Fine-tuning complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"ğŸ”¬ Evaluating model on test set...\\n\")\n",
        "\n",
        "test_loss, test_acc, test_precision, test_recall = model.evaluate(\n",
        "    X_test, y_test, verbose=0\n",
        ")\n",
        "\n",
        "f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall + 1e-7)\n",
        "\n",
        "print(f\"ğŸ“Š Test Results:\")\n",
        "print(f\"   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "print(f\"   Precision: {test_precision:.4f}\")\n",
        "print(f\"   Recall: {test_recall:.4f}\")\n",
        "print(f\"   F1-Score: {f1_score:.4f}\")\n",
        "print(f\"\\nğŸ¯ Model correctly classifies {test_acc*100:.1f}% of images!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "predictions = model.predict(X_test, verbose=0)\n",
        "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "fig, axes = plt.subplots(3, 5, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "indices = np.random.choice(len(X_test), 15, replace=False)\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "    axes[i].imshow(X_test[idx])\n",
        "    \n",
        "    true_label = 'Cat' if y_test[idx] == 0 else 'Dog'\n",
        "    pred_label = 'Cat' if predicted_classes[idx] == 0 else 'Dog'\n",
        "    confidence = predictions[idx][0] if predicted_classes[idx] == 1 else (1 - predictions[idx][0])\n",
        "    \n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    \n",
        "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}\\n({confidence*100:.1f}%)',\n",
        "                      fontsize=10, color=color, fontweight='bold')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Cats vs Dogs Predictions', fontsize=16, fontweight='bold', y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ Green = Correct | Red = Incorrect\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {},
      "source": [
        "## ğŸ˜ŠğŸ˜¢ğŸ˜¡ REAL AI EXAMPLE 2: Facial Emotion Detection\n",
        "\n",
        "**Project:** Build an emotion recognition system!\n",
        "\n",
        "**7 Emotions:**\n",
        "1. Happy ğŸ˜Š\n",
        "2. Sad ğŸ˜¢\n",
        "3. Angry ğŸ˜¡\n",
        "4. Surprise ğŸ˜²\n",
        "5. Fear ğŸ˜¨\n",
        "6. Disgust ğŸ¤¢\n",
        "7. Neutral ğŸ˜\n",
        "\n",
        "**Real-World Applications:**\n",
        "- **Customer Service**: Detect customer satisfaction\n",
        "- **Mental Health**: Monitor emotional states\n",
        "- **Education**: Gauge student engagement\n",
        "- **Marketing**: Analyze ad reactions\n",
        "- **Automotive**: Driver fatigue detection\n",
        "- **Gaming**: Adaptive difficulty based on emotions\n",
        "\n",
        "**2024-2025 Trend:** Emotion AI for human-computer interaction!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic emotion dataset\n",
        "print(\"ğŸ˜Š Creating Facial Emotion Recognition dataset...\\n\")\n",
        "\n",
        "# 7 emotions\n",
        "emotion_labels = ['Happy', 'Sad', 'Angry', 'Surprise', 'Fear', 'Disgust', 'Neutral']\n",
        "num_classes = len(emotion_labels)\n",
        "\n",
        "# Generate synthetic data (48x48 grayscale faces, like FER2013 dataset)\n",
        "num_samples_per_emotion = 100\n",
        "face_size = 48\n",
        "\n",
        "X_emotion = np.random.rand(\n",
        "    num_samples_per_emotion * num_classes, \n",
        "    face_size, \n",
        "    face_size, \n",
        "    1\n",
        ").astype('float32')\n",
        "\n",
        "y_emotion = np.repeat(np.arange(num_classes), num_samples_per_emotion)\n",
        "\n",
        "# Shuffle\n",
        "indices = np.random.permutation(len(X_emotion))\n",
        "X_emotion = X_emotion[indices]\n",
        "y_emotion = y_emotion[indices]\n",
        "\n",
        "# Split\n",
        "X_train_emo, X_test_emo, y_train_emo, y_test_emo = train_test_split(\n",
        "    X_emotion, y_emotion, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# One-hot encode\n",
        "y_train_emo_cat = keras.utils.to_categorical(y_train_emo, num_classes)\n",
        "y_test_emo_cat = keras.utils.to_categorical(y_test_emo, num_classes)\n",
        "\n",
        "print(f\"âœ… Emotion dataset created:\")\n",
        "print(f\"   Training samples: {len(X_train_emo)}\")\n",
        "print(f\"   Test samples: {len(X_test_emo)}\")\n",
        "print(f\"   Image size: {face_size}Ã—{face_size} (grayscale)\")\n",
        "print(f\"   Emotions: {', '.join(emotion_labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-19",
      "metadata": {},
      "source": [
        "### ğŸ—ï¸ Building Emotion Recognition CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-20",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build emotion recognition model\n",
        "print(\"ğŸ—ï¸ Building Emotion Recognition CNN...\\n\")\n",
        "\n",
        "emotion_model = models.Sequential([\n",
        "    # Input\n",
        "    layers.Input(shape=(48, 48, 1)),\n",
        "    \n",
        "    # Conv Block 1\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "    \n",
        "    # Conv Block 2\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "    \n",
        "    # Conv Block 3\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "    \n",
        "    # Dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    \n",
        "    # Output (7 emotions)\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "], name='Emotion_Recognition_CNN')\n",
        "\n",
        "print(\"ğŸ§  Emotion Recognition Model:\")\n",
        "emotion_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-21",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile model\n",
        "emotion_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"âœ… Emotion model compiled!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data augmentation for emotion recognition\n",
        "emotion_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "emotion_train_gen = emotion_datagen.flow(\n",
        "    X_train_emo, y_train_emo_cat, batch_size=32\n",
        ")\n",
        "\n",
        "print(\"âœ… Data augmentation configured for emotion recognition\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-23",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train emotion recognition model\n",
        "print(\"ğŸš€ Training Emotion Recognition model...\\n\")\n",
        "\n",
        "emotion_callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "]\n",
        "\n",
        "emotion_history = emotion_model.fit(\n",
        "    emotion_train_gen,\n",
        "    epochs=15,\n",
        "    validation_data=(X_test_emo, y_test_emo_cat),\n",
        "    callbacks=emotion_callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Emotion recognition training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate emotion model\n",
        "print(\"ğŸ”¬ Evaluating Emotion Recognition model...\\n\")\n",
        "\n",
        "emotion_loss, emotion_acc = emotion_model.evaluate(\n",
        "    X_test_emo, y_test_emo_cat, verbose=0\n",
        ")\n",
        "\n",
        "print(f\"ğŸ“Š Emotion Recognition Results:\")\n",
        "print(f\"   Test Accuracy: {emotion_acc:.4f} ({emotion_acc*100:.2f}%)\")\n",
        "print(f\"   Test Loss: {emotion_loss:.4f}\")\n",
        "print(f\"\\nğŸ¯ Model can recognize {num_classes} different emotions!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-25",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize emotion predictions\n",
        "emotion_predictions = emotion_model.predict(X_test_emo, verbose=0)\n",
        "emotion_pred_classes = np.argmax(emotion_predictions, axis=1)\n",
        "\n",
        "fig, axes = plt.subplots(3, 5, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "indices = np.random.choice(len(X_test_emo), 15, replace=False)\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "    axes[i].imshow(X_test_emo[idx].squeeze(), cmap='gray')\n",
        "    \n",
        "    true_emotion = emotion_labels[y_test_emo[idx]]\n",
        "    pred_emotion = emotion_labels[emotion_pred_classes[idx]]\n",
        "    confidence = emotion_predictions[idx][emotion_pred_classes[idx]] * 100\n",
        "    \n",
        "    color = 'green' if true_emotion == pred_emotion else 'red'\n",
        "    \n",
        "    axes[i].set_title(f'True: {true_emotion}\\nPred: {pred_emotion}\\n({confidence:.1f}%)',\n",
        "                      fontsize=10, color=color, fontweight='bold')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Emotion Recognition Predictions', fontsize=16, fontweight='bold', y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ˜Š Green = Correct | ğŸ˜¢ Red = Incorrect\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix for emotion recognition\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test_emo, emotion_pred_classes)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
        "plt.title('Emotion Recognition Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Emotion', fontsize=12)\n",
        "plt.xlabel('Predicted Emotion', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nğŸ“Š Detailed Classification Report:\\n\")\n",
        "print(classification_report(y_test_emo, emotion_pred_classes, \n",
        "                          target_names=emotion_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-27",
      "metadata": {},
      "source": [
        "## ğŸ¯ Why This Matters for 2024-2025 AI\n",
        "\n",
        "### ğŸš— **Autonomous Vehicles (Tesla FSD, Waymo)**\n",
        "\n",
        "**Computer Vision Stack:**\n",
        "```\n",
        "8 CAMERAS (360Â° view)\n",
        "    â†“\n",
        "OBJECT DETECTION (YOLOv8)\n",
        "    â”œâ”€â”€ Cars, pedestrians, cyclists\n",
        "    â”œâ”€â”€ Traffic lights, signs\n",
        "    â””â”€â”€ Road obstacles\n",
        "    â†“\n",
        "SEMANTIC SEGMENTATION (U-Net)\n",
        "    â”œâ”€â”€ Drivable area\n",
        "    â”œâ”€â”€ Lane markings\n",
        "    â””â”€â”€ Sidewalks, curbs\n",
        "    â†“\n",
        "DEPTH ESTIMATION\n",
        "    â””â”€â”€ 3D understanding\n",
        "    â†“\n",
        "SENSOR FUSION\n",
        "    â””â”€â”€ Combine vision + radar + GPS\n",
        "    â†“\n",
        "PATH PLANNING & CONTROL\n",
        "```\n",
        "\n",
        "**Impact:**\n",
        "- Tesla FSD processes 36 frames/sec from 8 cameras\n",
        "- Waymo drives 20M+ autonomous miles/year\n",
        "- All powered by CNNs and computer vision!\n",
        "\n",
        "### ğŸ¤– **Multimodal AI (GPT-4V, Gemini Vision)**\n",
        "\n",
        "**Vision Understanding Pipeline:**\n",
        "```\n",
        "IMAGE INPUT\n",
        "    â†“\n",
        "OBJECT DETECTION\n",
        "    â””â”€â”€ Identify all objects in scene\n",
        "    â†“\n",
        "SCENE UNDERSTANDING\n",
        "    â”œâ”€â”€ Spatial relationships\n",
        "    â”œâ”€â”€ Actions/activities\n",
        "    â””â”€â”€ Context\n",
        "    â†“\n",
        "VISION ENCODER (ResNet/ViT)\n",
        "    â””â”€â”€ Convert to embeddings\n",
        "    â†“\n",
        "FUSION WITH LANGUAGE MODEL\n",
        "    â†“\n",
        "TEXT OUTPUT\n",
        "    â””â”€â”€ \"This image shows a person walking a dog in a park...\"\n",
        "```\n",
        "\n",
        "**Your Skills â†’ GPT-4V:**\n",
        "- You: Cats vs Dogs classifier\n",
        "- GPT-4V: Understands ANY image + generates descriptions\n",
        "- **Same foundation, much larger scale!**\n",
        "\n",
        "### ğŸ¥ **Medical Imaging AI**\n",
        "\n",
        "**Clinical Applications:**\n",
        "```\n",
        "X-RAY / MRI / CT SCAN\n",
        "    â†“\n",
        "SEGMENTATION (U-Net)\n",
        "    â”œâ”€â”€ Organ boundaries\n",
        "    â”œâ”€â”€ Tumor regions\n",
        "    â””â”€â”€ Abnormalities\n",
        "    â†“\n",
        "CLASSIFICATION (ResNet)\n",
        "    â”œâ”€â”€ Disease vs Healthy\n",
        "    â”œâ”€â”€ Disease stage\n",
        "    â””â”€â”€ Urgency level\n",
        "    â†“\n",
        "CLINICAL DECISION SUPPORT\n",
        "```\n",
        "\n",
        "**Real Results:**\n",
        "- 94% accuracy in lung cancer detection\n",
        "- 96% accuracy in diabetic retinopathy\n",
        "- Reduces diagnosis time from hours to seconds\n",
        "- Saves lives through early detection!\n",
        "\n",
        "### ğŸ˜Š **Emotion AI (Human-Computer Interaction)**\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "**1. Customer Service Bots:**\n",
        "```\n",
        "Detect customer emotion â†’ Adapt response\n",
        "â”œâ”€â”€ Frustrated â†’ Escalate to human\n",
        "â”œâ”€â”€ Happy â†’ Continue automated\n",
        "â””â”€â”€ Confused â†’ Provide simpler explanation\n",
        "```\n",
        "\n",
        "**2. Education Technology:**\n",
        "```\n",
        "Monitor student emotion â†’ Adapt teaching\n",
        "â”œâ”€â”€ Bored â†’ Make content engaging\n",
        "â”œâ”€â”€ Confused â†’ Slow down, explain more\n",
        "â””â”€â”€ Engaged â†’ Continue pace\n",
        "```\n",
        "\n",
        "**3. Mental Health Monitoring:**\n",
        "```\n",
        "Track emotional patterns over time\n",
        "â””â”€â”€ Detect depression, anxiety early\n",
        "```\n",
        "\n",
        "**4. Automotive Safety:**\n",
        "```\n",
        "Driver emotion monitoring\n",
        "â”œâ”€â”€ Drowsy â†’ Alert warning\n",
        "â”œâ”€â”€ Distracted â†’ Attention reminder\n",
        "â””â”€â”€ Road rage â†’ Calming interventions\n",
        "```\n",
        "\n",
        "### ğŸ¯ **Production Deployment**\n",
        "\n",
        "**Your Models â†’ Production:**\n",
        "```\n",
        "1. MOBILE DEPLOYMENT\n",
        "   â”œâ”€â”€ Use MobileNet (lightweight)\n",
        "   â”œâ”€â”€ Convert to TensorFlow Lite\n",
        "   â””â”€â”€ Run on-device (iOS, Android)\n",
        "\n",
        "2. EDGE DEPLOYMENT\n",
        "   â”œâ”€â”€ NVIDIA Jetson (cameras, drones)\n",
        "   â”œâ”€â”€ Google Coral (IoT devices)\n",
        "   â””â”€â”€ Intel Movidius (embedded systems)\n",
        "\n",
        "3. CLOUD DEPLOYMENT\n",
        "   â”œâ”€â”€ AWS SageMaker\n",
        "   â”œâ”€â”€ Google AI Platform\n",
        "   â””â”€â”€ Azure ML\n",
        "\n",
        "4. WEB DEPLOYMENT\n",
        "   â”œâ”€â”€ TensorFlow.js (browser)\n",
        "   â””â”€â”€ Real-time inference\n",
        "```\n",
        "\n",
        "### ğŸ“Š **Industry Impact (2024):**\n",
        "\n",
        "```\n",
        "Computer Vision Market:\n",
        "â”œâ”€â”€ Market size: $17.7B (2024)\n",
        "â”œâ”€â”€ Growing at 19.6% CAGR\n",
        "â””â”€â”€ Projected: $47.4B by 2030\n",
        "\n",
        "Key Sectors:\n",
        "â”œâ”€â”€ Automotive: $8.4B (autonomous driving)\n",
        "â”œâ”€â”€ Healthcare: $3.2B (medical imaging)\n",
        "â”œâ”€â”€ Retail: $2.1B (visual search, checkout)\n",
        "â”œâ”€â”€ Security: $1.9B (surveillance, access control)\n",
        "â””â”€â”€ Manufacturing: $1.5B (quality control)\n",
        "```\n",
        "\n",
        "**ğŸ¯ You just learned the technologies powering a $17.7B industry!**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-28",
      "metadata": {},
      "source": [
        "## ğŸ¯ Interactive Exercise 1: Multi-Class Custom Classifier\n",
        "\n",
        "**Challenge:** Build a classifier for 5 different animal classes!\n",
        "\n",
        "**Classes:**\n",
        "1. Cats\n",
        "2. Dogs\n",
        "3. Birds\n",
        "4. Fish\n",
        "5. Horses\n",
        "\n",
        "**Your Task:**\n",
        "1. Create synthetic dataset (5 classes)\n",
        "2. Use transfer learning (MobileNetV2)\n",
        "3. Apply data augmentation\n",
        "4. Train and evaluate\n",
        "5. Visualize predictions with confidence scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCISE 1: Multi-Class Animal Classifier\n",
        "\n",
        "# TODO: Create dataset with 5 animal classes\n",
        "# animal_classes = ['Cat', 'Dog', 'Bird', 'Fish', 'Horse']\n",
        "# X_animals = ...\n",
        "# y_animals = ...\n",
        "\n",
        "# TODO: Split into train/val/test\n",
        "# X_train_animals, X_test_animals, y_train_animals, y_test_animals = ...\n",
        "\n",
        "# TODO: Build model with MobileNetV2 base\n",
        "# animal_model = ...\n",
        "\n",
        "# TODO: Add data augmentation\n",
        "# animal_datagen = ...\n",
        "\n",
        "# TODO: Train\n",
        "# animal_model.fit(...)\n",
        "\n",
        "# TODO: Evaluate and visualize\n",
        "\n",
        "print(\"Complete the TODOs above!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-30",
      "metadata": {},
      "source": [
        "### âœ… Solution to Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-31",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLUTION: Multi-Class Animal Classifier\n",
        "\n",
        "# Create dataset\n",
        "animal_classes = ['Cat', 'Dog', 'Bird', 'Fish', 'Horse']\n",
        "num_animal_classes = len(animal_classes)\n",
        "samples_per_class = 200\n",
        "\n",
        "X_animals = np.random.rand(\n",
        "    samples_per_class * num_animal_classes, 224, 224, 3\n",
        ").astype('float32')\n",
        "\n",
        "y_animals = np.repeat(np.arange(num_animal_classes), samples_per_class)\n",
        "\n",
        "# Shuffle\n",
        "indices = np.random.permutation(len(X_animals))\n",
        "X_animals = X_animals[indices]\n",
        "y_animals = y_animals[indices]\n",
        "\n",
        "# Split\n",
        "X_train_animals, X_test_animals, y_train_animals, y_test_animals = train_test_split(\n",
        "    X_animals, y_animals, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# One-hot encode\n",
        "y_train_animals_cat = keras.utils.to_categorical(y_train_animals, num_animal_classes)\n",
        "y_test_animals_cat = keras.utils.to_categorical(y_test_animals, num_animal_classes)\n",
        "\n",
        "print(f\"âœ… Animal dataset: {len(X_animals)} images, {num_animal_classes} classes\")\n",
        "\n",
        "# Build model\n",
        "animal_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "animal_base.trainable = False\n",
        "\n",
        "animal_model = models.Sequential([\n",
        "    animal_base,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_animal_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "animal_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nğŸ§  Animal Classifier Model:\")\n",
        "animal_model.summary()\n",
        "\n",
        "# Data augmentation\n",
        "animal_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2\n",
        ")\n",
        "\n",
        "animal_train_gen = animal_datagen.flow(X_train_animals, y_train_animals_cat, batch_size=32)\n",
        "\n",
        "# Train\n",
        "print(\"\\nğŸš€ Training Animal Classifier...\\n\")\n",
        "animal_history = animal_model.fit(\n",
        "    animal_train_gen,\n",
        "    epochs=10,\n",
        "    validation_data=(X_test_animals, y_test_animals_cat),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "animal_loss, animal_acc = animal_model.evaluate(X_test_animals, y_test_animals_cat, verbose=0)\n",
        "print(f\"\\nâœ… Animal Classifier Accuracy: {animal_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-32",
      "metadata": {},
      "source": [
        "## ğŸ¯ Interactive Exercise 2: Build a Simple Face Detector\n",
        "\n",
        "**Challenge:** Build a CNN to detect if an image contains a face or not (binary classification)!\n",
        "\n",
        "**Task:**\n",
        "1. Create dataset: Face vs No-Face\n",
        "2. Build CNN from scratch (no transfer learning)\n",
        "3. Train with data augmentation\n",
        "4. Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCISE 2: Face Detector\n",
        "\n",
        "# TODO: Create dataset (Face vs No-Face)\n",
        "# X_face_data = ...\n",
        "# y_face_data = ...  # 0 = no face, 1 = face\n",
        "\n",
        "# TODO: Build CNN from scratch\n",
        "# face_detector = models.Sequential([\n",
        "#     # Add Conv2D, MaxPooling, Dense layers\n",
        "# ])\n",
        "\n",
        "# TODO: Compile and train\n",
        "# face_detector.compile(...)\n",
        "# face_detector.fit(...)\n",
        "\n",
        "print(\"Complete the TODOs above!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-34",
      "metadata": {},
      "source": [
        "### âœ… Solution to Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLUTION: Face Detector\n",
        "\n",
        "# Create dataset\n",
        "num_samples = 1000\n",
        "X_face_data = np.random.rand(num_samples, 64, 64, 3).astype('float32')\n",
        "y_face_data = np.random.randint(0, 2, size=num_samples)  # Binary: 0 or 1\n",
        "\n",
        "# Split\n",
        "X_train_face, X_test_face, y_train_face, y_test_face = train_test_split(\n",
        "    X_face_data, y_face_data, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Build CNN\n",
        "face_detector = models.Sequential([\n",
        "    layers.Input(shape=(64, 64, 3)),\n",
        "    \n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "face_detector.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"ğŸ§  Face Detector Model:\")\n",
        "face_detector.summary()\n",
        "\n",
        "# Train\n",
        "print(\"\\nğŸš€ Training Face Detector...\\n\")\n",
        "face_history = face_detector.fit(\n",
        "    X_train_face, y_train_face,\n",
        "    batch_size=32,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "face_loss, face_acc = face_detector.evaluate(X_test_face, y_test_face, verbose=0)\n",
        "print(f\"\\nâœ… Face Detector Accuracy: {face_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-36",
      "metadata": {},
      "source": [
        "## ğŸ‰ Congratulations!\n",
        "\n",
        "**You just learned:**\n",
        "- âœ… Computer vision task taxonomy (classification, detection, segmentation)\n",
        "- âœ… YOLO concepts for real-time object detection\n",
        "- âœ… Image segmentation with U-Net architecture\n",
        "- âœ… Face recognition systems and FaceNet\n",
        "- âœ… Built a custom Cats vs Dogs classifier\n",
        "- âœ… Built a facial emotion detection system\n",
        "- âœ… How these technologies power autonomous vehicles\n",
        "- âœ… Production deployment strategies\n",
        "\n",
        "### ğŸ¯ Key Takeaways:\n",
        "\n",
        "1. **Computer Vision is Diverse**\n",
        "   - Classification: What is it?\n",
        "   - Detection: Where is it?\n",
        "   - Segmentation: Pixel-level understanding\n",
        "   - Each task requires different architectures\n",
        "\n",
        "2. **YOLO Revolutionized Object Detection**\n",
        "   - Single-stage detector (fast!)\n",
        "   - Real-time performance (45+ FPS)\n",
        "   - Powers autonomous vehicles, surveillance\n",
        "\n",
        "3. **Segmentation for Precise Understanding**\n",
        "   - U-Net: Encoder-decoder with skip connections\n",
        "   - Medical imaging gold standard\n",
        "   - Critical for autonomous driving\n",
        "\n",
        "4. **Face Recognition = Detection + Embedding**\n",
        "   - FaceNet with triplet loss\n",
        "   - 128D embeddings capture identity\n",
        "   - Used in security, authentication, social media\n",
        "\n",
        "5. **Emotion AI = Human-Computer Interaction**\n",
        "   - 7 basic emotions\n",
        "   - Applications in customer service, education, health\n",
        "   - Growing field in 2024-2025\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ¯ Real-World Impact:**\n",
        "\n",
        "```\n",
        "What You Built â†’ Industry Applications:\n",
        "\n",
        "Your Cats vs Dogs Classifier\n",
        "    â†“\n",
        "Medical AI: Cancer Detection (94% accuracy)\n",
        "Quality Control: Defect Detection (saves millions)\n",
        "E-commerce: Product Categorization (billions in sales)\n",
        "\n",
        "Your Emotion Detector\n",
        "    â†“\n",
        "Customer Service: Adaptive bots (better satisfaction)\n",
        "Education: Personalized learning (improved outcomes)\n",
        "Mental Health: Early intervention (saves lives)\n",
        "Automotive: Driver safety (prevents accidents)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ“š What's Next?**\n",
        "\n",
        "**Week 14: RNNs & Sequence Models**\n",
        "- Recurrent Neural Networks\n",
        "- LSTMs and GRUs\n",
        "- Time series prediction\n",
        "- Sequence-to-sequence models\n",
        "\n",
        "**Week 15: NLP & Text Processing**\n",
        "- Text preprocessing and embeddings\n",
        "- Word2Vec, GloVe\n",
        "- Sentiment analysis\n",
        "- Text generation\n",
        "\n",
        "**Week 16: Transformers & Attention**\n",
        "- Attention mechanisms\n",
        "- Transformer architecture\n",
        "- BERT, GPT concepts\n",
        "- Building towards understanding ChatGPT!\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ’¬ Remember:**\n",
        "\n",
        "*\"You just completed Week 13 and built production-quality computer vision systems! You understand the technology powering Tesla Autopilot, GPT-4V, and medical imaging AI. Next week, you'll learn about sequence models and time series, which are crucial for understanding language models and transformers!\"* ğŸš€\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ”— Week 13 Summary:**\n",
        "\n",
        "- **Day 1**: Built CNNs from scratch, understood convolution operations\n",
        "- **Day 2**: Mastered transfer learning, explored ResNet, VGG, MobileNet\n",
        "- **Day 3**: Applied CV to real problems (detection, segmentation, recognition)\n",
        "\n",
        "**You're now ready for advanced deep learning topics!** ğŸ“"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
