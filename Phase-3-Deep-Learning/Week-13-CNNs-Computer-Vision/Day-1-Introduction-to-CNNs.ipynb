{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Day 1: Introduction to Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**ðŸŽ¯ Goal:** Understand CNNs and build an image classifier from scratch\n",
    "\n",
    "**â±ï¸ Time:** 60-75 minutes\n",
    "\n",
    "**ðŸŒŸ Why This Matters for AI:**\n",
    "- CNNs power computer vision in GPT-4V, Gemini Vision, and DALL-E\n",
    "- Used in autonomous vehicles, medical imaging, and facial recognition\n",
    "- Foundation of multimodal AI (combining vision + language)\n",
    "- Critical for RAG systems that process images and documents\n",
    "- Understanding CNNs helps you build vision-based Agentic AI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ðŸ–¼ï¸ What are Convolutional Neural Networks?\n",
    "\n",
    "**Convolutional Neural Networks (CNNs)** are specialized neural networks designed for processing grid-like data, especially **images**!\n",
    "\n",
    "### ðŸ§  Why Not Use Regular Neural Networks for Images?\n",
    "\n",
    "**Problem with fully-connected networks:**\n",
    "```\n",
    "Image: 224Ã—224 pixels Ã— 3 colors = 150,528 inputs\n",
    "Hidden layer: 1000 neurons\n",
    "Weights needed: 150,528 Ã— 1000 = 150 MILLION parameters!\n",
    "\n",
    "âŒ Too many parameters\n",
    "âŒ Ignores spatial structure\n",
    "âŒ Doesn't handle position variations\n",
    "```\n",
    "\n",
    "**Solution: CNNs!**\n",
    "```\n",
    "âœ… Share weights across the image (fewer parameters)\n",
    "âœ… Preserve spatial structure\n",
    "âœ… Learn hierarchical features (edges â†’ shapes â†’ objects)\n",
    "âœ… Translation invariant (cat anywhere = cat!)\n",
    "```\n",
    "\n",
    "### ðŸŽ¯ Real-World CNN Applications (2024-2025):\n",
    "\n",
    "**ðŸ¤– Multimodal AI:**\n",
    "- **GPT-4V**: Uses CNN-based vision encoder to understand images\n",
    "- **Gemini Vision**: Processes images and videos with CNNs\n",
    "- **Claude with Vision**: Analyzes images using convolutional layers\n",
    "\n",
    "**ðŸŽ¨ Generative AI:**\n",
    "- **DALL-E 3**: CNNs in the image generation pipeline\n",
    "- **Stable Diffusion**: Uses convolutional U-Net architecture\n",
    "- **Midjourney**: CNN-based image synthesis\n",
    "\n",
    "**ðŸš— Autonomous Vehicles:**\n",
    "- Object detection (pedestrians, cars, signs)\n",
    "- Lane detection and segmentation\n",
    "- Depth estimation\n",
    "\n",
    "**ðŸ¥ Medical Imaging:**\n",
    "- Cancer detection in X-rays and MRIs\n",
    "- Disease diagnosis from medical scans\n",
    "- Organ segmentation\n",
    "\n",
    "**ðŸ“± Everyday Applications:**\n",
    "- Face unlock (iPhone Face ID)\n",
    "- Google Photos search (\"find pictures of cats\")\n",
    "- Instagram filters and effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## ðŸ”¬ How CNNs See Images: The Building Blocks\n",
    "\n",
    "CNNs learn to see images through **hierarchical feature learning**:\n",
    "\n",
    "```\n",
    "LOW-LEVEL FEATURES (Early Layers)\n",
    "â”œâ”€â”€ Edges (horizontal, vertical, diagonal)\n",
    "â”œâ”€â”€ Colors and textures\n",
    "â””â”€â”€ Simple patterns\n",
    "\n",
    "MID-LEVEL FEATURES (Middle Layers)\n",
    "â”œâ”€â”€ Corners and curves\n",
    "â”œâ”€â”€ Simple shapes (circles, rectangles)\n",
    "â””â”€â”€ Texture patterns\n",
    "\n",
    "HIGH-LEVEL FEATURES (Deep Layers)\n",
    "â”œâ”€â”€ Object parts (eyes, wheels, windows)\n",
    "â”œâ”€â”€ Complex shapes\n",
    "â””â”€â”€ Complete objects (faces, cars, animals)\n",
    "```\n",
    "\n",
    "### ðŸŽ¯ Three Core Components:\n",
    "\n",
    "#### 1ï¸âƒ£ **Convolutional Layers** - Feature Detectors\n",
    "- Apply **filters (kernels)** to detect patterns\n",
    "- Each filter learns one pattern (edge, texture, etc.)\n",
    "- Slide filter across the image (convolution operation)\n",
    "- Output: **Feature maps** (where patterns were found)\n",
    "\n",
    "```\n",
    "Filter (3Ã—3 edge detector):\n",
    "[-1, -1, -1]\n",
    "[ 0,  0,  0]\n",
    "[ 1,  1,  1]\n",
    "\n",
    "Convolves across image â†’ Detects horizontal edges!\n",
    "```\n",
    "\n",
    "#### 2ï¸âƒ£ **Pooling Layers** - Downsampling\n",
    "- Reduce spatial dimensions (make feature maps smaller)\n",
    "- Keep important information, discard redundant details\n",
    "- **Max Pooling**: Take maximum value in each region\n",
    "- **Average Pooling**: Take average value\n",
    "\n",
    "```\n",
    "Max Pooling (2Ã—2):\n",
    "[1, 3]    â†’    [3]\n",
    "[2, 0]         \n",
    "\n",
    "Takes max value, reduces size by 2Ã—\n",
    "```\n",
    "\n",
    "#### 3ï¸âƒ£ **Activation Functions** - Non-linearity\n",
    "- **ReLU**: Most common (max(0, x))\n",
    "- Adds non-linearity to learn complex patterns\n",
    "- Applied after convolution\n",
    "\n",
    "### ðŸ—ï¸ Typical CNN Architecture:\n",
    "\n",
    "```\n",
    "INPUT IMAGE (224Ã—224Ã—3)\n",
    "    â†“\n",
    "CONV Layer (32 filters, 3Ã—3) â†’ ReLU\n",
    "    â†“\n",
    "MAX POOL (2Ã—2)\n",
    "    â†“\n",
    "CONV Layer (64 filters, 3Ã—3) â†’ ReLU\n",
    "    â†“\n",
    "MAX POOL (2Ã—2)\n",
    "    â†“\n",
    "CONV Layer (128 filters, 3Ã—3) â†’ ReLU\n",
    "    â†“\n",
    "FLATTEN\n",
    "    â†“\n",
    "DENSE Layer (128 neurons) â†’ ReLU\n",
    "    â†“\n",
    "OUTPUT (10 classes) â†’ Softmax\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## ðŸ” Understanding Convolution Operation\n",
    "\n",
    "**Convolution** is the core operation in CNNs!\n",
    "\n",
    "### ðŸ“Š How It Works:\n",
    "\n",
    "1. **Take a filter (kernel)** - small matrix (e.g., 3Ã—3)\n",
    "2. **Slide it across the image** (stride = how many pixels to move)\n",
    "3. **Element-wise multiply** filter with image patch\n",
    "4. **Sum the results** - this is one output value\n",
    "5. **Repeat** across the entire image\n",
    "\n",
    "### ðŸŽ¯ Key Parameters:\n",
    "\n",
    "**Filter Size:**\n",
    "- Common: 3Ã—3, 5Ã—5, 7Ã—7\n",
    "- Larger filters = larger receptive field\n",
    "\n",
    "**Stride:**\n",
    "- Stride = 1: Move filter 1 pixel at a time (more detail)\n",
    "- Stride = 2: Move filter 2 pixels (faster, smaller output)\n",
    "\n",
    "**Padding:**\n",
    "- **Valid**: No padding (output smaller than input)\n",
    "- **Same**: Add padding to keep output size same as input\n",
    "\n",
    "**Number of Filters:**\n",
    "- More filters = learn more patterns\n",
    "- Each filter produces one feature map\n",
    "- 32 filters â†’ 32 feature maps\n",
    "\n",
    "### ðŸ§® Output Size Calculation:\n",
    "\n",
    "```\n",
    "Output size = (Input - Filter + 2Ã—Padding) / Stride + 1\n",
    "\n",
    "Example:\n",
    "Input: 28Ã—28\n",
    "Filter: 3Ã—3\n",
    "Padding: 0\n",
    "Stride: 1\n",
    "\n",
    "Output = (28 - 3 + 0) / 1 + 1 = 26Ã—26\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow numpy matplotlib scikit-learn --quiet\n",
    "\n",
    "print(\"âœ… Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"ðŸ“š Libraries loaded!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Visualizing Convolution: Edge Detection Example\n",
    "\n",
    "Let's see how a simple filter can detect edges in images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple image with vertical and horizontal edges\n",
    "simple_image = np.zeros((10, 10))\n",
    "simple_image[3:7, 3:7] = 1  # White square in the middle\n",
    "\n",
    "# Define edge detection filters\n",
    "horizontal_edge_filter = np.array([\n",
    "    [-1, -1, -1],\n",
    "    [ 0,  0,  0],\n",
    "    [ 1,  1,  1]\n",
    "])\n",
    "\n",
    "vertical_edge_filter = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1]\n",
    "])\n",
    "\n",
    "# Manual convolution function\n",
    "def convolve2d(image, kernel):\n",
    "    \"\"\"\n",
    "    Simple 2D convolution (valid padding)\n",
    "    \"\"\"\n",
    "    img_h, img_w = image.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    output_h = img_h - kernel_h + 1\n",
    "    output_w = img_w - kernel_w + 1\n",
    "    \n",
    "    output = np.zeros((output_h, output_w))\n",
    "    \n",
    "    for i in range(output_h):\n",
    "        for j in range(output_w):\n",
    "            # Extract patch and apply filter\n",
    "            patch = image[i:i+kernel_h, j:j+kernel_w]\n",
    "            output[i, j] = np.sum(patch * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Apply filters\n",
    "horizontal_edges = convolve2d(simple_image, horizontal_edge_filter)\n",
    "vertical_edges = convolve2d(simple_image, vertical_edge_filter)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "axes[0, 0].imshow(simple_image, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(horizontal_edge_filter, cmap='coolwarm')\n",
    "axes[0, 1].set_title('Horizontal Edge Filter', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(horizontal_edges, cmap='gray')\n",
    "axes[1, 0].set_title('Horizontal Edges Detected', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(vertical_edges, cmap='gray')\n",
    "axes[1, 1].set_title('Vertical Edges Detected', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸŽ¯ Convolution Operation Explained:\")\n",
    "print(\"   - Horizontal filter detects top and bottom edges\")\n",
    "print(\"   - Vertical filter detects left and right edges\")\n",
    "print(\"   - CNNs learn these filters automatically during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## ðŸš€ Building a CNN from Scratch with TensorFlow\n",
    "\n",
    "Let's build a real CNN to classify images from the **CIFAR-10 dataset**!\n",
    "\n",
    "### ðŸ“Š CIFAR-10 Dataset:\n",
    "- 60,000 color images (32Ã—32 pixels)\n",
    "- 10 classes: airplane, car, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "- 50,000 training images, 10,000 test images\n",
    "- Real-world benchmark dataset used in AI research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "print(\"ðŸ“¥ Loading CIFAR-10 dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded!\")\n",
    "print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]}\")\n",
    "print(f\"   Image shape: {X_train.shape[1:]}\")\n",
    "print(f\"   Classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(15):\n",
    "    axes[i].imshow(X_train[i])\n",
    "    axes[i].set_title(f'{class_names[y_train[i][0]]}', fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('CIFAR-10 Sample Images', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸŽ¯ Our goal: Train a CNN to classify these images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "print(\"ðŸ”§ Preprocessing data...\")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(\"âœ… Data preprocessed!\")\n",
    "print(f\"   X_train range: [{X_train.min():.2f}, {X_train.max():.2f}]\")\n",
    "print(f\"   y_train shape: {y_train_cat.shape}\")\n",
    "print(f\"   Example label: {y_train[0]} â†’ {y_train_cat[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Building the CNN Architecture\n",
    "\n",
    "We'll create a CNN with:\n",
    "- **3 Convolutional blocks** (Conv2D + ReLU + MaxPooling)\n",
    "- **Dropout layers** (prevent overfitting)\n",
    "- **Dense layers** (classification)\n",
    "- **Softmax output** (10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "def build_cnn():\n",
    "    \"\"\"\n",
    "    Build a CNN for CIFAR-10 classification\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv Block 1: 32 filters (3Ã—3)\n",
    "    - Conv Block 2: 64 filters (3Ã—3)\n",
    "    - Conv Block 3: 128 filters (3Ã—3)\n",
    "    - Dense layers with Dropout\n",
    "    - Output: 10 classes (Softmax)\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(32, 32, 3)),\n",
    "        \n",
    "        # Convolutional Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Flatten and Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')  # 10 classes\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = build_cnn()\n",
    "\n",
    "print(\"ðŸ§  CNN Model created!\")\n",
    "print(\"\\nðŸ“Š Model Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… Model compiled!\")\n",
    "print(\"\\nðŸŽ¯ Configuration:\")\n",
    "print(\"   Optimizer: Adam\")\n",
    "print(\"   Loss: Categorical Crossentropy\")\n",
    "print(\"   Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Training the CNN\n",
    "\n",
    "Now let's train our CNN on CIFAR-10!\n",
    "\n",
    "**Training tips:**\n",
    "- Use a validation split to monitor overfitting\n",
    "- Early stopping prevents training too long\n",
    "- Learning rate reduction improves final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "print(\"   This may take 5-10 minutes depending on your hardware\")\n",
    "print(\"   Tip: Use GPU for faster training!\\n\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=64,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Training History Insights:\")\n",
    "print(f\"   Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"   Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## ðŸ“Š Evaluating the Model\n",
    "\n",
    "Let's test our CNN on unseen data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"ðŸ”¬ Evaluating model on test set...\\n\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"âœ… Test Results:\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nðŸŽ¯ This means our CNN correctly classifies {test_accuracy*100:.1f}% of images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(3, 5, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Randomly select 15 test images\n",
    "indices = np.random.choice(len(X_test), 15, replace=False)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[i].imshow(X_test[idx])\n",
    "    \n",
    "    true_label = class_names[y_test[idx][0]]\n",
    "    pred_label = class_names[predicted_classes[idx]]\n",
    "    confidence = predictions[idx][predicted_classes[idx]] * 100\n",
    "    \n",
    "    # Color: green if correct, red if wrong\n",
    "    color = 'green' if true_label == pred_label else 'red'\n",
    "    \n",
    "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}\\n({confidence:.1f}%)',\n",
    "                      fontsize=10, color=color, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('CNN Predictions on Test Images', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸŽ¯ Green = Correct prediction | Red = Incorrect prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## ðŸ” Visualizing CNN Feature Maps\n",
    "\n",
    "Let's peek inside our CNN to see what features it learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that outputs intermediate layer activations\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]  # First 8 layers\n",
    "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# Get activations for a test image\n",
    "test_img = X_test[0:1]  # First test image\n",
    "activations = activation_model.predict(test_img, verbose=0)\n",
    "\n",
    "# Visualize feature maps from first convolutional layer\n",
    "first_layer_activation = activations[0]\n",
    "print(f\"First convolutional layer output shape: {first_layer_activation.shape}\")\n",
    "print(f\"Number of feature maps: {first_layer_activation.shape[-1]}\")\n",
    "\n",
    "# Plot first 16 feature maps\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(16):\n",
    "    axes[i].imshow(first_layer_activation[0, :, :, i], cmap='viridis')\n",
    "    axes[i].set_title(f'Filter {i+1}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Feature Maps from First Convolutional Layer', \n",
    "             fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ What You're Seeing:\")\n",
    "print(\"   - Each image shows what one filter detected\")\n",
    "print(\"   - Bright areas = strong activation (pattern detected)\")\n",
    "print(\"   - Dark areas = weak activation (pattern not present)\")\n",
    "print(\"   - Different filters learn different patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Why This Matters for Modern AI (2024-2025)\n",
    "\n",
    "The CNN you just built uses the **same principles** powering cutting-edge multimodal AI!\n",
    "\n",
    "### ðŸ¤– **GPT-4V & Gemini Vision (Multimodal AI)**\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "IMAGE INPUT\n",
    "    â†“\n",
    "CNN-based Vision Encoder (ViT - Vision Transformer)\n",
    "    â”œâ”€â”€ Patch embedding (split image into patches)\n",
    "    â”œâ”€â”€ Convolutional processing\n",
    "    â””â”€â”€ Feature extraction\n",
    "    â†“\n",
    "IMAGE EMBEDDINGS (vectors)\n",
    "    â†“\n",
    "FUSION LAYER (combine with text)\n",
    "    â†“\n",
    "TRANSFORMER LLM\n",
    "    â†“\n",
    "TEXT OUTPUT (describes image!)\n",
    "```\n",
    "\n",
    "**Your CNN â†’ GPT-4V:**\n",
    "- Your CNN: 3 conv layers, ~500K parameters\n",
    "- GPT-4V Vision: ~40 layers, ~1B parameters\n",
    "- **Same convolution operation, bigger scale!**\n",
    "\n",
    "### ðŸŽ¨ **DALL-E 3 & Stable Diffusion (Image Generation)**\n",
    "\n",
    "**U-Net Architecture (Diffusion Models):**\n",
    "```\n",
    "NOISY IMAGE\n",
    "    â†“\n",
    "ENCODER (Convolutional downsampling)\n",
    "    â”œâ”€â”€ Conv + Pool (extract features)\n",
    "    â”œâ”€â”€ Conv + Pool (deeper features)\n",
    "    â””â”€â”€ Bottleneck\n",
    "    â†“\n",
    "DECODER (Convolutional upsampling)\n",
    "    â”œâ”€â”€ Conv + Upsample (reconstruct)\n",
    "    â”œâ”€â”€ Conv + Upsample (refine)\n",
    "    â””â”€â”€ Final Conv\n",
    "    â†“\n",
    "CLEAN IMAGE\n",
    "```\n",
    "\n",
    "- Uses **convolutional layers** at every step\n",
    "- Your edge detection â†’ DALL-E's feature extraction\n",
    "- Iteratively denoises images using CNNs!\n",
    "\n",
    "### ðŸš— **Tesla Autopilot & Self-Driving Cars**\n",
    "\n",
    "**Vision System:**\n",
    "```\n",
    "8 CAMERAS (surrounding vehicle)\n",
    "    â†“\n",
    "CNN Feature Extractors (ResNet-50 backbone)\n",
    "    â†“\n",
    "Feature Pyramid Network (multi-scale features)\n",
    "    â†“\n",
    "DETECTION HEADS\n",
    "    â”œâ”€â”€ Object detection (cars, pedestrians)\n",
    "    â”œâ”€â”€ Lane detection (drivable area)\n",
    "    â”œâ”€â”€ Depth estimation (3D understanding)\n",
    "    â””â”€â”€ Segmentation (semantic scene)\n",
    "    â†“\n",
    "PLANNING & CONTROL\n",
    "```\n",
    "\n",
    "- Processes 36 frames/second from 8 cameras\n",
    "- All using convolutional neural networks!\n",
    "\n",
    "### ðŸ“Š **Scale Comparison:**\n",
    "\n",
    "```\n",
    "Your CIFAR-10 CNN:        ~500K parameters\n",
    "ResNet-50:                25M parameters\n",
    "Vision Transformer (ViT): 86M parameters\n",
    "GPT-4V Vision Encoder:    ~1B parameters (estimated)\n",
    "\n",
    "Same building blocks, different scale!\n",
    "```\n",
    "\n",
    "### ðŸŽ¯ **Real-World Impact:**\n",
    "\n",
    "1. **Medical Diagnosis**: CNNs detect cancer in X-rays with 94%+ accuracy\n",
    "2. **Climate Monitoring**: Satellite image analysis for deforestation\n",
    "3. **Agriculture**: Crop disease detection from drone images\n",
    "4. **Security**: Face recognition in airports and smartphones\n",
    "5. **Retail**: Visual search (Google Lens, Pinterest)\n",
    "\n",
    "**You just built the foundation of computer vision AI!** ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ YOUR TURN: Interactive Exercise\n",
    "\n",
    "**Challenge:** Build a CNN to classify Fashion-MNIST!\n",
    "\n",
    "**Dataset:** Fashion-MNIST\n",
    "- 70,000 grayscale images (28Ã—28)\n",
    "- 10 classes: T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
    "\n",
    "**Your Task:**\n",
    "1. Load Fashion-MNIST dataset\n",
    "2. Build a CNN with at least 2 convolutional blocks\n",
    "3. Train for 10 epochs\n",
    "4. Achieve >85% test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!\n",
    "\n",
    "# Step 1: Load Fashion-MNIST\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# TODO: Load the dataset\n",
    "# (X_train_fashion, y_train_fashion), (X_test_fashion, y_test_fashion) = ...\n",
    "\n",
    "# Step 2: Preprocess data\n",
    "# TODO: Normalize to [0, 1]\n",
    "# TODO: Reshape to (28, 28, 1) for grayscale\n",
    "# TODO: One-hot encode labels\n",
    "\n",
    "# Step 3: Build CNN\n",
    "# TODO: Create Sequential model\n",
    "# TODO: Add Conv2D + MaxPooling layers\n",
    "# TODO: Add Dense layers\n",
    "\n",
    "# Step 4: Compile and train\n",
    "# TODO: Compile with optimizer, loss, metrics\n",
    "# TODO: Train for 10 epochs\n",
    "\n",
    "# Step 5: Evaluate\n",
    "# TODO: Test accuracy\n",
    "\n",
    "print(\"Complete the TODOs above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### âœ… Solution (Try on your own first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Load dataset\n",
    "(X_train_fashion, y_train_fashion), (X_test_fashion, y_test_fashion) = fashion_mnist.load_data()\n",
    "\n",
    "# Preprocess\n",
    "X_train_fashion = X_train_fashion.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test_fashion = X_test_fashion.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "y_train_fashion_cat = keras.utils.to_categorical(y_train_fashion, 10)\n",
    "y_test_fashion_cat = keras.utils.to_categorical(y_test_fashion, 10)\n",
    "\n",
    "# Build model\n",
    "fashion_model = keras.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    \n",
    "    # Conv Block 1\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Conv Block 2\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Dense layers\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "fashion_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"ðŸ§  Fashion-MNIST CNN Model:\")\n",
    "fashion_model.summary()\n",
    "\n",
    "# Train\n",
    "print(\"\\nðŸš€ Training Fashion-MNIST CNN...\")\n",
    "fashion_history = fashion_model.fit(\n",
    "    X_train_fashion, y_train_fashion_cat,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "fashion_test_loss, fashion_test_acc = fashion_model.evaluate(\n",
    "    X_test_fashion, y_test_fashion_cat, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Fashion-MNIST Results:\")\n",
    "print(f\"   Test Accuracy: {fashion_test_acc:.4f} ({fashion_test_acc*100:.2f}%)\")\n",
    "\n",
    "if fashion_test_acc > 0.85:\n",
    "    print(\"   ðŸŽ‰ Challenge completed! Accuracy > 85%\")\n",
    "else:\n",
    "    print(\"   ðŸ’¡ Try adding more layers or training longer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "**You just learned:**\n",
    "- âœ… What CNNs are and why they're essential for computer vision\n",
    "- âœ… Convolutional layers, pooling layers, and feature maps\n",
    "- âœ… How CNNs see images (edge detection â†’ patterns â†’ objects)\n",
    "- âœ… Built a CNN from scratch with TensorFlow/Keras\n",
    "- âœ… Trained a real image classifier on CIFAR-10 dataset\n",
    "- âœ… Visualized CNN feature maps and activations\n",
    "- âœ… How CNNs power GPT-4V, DALL-E, and autonomous vehicles\n",
    "\n",
    "### ðŸŽ¯ Key Takeaways:\n",
    "\n",
    "1. **CNNs are designed for images**\n",
    "   - Share weights across spatial dimensions\n",
    "   - Preserve spatial structure\n",
    "   - Learn hierarchical features\n",
    "\n",
    "2. **Convolution = pattern detection**\n",
    "   - Each filter learns one pattern\n",
    "   - Early layers: edges and textures\n",
    "   - Deep layers: complex objects\n",
    "\n",
    "3. **Pooling = downsampling**\n",
    "   - Reduces spatial dimensions\n",
    "   - Makes network more robust\n",
    "   - Increases receptive field\n",
    "\n",
    "4. **Same principles, different scales**\n",
    "   - Your CNN: ~500K parameters\n",
    "   - GPT-4V Vision: ~1B parameters\n",
    "   - **You understand the foundation!**\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ“š Next Lesson:** Day 2 - Advanced CNN Architectures\n",
    "- LeNet, AlexNet, VGG, ResNet\n",
    "- Transfer learning (use pre-trained models!)\n",
    "- Fine-tuning for custom tasks\n",
    "- Data augmentation techniques\n",
    "- MobileNetV2 for mobile deployment\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¬ Remember:**\n",
    "\n",
    "*\"You just built the computer vision technology that powers GPT-4V, Tesla Autopilot, and medical imaging AI! CNNs are the eyes of modern AI systems. In the next lesson, you'll learn how to use pre-trained models and transfer learning to build production-ready vision systems with minimal training!\"* ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ”— Connections to Modern AI:**\n",
    "- **Multimodal LLMs**: CNNs encode images before language processing\n",
    "- **RAG with Vision**: CNNs extract features from document images\n",
    "- **Agentic AI**: Vision-based agents use CNNs to perceive environment\n",
    "- **Diffusion Models**: Use convolutional U-Nets for image generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
