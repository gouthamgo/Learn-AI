{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 1: Reinforcement Learning Fundamentals\n",
    "\n",
    "**üéØ Goal:** Master RL fundamentals - how AI agents learn by trial and error (like humans!)\n",
    "\n",
    "**‚è±Ô∏è Time:** 90-120 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- RL powers self-driving cars, robotics, and game-playing AI\n",
    "- ChatGPT uses RLHF (Reinforcement Learning from Human Feedback) for alignment\n",
    "- AlphaGo beat world champion using RL (40 million self-play games!)\n",
    "- Autonomous agents and multi-agent systems use RL\n",
    "- Real-world optimization: energy grids, traffic systems, resource allocation\n",
    "- Foundation of AGI research - learning from interaction\n",
    "- Used in robotics (Boston Dynamics), games (OpenAI Five), and drug discovery\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-is-rl",
   "metadata": {},
   "source": [
    "## ü§î What is Reinforcement Learning?\n",
    "\n",
    "**Reinforcement Learning = Learning by trial and error through rewards and punishments**\n",
    "\n",
    "**Human Analogy:**\n",
    "Learning to ride a bike:\n",
    "- **Try action:** Pedal and steer\n",
    "- **Get feedback:** Stay balanced (+reward) OR fall (-punishment)\n",
    "- **Learn:** Adjust actions to maximize staying upright\n",
    "- **Improve:** Eventually ride without falling!\n",
    "\n",
    "**In AI:**\n",
    "- **Agent:** The learner/decision maker (you on the bike)\n",
    "- **Environment:** The world the agent interacts with (road, gravity)\n",
    "- **Actions:** Choices the agent makes (pedal, steer left/right)\n",
    "- **State:** Current situation (speed, balance, position)\n",
    "- **Reward:** Feedback signal (+1 for staying balanced, -1 for falling)\n",
    "- **Policy:** Strategy for choosing actions (what to do in each state)\n",
    "\n",
    "### üéØ RL vs Supervised Learning vs Unsupervised Learning\n",
    "\n",
    "| Feature | Supervised Learning | Unsupervised Learning | Reinforcement Learning |\n",
    "|---------|---------------------|----------------------|------------------------|\n",
    "| **Learning from** | Labeled examples | Unlabeled data | Trial and error |\n",
    "| **Feedback** | Correct answer given | No feedback | Reward/punishment |\n",
    "| **Goal** | Predict labels | Find patterns | Maximize cumulative reward |\n",
    "| **Example** | Image classification | Clustering | Game playing |\n",
    "| **Training data** | (X, Y) pairs | X only | (State, Action, Reward) sequences |\n",
    "\n",
    "### üéØ Real-World Applications (2024-2025)\n",
    "\n",
    "**Where RL is used:**\n",
    "1. **ChatGPT/Claude:** RLHF fine-tunes models to be helpful and safe\n",
    "2. **Self-Driving Cars:** Learn to navigate traffic (Tesla, Waymo)\n",
    "3. **Robotics:** Boston Dynamics robots learn to walk, jump, dance\n",
    "4. **Game AI:** AlphaGo, OpenAI Five (Dota 2), DeepMind's AlphaStar (StarCraft)\n",
    "5. **Recommendation Systems:** YouTube, Netflix optimize for engagement\n",
    "6. **Resource Optimization:** Google data centers (40% energy savings!)\n",
    "7. **Trading Algorithms:** Financial markets\n",
    "8. **Healthcare:** Treatment optimization, drug discovery\n",
    "\n",
    "**The Revolution:**\n",
    "- **2013:** DeepMind's DQN plays Atari games at human level\n",
    "- **2016:** AlphaGo beats Lee Sedol (Go world champion)\n",
    "- **2017:** AlphaZero masters Chess, Shogi, Go from scratch\n",
    "- **2019:** OpenAI Five beats Dota 2 world champions\n",
    "- **2022:** ChatGPT uses RLHF for alignment\n",
    "- **2024-2025:** RL powers autonomous agents and robotics breakthroughs\n",
    "\n",
    "Let's build an RL agent from scratch! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Make plots beautiful\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"Let's build RL agents from scratch! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rl-components",
   "metadata": {},
   "source": [
    "## üß© The RL Framework: Agent-Environment Interaction\n",
    "\n",
    "**The RL Loop:**\n",
    "\n",
    "```\n",
    "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ    Agent     ‚îÇ\n",
    "     ‚îÇ  (Learner)   ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ\n",
    "       Action (a_t)\n",
    "            ‚îÇ\n",
    "            ‚Üì\n",
    "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ Environment  ‚îÇ\n",
    "     ‚îÇ   (World)    ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ\n",
    "    State (s_t+1), Reward (r_t+1)\n",
    "            ‚îÇ\n",
    "            ‚Üì\n",
    "     [Repeat forever]\n",
    "```\n",
    "\n",
    "**At each time step t:**\n",
    "1. **Agent** observes state s_t\n",
    "2. **Agent** takes action a_t (based on policy œÄ)\n",
    "3. **Environment** transitions to new state s_t+1\n",
    "4. **Environment** gives reward r_t+1\n",
    "5. **Agent** learns from experience (s_t, a_t, r_t+1, s_t+1)\n",
    "6. **Repeat!**\n",
    "\n",
    "### üéØ Key Concepts:\n",
    "\n",
    "**1. State (s):**\n",
    "- Complete description of the environment\n",
    "- Example (Grid world): (row, col) position\n",
    "- Example (Chess): Board configuration\n",
    "- Example (Self-driving): Speed, position, nearby cars, traffic lights\n",
    "\n",
    "**2. Action (a):**\n",
    "- Choice the agent makes\n",
    "- Example (Grid world): {UP, DOWN, LEFT, RIGHT}\n",
    "- Example (Chess): Move a piece\n",
    "- Example (Self-driving): {Accelerate, Brake, Turn}\n",
    "\n",
    "**3. Reward (r):**\n",
    "- Immediate feedback signal\n",
    "- Example (Grid world): +10 for goal, -1 for each step\n",
    "- Example (Chess): +1 for win, 0 for draw, -1 for loss\n",
    "- Example (Self-driving): +1 for safe driving, -100 for crash\n",
    "\n",
    "**4. Policy (œÄ):**\n",
    "- Strategy for choosing actions\n",
    "- œÄ(s) = action to take in state s\n",
    "- Goal: Find optimal policy œÄ* that maximizes rewards!\n",
    "\n",
    "**5. Value Function (V):**\n",
    "- Expected total reward from a state\n",
    "- V(s) = \"How good is this state?\"\n",
    "- Considers future rewards, not just immediate\n",
    "\n",
    "Let's implement a simple environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gridworld-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Grid World Environment\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Simple 4x4 grid world:\n",
    "    - Agent starts at (0, 0)\n",
    "    - Goal at (3, 3)\n",
    "    - Walls at (1, 1) and (2, 2)\n",
    "    - Actions: UP, DOWN, LEFT, RIGHT\n",
    "    - Rewards: +10 for goal, -1 for each step, -5 for hitting wall\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (size-1, size-1)\n",
    "        self.walls = [(1, 1), (2, 2)]  # Obstacles\n",
    "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "        self.action_effects = {\n",
    "            'UP': (-1, 0),\n",
    "            'DOWN': (1, 0),\n",
    "            'LEFT': (0, -1),\n",
    "            'RIGHT': (0, 1)\n",
    "        }\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to starting position\"\"\"\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action and return (next_state, reward, done)\n",
    "        \"\"\"\n",
    "        # Calculate next position\n",
    "        delta = self.action_effects[action]\n",
    "        next_state = (self.state[0] + delta[0], self.state[1] + delta[1])\n",
    "        \n",
    "        # Check if next state is valid\n",
    "        if self._is_valid(next_state):\n",
    "            self.state = next_state\n",
    "            reward = -1  # Step cost\n",
    "        else:\n",
    "            # Hit wall or boundary - stay in place\n",
    "            reward = -5  # Penalty for hitting wall\n",
    "        \n",
    "        # Check if reached goal\n",
    "        done = False\n",
    "        if self.state == self.goal:\n",
    "            reward = 10  # Goal reward!\n",
    "            done = True\n",
    "        \n",
    "        return self.state, reward, done\n",
    "    \n",
    "    def _is_valid(self, state):\n",
    "        \"\"\"Check if state is within bounds and not a wall\"\"\"\n",
    "        row, col = state\n",
    "        \n",
    "        # Check bounds\n",
    "        if row < 0 or row >= self.size or col < 0 or col >= self.size:\n",
    "            return False\n",
    "        \n",
    "        # Check walls\n",
    "        if state in self.walls:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Visualize the grid world\"\"\"\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        \n",
    "        # Mark walls\n",
    "        for wall in self.walls:\n",
    "            grid[wall] = -1\n",
    "        \n",
    "        # Mark goal\n",
    "        grid[self.goal] = 2\n",
    "        \n",
    "        # Mark agent\n",
    "        grid[self.state] = 1\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(grid, cmap='RdYlGn', vmin=-1, vmax=2)\n",
    "        \n",
    "        # Add grid lines\n",
    "        for i in range(self.size + 1):\n",
    "            plt.axhline(i - 0.5, color='black', linewidth=2)\n",
    "            plt.axvline(i - 0.5, color='black', linewidth=2)\n",
    "        \n",
    "        # Add labels\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if (i, j) == self.state:\n",
    "                    plt.text(j, i, 'ü§ñ', ha='center', va='center', fontsize=30)\n",
    "                elif (i, j) == self.goal:\n",
    "                    plt.text(j, i, 'üéØ', ha='center', va='center', fontsize=30)\n",
    "                elif (i, j) in self.walls:\n",
    "                    plt.text(j, i, 'üß±', ha='center', va='center', fontsize=30)\n",
    "        \n",
    "        plt.xlim(-0.5, self.size - 0.5)\n",
    "        plt.ylim(self.size - 0.5, -0.5)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title('Grid World: ü§ñ=Agent, üéØ=Goal, üß±=Wall', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create environment\n",
    "env = GridWorld(size=4)\n",
    "\n",
    "print(\"‚úÖ Grid World Environment Created!\")\n",
    "print(f\"\\nEnvironment Details:\")\n",
    "print(f\"  Grid size: {env.size}x{env.size}\")\n",
    "print(f\"  Start: {env.start}\")\n",
    "print(f\"  Goal: {env.goal}\")\n",
    "print(f\"  Walls: {env.walls}\")\n",
    "print(f\"  Actions: {env.actions}\")\n",
    "print(f\"\\nRewards:\")\n",
    "print(f\"  +10 for reaching goal\")\n",
    "print(f\"  -1 for each step (encourages shortest path)\")\n",
    "print(f\"  -5 for hitting wall/boundary\")\n",
    "\n",
    "# Visualize initial state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the environment with random actions\n",
    "\n",
    "print(\"üéÆ Testing environment with random actions...\\n\")\n",
    "\n",
    "state = env.reset()\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "total_reward = 0\n",
    "for step in range(10):\n",
    "    # Random action\n",
    "    action = random.choice(env.actions)\n",
    "    \n",
    "    # Take step\n",
    "    next_state, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    print(f\"Step {step + 1}: Action={action:6s}, State={next_state}, Reward={reward:3}, Done={done}\")\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\\nüéâ Reached goal in {step + 1} steps!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal reward: {total_reward}\")\n",
    "print(\"\\nüí° Notice: Random actions are inefficient! We need a smarter policy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mdp",
   "metadata": {},
   "source": [
    "## üé≤ Markov Decision Processes (MDPs)\n",
    "\n",
    "**MDP = Mathematical framework for modeling RL problems**\n",
    "\n",
    "### Definition:\n",
    "\n",
    "An MDP is defined by a tuple (S, A, P, R, Œ≥):\n",
    "\n",
    "**1. S:** Set of states\n",
    "- All possible situations\n",
    "- Example: All (row, col) positions in grid\n",
    "\n",
    "**2. A:** Set of actions\n",
    "- All possible choices\n",
    "- Example: {UP, DOWN, LEFT, RIGHT}\n",
    "\n",
    "**3. P:** Transition probability P(s'|s,a)\n",
    "- Probability of reaching state s' from state s after action a\n",
    "- Example: In deterministic grid, P = 1.0 for intended direction\n",
    "- Example: In stochastic grid, P = 0.8 for intended, 0.1 for each side\n",
    "\n",
    "**4. R:** Reward function R(s,a,s')\n",
    "- Immediate reward for transition\n",
    "- Example: +10 for goal, -1 for step\n",
    "\n",
    "**5. Œ≥ (gamma):** Discount factor (0 ‚â§ Œ≥ ‚â§ 1)\n",
    "- How much we value future rewards\n",
    "- Œ≥ = 0: Only care about immediate reward (myopic)\n",
    "- Œ≥ = 1: Future rewards as important as immediate (far-sighted)\n",
    "- Œ≥ = 0.9: Common choice (balance)\n",
    "\n",
    "### üéØ The Markov Property:\n",
    "\n",
    "**\"The future is independent of the past given the present\"**\n",
    "\n",
    "Mathematically:\n",
    "```\n",
    "P(s_t+1 | s_t, s_t-1, s_t-2, ..., s_0) = P(s_t+1 | s_t)\n",
    "```\n",
    "\n",
    "**Meaning:**\n",
    "- Current state contains all relevant information\n",
    "- Don't need to remember entire history\n",
    "- Simplifies learning significantly!\n",
    "\n",
    "**Example:**\n",
    "- ‚úÖ Chess: Current board position is enough (Markov)\n",
    "- ‚ùå Poker: Need to remember previous bets/actions (Partially Observable)\n",
    "\n",
    "### üéØ Goal: Maximize Return\n",
    "\n",
    "**Return (G_t):** Total discounted reward from time t\n",
    "\n",
    "```\n",
    "G_t = r_t+1 + Œ≥*r_t+2 + Œ≥¬≤*r_t+3 + Œ≥¬≥*r_t+4 + ...\n",
    "    = Œ£ Œ≥^k * r_t+k+1  (sum from k=0 to ‚àû)\n",
    "```\n",
    "\n",
    "**Example (Œ≥=0.9):**\n",
    "```\n",
    "Rewards: [1, 1, 1, 10]\n",
    "Return = 1 + 0.9*1 + 0.81*1 + 0.729*10 = 10.0\n",
    "```\n",
    "\n",
    "**Why discount?**\n",
    "1. **Uncertainty:** Future is uncertain\n",
    "2. **Mathematical convenience:** Ensures finite return\n",
    "3. **Preference:** Immediate rewards preferred (human psychology!)\n",
    "4. **Convergence:** Helps algorithms converge\n",
    "\n",
    "Let's visualize MDPs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-mdp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize discount factor effect\n",
    "\n",
    "def calculate_return(rewards, gamma):\n",
    "    \"\"\"Calculate discounted return\"\"\"\n",
    "    G = 0\n",
    "    for t, r in enumerate(rewards):\n",
    "        G += (gamma ** t) * r\n",
    "    return G\n",
    "\n",
    "# Example reward sequences\n",
    "rewards_immediate = [10, 0, 0, 0, 0]  # Immediate reward\n",
    "rewards_delayed = [0, 0, 0, 0, 10]    # Delayed reward\n",
    "\n",
    "gammas = np.linspace(0, 1, 20)\n",
    "returns_immediate = [calculate_return(rewards_immediate, g) for g in gammas]\n",
    "returns_delayed = [calculate_return(rewards_delayed, g) for g in gammas]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Return vs Gamma\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gammas, returns_immediate, 'b-o', label='Immediate reward [10,0,0,0,0]', linewidth=2)\n",
    "plt.plot(gammas, returns_delayed, 'r-s', label='Delayed reward [0,0,0,0,10]', linewidth=2)\n",
    "plt.xlabel('Discount Factor (Œ≥)', fontsize=12)\n",
    "plt.ylabel('Total Return', fontsize=12)\n",
    "plt.title('Effect of Discount Factor on Return', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axvline(0.9, color='green', linestyle='--', alpha=0.5, label='Common choice')\n",
    "\n",
    "# Plot 2: Discount weights over time\n",
    "plt.subplot(1, 2, 2)\n",
    "time_steps = np.arange(20)\n",
    "for gamma in [0.5, 0.9, 0.99]:\n",
    "    weights = [gamma ** t for t in time_steps]\n",
    "    plt.plot(time_steps, weights, 'o-', label=f'Œ≥={gamma}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Time Step', fontsize=12)\n",
    "plt.ylabel('Discount Weight (Œ≥^t)', fontsize=12)\n",
    "plt.title('How Much Future Rewards Count', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Observations:\")\n",
    "print(\"  Left plot: Higher Œ≥ ‚Üí delayed rewards matter more\")\n",
    "print(\"  Right plot: Lower Œ≥ ‚Üí only nearby rewards matter\")\n",
    "print(\"\\nüí° Common choices:\")\n",
    "print(\"  Œ≥ = 0.9:  Games, robotics (balance short/long term)\")\n",
    "print(\"  Œ≥ = 0.99: Finance, long-term planning\")\n",
    "print(\"  Œ≥ = 0.5:  Very myopic (rare in practice)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q-learning",
   "metadata": {},
   "source": [
    "## üß† Q-Learning Algorithm\n",
    "\n",
    "**Q-Learning = Learn action-values (Q-values) to find optimal policy**\n",
    "\n",
    "### What is Q?\n",
    "\n",
    "**Q(s, a) = Expected return from taking action a in state s, then following optimal policy**\n",
    "\n",
    "- Q stands for \"Quality\" of action\n",
    "- Q(s, a) tells us: \"How good is action a in state s?\"\n",
    "- Higher Q ‚Üí better action!\n",
    "\n",
    "**Example (Grid World):**\n",
    "```\n",
    "State (1, 1), next to goal (1, 2):\n",
    "  Q(s, RIGHT) = 9  (leads to goal!)\n",
    "  Q(s, LEFT)  = -5 (moves away)\n",
    "  Q(s, UP)    = -3\n",
    "  Q(s, DOWN)  = -4\n",
    "  \n",
    "Best action: RIGHT (highest Q-value)\n",
    "```\n",
    "\n",
    "### The Bellman Equation:\n",
    "\n",
    "**Optimal Q-value satisfies:**\n",
    "```\n",
    "Q*(s, a) = R(s,a) + Œ≥ * max_a' Q*(s', a')\n",
    "```\n",
    "\n",
    "**In words:**\n",
    "- Q-value = Immediate reward + Discounted best future Q-value\n",
    "- Recursive definition (dynamic programming!)\n",
    "\n",
    "### Q-Learning Update Rule:\n",
    "\n",
    "**After taking action a in state s, receiving reward r, reaching state s':**\n",
    "\n",
    "```\n",
    "Q(s, a) ‚Üê Q(s, a) + Œ± * [r + Œ≥ * max_a' Q(s', a') - Q(s, a)]\n",
    "                        Ô∏∏‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅÔ∏∏\n",
    "                                  TD Error (Œ¥)\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "- **Œ± (alpha):** Learning rate (0 < Œ± ‚â§ 1)\n",
    "  - Œ± = 0.1: Slow, stable learning\n",
    "  - Œ± = 0.5: Faster learning\n",
    "  - Œ± = 1.0: Only remember latest sample\n",
    "\n",
    "- **TD Error (Œ¥):** How much we were wrong\n",
    "  - Œ¥ > 0: We underestimated ‚Üí increase Q\n",
    "  - Œ¥ < 0: We overestimated ‚Üí decrease Q\n",
    "\n",
    "- **Target:** r + Œ≥ * max_a' Q(s', a')\n",
    "  - What Q(s,a) \"should be\" based on new experience\n",
    "\n",
    "### üéØ Q-Learning Algorithm:\n",
    "\n",
    "```python\n",
    "1. Initialize Q(s, a) = 0 for all state-action pairs\n",
    "2. For each episode:\n",
    "     a. Reset environment to start state s\n",
    "     b. While not done:\n",
    "          i.   Choose action a using Œµ-greedy policy\n",
    "          ii.  Take action a, observe r, s'\n",
    "          iii. Update: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥*max_a' Q(s',a') - Q(s,a)]\n",
    "          iv.  s ‚Üê s'\n",
    "3. Return learned Q-table\n",
    "```\n",
    "\n",
    "### üéØ Exploration vs Exploitation:\n",
    "\n",
    "**The Dilemma:**\n",
    "- **Exploit:** Choose action with highest Q-value (greedy)\n",
    "- **Explore:** Try random actions (discover better strategies)\n",
    "\n",
    "**Œµ-greedy Policy:**\n",
    "- With probability Œµ: Random action (explore)\n",
    "- With probability 1-Œµ: Best action (exploit)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Œµ = 0.1:\n",
    "  - 10% of time: explore (random)\n",
    "  - 90% of time: exploit (greedy)\n",
    "```\n",
    "\n",
    "**Decay Œµ over time:**\n",
    "- Start high (Œµ = 1.0): Explore a lot\n",
    "- End low (Œµ = 0.01): Mostly exploit\n",
    "- Balances exploration and exploitation!\n",
    "\n",
    "Let's implement Q-Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q-learning-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning Agent\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent for grid world\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, actions, learning_rate=0.1, discount_factor=0.9, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.actions = actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Q-table: dictionary {state: {action: Q-value}}\n",
    "        self.q_table = defaultdict(lambda: {action: 0.0 for action in actions})\n",
    "        \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"\n",
    "        Choose action using Œµ-greedy policy\n",
    "        \"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            # Exploit: best action\n",
    "            q_values = self.q_table[state]\n",
    "            max_q = max(q_values.values())\n",
    "            # Handle ties: randomly choose among best actions\n",
    "            best_actions = [a for a in self.actions if q_values[a] == max_q]\n",
    "            return random.choice(best_actions)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update Q-value using Q-learning rule\n",
    "        \"\"\"\n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        # Calculate target\n",
    "        if done:\n",
    "            target = reward  # No future rewards\n",
    "        else:\n",
    "            # Best Q-value for next state\n",
    "            max_next_q = max(self.q_table[next_state].values())\n",
    "            target = reward + self.gamma * max_next_q\n",
    "        \n",
    "        # TD error\n",
    "        td_error = target - current_q\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.q_table[state][action] = current_q + self.lr * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"‚úÖ Q-Learning Agent implemented!\")\n",
    "print(\"\\nüéØ Key features:\")\n",
    "print(\"  - Q-table stores Q(s,a) for all state-action pairs\")\n",
    "print(\"  - Œµ-greedy policy balances exploration/exploitation\")\n",
    "print(\"  - Decaying Œµ: explore less over time\")\n",
    "print(\"  - TD learning: update from experience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-q-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Q-Learning Agent\n",
    "\n",
    "def train_q_learning(env, agent, num_episodes=500, max_steps=100):\n",
    "    \"\"\"\n",
    "    Train Q-learning agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilon_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Choose action\n",
    "            action = agent.get_action(state, training=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Update Q-table\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_length = np.mean(episode_lengths[-100:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} - Avg Reward: {avg_reward:.2f}, Avg Length: {avg_length:.2f}, Œµ: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths, epsilon_history\n",
    "\n",
    "# Create agent and environment\n",
    "env = GridWorld(size=4)\n",
    "agent = QLearningAgent(\n",
    "    actions=env.actions,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.9,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training Q-Learning agent...\\n\")\n",
    "rewards, lengths, epsilons = train_q_learning(env, agent, num_episodes=500)\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Episode rewards\n",
    "ax = axes[0, 0]\n",
    "ax.plot(rewards, alpha=0.3, color='blue', label='Raw')\n",
    "# Moving average\n",
    "window = 50\n",
    "moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(rewards)), moving_avg, color='red', linewidth=2, label=f'{window}-episode avg')\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Total Reward', fontsize=12)\n",
    "ax.set_title('üìà Learning Progress: Episode Rewards', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode lengths\n",
    "ax = axes[0, 1]\n",
    "ax.plot(lengths, alpha=0.3, color='green', label='Raw')\n",
    "moving_avg_len = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(lengths)), moving_avg_len, color='orange', linewidth=2, label=f'{window}-episode avg')\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Steps to Goal', fontsize=12)\n",
    "ax.set_title('‚è±Ô∏è Efficiency: Steps per Episode', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Epsilon decay\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epsilons, color='purple', linewidth=2)\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Epsilon (Œµ)', fontsize=12)\n",
    "ax.set_title('üîç Exploration Rate (Œµ) Decay', fontsize=13, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.axhline(0.1, color='red', linestyle='--', alpha=0.5, label='10% exploration')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 4: Final 50 episodes statistics\n",
    "ax = axes[1, 1]\n",
    "final_rewards = rewards[-50:]\n",
    "ax.hist(final_rewards, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax.axvline(np.mean(final_rewards), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(final_rewards):.2f}')\n",
    "ax.set_xlabel('Total Reward', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('üìä Final 50 Episodes: Reward Distribution', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Training Summary:\")\n",
    "print(f\"  First 50 episodes avg reward: {np.mean(rewards[:50]):.2f}\")\n",
    "print(f\"  Last 50 episodes avg reward: {np.mean(rewards[-50:]):.2f}\")\n",
    "print(f\"  Improvement: {np.mean(rewards[-50:]) - np.mean(rewards[:50]):.2f}\")\n",
    "print(f\"\\n  First 50 episodes avg length: {np.mean(lengths[:50]):.1f} steps\")\n",
    "print(f\"  Last 50 episodes avg length: {np.mean(lengths[-50:]):.1f} steps\")\n",
    "print(f\"\\nüí° Agent learned to reach goal more efficiently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-ai-example",
   "metadata": {},
   "source": [
    "## üåü Real AI Example: Visualizing Learned Policy\n",
    "\n",
    "**Let's see what the agent learned!**\n",
    "\n",
    "We'll visualize:\n",
    "1. **Q-values:** How good each action is in each state\n",
    "2. **Policy:** Best action in each state (arrows)\n",
    "3. **Value function:** How valuable each state is\n",
    "\n",
    "This is exactly how researchers visualize RL agents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned policy and Q-values\n",
    "\n",
    "def visualize_policy(agent, env):\n",
    "    \"\"\"\n",
    "    Visualize learned policy with arrows\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Create value function grid\n",
    "    value_grid = np.zeros((env.size, env.size))\n",
    "    \n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            state = (i, j)\n",
    "            if state in env.walls:\n",
    "                value_grid[i, j] = -10  # Wall marker\n",
    "            else:\n",
    "                # State value = max Q-value\n",
    "                value_grid[i, j] = max(agent.q_table[state].values())\n",
    "    \n",
    "    # Plot 1: Value function heatmap\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(value_grid, cmap='RdYlGn', interpolation='nearest')\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(env.size + 1):\n",
    "        ax.axhline(i - 0.5, color='black', linewidth=2)\n",
    "        ax.axvline(i - 0.5, color='black', linewidth=2)\n",
    "    \n",
    "    # Add values as text\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if (i, j) in env.walls:\n",
    "                ax.text(j, i, 'üß±', ha='center', va='center', fontsize=25)\n",
    "            elif (i, j) == env.goal:\n",
    "                ax.text(j, i, 'üéØ', ha='center', va='center', fontsize=25)\n",
    "            else:\n",
    "                ax.text(j, i, f'{value_grid[i, j]:.1f}', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(-0.5, env.size - 0.5)\n",
    "    ax.set_ylim(env.size - 0.5, -0.5)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('üéØ State Values V(s) = max_a Q(s,a)', fontsize=13, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, label='Value')\n",
    "    \n",
    "    # Plot 2: Policy arrows\n",
    "    ax = axes[1]\n",
    "    ax.imshow(np.zeros((env.size, env.size)), cmap='gray', vmin=0, vmax=1, alpha=0.1)\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(env.size + 1):\n",
    "        ax.axhline(i - 0.5, color='black', linewidth=2)\n",
    "        ax.axvline(i - 0.5, color='black', linewidth=2)\n",
    "    \n",
    "    # Arrow directions\n",
    "    arrow_map = {\n",
    "        'UP': '‚Üë',\n",
    "        'DOWN': '‚Üì',\n",
    "        'LEFT': '‚Üê',\n",
    "        'RIGHT': '‚Üí'\n",
    "    }\n",
    "    \n",
    "    # Draw arrows for best action in each state\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            state = (i, j)\n",
    "            if state in env.walls:\n",
    "                ax.text(j, i, 'üß±', ha='center', va='center', fontsize=25)\n",
    "            elif state == env.goal:\n",
    "                ax.text(j, i, 'üéØ', ha='center', va='center', fontsize=25)\n",
    "            else:\n",
    "                # Get best action\n",
    "                q_values = agent.q_table[state]\n",
    "                best_action = max(q_values, key=q_values.get)\n",
    "                arrow = arrow_map[best_action]\n",
    "                ax.text(j, i, arrow, ha='center', va='center', fontsize=30, fontweight='bold', color='blue')\n",
    "    \n",
    "    ax.set_xlim(-0.5, env.size - 0.5)\n",
    "    ax.set_ylim(env.size - 0.5, -0.5)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('üß≠ Learned Policy œÄ(s) = argmax_a Q(s,a)', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize learned policy\n",
    "visualize_policy(agent, env)\n",
    "\n",
    "print(\"\\nüéØ Policy Interpretation:\")\n",
    "print(\"  Left: State values show 'how good' each state is\")\n",
    "print(\"  Right: Arrows show best action from each state\")\n",
    "print(\"  Notice: Arrows point toward goal (optimal path!)\")\n",
    "print(\"\\nüí° This is how DeepMind visualizes AlphaGo's strategy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-learned-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test learned policy (no exploration)\n",
    "\n",
    "print(\"üéÆ Testing learned policy (exploitation only)...\\n\")\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "path = [state]\n",
    "\n",
    "print(f\"Start: {state}\")\n",
    "\n",
    "for step in range(20):\n",
    "    # Choose best action (no exploration)\n",
    "    action = agent.get_action(state, training=False)\n",
    "    \n",
    "    # Take action\n",
    "    next_state, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "    path.append(next_state)\n",
    "    \n",
    "    print(f\"Step {step + 1}: {state} --{action:6s}--> {next_state} (reward: {reward:3})\")\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\\nüéâ Reached goal in {step + 1} steps!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nPath taken: {' ‚Üí '.join([str(s) for s in path])}\")\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(f\"\\nüí° Agent learned optimal (or near-optimal) policy!\")\n",
    "print(f\"   Optimal path length: {env.size - 1 + env.size - 1} = {2*(env.size-1)} steps (Manhattan distance)\")\n",
    "print(f\"   Agent's path: {step + 1} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercises\n",
    "\n",
    "Test your understanding of RL fundamentals!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### Exercise 1: Modify Reward Structure\n",
    "\n",
    "**Task:** Change the grid world rewards and retrain\n",
    "\n",
    "**Experiment with:**\n",
    "1. Higher goal reward (+100 instead of +10)\n",
    "2. No step penalty (0 instead of -1)\n",
    "3. Larger wall penalty (-10 instead of -5)\n",
    "\n",
    "**Question:** How does each change affect learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Modify GridWorld class rewards and retrain\n",
    "# Compare learning curves\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for solution & insights</summary>\n",
    "\n",
    "```python\n",
    "# Higher goal reward:\n",
    "# - Stronger signal ‚Üí faster learning\n",
    "# - But: doesn't change optimal policy\n",
    "\n",
    "# No step penalty:\n",
    "# - Agent doesn't care about path length\n",
    "# - May wander randomly (not efficient)\n",
    "# - Step penalty encourages shortest path!\n",
    "\n",
    "# Larger wall penalty:\n",
    "# - Agent avoids walls more strongly\n",
    "# - May find safer (but longer) paths\n",
    "# - Good for robotics (avoid collisions!)\n",
    "```\n",
    "\n",
    "**Key Insight:** Reward design (reward shaping) is crucial for RL!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### Exercise 2: Tune Hyperparameters\n",
    "\n",
    "**Task:** Experiment with different hyperparameters\n",
    "\n",
    "**Try:**\n",
    "1. Learning rate: Œ± = 0.01, 0.1, 0.5, 1.0\n",
    "2. Discount factor: Œ≥ = 0.5, 0.9, 0.99\n",
    "3. Epsilon decay: 0.99, 0.995, 0.999\n",
    "\n",
    "**Question:** Which combination learns fastest? Which learns best final policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create agents with different hyperparameters\n",
    "# Train and compare\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for solution & insights</summary>\n",
    "\n",
    "```python\n",
    "# Learning rate (Œ±):\n",
    "# - Too low (0.01): Slow learning, but stable\n",
    "# - Too high (1.0): Fast initial learning, but unstable\n",
    "# - Optimal: 0.1-0.3 for most problems\n",
    "\n",
    "# Discount factor (Œ≥):\n",
    "# - Low (0.5): Myopic, only sees nearby rewards\n",
    "# - High (0.99): Far-sighted, considers long-term\n",
    "# - For grid world: 0.9 works well\n",
    "\n",
    "# Epsilon decay:\n",
    "# - Fast (0.99): Exploits early, may miss better solutions\n",
    "# - Slow (0.999): Explores longer, finds better policy\n",
    "# - Trade-off: exploration time vs convergence speed\n",
    "```\n",
    "\n",
    "**Best Practice:** Start with standard values (Œ±=0.1, Œ≥=0.9, Œµ_decay=0.995), then tune!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3",
   "metadata": {},
   "source": [
    "### Exercise 3: Calculate Return by Hand\n",
    "\n",
    "**Given:**\n",
    "- Reward sequence: [1, 2, 3, 10]\n",
    "- Discount factor: Œ≥ = 0.9\n",
    "\n",
    "**Task:** Calculate the total discounted return G\n",
    "\n",
    "**Formula:** G = r‚ÇÅ + Œ≥r‚ÇÇ + Œ≥¬≤r‚ÇÉ + Œ≥¬≥r‚ÇÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "rewards = [1, 2, 3, 10]\n",
    "gamma = 0.9\n",
    "\n",
    "# Calculate G\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "G = 1 + 0.9*2 + 0.81*3 + 0.729*10\n",
    "G = 1 + 1.8 + 2.43 + 7.29\n",
    "G = 12.52\n",
    "\n",
    "# Or using code:\n",
    "G = sum([gamma**i * r for i, r in enumerate(rewards)])\n",
    "print(f\"Return: {G:.2f}\")\n",
    "```\n",
    "\n",
    "**Notice:** Later rewards count less (7.29 instead of 10)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**You just learned:**\n",
    "\n",
    "### 1. **What is Reinforcement Learning?**\n",
    "   - ‚úÖ Learning by trial and error through rewards\n",
    "   - ‚úÖ Agent-environment interaction loop\n",
    "   - ‚úÖ Different from supervised/unsupervised learning\n",
    "   - **Powers:** Game AI, robotics, ChatGPT (RLHF)\n",
    "\n",
    "### 2. **Markov Decision Processes (MDPs)**\n",
    "   - ‚úÖ Mathematical framework: (S, A, P, R, Œ≥)\n",
    "   - ‚úÖ Markov property: future independent of past given present\n",
    "   - ‚úÖ Discount factor Œ≥ balances short/long-term rewards\n",
    "   - **Used in:** All RL algorithms, planning, control\n",
    "\n",
    "### 3. **Q-Learning Algorithm**\n",
    "   - ‚úÖ Learn Q(s,a) = \"quality\" of actions\n",
    "   - ‚úÖ Bellman equation: Q*(s,a) = r + Œ≥ max Q(s',a')\n",
    "   - ‚úÖ TD learning: update from experience\n",
    "   - ‚úÖ Œµ-greedy: balance exploration/exploitation\n",
    "   - **Breakthrough:** First RL algorithm to master Atari games (DQN 2013)\n",
    "\n",
    "### 4. **Key Concepts**\n",
    "   - ‚úÖ Policy œÄ: strategy for choosing actions\n",
    "   - ‚úÖ Value function V(s): expected return from state\n",
    "   - ‚úÖ Exploration vs exploitation trade-off\n",
    "   - ‚úÖ Reward shaping affects learning\n",
    "\n",
    "### üåü Real-World Impact (2024-2025):\n",
    "\n",
    "**What You Can Build:**\n",
    "- üéÆ **Game AI** (Chess, Go, Poker agents)\n",
    "- ü§ñ **Robotics** (Navigation, manipulation)\n",
    "- üí¨ **Chatbots** with RLHF (like ChatGPT)\n",
    "- üöó **Autonomous vehicles** (path planning)\n",
    "- üìä **Resource optimization** (energy, traffic)\n",
    "- üí∞ **Trading algorithms** (financial markets)\n",
    "\n",
    "**Modern Applications:**\n",
    "- **ChatGPT:** Uses RLHF to align with human preferences\n",
    "- **AlphaGo:** Beat world champion with self-play RL\n",
    "- **Tesla Autopilot:** RL for decision making\n",
    "- **DeepMind:** Energy optimization (40% savings!)\n",
    "- **OpenAI Five:** Dota 2 world championship\n",
    "- **Boston Dynamics:** Robots learn to walk/jump\n",
    "\n",
    "### üìä Q-Learning vs Other Methods:\n",
    "\n",
    "| Feature | Q-Learning | Deep Q-Network (DQN) | Policy Gradient |\n",
    "|---------|------------|---------------------|------------------|\n",
    "| Function approximation | ‚ùå Table | ‚úÖ Neural network | ‚úÖ Neural network |\n",
    "| Discrete actions | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes & continuous |\n",
    "| Off-policy | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No (on-policy) |\n",
    "| Sample efficiency | ‚úÖ Good | ‚úÖ Good | ‚ùå Poor |\n",
    "| Stability | ‚úÖ Stable | ‚ö†Ô∏è Can diverge | ‚ö†Ô∏è High variance |\n",
    "| Used for | Small state spaces | Atari games | Robotics, continuous control |\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now understand:\n",
    "- How RL agents learn from interaction\n",
    "- The foundations of DeepMind's AlphaGo\n",
    "- How ChatGPT uses RLHF for alignment\n",
    "- The core algorithm behind game-playing AI\n",
    "\n",
    "**Next:** We'll learn Deep Q-Networks (DQN) and modern deep RL! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "**Practice Exercises:**\n",
    "1. Implement a larger grid world (8x8, 10x10)\n",
    "2. Add stochastic transitions (actions succeed 80% of time)\n",
    "3. Implement SARSA (on-policy variant of Q-learning)\n",
    "4. Try different exploration strategies (Boltzmann, UCB)\n",
    "\n",
    "**Coming Next:**\n",
    "- **Day 2:** Deep Q-Networks (DQN), Policy Gradients, Actor-Critic\n",
    "- **Day 3:** Advanced RL - Multi-agent, AlphaZero, Real-world applications\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Deep Dive Resources:**\n",
    "- Sutton & Barto: \"Reinforcement Learning: An Introduction\" (THE textbook)\n",
    "- DeepMind x UCL RL Lecture Series (YouTube)\n",
    "- OpenAI Spinning Up in Deep RL\n",
    "- Gymnasium (OpenAI Gym successor) documentation\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: RL is how AI learns to master games, control robots, and align language models. You now know the foundations!* üåü\n",
    "\n",
    "**üéØ You understand how AlphaGo learned to beat humans at Go!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
