{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 3: Advanced RL Applications\n",
    "\n",
    "**üéØ Goal:** Master advanced RL - multi-agent systems, AlphaGo, real-world applications\n",
    "\n",
    "**‚è±Ô∏è Time:** 90-120 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- AlphaGo/AlphaZero revolutionized game AI - beat world champions at Go, Chess, Shogi\n",
    "- Multi-agent RL powers autonomous vehicle coordination and swarm robotics\n",
    "- RL optimizes Google data centers (40% energy savings = millions of dollars!)\n",
    "- Robotics uses RL for manipulation, walking, and complex tasks\n",
    "- ChatGPT uses RLHF (Reinforcement Learning from Human Feedback)\n",
    "- Self-driving cars, drones, and warehouse robots use RL for decision-making\n",
    "- RL in finance: trading algorithms, portfolio optimization\n",
    "- Healthcare: treatment optimization, drug discovery, personalized medicine\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-rl-overview",
   "metadata": {},
   "source": [
    "## üåç Real-World RL: Beyond Games\n",
    "\n",
    "**RL has moved from games to real-world impact!**\n",
    "\n",
    "### üéØ 2024-2025 RL Applications:\n",
    "\n",
    "**1. Large Language Models (LLMs):**\n",
    "- **ChatGPT/GPT-4:** RLHF fine-tunes models to be helpful, harmless, honest\n",
    "- **Claude/Gemini:** Constitutional AI with RL\n",
    "- **Reward model:** Human preferences ‚Üí RL objective\n",
    "- **Algorithm:** PPO (Proximal Policy Optimization)\n",
    "\n",
    "**2. Robotics:**\n",
    "- **Boston Dynamics:** Atlas robot learns to parkour, backflips\n",
    "- **Tesla Bot:** RL for manipulation and navigation\n",
    "- **Warehouse robots:** Amazon, Ocado use RL for coordination\n",
    "- **Surgical robots:** Learn precise movements\n",
    "\n",
    "**3. Autonomous Vehicles:**\n",
    "- **Waymo/Tesla:** Decision making in traffic\n",
    "- **Drones:** Path planning, obstacle avoidance\n",
    "- **Multi-agent:** Vehicle-to-vehicle coordination\n",
    "\n",
    "**4. Resource Optimization:**\n",
    "- **Google Data Centers:** 40% cooling cost reduction\n",
    "- **Energy grids:** Load balancing, renewable integration\n",
    "- **Traffic lights:** Adaptive timing reduces congestion\n",
    "- **Supply chains:** Inventory and logistics optimization\n",
    "\n",
    "**5. Finance & Trading:**\n",
    "- **Algorithmic trading:** High-frequency trading strategies\n",
    "- **Portfolio optimization:** Risk-adjusted returns\n",
    "- **Fraud detection:** Sequential decision making\n",
    "\n",
    "**6. Healthcare:**\n",
    "- **Treatment optimization:** Personalized medicine\n",
    "- **Drug discovery:** Molecular design\n",
    "- **ICU management:** Ventilator and medication dosing\n",
    "- **Radiation therapy:** Adaptive treatment planning\n",
    "\n",
    "**7. Recommendation Systems:**\n",
    "- **YouTube/Netflix:** Maximize long-term engagement\n",
    "- **E-commerce:** Product recommendations\n",
    "- **News feeds:** Content personalization\n",
    "\n",
    "### üìà The Evolution:\n",
    "\n",
    "```\n",
    "2013: DQN plays Atari\n",
    "2016: AlphaGo beats Lee Sedol\n",
    "2017: AlphaZero masters Chess/Go/Shogi\n",
    "2018: OpenAI Five plays Dota 2\n",
    "2019: OpenAI Five beats world champions\n",
    "2020: AlphaStar masters StarCraft II\n",
    "2022: ChatGPT uses RLHF\n",
    "2024: RL in robotics, autonomous vehicles, data centers\n",
    "2025: Widespread RL deployment in industry\n",
    "```\n",
    "\n",
    "**Key Insight:** RL has moved from research toy problems ‚Üí real-world impact!\n",
    "\n",
    "Let's explore advanced RL! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output, HTML\n",
    "import time\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make plots beautiful\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"Let's explore advanced RL applications! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-agent-rl",
   "metadata": {},
   "source": [
    "## ü§ù Multi-Agent Reinforcement Learning\n",
    "\n",
    "**Multi-Agent RL = Multiple agents learning simultaneously in shared environment**\n",
    "\n",
    "### Why Multi-Agent?\n",
    "\n",
    "**Single-agent problems:**\n",
    "- Agent vs static environment\n",
    "- Example: Single robot navigating maze\n",
    "\n",
    "**Multi-agent problems:**\n",
    "- Multiple agents interacting\n",
    "- Environment changes due to other agents!\n",
    "- Example: Self-driving cars in traffic, multi-player games\n",
    "\n",
    "### üéØ Types of Multi-Agent Settings:\n",
    "\n",
    "**1. Cooperative (All agents work together):**\n",
    "- **Goal:** Maximize team reward\n",
    "- **Examples:**\n",
    "  - Warehouse robots coordinating\n",
    "  - Soccer playing robots\n",
    "  - Drone swarms\n",
    "  - Traffic light coordination\n",
    "- **Challenge:** Credit assignment (which agent contributed?)\n",
    "\n",
    "**2. Competitive (Agents oppose each other):**\n",
    "- **Goal:** Beat opponent\n",
    "- **Examples:**\n",
    "  - Chess, Go, Poker\n",
    "  - 1v1 games\n",
    "  - Adversarial scenarios\n",
    "- **Challenge:** Non-stationary environment (opponent adapts!)\n",
    "\n",
    "**3. Mixed (Cooperation + Competition):**\n",
    "- **Goal:** Team vs team\n",
    "- **Examples:**\n",
    "  - Dota 2 (5v5), StarCraft\n",
    "  - Autonomous vehicle coordination\n",
    "  - Economic markets\n",
    "- **Challenge:** Both credit assignment AND non-stationarity\n",
    "\n",
    "### üéØ Key Challenges:\n",
    "\n",
    "**1. Non-Stationarity:**\n",
    "- Other agents learning ‚Üí environment changes\n",
    "- Breaks Markov assumption!\n",
    "- Solution: Model other agents, centralized training\n",
    "\n",
    "**2. Credit Assignment:**\n",
    "- Which agent caused team success/failure?\n",
    "- Global reward but local actions\n",
    "- Solution: Individual reward shaping, counterfactual reasoning\n",
    "\n",
    "**3. Scalability:**\n",
    "- Joint action space grows exponentially\n",
    "- Communication overhead\n",
    "- Solution: Decentralized execution, communication protocols\n",
    "\n",
    "### üåü Famous Multi-Agent RL Systems:\n",
    "\n",
    "**1. OpenAI Five (2019):**\n",
    "- 5 agents play Dota 2 (5v5 team game)\n",
    "- Beat world champion team OG\n",
    "- 10,000 years of gameplay per day!\n",
    "- Used PPO with team spirit reward\n",
    "\n",
    "**2. AlphaStar (2019):**\n",
    "- DeepMind's StarCraft II agent\n",
    "- League training (agents play each other)\n",
    "- Reached Grandmaster level\n",
    "\n",
    "**3. Google Data Centers (2016-present):**\n",
    "- Multiple cooling units coordinate\n",
    "- 40% energy reduction\n",
    "- Saved millions of dollars\n",
    "\n",
    "**4. Autonomous Vehicle Coordination:**\n",
    "- Multiple cars negotiate intersections\n",
    "- Platooning (convoy formation)\n",
    "- V2V (vehicle-to-vehicle) communication\n",
    "\n",
    "Let's implement a simple multi-agent system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-agent-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Multi-Agent Environment: Cooperative Treasure Hunt\n",
    "\n",
    "class MultiAgentTreasureHunt:\n",
    "    \"\"\"\n",
    "    Cooperative multi-agent environment\n",
    "    \n",
    "    - Multiple agents on a grid\n",
    "    - Goal: Collect all treasures\n",
    "    - Reward: Shared among all agents\n",
    "    - Challenge: Agents must coordinate!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=8, num_agents=2, num_treasures=3):\n",
    "        self.size = size\n",
    "        self.num_agents = num_agents\n",
    "        self.num_treasures = num_treasures\n",
    "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT', 'STAY']\n",
    "        self.action_effects = {\n",
    "            'UP': (-1, 0),\n",
    "            'DOWN': (1, 0),\n",
    "            'LEFT': (0, -1),\n",
    "            'RIGHT': (0, 1),\n",
    "            'STAY': (0, 0)\n",
    "        }\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        # Random agent positions\n",
    "        self.agent_positions = []\n",
    "        for _ in range(self.num_agents):\n",
    "            pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "            self.agent_positions.append(pos)\n",
    "        \n",
    "        # Random treasure positions\n",
    "        self.treasures = set()\n",
    "        while len(self.treasures) < self.num_treasures:\n",
    "            pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "            if pos not in self.agent_positions:\n",
    "                self.treasures.add(pos)\n",
    "        \n",
    "        self.collected = 0\n",
    "        self.steps = 0\n",
    "        return self._get_observations()\n",
    "    \n",
    "    def _get_observations(self):\n",
    "        \"\"\"Get observations for all agents\"\"\"\n",
    "        observations = []\n",
    "        for agent_pos in self.agent_positions:\n",
    "            # Simple observation: agent position + nearest treasure\n",
    "            if self.treasures:\n",
    "                nearest = min(self.treasures, \n",
    "                            key=lambda t: abs(t[0]-agent_pos[0]) + abs(t[1]-agent_pos[1]))\n",
    "                obs = [agent_pos[0], agent_pos[1], nearest[0], nearest[1]]\n",
    "            else:\n",
    "                obs = [agent_pos[0], agent_pos[1], -1, -1]\n",
    "            observations.append(np.array(obs))\n",
    "        return observations\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Take actions for all agents\n",
    "        \n",
    "        Args:\n",
    "            actions: List of action indices for each agent\n",
    "        \n",
    "        Returns:\n",
    "            observations, reward, done, info\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        \n",
    "        # Move agents\n",
    "        new_positions = []\n",
    "        for i, action_idx in enumerate(actions):\n",
    "            action = self.actions[action_idx]\n",
    "            delta = self.action_effects[action]\n",
    "            new_pos = (self.agent_positions[i][0] + delta[0],\n",
    "                      self.agent_positions[i][1] + delta[1])\n",
    "            \n",
    "            # Check bounds\n",
    "            if 0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size:\n",
    "                new_positions.append(new_pos)\n",
    "            else:\n",
    "                new_positions.append(self.agent_positions[i])  # Stay in bounds\n",
    "                reward -= 1  # Small penalty for hitting wall\n",
    "        \n",
    "        self.agent_positions = new_positions\n",
    "        \n",
    "        # Check treasure collection\n",
    "        for pos in self.agent_positions:\n",
    "            if pos in self.treasures:\n",
    "                self.treasures.remove(pos)\n",
    "                self.collected += 1\n",
    "                reward += 10  # Big reward for treasure!\n",
    "        \n",
    "        # Step penalty (encourages efficiency)\n",
    "        reward -= 0.1\n",
    "        \n",
    "        self.steps += 1\n",
    "        done = len(self.treasures) == 0 or self.steps >= 100\n",
    "        \n",
    "        observations = self._get_observations()\n",
    "        \n",
    "        info = {\n",
    "            'collected': self.collected,\n",
    "            'remaining': len(self.treasures)\n",
    "        }\n",
    "        \n",
    "        return observations, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Visualize environment\"\"\"\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        \n",
    "        # Mark treasures\n",
    "        for treasure in self.treasures:\n",
    "            grid[treasure] = 2\n",
    "        \n",
    "        # Mark agents\n",
    "        for i, agent_pos in enumerate(self.agent_positions):\n",
    "            grid[agent_pos] = 1 + i * 0.3  # Different colors for agents\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(grid, cmap='viridis', interpolation='nearest')\n",
    "        \n",
    "        # Add grid lines\n",
    "        for i in range(self.size + 1):\n",
    "            plt.axhline(i - 0.5, color='white', linewidth=1)\n",
    "            plt.axvline(i - 0.5, color='white', linewidth=1)\n",
    "        \n",
    "        # Add emoji\n",
    "        for i, agent_pos in enumerate(self.agent_positions):\n",
    "            plt.text(agent_pos[1], agent_pos[0], f'ü§ñ{i+1}', \n",
    "                    ha='center', va='center', fontsize=20)\n",
    "        \n",
    "        for treasure in self.treasures:\n",
    "            plt.text(treasure[1], treasure[0], 'üíé', \n",
    "                    ha='center', va='center', fontsize=20)\n",
    "        \n",
    "        plt.xlim(-0.5, self.size - 0.5)\n",
    "        plt.ylim(self.size - 0.5, -0.5)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(f'Multi-Agent Treasure Hunt\\nCollected: {self.collected}/{self.num_treasures}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test environment\n",
    "env = MultiAgentTreasureHunt(size=8, num_agents=2, num_treasures=3)\n",
    "\n",
    "print(\"‚úÖ Multi-Agent Environment Created!\")\n",
    "print(f\"\\nSetup:\")\n",
    "print(f\"  Grid size: {env.size}x{env.size}\")\n",
    "print(f\"  Agents: {env.num_agents}\")\n",
    "print(f\"  Treasures: {env.num_treasures}\")\n",
    "print(f\"  Actions: {env.actions}\")\n",
    "print(f\"\\nGoal: Agents cooperate to collect all treasures!\")\n",
    "print(f\"Reward: Shared among agents (+10 per treasure, -0.1 per step)\")\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alphago-alphazero",
   "metadata": {},
   "source": [
    "## üèÜ AlphaGo & AlphaZero: Mastering Games\n",
    "\n",
    "**AlphaGo (2016) = RL + Neural Networks + Tree Search**\n",
    "\n",
    "### The AlphaGo Revolution:\n",
    "\n",
    "**March 2016:** AlphaGo beats Lee Sedol 4-1\n",
    "- Lee Sedol: 18-time world champion, top 3 player\n",
    "- Go: More complex than Chess (10^170 possible games!)\n",
    "- \"Impossible\" task 10 years earlier\n",
    "- Shocked AI community and general public\n",
    "\n",
    "### üéØ Why Go Was Hard:\n",
    "\n",
    "**Comparison:**\n",
    "- **Chess:** ~10^120 possible games\n",
    "- **Go:** ~10^170 possible games (more than atoms in universe!)\n",
    "- **Board size:** Go = 19√ó19 = 361 positions\n",
    "- **Average game length:** ~150 moves (vs ~40 in chess)\n",
    "- **Branching factor:** ~250 legal moves per position (vs ~35 in chess)\n",
    "\n",
    "**Traditional approaches failed:**\n",
    "- Minimax: Too many positions to search\n",
    "- Evaluation: Hard to evaluate board position (who's winning?)\n",
    "- Human knowledge: Patterns too complex to encode\n",
    "\n",
    "### üéØ AlphaGo Architecture:\n",
    "\n",
    "**Three Key Components:**\n",
    "\n",
    "**1. Policy Network (œÄ):**\n",
    "- **Input:** Board position\n",
    "- **Output:** Probability for each move\n",
    "- **Training:** \n",
    "  - Supervised learning on human games (imitate experts)\n",
    "  - Reinforcement learning through self-play\n",
    "- **Purpose:** Suggest good moves (narrows search)\n",
    "\n",
    "**2. Value Network (V):**\n",
    "- **Input:** Board position\n",
    "- **Output:** Win probability (who's winning?)\n",
    "- **Training:** Self-play games\n",
    "- **Purpose:** Evaluate positions without playing to end\n",
    "\n",
    "**3. Monte Carlo Tree Search (MCTS):**\n",
    "- **Purpose:** Look ahead and plan\n",
    "- **Process:**\n",
    "  1. **Selection:** Navigate tree using UCB (Upper Confidence Bound)\n",
    "  2. **Expansion:** Add new node\n",
    "  3. **Simulation:** Play out using policy network\n",
    "  4. **Backup:** Update values\n",
    "- **Combines:** Neural networks (intuition) + search (planning)\n",
    "\n",
    "### üéØ Training Pipeline:\n",
    "\n",
    "```\n",
    "Phase 1: Supervised Learning\n",
    "  - Train policy network on 30M human expert games\n",
    "  - Learn to imitate human play\n",
    "  - Accuracy: 57% (predict expert move)\n",
    "\n",
    "Phase 2: Reinforcement Learning\n",
    "  - Policy network plays itself\n",
    "  - Update network to maximize win rate\n",
    "  - Self-play: Millions of games\n",
    "\n",
    "Phase 3: Value Network Training\n",
    "  - Learn to predict winner from position\n",
    "  - Train on self-play games\n",
    "\n",
    "Phase 4: MCTS Integration\n",
    "  - Combine all components\n",
    "  - Search ~10,000 positions per move\n",
    "```\n",
    "\n",
    "### üéØ AlphaGo Zero (2017): Even Better!\n",
    "\n",
    "**Revolutionary Changes:**\n",
    "- ‚ùå **No human data!** Pure self-play from scratch\n",
    "- ‚úÖ **Tabula rasa:** Start with random play\n",
    "- ‚úÖ **Single network:** Combined policy + value\n",
    "- ‚úÖ **Simpler:** No handcrafted features\n",
    "\n",
    "**Results:**\n",
    "- Trained in 3 days (vs weeks for AlphaGo)\n",
    "- Beat AlphaGo 100-0!\n",
    "- Discovered novel strategies (never seen in human play)\n",
    "\n",
    "**Key Insight:** Self-play RL > human knowledge!\n",
    "\n",
    "### üéØ AlphaZero (2017): Generalization\n",
    "\n",
    "**One algorithm, three games:**\n",
    "- **Chess:** Beat Stockfish (world's best chess engine)\n",
    "- **Shogi (Japanese chess):** Beat Elmo (champion program)\n",
    "- **Go:** Beat AlphaGo Zero\n",
    "\n",
    "**Training time:**\n",
    "- Chess: 4 hours (superhuman!)\n",
    "- Shogi: 2 hours\n",
    "- Go: 8 hours\n",
    "\n",
    "**Impact:**\n",
    "- Proved RL can master complex games from scratch\n",
    "- Inspired research in other domains\n",
    "- Showed power of self-play + search\n",
    "\n",
    "### üéØ Core Algorithm: Self-Play + MCTS\n",
    "\n",
    "```python\n",
    "# Simplified AlphaZero Training Loop\n",
    "\n",
    "1. Initialize neural network Œ∏ (random weights)\n",
    "\n",
    "2. For iteration = 1 to N:\n",
    "     \n",
    "     a. Self-Play (generate training data):\n",
    "        For game = 1 to M:\n",
    "            While not game_over:\n",
    "                # Use MCTS with neural network\n",
    "                action_probs = MCTS(state, network)\n",
    "                action = sample(action_probs)\n",
    "                state = take_action(state, action)\n",
    "                \n",
    "                # Store (state, action_probs, outcome)\n",
    "                replay_buffer.add(state, action_probs, None)\n",
    "            \n",
    "            # Backfill outcomes (who won?)\n",
    "            for experience in game:\n",
    "                experience.outcome = game_result\n",
    "     \n",
    "     b. Train Network:\n",
    "        For batch in replay_buffer:\n",
    "            # Policy loss: Match MCTS probabilities\n",
    "            # Value loss: Predict game outcome\n",
    "            loss = policy_loss + value_loss\n",
    "            update(Œ∏, loss)\n",
    "     \n",
    "     c. Evaluate:\n",
    "        Play new_network vs old_network\n",
    "        If win_rate > 55%:\n",
    "            Replace old_network with new_network\n",
    "```\n",
    "\n",
    "**Why MCTS + Neural Networks?**\n",
    "- **MCTS alone:** Too slow, needs millions of simulations\n",
    "- **Neural networks alone:** No lookahead, makes mistakes\n",
    "- **Combined:** Fast expert intuition + strategic planning!\n",
    "\n",
    "Let's implement a simple MCTS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-mcts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Monte Carlo Tree Search\n",
    "\n",
    "import math\n",
    "\n",
    "class MCTSNode:\n",
    "    \"\"\"\n",
    "    Node in Monte Carlo Tree Search\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.visits = 0\n",
    "        self.value = 0.0\n",
    "    \n",
    "    def is_fully_expanded(self, actions):\n",
    "        \"\"\"Check if all actions explored\"\"\"\n",
    "        return len(self.children) == len(actions)\n",
    "    \n",
    "    def best_child(self, c=1.41):\n",
    "        \"\"\"\n",
    "        Select best child using UCB1 (Upper Confidence Bound)\n",
    "        \n",
    "        UCB = value + c * sqrt(log(parent_visits) / visits)\n",
    "               Ô∏∏‚îÄ‚îÄ‚îÄÔ∏∏   Ô∏∏‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄÔ∏∏\n",
    "            Exploitation        Exploration\n",
    "        \"\"\"\n",
    "        best_score = -float('inf')\n",
    "        best_child = None\n",
    "        \n",
    "        for action, child in self.children.items():\n",
    "            if child.visits == 0:\n",
    "                ucb = float('inf')  # Explore unvisited nodes first\n",
    "            else:\n",
    "                # UCB1 formula\n",
    "                exploit = child.value / child.visits\n",
    "                explore = c * math.sqrt(math.log(self.visits) / child.visits)\n",
    "                ucb = exploit + explore\n",
    "            \n",
    "            if ucb > best_score:\n",
    "                best_score = ucb\n",
    "                best_child = child\n",
    "        \n",
    "        return best_child\n",
    "    \n",
    "    def update(self, value):\n",
    "        \"\"\"Update node statistics\"\"\"\n",
    "        self.visits += 1\n",
    "        self.value += value\n",
    "\n",
    "class SimpleMCTS:\n",
    "    \"\"\"\n",
    "    Simplified MCTS for demonstration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_simulations=100):\n",
    "        self.num_simulations = num_simulations\n",
    "    \n",
    "    def search(self, root_state, get_actions_fn, take_action_fn, is_terminal_fn, evaluate_fn):\n",
    "        \"\"\"\n",
    "        Run MCTS from root state\n",
    "        \n",
    "        Args:\n",
    "            root_state: Initial state\n",
    "            get_actions_fn: Function to get legal actions\n",
    "            take_action_fn: Function to take action and get next state\n",
    "            is_terminal_fn: Function to check if state is terminal\n",
    "            evaluate_fn: Function to evaluate terminal state\n",
    "        \n",
    "        Returns:\n",
    "            Best action\n",
    "        \"\"\"\n",
    "        root = MCTSNode(root_state)\n",
    "        \n",
    "        for _ in range(self.num_simulations):\n",
    "            node = root\n",
    "            state = root_state\n",
    "            path = [node]\n",
    "            \n",
    "            # 1. Selection: Navigate to leaf\n",
    "            while not is_terminal_fn(state) and node.is_fully_expanded(get_actions_fn(state)):\n",
    "                node = node.best_child()\n",
    "                path.append(node)\n",
    "                state = node.state\n",
    "            \n",
    "            # 2. Expansion: Add new child\n",
    "            if not is_terminal_fn(state):\n",
    "                actions = get_actions_fn(state)\n",
    "                untried_actions = [a for a in actions if a not in node.children]\n",
    "                \n",
    "                if untried_actions:\n",
    "                    action = random.choice(untried_actions)\n",
    "                    state = take_action_fn(state, action)\n",
    "                    child = MCTSNode(state, parent=node)\n",
    "                    node.children[action] = child\n",
    "                    node = child\n",
    "                    path.append(node)\n",
    "            \n",
    "            # 3. Simulation: Play out randomly\n",
    "            while not is_terminal_fn(state):\n",
    "                actions = get_actions_fn(state)\n",
    "                action = random.choice(actions)\n",
    "                state = take_action_fn(state, action)\n",
    "            \n",
    "            # 4. Backpropagation: Update values\n",
    "            value = evaluate_fn(state)\n",
    "            for node in path:\n",
    "                node.update(value)\n",
    "        \n",
    "        # Return most visited action (robust)\n",
    "        best_action = max(root.children.items(), \n",
    "                         key=lambda x: x[1].visits)[0]\n",
    "        return best_action\n",
    "\n",
    "print(\"‚úÖ MCTS Implemented!\")\n",
    "print(\"\\nüéØ MCTS Process:\")\n",
    "print(\"  1. Selection: Navigate tree using UCB\")\n",
    "print(\"  2. Expansion: Add new node\")\n",
    "print(\"  3. Simulation: Random playout\")\n",
    "print(\"  4. Backpropagation: Update statistics\")\n",
    "print(\"\\nüí° AlphaGo uses neural networks instead of random playout!\")\n",
    "print(\"   Policy network suggests moves, value network evaluates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rl-robotics",
   "metadata": {},
   "source": [
    "## ü§ñ RL for Robotics\n",
    "\n",
    "**Robotics = RL's real-world proving ground**\n",
    "\n",
    "### Why RL for Robotics?\n",
    "\n",
    "**Traditional robotics:**\n",
    "- Hand-engineered controllers\n",
    "- Requires exact models of physics\n",
    "- Brittle (breaks on unexpected situations)\n",
    "- Hard to generalize\n",
    "\n",
    "**RL robotics:**\n",
    "- Learn from trial and error\n",
    "- No physics model needed\n",
    "- Adapts to new situations\n",
    "- Generalizes across tasks\n",
    "\n",
    "### üéØ Challenges:\n",
    "\n",
    "**1. Sample Efficiency:**\n",
    "- Real robots are slow (100x slower than simulation)\n",
    "- Expensive (robot time costs money)\n",
    "- Dangerous (crashing costs $$$)\n",
    "- **Solution:** Sim-to-real transfer, model-based RL\n",
    "\n",
    "**2. Safety:**\n",
    "- Random exploration can damage robot\n",
    "- Unsafe actions can hurt humans\n",
    "- **Solution:** Safe RL, human demonstrations, constrained optimization\n",
    "\n",
    "**3. Continuous Control:**\n",
    "- Joint angles, velocities (continuous)\n",
    "- High-dimensional action space\n",
    "- **Solution:** Actor-critic methods (PPO, SAC, TD3)\n",
    "\n",
    "**4. Sim-to-Real Gap:**\n",
    "- Simulation ‚â† reality (physics, friction, noise)\n",
    "- Policy trained in sim fails on real robot\n",
    "- **Solution:** Domain randomization, dynamics model learning\n",
    "\n",
    "### üåü Success Stories (2024-2025):\n",
    "\n",
    "**1. Boston Dynamics (Atlas, Spot):**\n",
    "- **Atlas:** Humanoid robot does parkour, backflips\n",
    "- **Spot:** Quadruped navigates complex terrain\n",
    "- **Methods:** Model-predictive control + RL\n",
    "- **Training:** Sim-to-real with domain randomization\n",
    "\n",
    "**2. Berkeley Robot Learning:**\n",
    "- Robotic manipulation (grasping, assembly)\n",
    "- Learning from demonstrations + RL\n",
    "- Can fold laundry, open doors, sort objects\n",
    "\n",
    "**3. OpenAI Robotics (Dactyl):**\n",
    "- Robotic hand solves Rubik's cube\n",
    "- Trained entirely in simulation\n",
    "- Domain randomization for sim-to-real\n",
    "- **Key:** Randomize physics parameters in sim\n",
    "\n",
    "**4. Tesla Bot (Optimus):**\n",
    "- Humanoid robot for general tasks\n",
    "- Uses imitation learning + RL\n",
    "- Goal: Human-level dexterity\n",
    "\n",
    "**5. Warehouse Robotics:**\n",
    "- **Amazon:** Kiva robots (navigation + coordination)\n",
    "- **Ocado:** Swarm robotics in warehouses\n",
    "- **Multi-agent RL:** Coordinate thousands of robots\n",
    "\n",
    "### üéØ Common Robotics Tasks:\n",
    "\n",
    "**1. Manipulation:**\n",
    "- Grasping objects (pick and place)\n",
    "- Assembly tasks\n",
    "- Tool use\n",
    "- **Challenge:** Contact-rich, precise control\n",
    "- **Methods:** SAC, TD3, PPO\n",
    "\n",
    "**2. Locomotion:**\n",
    "- Walking (bipedal, quadrupedal)\n",
    "- Running, jumping\n",
    "- Navigating terrain\n",
    "- **Challenge:** Balance, coordination\n",
    "- **Methods:** PPO, TRPO, evolutionary algorithms\n",
    "\n",
    "**3. Navigation:**\n",
    "- Path planning\n",
    "- Obstacle avoidance\n",
    "- SLAM (Simultaneous Localization and Mapping)\n",
    "- **Challenge:** Partial observability, dynamics\n",
    "- **Methods:** DQN, A3C, PPO\n",
    "\n",
    "**4. Manipulation + Navigation:**\n",
    "- Mobile manipulation\n",
    "- Fetch and carry\n",
    "- **Challenge:** Hierarchical control\n",
    "- **Methods:** Hierarchical RL, options framework\n",
    "\n",
    "### üéØ Key Algorithms for Robotics:\n",
    "\n",
    "**1. PPO (Proximal Policy Optimization):**\n",
    "- Most popular for robotics\n",
    "- Stable, robust\n",
    "- On-policy (learns from current policy)\n",
    "- Used in: OpenAI Five, Boston Dynamics\n",
    "\n",
    "**2. SAC (Soft Actor-Critic):**\n",
    "- Off-policy (sample efficient)\n",
    "- Maximum entropy (encourages exploration)\n",
    "- State-of-the-art for continuous control\n",
    "- Used in: Robotic manipulation\n",
    "\n",
    "**3. TD3 (Twin Delayed DDPG):**\n",
    "- Improved version of DDPG\n",
    "- Addresses overestimation bias\n",
    "- Great for continuous control\n",
    "- Used in: Locomotion, manipulation\n",
    "\n",
    "### üéØ Sim-to-Real Transfer:\n",
    "\n",
    "**Problem:** Training on real robot is slow/expensive\n",
    "\n",
    "**Solution:** Train in simulation, transfer to real robot\n",
    "\n",
    "**Domain Randomization:**\n",
    "```python\n",
    "# Randomize simulation parameters\n",
    "for episode in training:\n",
    "    # Randomize physics\n",
    "    friction = random.uniform(0.5, 2.0)\n",
    "    mass = random.uniform(0.8, 1.2)\n",
    "    \n",
    "    # Randomize appearance\n",
    "    colors = random_colors()\n",
    "    textures = random_textures()\n",
    "    lighting = random_lighting()\n",
    "    \n",
    "    # Randomize sensing\n",
    "    sensor_noise = random_noise()\n",
    "    \n",
    "    # Train on this randomized environment\n",
    "    train_episode(env_with_randomization)\n",
    "```\n",
    "\n",
    "**Result:** Policy learns to be robust to variations ‚Üí works on real robot!\n",
    "\n",
    "**Success:** OpenAI's Dactyl (Rubik's cube), ANYmal quadruped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rl-optimization",
   "metadata": {},
   "source": [
    "## üéØ Real AI Example: Resource Optimization\n",
    "\n",
    "**RL for Real-World Optimization**\n",
    "\n",
    "### Google Data Center Cooling (2016)\n",
    "\n",
    "**Problem:**\n",
    "- Data centers consume massive energy\n",
    "- Cooling costs ~40% of total energy\n",
    "- Manual tuning is suboptimal\n",
    "\n",
    "**Solution: DeepMind's RL System**\n",
    "\n",
    "**Setup:**\n",
    "- **State:** Temperature sensors, pump speeds, weather\n",
    "- **Actions:** Adjust cooling system settings\n",
    "- **Reward:** -energy_consumption (minimize)\n",
    "- **Constraints:** Keep servers cool (< max temperature)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Sensors (120+ data points)\n",
    "    ‚Üì\n",
    "Deep Neural Network\n",
    "    ‚Üì\n",
    "Actions (cooling controls)\n",
    "    ‚Üì\n",
    "Safety Layer (human-in-loop)\n",
    "    ‚Üì\n",
    "Data Center\n",
    "```\n",
    "\n",
    "**Results:**\n",
    "- üí∞ **40% reduction in cooling energy**\n",
    "- üí∞ **15% reduction in total PUE (Power Usage Effectiveness)**\n",
    "- üí∞ **Millions of dollars saved per year**\n",
    "- üå± **Significant carbon footprint reduction**\n",
    "\n",
    "**Key Insight:** RL discovered patterns humans couldn't see!\n",
    "\n",
    "### Other Optimization Applications:\n",
    "\n",
    "**1. Traffic Light Control:**\n",
    "- Adaptive timing based on traffic flow\n",
    "- Multi-agent: Coordinate multiple intersections\n",
    "- **Result:** 20-30% reduction in wait times\n",
    "\n",
    "**2. Energy Grid Management:**\n",
    "- Balance supply and demand\n",
    "- Integrate renewable energy (solar, wind)\n",
    "- Demand response optimization\n",
    "- **Result:** Better renewable integration\n",
    "\n",
    "**3. Supply Chain Optimization:**\n",
    "- Inventory management\n",
    "- Warehouse robot coordination\n",
    "- Delivery route optimization\n",
    "- **Result:** Lower costs, faster delivery\n",
    "\n",
    "**4. Financial Trading:**\n",
    "- Portfolio optimization\n",
    "- Market making\n",
    "- Risk management\n",
    "- **Result:** Better risk-adjusted returns\n",
    "\n",
    "Let's implement a simple resource optimization problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resource-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Resource Optimization: Energy Management\n",
    "\n",
    "class EnergyManagementEnv:\n",
    "    \"\"\"\n",
    "    Simplified energy management environment\n",
    "    \n",
    "    Goal: Minimize energy cost while meeting demand\n",
    "    - Can use grid power (expensive) or battery (cheaper but limited)\n",
    "    - Solar power available (varies by time)\n",
    "    - Battery has charge/discharge limits\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_steps=24):\n",
    "        self.max_steps = max_steps  # 24 hours\n",
    "        self.battery_capacity = 100.0  # kWh\n",
    "        self.max_charge_rate = 10.0  # kW\n",
    "        self.max_discharge_rate = 10.0  # kW\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        self.step_count = 0\n",
    "        self.battery_level = 50.0  # Start at 50% charge\n",
    "        self.total_cost = 0.0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_demand(self, hour):\n",
    "        \"\"\"Energy demand varies by hour (kW)\"\"\"\n",
    "        # Simple pattern: higher during day\n",
    "        base = 20\n",
    "        variation = 15 * np.sin((hour - 6) * np.pi / 12)\n",
    "        return max(5, base + variation)\n",
    "    \n",
    "    def _get_solar(self, hour):\n",
    "        \"\"\"Solar generation varies by hour (kW)\"\"\"\n",
    "        # Solar only during day (6am - 6pm)\n",
    "        if 6 <= hour < 18:\n",
    "            return 15 * np.sin((hour - 6) * np.pi / 12)\n",
    "        return 0.0\n",
    "    \n",
    "    def _get_grid_price(self, hour):\n",
    "        \"\"\"Grid electricity price varies by hour ($/kWh)\"\"\"\n",
    "        # Peak pricing during day\n",
    "        if 9 <= hour < 21:  # Peak hours\n",
    "            return 0.30\n",
    "        else:  # Off-peak\n",
    "            return 0.10\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Get current state\"\"\"\n",
    "        hour = self.step_count % 24\n",
    "        demand = self._get_demand(hour)\n",
    "        solar = self._get_solar(hour)\n",
    "        price = self._get_grid_price(hour)\n",
    "        \n",
    "        return np.array([\n",
    "            self.battery_level / self.battery_capacity,  # Normalized battery level\n",
    "            hour / 24.0,  # Normalized hour\n",
    "            demand / 50.0,  # Normalized demand\n",
    "            solar / 20.0,  # Normalized solar\n",
    "            price / 0.30  # Normalized price\n",
    "        ])\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action: [battery_charge_rate]\n",
    "        Positive = charge, Negative = discharge\n",
    "        \n",
    "        Action space: -1 to 1 (scaled to max charge/discharge rate)\n",
    "        \"\"\"\n",
    "        hour = self.step_count % 24\n",
    "        demand = self._get_demand(hour)\n",
    "        solar = self._get_solar(hour)\n",
    "        price = self._get_grid_price(hour)\n",
    "        \n",
    "        # Scale action to charge/discharge rate\n",
    "        if action > 0:\n",
    "            battery_action = action * self.max_charge_rate  # Charge\n",
    "        else:\n",
    "            battery_action = action * self.max_discharge_rate  # Discharge\n",
    "        \n",
    "        # Clip battery action based on current level\n",
    "        if battery_action > 0:  # Charging\n",
    "            battery_action = min(battery_action, self.battery_capacity - self.battery_level)\n",
    "        else:  # Discharging\n",
    "            battery_action = max(battery_action, -self.battery_level)\n",
    "        \n",
    "        # Update battery\n",
    "        self.battery_level += battery_action\n",
    "        \n",
    "        # Calculate energy from sources\n",
    "        net_demand = demand - solar + battery_action  # Total demand after solar and battery\n",
    "        grid_power = max(0, net_demand)  # Grid makes up difference\n",
    "        \n",
    "        # Calculate cost\n",
    "        cost = grid_power * price\n",
    "        self.total_cost += cost\n",
    "        \n",
    "        # Penalty for not meeting demand (emergency)\n",
    "        if net_demand > grid_power:\n",
    "            cost += 100  # Large penalty\n",
    "        \n",
    "        # Reward is negative cost (minimize cost)\n",
    "        reward = -cost\n",
    "        \n",
    "        self.step_count += 1\n",
    "        done = self.step_count >= self.max_steps\n",
    "        \n",
    "        return self._get_state(), reward, done, {\n",
    "            'cost': cost,\n",
    "            'total_cost': self.total_cost,\n",
    "            'battery_level': self.battery_level,\n",
    "            'grid_power': grid_power\n",
    "        }\n",
    "\n",
    "# Test environment\n",
    "env = EnergyManagementEnv(max_steps=24)\n",
    "state = env.reset()\n",
    "\n",
    "print(\"‚úÖ Energy Management Environment Created!\")\n",
    "print(f\"\\nSetup:\")\n",
    "print(f\"  Battery capacity: {env.battery_capacity} kWh\")\n",
    "print(f\"  Max charge rate: {env.max_charge_rate} kW\")\n",
    "print(f\"  Time horizon: {env.max_steps} hours\")\n",
    "print(f\"\\nGoal: Minimize electricity cost!\")\n",
    "print(f\"  - Use solar when available (free!)\")\n",
    "print(f\"  - Charge battery during off-peak (cheap)\")\n",
    "print(f\"  - Discharge battery during peak (avoid expensive grid)\")\n",
    "\n",
    "# Visualize one day\n",
    "hours = np.arange(24)\n",
    "demand = [env._get_demand(h) for h in hours]\n",
    "solar = [env._get_solar(h) for h in hours]\n",
    "prices = [env._get_grid_price(h) for h in hours]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot power\n",
    "ax = axes[0]\n",
    "ax.plot(hours, demand, 'r-o', label='Demand', linewidth=2)\n",
    "ax.plot(hours, solar, 'y-s', label='Solar Generation', linewidth=2)\n",
    "ax.fill_between(hours, 0, solar, alpha=0.3, color='yellow')\n",
    "ax.set_xlabel('Hour of Day', fontsize=12)\n",
    "ax.set_ylabel('Power (kW)', fontsize=12)\n",
    "ax.set_title('‚ö° Energy Demand and Solar Generation', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot prices\n",
    "ax = axes[1]\n",
    "ax.plot(hours, prices, 'g-o', linewidth=2, markersize=8)\n",
    "ax.fill_between(hours, 0, prices, alpha=0.3, color='green')\n",
    "ax.set_xlabel('Hour of Day', fontsize=12)\n",
    "ax.set_ylabel('Price ($/kWh)', fontsize=12)\n",
    "ax.set_title('üí∞ Electricity Price (Peak vs Off-Peak)', fontsize=13, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° RL Strategy:\")\n",
    "print(\"  - Night (cheap): Charge battery\")\n",
    "print(\"  - Day (expensive): Use solar + discharge battery\")\n",
    "print(\"  - Minimize grid usage during peak hours!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercises\n",
    "\n",
    "Test your understanding of advanced RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### Exercise 1: Multi-Agent Credit Assignment\n",
    "\n",
    "**Scenario:** Two robots cooperate to move a heavy box. The box only moves if both push.\n",
    "\n",
    "**Problem:** They get reward +10 for success, 0 for failure. How to assign credit?\n",
    "\n",
    "**Question:** Why is credit assignment hard? What are possible solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for answer</summary>\n",
    "\n",
    "**Why Credit Assignment is Hard:**\n",
    "\n",
    "1. **Global reward, individual actions:**\n",
    "   - Both get same reward (+10)\n",
    "   - But one might have pushed harder!\n",
    "   - How to know who contributed more?\n",
    "\n",
    "2. **Necessary cooperation:**\n",
    "   - Box doesn't move unless BOTH push\n",
    "   - Individual actions seem useless alone\n",
    "   - Hard to learn without exploration\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "**1. Shaped Rewards:**\n",
    "   ```python\n",
    "   # Instead of just success reward:\n",
    "   reward = 10 * (movement_distance)\n",
    "   # Gives feedback even for partial success\n",
    "   ```\n",
    "\n",
    "**2. Difference Rewards:**\n",
    "   ```python\n",
    "   # Agent i's reward:\n",
    "   D_i = G(all agents) - G(all except i)\n",
    "   # Measures agent i's marginal contribution\n",
    "   ```\n",
    "\n",
    "**3. Centralized Training, Decentralized Execution (CTDE):**\n",
    "   - Training: Critic sees all agents' actions\n",
    "   - Execution: Each agent acts independently\n",
    "   - Used in OpenAI Five, QMIX\n",
    "\n",
    "**4. Communication:**\n",
    "   - Agents share information\n",
    "   - Learn communication protocol\n",
    "   - Example: \"I'm pushing now!\"\n",
    "\n",
    "**Real-World:** OpenAI Five uses team spirit reward (mix of individual and team rewards)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### Exercise 2: AlphaGo vs Traditional Game AI\n",
    "\n",
    "**Question:** Why couldn't traditional game AI (minimax, alpha-beta pruning) solve Go?\n",
    "\n",
    "**Compare to Chess, where these methods work well.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for answer</summary>\n",
    "\n",
    "**Why Traditional AI Failed at Go:**\n",
    "\n",
    "**1. Branching Factor:**\n",
    "   - **Chess:** ~35 legal moves per position\n",
    "   - **Go:** ~250 legal moves per position\n",
    "   - **Impact:** Search tree grows exponentially faster!\n",
    "\n",
    "**2. Game Length:**\n",
    "   - **Chess:** Average ~40 moves\n",
    "   - **Go:** Average ~150 moves\n",
    "   - **Impact:** Need to look much deeper\n",
    "\n",
    "**3. Position Evaluation:**\n",
    "   - **Chess:** Material count works (queen=9, rook=5, etc.)\n",
    "   - **Go:** No simple evaluation function!\n",
    "     - All stones equal value\n",
    "     - Territory hard to count mid-game\n",
    "     - Strategic patterns subtle\n",
    "   - **Impact:** Can't prune bad moves effectively\n",
    "\n",
    "**4. Search Space:**\n",
    "   ```\n",
    "   Chess: 10^120 possible games\n",
    "   Go:    10^170 possible games\n",
    "   \n",
    "   For comparison:\n",
    "   Atoms in observable universe: 10^80\n",
    "   ```\n",
    "\n",
    "**AlphaGo's Solutions:**\n",
    "\n",
    "1. **Policy Network:** \n",
    "   - Suggests plausible moves\n",
    "   - Reduces branching from 250 ‚Üí ~5-10 good moves\n",
    "\n",
    "2. **Value Network:**\n",
    "   - Evaluates position without playing to end\n",
    "   - Learned from millions of games\n",
    "\n",
    "3. **MCTS:**\n",
    "   - Focuses search on promising variations\n",
    "   - Balances exploration/exploitation\n",
    "\n",
    "4. **Deep Learning:**\n",
    "   - Learns patterns from data\n",
    "   - Generalizes to new positions\n",
    "\n",
    "**Key Insight:** Neural networks provide the \"intuition\" that humans use, making search tractable!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3",
   "metadata": {},
   "source": [
    "### Exercise 3: Real-World RL Challenges\n",
    "\n",
    "**Question:** You're deploying RL for a self-driving car. What are the main challenges compared to training in simulation?\n",
    "\n",
    "**Consider:** Safety, sample efficiency, robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for answer</summary>\n",
    "\n",
    "**Real-World RL Challenges for Self-Driving:**\n",
    "\n",
    "**1. Safety:**\n",
    "   - **Problem:** Random exploration can cause crashes\n",
    "   - **Solutions:**\n",
    "     - Start with human demonstrations (imitation learning)\n",
    "     - Safe exploration with constraints\n",
    "     - Human-in-the-loop (overseer can intervene)\n",
    "     - Gradual deployment (parking lots ‚Üí highways)\n",
    "\n",
    "**2. Sample Efficiency:**\n",
    "   - **Problem:** Real driving is slow, expensive\n",
    "     - 1 hour real driving vs 1000x faster in sim\n",
    "     - Cost of car, sensors, human operators\n",
    "   - **Solutions:**\n",
    "     - Train mostly in simulation\n",
    "     - Sim-to-real transfer with domain randomization\n",
    "     - Off-policy algorithms (reuse data)\n",
    "     - Model-based RL (learn world model)\n",
    "\n",
    "**3. Sim-to-Real Gap:**\n",
    "   - **Problem:** Simulation ‚â† reality\n",
    "     - Physics inaccuracies\n",
    "     - Sensor noise\n",
    "     - Weather, lighting variations\n",
    "     - Unpredictable human drivers\n",
    "   - **Solutions:**\n",
    "     - Domain randomization (vary sim parameters)\n",
    "     - Real-world fine-tuning\n",
    "     - Robust policies (uncertainty-aware)\n",
    "     - Hybrid approach (sim pre-training + real fine-tuning)\n",
    "\n",
    "**4. Non-Stationarity:**\n",
    "   - **Problem:** Environment changes\n",
    "     - Traffic patterns evolve\n",
    "     - Weather conditions vary\n",
    "     - Roads change (construction)\n",
    "   - **Solutions:**\n",
    "     - Continuous learning\n",
    "     - Adaptation mechanisms\n",
    "     - Ensemble policies\n",
    "\n",
    "**5. Multi-Agent Interactions:**\n",
    "   - **Problem:** Other drivers adapt\n",
    "     - Humans react to autonomous car\n",
    "     - Mixed autonomy scenarios\n",
    "   - **Solutions:**\n",
    "     - Model other agents\n",
    "     - Conservative policies\n",
    "     - Defensive driving\n",
    "\n",
    "**6. Reward Design:**\n",
    "   - **Problem:** What to optimize?\n",
    "     - Safety + efficiency + comfort + legality\n",
    "     - Hard to balance multiple objectives\n",
    "   - **Solutions:**\n",
    "     - Multi-objective RL\n",
    "     - Inverse RL (learn from humans)\n",
    "     - Careful reward shaping\n",
    "\n",
    "**7. Rare Events:**\n",
    "   - **Problem:** Critical situations rare\n",
    "     - Crashes happen 1 in millions of miles\n",
    "     - Hard to learn from rare events\n",
    "   - **Solutions:**\n",
    "     - Adversarial testing\n",
    "     - Scenario generation\n",
    "     - Transfer from similar situations\n",
    "\n",
    "**Industry Approach (Waymo, Tesla):**\n",
    "1. Massive simulation (billions of miles)\n",
    "2. Imitation learning from human drivers\n",
    "3. Gradual RL fine-tuning\n",
    "4. Extensive real-world testing\n",
    "5. Fleet learning (aggregate data from all cars)\n",
    "\n",
    "**Key Takeaway:** Real-world RL requires careful engineering beyond the core algorithm!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**You just learned:**\n",
    "\n",
    "### 1. **Multi-Agent RL**\n",
    "   - ‚úÖ Multiple agents in shared environment\n",
    "   - ‚úÖ Cooperative, competitive, mixed settings\n",
    "   - ‚úÖ Credit assignment challenge\n",
    "   - ‚úÖ Non-stationarity (agents adapt)\n",
    "   - **Used in:** OpenAI Five (Dota 2), warehouse robots, traffic systems\n",
    "\n",
    "### 2. **AlphaGo & AlphaZero**\n",
    "   - ‚úÖ Revolutionized game AI (beat world champions)\n",
    "   - ‚úÖ Combines: Neural networks + MCTS + self-play\n",
    "   - ‚úÖ Policy network (suggests moves) + Value network (evaluates)\n",
    "   - ‚úÖ AlphaZero: Tabula rasa learning (no human data!)\n",
    "   - **Impact:** Proved RL can surpass human knowledge\n",
    "\n",
    "### 3. **RL for Robotics**\n",
    "   - ‚úÖ Learn complex behaviors (walking, manipulation)\n",
    "   - ‚úÖ Sim-to-real transfer with domain randomization\n",
    "   - ‚úÖ Challenges: safety, sample efficiency\n",
    "   - ‚úÖ Algorithms: PPO, SAC, TD3\n",
    "   - **Used in:** Boston Dynamics, Tesla Bot, warehouse robots\n",
    "\n",
    "### 4. **Resource Optimization**\n",
    "   - ‚úÖ Google data centers: 40% energy savings\n",
    "   - ‚úÖ Traffic light coordination\n",
    "   - ‚úÖ Energy grid management\n",
    "   - ‚úÖ Supply chain optimization\n",
    "   - **Impact:** Millions of dollars saved, reduced carbon footprint\n",
    "\n",
    "### üåü Real-World Impact (2024-2025):\n",
    "\n",
    "**What You Can Build:**\n",
    "- ü§ñ **Multi-agent systems:** Warehouse robots, traffic coordination\n",
    "- üéÆ **Game AI:** Chess/Go agents with MCTS\n",
    "- üè≠ **Optimization:** Energy, supply chain, scheduling\n",
    "- ü§ñ **Robotics:** Navigation, manipulation (sim-to-real)\n",
    "- üí¨ **LLM fine-tuning:** RLHF like ChatGPT\n",
    "- üìä **Recommendation:** Personalization with RL\n",
    "\n",
    "**Modern Applications:**\n",
    "- **ChatGPT (2022):** RLHF with PPO\n",
    "- **AlphaFold 2 (2020):** Protein folding (Nobel Prize!)\n",
    "- **OpenAI Five (2019):** Beat Dota 2 champions\n",
    "- **AlphaStar (2019):** StarCraft II Grandmaster\n",
    "- **Waymo (2024):** Autonomous taxis in SF/Phoenix\n",
    "- **Boston Dynamics (2024):** Atlas parkour, Spot deployment\n",
    "\n",
    "### üìä RL Algorithm Selection Guide:\n",
    "\n",
    "| Application | Best Algorithm | Why? |\n",
    "|-------------|---------------|------|\n",
    "| **Atari games** | DQN, Rainbow | Discrete actions, image input |\n",
    "| **Robotics** | PPO, SAC, TD3 | Continuous control, stability |\n",
    "| **LLM fine-tuning** | PPO | Stable, large-scale |\n",
    "| **Multi-agent** | QMIX, MADDPG | Credit assignment |\n",
    "| **Board games** | AlphaZero (MCTS) | Perfect information, planning |\n",
    "| **Real-time strategy** | AlphaStar | Partial obs, multi-agent |\n",
    "| **Optimization** | PPO, SAC | General purpose |\n",
    "\n",
    "### üéØ RL Development Stack (2024):\n",
    "\n",
    "**Environments:**\n",
    "- **Gymnasium:** Standard RL environments (successor to OpenAI Gym)\n",
    "- **MuJoCo:** Physics simulation for robotics\n",
    "- **Unity ML-Agents:** 3D environments\n",
    "- **PettingZoo:** Multi-agent environments\n",
    "\n",
    "**Libraries:**\n",
    "- **Stable-Baselines3:** Pre-implemented algorithms (PPO, SAC, etc.)\n",
    "- **RLlib (Ray):** Scalable RL, distributed training\n",
    "- **CleanRL:** Clean, single-file implementations\n",
    "- **TensorFlow Agents:** Google's RL library\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now understand:\n",
    "- How AlphaGo beat the world champion\n",
    "- Multi-agent coordination in real systems\n",
    "- RL's impact on robotics and optimization\n",
    "- Real-world RL deployment challenges\n",
    "- Complete RL pipeline from basics to applications\n",
    "\n",
    "**You're ready to build real RL systems!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "**Practice Projects:**\n",
    "1. **Multi-Agent Game:**\n",
    "   - Implement multi-agent tag or hide-and-seek\n",
    "   - Try cooperative and competitive scenarios\n",
    "\n",
    "2. **MCTS for Chess/Connect Four:**\n",
    "   - Implement full MCTS with neural network evaluation\n",
    "   - Compare to minimax\n",
    "\n",
    "3. **Sim-to-Real:**\n",
    "   - Train robot in simulation (PyBullet/MuJoCo)\n",
    "   - Add domain randomization\n",
    "   - Test on real robot (if available)\n",
    "\n",
    "4. **Resource Optimization:**\n",
    "   - Extend energy management to full day/week\n",
    "   - Add more constraints (battery degradation)\n",
    "   - Try different RL algorithms\n",
    "\n",
    "5. **RLHF for LLMs:**\n",
    "   - Fine-tune small language model with PPO\n",
    "   - Implement reward model from human preferences\n",
    "   - Compare before/after fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Continue Learning:**\n",
    "\n",
    "**Advanced Topics:**\n",
    "- **Model-Based RL:** Learn world models, plan with them\n",
    "- **Offline RL:** Learn from fixed datasets (no exploration)\n",
    "- **Inverse RL:** Learn reward function from demonstrations\n",
    "- **Meta-RL:** Learn to learn (adapt quickly to new tasks)\n",
    "- **Hierarchical RL:** Learn skills and when to use them\n",
    "\n",
    "**Resources:**\n",
    "- **Courses:**\n",
    "  - David Silver's RL Course (DeepMind)\n",
    "  - CS285: Deep RL (UC Berkeley, Sergey Levine)\n",
    "  - Spinning Up in Deep RL (OpenAI)\n",
    "\n",
    "- **Books:**\n",
    "  - Sutton & Barto: \"RL: An Introduction\" (the bible)\n",
    "  - Graesser & Keng: \"Foundations of Deep RL\"\n",
    "\n",
    "- **Papers:**\n",
    "  - DQN (Mnih et al., 2015)\n",
    "  - AlphaGo (Silver et al., 2016)\n",
    "  - PPO (Schulman et al., 2017)\n",
    "  - AlphaZero (Silver et al., 2017)\n",
    "\n",
    "- **Code:**\n",
    "  ```bash\n",
    "  # Install RL ecosystem\n",
    "  pip install gymnasium\n",
    "  pip install stable-baselines3\n",
    "  pip install sb3-contrib\n",
    "  pip install pettingzoo  # Multi-agent\n",
    "  pip install mujoco  # Robotics sim\n",
    "  ```\n",
    "\n",
    "**Community:**\n",
    "- r/reinforcementlearning (Reddit)\n",
    "- Papers with Code (RL section)\n",
    "- OpenAI Gym leaderboards\n",
    "- Hugging Face RL course\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ You're Now an RL Practitioner!**\n",
    "\n",
    "You understand:\n",
    "- ‚úÖ RL fundamentals (MDPs, Q-learning, policy gradients)\n",
    "- ‚úÖ Deep RL (DQN, PPO, Actor-Critic)\n",
    "- ‚úÖ Advanced applications (multi-agent, games, robotics)\n",
    "- ‚úÖ Real-world deployment considerations\n",
    "\n",
    "**Ready to:**\n",
    "- Build game-playing AI\n",
    "- Optimize real-world systems\n",
    "- Train robots in simulation\n",
    "- Fine-tune language models\n",
    "- Contribute to cutting-edge RL research\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: RL is the closest we have to artificial general intelligence. Agents that learn from interaction can master any task - from games to robotics to optimizing our world!* üåü\n",
    "\n",
    "**üèÜ You now understand the AI behind AlphaGo, ChatGPT, and Boston Dynamics robots!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
