{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 2: Deep Reinforcement Learning\n",
    "\n",
    "**üéØ Goal:** Master Deep RL - combining neural networks with RL (like DeepMind's Atari agents!)\n",
    "\n",
    "**‚è±Ô∏è Time:** 90-120 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Deep Q-Networks (DQN) revolutionized RL in 2013 - first to play Atari at human level\n",
    "- Policy gradients power modern robotics (Boston Dynamics, Tesla robots)\n",
    "- Actor-Critic methods used in AlphaGo, OpenAI Five, and Dota 2 AI\n",
    "- Foundation of ChatGPT's RLHF (Proximal Policy Optimization)\n",
    "- Powers self-driving cars, drone control, and robotic manipulation\n",
    "- Enables learning in high-dimensional state spaces (images, sensor data)\n",
    "- Used in Google data centers, recommendation systems, and game AI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deep-rl-intro",
   "metadata": {},
   "source": [
    "## ü§î Why Deep Reinforcement Learning?\n",
    "\n",
    "**Problem with Tabular Q-Learning:**\n",
    "\n",
    "**Yesterday we learned Q-learning with tables:**\n",
    "- Grid world: 16 states √ó 4 actions = 64 Q-values ‚úÖ\n",
    "- Chess: ~10‚Å¥‚Å∞ states √ó moves ‚Üí IMPOSSIBLE to store! ‚ùå\n",
    "- Atari games: 210√ó160√ó3 pixels = 100,800 dimensions ‚Üí IMPOSSIBLE! ‚ùå\n",
    "\n",
    "**The Solution: Function Approximation**\n",
    "\n",
    "Instead of table Q(s, a), use a function approximator:\n",
    "```\n",
    "Q(s, a) ‚âà Q(s, a; Œ∏)   (neural network with parameters Œ∏)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Handle large/continuous state spaces\n",
    "- ‚úÖ Generalization: similar states ‚Üí similar Q-values\n",
    "- ‚úÖ Learn from raw pixels (end-to-end)\n",
    "- ‚úÖ Share knowledge across states\n",
    "\n",
    "### üéØ The Deep RL Revolution\n",
    "\n",
    "**Timeline:**\n",
    "- **2013:** DeepMind's DQN plays Atari games (Nature paper 2015)\n",
    "- **2015:** DQN beats human experts on 29/49 Atari games\n",
    "- **2016:** AlphaGo beats Lee Sedol using policy networks\n",
    "- **2017:** AlphaZero masters Chess/Shogi/Go from scratch\n",
    "- **2019:** OpenAI Five beats Dota 2 world champions (PPO)\n",
    "- **2022:** ChatGPT fine-tuned with PPO (RLHF)\n",
    "- **2024-2025:** Deep RL in robotics, autonomous vehicles, drug discovery\n",
    "\n",
    "**Key Innovations:**\n",
    "1. **DQN (2013):** CNN + Q-learning + experience replay\n",
    "2. **Policy Gradients:** Directly optimize policy (not Q-values)\n",
    "3. **Actor-Critic:** Combine value functions + policy gradients\n",
    "4. **PPO (2017):** Stable, robust, used in ChatGPT\n",
    "\n",
    "Let's build Deep RL from scratch! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Make plots beautiful\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"Let's build Deep RL agents! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dqn-theory",
   "metadata": {},
   "source": [
    "## üß† Deep Q-Networks (DQN)\n",
    "\n",
    "**DQN = Q-Learning + Deep Neural Networks**\n",
    "\n",
    "### Core Idea:\n",
    "\n",
    "**Replace Q-table with neural network:**\n",
    "```\n",
    "Old: Q-table[state][action] = value\n",
    "New: Q(state; Œ∏) ‚Üí [Q-value for each action]\n",
    "```\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (state)\n",
    "    ‚Üì\n",
    "Hidden Layer 1 (ReLU)\n",
    "    ‚Üì\n",
    "Hidden Layer 2 (ReLU)\n",
    "    ‚Üì\n",
    "Output Layer (Q-value for each action)\n",
    "```\n",
    "\n",
    "### üéØ Key Innovations of DQN:\n",
    "\n",
    "**1. Experience Replay:**\n",
    "- Store transitions (s, a, r, s') in replay buffer\n",
    "- Sample random mini-batches for training\n",
    "- Breaks correlation between consecutive samples\n",
    "- More sample efficient (reuse experiences)\n",
    "\n",
    "**Why it helps:**\n",
    "```\n",
    "Without replay: [exp1, exp2, exp3, ...] ‚Üí highly correlated\n",
    "With replay:    [exp47, exp2, exp95, ...] ‚Üí independent samples\n",
    "```\n",
    "\n",
    "**2. Target Network:**\n",
    "- Two networks: Q-network (online) and Target network (frozen)\n",
    "- Target network updated every C steps\n",
    "- Stabilizes training (moving target problem)\n",
    "\n",
    "**Loss Function:**\n",
    "```\n",
    "Loss = (r + Œ≥ * max_a' Q_target(s', a') - Q(s, a))¬≤\n",
    "       Ô∏∏‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅÔ∏∏\n",
    "                     TD Error\n",
    "```\n",
    "\n",
    "**3. CNN for Image Processing (Atari):**\n",
    "- Convolutional layers extract visual features\n",
    "- Input: 84√ó84√ó4 grayscale frames (4 = frame stacking)\n",
    "- Output: Q-value for each action\n",
    "\n",
    "### üéØ DQN Algorithm:\n",
    "\n",
    "```\n",
    "1. Initialize Q-network with random weights Œ∏\n",
    "2. Initialize target network with weights Œ∏‚Åª = Œ∏\n",
    "3. Initialize replay buffer D\n",
    "4. For each episode:\n",
    "     a. Observe initial state s\n",
    "     b. For each step:\n",
    "          i.   Choose action a using Œµ-greedy\n",
    "          ii.  Execute a, observe r, s'\n",
    "          iii. Store (s, a, r, s', done) in D\n",
    "          iv.  Sample random mini-batch from D\n",
    "          v.   Compute target: y = r + Œ≥*max_a' Q(s',a'; Œ∏‚Åª)\n",
    "          vi.  Update Œ∏ by minimizing (y - Q(s,a; Œ∏))¬≤\n",
    "          vii. Every C steps: Œ∏‚Åª ‚Üê Œ∏\n",
    "```\n",
    "\n",
    "Let's implement DQN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dqn-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-Network Architecture\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network\n",
    "    \n",
    "    Input: State representation\n",
    "    Output: Q-values for all actions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: State tensor (batch_size, state_size)\n",
    "        \n",
    "        Returns:\n",
    "            Q-values for all actions (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation on output\n",
    "        return x\n",
    "\n",
    "# Test the network\n",
    "state_size = 4  # Example: CartPole has 4 state variables\n",
    "action_size = 2  # Example: 2 actions (left, right)\n",
    "\n",
    "dqn = DQN(state_size, action_size).to(device)\n",
    "\n",
    "# Test forward pass\n",
    "dummy_state = torch.randn(1, state_size).to(device)\n",
    "q_values = dqn(dummy_state)\n",
    "\n",
    "print(\"‚úÖ DQN Network Created!\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Input size: {state_size}\")\n",
    "print(f\"  Hidden layers: 128 ‚Üí 128\")\n",
    "print(f\"  Output size: {action_size}\")\n",
    "print(f\"\\nNetwork:\")\n",
    "print(dqn)\n",
    "print(f\"\\nTest output shape: {q_values.shape}\")\n",
    "print(f\"Q-values: {q_values.detach().cpu().numpy()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "replay-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience Replay Buffer\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience replay buffer for DQN\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample random mini-batch\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Separate components\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.uint8)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Test replay buffer\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "\n",
    "# Add some experiences\n",
    "for i in range(100):\n",
    "    state = np.random.randn(4)\n",
    "    action = np.random.randint(2)\n",
    "    reward = np.random.randn()\n",
    "    next_state = np.random.randn(4)\n",
    "    done = False\n",
    "    buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(\"‚úÖ Replay Buffer Implemented!\")\n",
    "print(f\"\\nBuffer size: {len(buffer)}\")\n",
    "print(f\"Capacity: {buffer.buffer.maxlen}\")\n",
    "\n",
    "# Sample a mini-batch\n",
    "if len(buffer) >= 32:\n",
    "    states, actions, rewards, next_states, dones = buffer.sample(32)\n",
    "    print(f\"\\nSampled mini-batch:\")\n",
    "    print(f\"  States shape: {states.shape}\")\n",
    "    print(f\"  Actions shape: {actions.shape}\")\n",
    "    print(f\"  Rewards shape: {rewards.shape}\")\n",
    "    print(f\"\\nüí° Random sampling breaks temporal correlation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dqn-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Complete DQN agent with experience replay and target network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99, \n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,\n",
    "                 buffer_size=10000, batch_size=64, target_update=10):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.update_counter = 0\n",
    "        \n",
    "        # Q-network and target network\n",
    "        self.q_network = DQN(state_size, action_size).to(device)\n",
    "        self.target_network = DQN(state_size, action_size).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()  # Target network in eval mode\n",
    "        \n",
    "        # Optimizer and replay buffer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"Choose action using Œµ-greedy policy\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        \n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train on mini-batch from replay buffer\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample mini-batch\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Target Q-values (use target network)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"‚úÖ DQN Agent Implemented!\")\n",
    "print(\"\\nüéØ Key Components:\")\n",
    "print(\"  1. Q-Network: Approximates Q(s,a)\")\n",
    "print(\"  2. Target Network: Stabilizes training\")\n",
    "print(\"  3. Replay Buffer: Breaks correlation\")\n",
    "print(\"  4. Œµ-greedy: Exploration/exploitation\")\n",
    "print(\"\\nüí° This is the algorithm that mastered Atari games!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cartpole-env",
   "metadata": {},
   "source": [
    "## üéÆ Real AI Example: CartPole with DQN\n",
    "\n",
    "**Task:** Balance a pole on a moving cart\n",
    "\n",
    "**CartPole Environment:**\n",
    "- **State:** [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "- **Actions:** 0 = push left, 1 = push right\n",
    "- **Reward:** +1 for each timestep the pole stays upright\n",
    "- **Done:** Pole falls beyond ¬±12¬∞ or cart moves beyond ¬±2.4 units\n",
    "- **Goal:** Keep pole balanced for 200+ steps\n",
    "\n",
    "**Why CartPole?**\n",
    "- Classic control problem\n",
    "- Simple but non-trivial\n",
    "- Tests continuous state handling\n",
    "- Fast training (see results in minutes!)\n",
    "\n",
    "**Real-World Analogy:**\n",
    "- Balancing humanoid robots (Boston Dynamics)\n",
    "- Drone stabilization\n",
    "- Inverted pendulum control\n",
    "\n",
    "Let's train DQN on CartPole!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cartpole-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CartPole environment (gym-like interface)\n",
    "\n",
    "class CartPoleEnv:\n",
    "    \"\"\"\n",
    "    Simplified CartPole environment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.length = 0.5\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        \n",
    "        # Thresholds\n",
    "        self.theta_threshold = 12 * 2 * np.pi / 360  # ¬±12 degrees\n",
    "        self.x_threshold = 2.4\n",
    "        \n",
    "        self.state = None\n",
    "        self.steps = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        self.state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps = 0\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take action\"\"\"\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        \n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = np.cos(theta)\n",
    "        sintheta = np.sin(theta)\n",
    "        \n",
    "        # Physics simulation\n",
    "        temp = (force + self.masspole * self.length * theta_dot**2 * sintheta) / (self.masscart + self.masspole)\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / \\\n",
    "                   (self.length * (4.0/3.0 - self.masspole * costheta**2 / (self.masscart + self.masspole)))\n",
    "        xacc = temp - self.masspole * self.length * thetaacc * costheta / (self.masscart + self.masspole)\n",
    "        \n",
    "        # Update state\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "        \n",
    "        self.state = np.array([x, x_dot, theta, theta_dot])\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Check termination\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold\n",
    "            or theta > self.theta_threshold\n",
    "            or self.steps >= 500\n",
    "        )\n",
    "        \n",
    "        reward = 1.0 if not done else 0.0\n",
    "        \n",
    "        return self.state.copy(), reward, done\n",
    "\n",
    "# Test environment\n",
    "env = CartPoleEnv()\n",
    "state = env.reset()\n",
    "\n",
    "print(\"‚úÖ CartPole Environment Created!\")\n",
    "print(f\"\\nState: {state}\")\n",
    "print(f\"  [cart_pos, cart_vel, pole_angle, pole_angular_vel]\")\n",
    "print(f\"\\nActions: 0 = Left, 1 = Right\")\n",
    "print(f\"Goal: Balance pole for 200+ steps\")\n",
    "\n",
    "# Test random policy\n",
    "total_reward = 0\n",
    "for _ in range(100):\n",
    "    action = np.random.randint(2)\n",
    "    state, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"\\nüé≤ Random policy reward: {total_reward}\")\n",
    "print(f\"üí° DQN should achieve 200+ reward!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-dqn-cartpole",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DQN on CartPole\n",
    "\n",
    "def train_dqn(env, agent, num_episodes=300):\n",
    "    \"\"\"\n",
    "    Train DQN agent on CartPole\n",
    "    \"\"\"\n",
    "    rewards_history = []\n",
    "    losses_history = []\n",
    "    epsilon_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_losses = []\n",
    "        \n",
    "        for step in range(500):\n",
    "            # Choose action\n",
    "            action = agent.get_action(state, training=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Store in replay buffer\n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train\n",
    "            loss = agent.train()\n",
    "            if loss is not None:\n",
    "                episode_losses.append(loss)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Record metrics\n",
    "        rewards_history.append(episode_reward)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        if episode_losses:\n",
    "            losses_history.append(np.mean(episode_losses))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-50:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} - Avg Reward: {avg_reward:.2f}, Œµ: {agent.epsilon:.3f}\")\n",
    "            \n",
    "            if avg_reward >= 195:\n",
    "                print(f\"\\nüéâ Solved! Average reward {avg_reward:.2f} >= 195\")\n",
    "                break\n",
    "    \n",
    "    return rewards_history, losses_history, epsilon_history\n",
    "\n",
    "# Create agent and train\n",
    "env = CartPoleEnv()\n",
    "agent = DQNAgent(\n",
    "    state_size=4,\n",
    "    action_size=2,\n",
    "    learning_rate=0.001,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01,\n",
    "    buffer_size=10000,\n",
    "    batch_size=64,\n",
    "    target_update=10\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training DQN on CartPole...\\n\")\n",
    "rewards, losses, epsilons = train_dqn(env, agent, num_episodes=300)\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-dqn-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DQN training\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Episode rewards\n",
    "ax = axes[0, 0]\n",
    "ax.plot(rewards, alpha=0.3, color='blue', label='Raw')\n",
    "window = 20\n",
    "if len(rewards) >= window:\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), moving_avg, color='red', linewidth=2, label=f'{window}-episode avg')\n",
    "ax.axhline(195, color='green', linestyle='--', linewidth=2, label='Solved threshold')\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Total Reward', fontsize=12)\n",
    "ax.set_title('üìà DQN Learning Progress', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Training loss\n",
    "ax = axes[0, 1]\n",
    "if losses:\n",
    "    ax.plot(losses, color='orange', alpha=0.5)\n",
    "    if len(losses) >= 20:\n",
    "        loss_smooth = np.convolve(losses, np.ones(20)/20, mode='valid')\n",
    "        ax.plot(range(19, len(losses)), loss_smooth, color='red', linewidth=2, label='Smoothed')\n",
    "ax.set_xlabel('Training Step', fontsize=12)\n",
    "ax.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "ax.set_title('üìâ Training Loss', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Epsilon decay\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epsilons, color='purple', linewidth=2)\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Epsilon (Œµ)', fontsize=12)\n",
    "ax.set_title('üîç Exploration Rate Decay', fontsize=13, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Performance distribution\n",
    "ax = axes[1, 1]\n",
    "if len(rewards) >= 50:\n",
    "    final_rewards = rewards[-50:]\n",
    "    ax.hist(final_rewards, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(np.mean(final_rewards), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(final_rewards):.1f}')\n",
    "    ax.axvline(195, color='green', linestyle='--', linewidth=2, label='Solved: 195')\n",
    "ax.set_xlabel('Total Reward', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('üìä Final 50 Episodes Performance', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Training Summary:\")\n",
    "print(f\"  Total episodes: {len(rewards)}\")\n",
    "print(f\"  Final 10 episodes avg: {np.mean(rewards[-10:]):.2f}\")\n",
    "print(f\"  Best episode: {max(rewards):.0f}\")\n",
    "if np.mean(rewards[-50:]) >= 195:\n",
    "    print(f\"\\nüéâ Problem SOLVED! Agent consistently balances pole!\")\n",
    "else:\n",
    "    print(f\"\\nüí° Agent learning but needs more training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "policy-gradients",
   "metadata": {},
   "source": [
    "## üéØ Policy Gradients\n",
    "\n",
    "**Policy Gradients = Directly optimize the policy (not Q-values)**\n",
    "\n",
    "### Key Difference:\n",
    "\n",
    "**Value-based (DQN):**\n",
    "```\n",
    "Learn Q(s,a) ‚Üí Derive policy: œÄ(s) = argmax_a Q(s,a)\n",
    "```\n",
    "\n",
    "**Policy-based (Policy Gradient):**\n",
    "```\n",
    "Directly learn policy: œÄ(a|s; Œ∏) = probability of action a in state s\n",
    "```\n",
    "\n",
    "### üéØ Why Policy Gradients?\n",
    "\n",
    "**Advantages:**\n",
    "1. **Continuous actions:** DQN struggles, PG excels\n",
    "   - Example: Robot arm angles, steering wheel position\n",
    "2. **Stochastic policies:** PG naturally outputs probabilities\n",
    "   - Example: Rock-paper-scissors (need randomness!)\n",
    "3. **Better convergence:** In some problems (e.g., robotics)\n",
    "4. **Simplicity:** No need for Q-function approximation\n",
    "\n",
    "**Disadvantages:**\n",
    "1. **Sample inefficient:** Needs many episodes\n",
    "2. **High variance:** Gradients can be noisy\n",
    "3. **Local optima:** Can get stuck\n",
    "\n",
    "### üéØ REINFORCE Algorithm:\n",
    "\n",
    "**Core Idea:** Increase probability of good actions, decrease bad ones\n",
    "\n",
    "**Objective:**\n",
    "```\n",
    "J(Œ∏) = E[Œ£ r_t]  (expected total reward)\n",
    "```\n",
    "\n",
    "**Policy Gradient Theorem:**\n",
    "```\n",
    "‚àáJ(Œ∏) = E[‚àá log œÄ(a|s; Œ∏) * G_t]\n",
    "```\n",
    "\n",
    "**In words:**\n",
    "- If action led to high return G_t ‚Üí increase its probability\n",
    "- If action led to low return ‚Üí decrease its probability\n",
    "\n",
    "**Update Rule:**\n",
    "```\n",
    "Œ∏ ‚Üê Œ∏ + Œ± * ‚àá log œÄ(a_t|s_t; Œ∏) * G_t\n",
    "```\n",
    "\n",
    "### üéØ REINFORCE Algorithm:\n",
    "\n",
    "```\n",
    "1. Initialize policy network œÄ(a|s; Œ∏)\n",
    "2. For each episode:\n",
    "     a. Generate episode using œÄ: (s_0,a_0,r_1), (s_1,a_1,r_2), ...\n",
    "     b. For each step t:\n",
    "          i.  Calculate return: G_t = Œ£_{k=t}^T Œ≥^(k-t) * r_k\n",
    "          ii. Update: Œ∏ ‚Üê Œ∏ + Œ± * ‚àá log œÄ(a_t|s_t; Œ∏) * G_t\n",
    "```\n",
    "\n",
    "**Key Insight:** This is Monte Carlo - wait until episode ends, then update!\n",
    "\n",
    "Let's implement REINFORCE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "policy-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Network\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy network that outputs action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Returns:\n",
    "            Action probabilities (softmax output)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=-1)  # Convert to probabilities\n",
    "\n",
    "# Test policy network\n",
    "policy_net = PolicyNetwork(state_size=4, action_size=2).to(device)\n",
    "\n",
    "dummy_state = torch.randn(1, 4).to(device)\n",
    "action_probs = policy_net(dummy_state)\n",
    "\n",
    "print(\"‚úÖ Policy Network Created!\")\n",
    "print(f\"\\nOutput: Action probabilities (sum to 1.0)\")\n",
    "print(f\"Action probs: {action_probs.detach().cpu().numpy()[0]}\")\n",
    "print(f\"Sum: {action_probs.sum().item():.4f}\")\n",
    "print(f\"\\nüí° Stochastic policy: sample from this distribution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reinforce-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE Agent\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"\n",
    "    REINFORCE policy gradient agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy = PolicyNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Episode memory\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Sample action from policy\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action_probs = self.policy(state)\n",
    "        \n",
    "        # Sample action from probability distribution\n",
    "        action_dist = torch.distributions.Categorical(action_probs)\n",
    "        action = action_dist.sample()\n",
    "        \n",
    "        # Store log probability for training\n",
    "        self.log_probs.append(action_dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def train_episode(self):\n",
    "        \"\"\"\n",
    "        Train on collected episode\n",
    "        \"\"\"\n",
    "        # Calculate returns (discounted cumulative rewards)\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        # Normalize returns (reduces variance)\n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        if len(returns) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)  # Negative for gradient ascent\n",
    "        \n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode memory\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return policy_loss.item()\n",
    "\n",
    "print(\"‚úÖ REINFORCE Agent Implemented!\")\n",
    "print(\"\\nüéØ Key differences from DQN:\")\n",
    "print(\"  1. Outputs action probabilities (not Q-values)\")\n",
    "print(\"  2. Samples actions stochastically\")\n",
    "print(\"  3. Updates after full episode (Monte Carlo)\")\n",
    "print(\"  4. No replay buffer or target network\")\n",
    "print(\"\\nüí° Used in robotics with continuous actions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actor-critic",
   "metadata": {},
   "source": [
    "## üé≠ Actor-Critic Methods\n",
    "\n",
    "**Actor-Critic = Combine value-based + policy-based approaches**\n",
    "\n",
    "### The Best of Both Worlds:\n",
    "\n",
    "**Two networks:**\n",
    "1. **Actor (Policy):** Chooses actions œÄ(a|s; Œ∏)\n",
    "2. **Critic (Value):** Evaluates actions V(s; w)\n",
    "\n",
    "**Why both?**\n",
    "- **Policy gradients:** High variance, slow learning\n",
    "- **Value functions:** Lower variance estimates\n",
    "- **Combination:** Actor learns policy, Critic reduces variance!\n",
    "\n",
    "### üéØ How It Works:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  State  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "     ‚îÇ\n",
    "     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ        ‚îÇ\n",
    "     ‚Üì        ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Actor  ‚îÇ ‚îÇ Critic ‚îÇ\n",
    "‚îÇ(Policy)‚îÇ ‚îÇ(Value) ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îÇ          ‚îÇ\n",
    "    ‚Üì          ‚Üì\n",
    "  Action    Advantage\n",
    "             (TD Error)\n",
    "```\n",
    "\n",
    "**Update Process:**\n",
    "1. **Actor** takes action a\n",
    "2. **Critic** evaluates: Œ¥ = r + Œ≥V(s') - V(s)  (TD error)\n",
    "3. **Actor** updates: Œ∏ ‚Üê Œ∏ + Œ± * Œ¥ * ‚àá log œÄ(a|s; Œ∏)\n",
    "4. **Critic** updates: w ‚Üê w + Œ≤ * Œ¥ * ‚àáV(s; w)\n",
    "\n",
    "**Key Insight:** TD error Œ¥ acts as \"advantage\" - how much better action was than expected!\n",
    "\n",
    "### üéØ Advantages:\n",
    "\n",
    "**vs DQN:**\n",
    "- ‚úÖ Works with continuous actions\n",
    "- ‚úÖ More stable than pure policy gradients\n",
    "- ‚úÖ Can learn online (no replay buffer needed)\n",
    "\n",
    "**vs REINFORCE:**\n",
    "- ‚úÖ Lower variance (critic provides baseline)\n",
    "- ‚úÖ Can update after each step (not just episodes)\n",
    "- ‚úÖ Faster learning\n",
    "\n",
    "### üåü Famous Actor-Critic Algorithms:\n",
    "\n",
    "1. **A3C (2016):** Asynchronous Advantage Actor-Critic\n",
    "2. **A2C:** Synchronous version of A3C\n",
    "3. **PPO (2017):** Proximal Policy Optimization (ChatGPT!)\n",
    "4. **SAC (2018):** Soft Actor-Critic (robotics)\n",
    "5. **TD3 (2018):** Twin Delayed DDPG (continuous control)\n",
    "\n",
    "**PPO is THE algorithm for:**\n",
    "- ChatGPT/GPT-4 fine-tuning (RLHF)\n",
    "- OpenAI Five (Dota 2)\n",
    "- Many robotics applications\n",
    "\n",
    "Let's implement a simple Actor-Critic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actor-critic-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic Agent\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Actor-Critic network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Actor head (policy)\n",
    "        self.actor = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        # Critic head (value)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Returns:\n",
    "            action_probs: Policy output\n",
    "            state_value: Value estimate\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Actor output\n",
    "        action_probs = F.softmax(self.actor(x), dim=-1)\n",
    "        \n",
    "        # Critic output\n",
    "        state_value = self.critic(x)\n",
    "        \n",
    "        return action_probs, state_value\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    \"\"\"\n",
    "    Actor-Critic agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Combined network\n",
    "        self.model = ActorCritic(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Episode memory\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Sample action from policy and get value estimate\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action_probs, state_value = self.model(state)\n",
    "        \n",
    "        # Sample action\n",
    "        action_dist = torch.distributions.Categorical(action_probs)\n",
    "        action = action_dist.sample()\n",
    "        \n",
    "        # Store for training\n",
    "        self.log_probs.append(action_dist.log_prob(action))\n",
    "        self.values.append(state_value)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def train_episode(self):\n",
    "        \"\"\"\n",
    "        Train on collected episode using advantage\n",
    "        \"\"\"\n",
    "        # Calculate returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        \n",
    "        # Calculate advantages\n",
    "        values = torch.cat(self.values)\n",
    "        advantages = returns - values.detach().squeeze()\n",
    "        \n",
    "        # Actor loss (policy gradient with advantage)\n",
    "        actor_loss = []\n",
    "        for log_prob, advantage in zip(self.log_probs, advantages):\n",
    "            actor_loss.append(-log_prob * advantage)\n",
    "        actor_loss = torch.stack(actor_loss).sum()\n",
    "        \n",
    "        # Critic loss (MSE between value and return)\n",
    "        critic_loss = F.mse_loss(values.squeeze(), returns)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = actor_loss + critic_loss\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear memory\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "print(\"‚úÖ Actor-Critic Agent Implemented!\")\n",
    "print(\"\\nüé≠ Architecture:\")\n",
    "print(\"  Actor: Learns policy œÄ(a|s)\")\n",
    "print(\"  Critic: Learns value V(s)\")\n",
    "print(\"  Advantage: A = G - V(s) (how much better than expected)\")\n",
    "print(\"\\nüí° This is the foundation of AlphaGo and PPO (ChatGPT)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercises\n",
    "\n",
    "Test your understanding of Deep RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### Exercise 1: DQN vs Tabular Q-Learning\n",
    "\n",
    "**Question:** Why can't we use tabular Q-learning for Atari games?\n",
    "\n",
    "**Consider:**\n",
    "- State space size\n",
    "- Memory requirements\n",
    "- Generalization needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for answer</summary>\n",
    "\n",
    "**Why DQN is necessary for Atari:**\n",
    "\n",
    "1. **Massive state space:**\n",
    "   - Atari screen: 210√ó160√ó3 RGB pixels\n",
    "   - Each pixel: 0-255 (256 values)\n",
    "   - Total states: 256^(210√ó160√ó3) ‚âà 10^120,000\n",
    "   - Universe atoms: ~10^80 (impossibly large!)\n",
    "\n",
    "2. **Memory requirements:**\n",
    "   - Table: Need to store Q-value for each state-action\n",
    "   - With 18 actions: 18 √ó 10^120,000 values\n",
    "   - Even 1 byte per value ‚Üí impossible to store!\n",
    "\n",
    "3. **Never see same state twice:**\n",
    "   - Pixel-perfect states rarely repeat\n",
    "   - Tabular: No generalization\n",
    "   - DQN: Similar screens ‚Üí similar Q-values\n",
    "\n",
    "4. **CNN advantages:**\n",
    "   - Learns visual features (edges, objects)\n",
    "   - Generalizes across similar situations\n",
    "   - Compact representation (millions of parameters vs infinite table)\n",
    "\n",
    "**Key Insight:** Function approximation (neural networks) is essential for high-dimensional state spaces!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### Exercise 2: Experience Replay Benefits\n",
    "\n",
    "**Task:** Explain why experience replay improves DQN training\n",
    "\n",
    "**Hint:** Think about:\n",
    "- Correlation between consecutive samples\n",
    "- Data efficiency\n",
    "- Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for answer</summary>\n",
    "\n",
    "**Why Experience Replay Works:**\n",
    "\n",
    "1. **Breaks temporal correlation:**\n",
    "   ```\n",
    "   Without replay:\n",
    "   [s1‚Üís2‚Üís3‚Üís4]  (highly correlated)\n",
    "   Network overfits to recent trajectory!\n",
    "   \n",
    "   With replay:\n",
    "   [s47, s2, s95, s12]  (random samples)\n",
    "   Independent samples ‚Üí better generalization\n",
    "   ```\n",
    "\n",
    "2. **Data efficiency:**\n",
    "   - Each experience used multiple times\n",
    "   - Sampled in different mini-batches\n",
    "   - Better use of expensive interactions\n",
    "\n",
    "3. **Stability:**\n",
    "   - Reduces variance in updates\n",
    "   - Smooths out noisy gradients\n",
    "   - Prevents catastrophic forgetting\n",
    "\n",
    "4. **Batch learning:**\n",
    "   - Mini-batch gradient descent more stable than online\n",
    "   - Better hardware utilization (GPU)\n",
    "\n",
    "**Real Impact:** DQN without replay fails to learn Atari games!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3",
   "metadata": {},
   "source": [
    "### Exercise 3: When to Use Which Algorithm?\n",
    "\n",
    "**Scenario:** You're choosing an RL algorithm for these problems. Which would you use?\n",
    "\n",
    "1. **Atari Pong:** Discrete actions, image observations\n",
    "2. **Robot arm control:** Continuous joint angles\n",
    "3. **Poker bot:** Needs randomness in strategy\n",
    "4. **Self-driving car:** High-dimensional continuous control\n",
    "\n",
    "**Choices:** DQN, Policy Gradients, Actor-Critic (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for answer</summary>\n",
    "\n",
    "**Algorithm Selection Guide:**\n",
    "\n",
    "1. **Atari Pong ‚Üí DQN**\n",
    "   - Discrete actions (up/down)\n",
    "   - Image processing (CNN)\n",
    "   - DQN proven to work well\n",
    "   - Sample efficient\n",
    "\n",
    "2. **Robot arm control ‚Üí Actor-Critic (SAC/TD3)**\n",
    "   - Continuous actions (joint angles)\n",
    "   - DQN can't handle continuous actions directly\n",
    "   - SAC/TD3 designed for robotics\n",
    "   - Stable learning\n",
    "\n",
    "3. **Poker bot ‚Üí Policy Gradients**\n",
    "   - Needs stochastic policy (randomness)\n",
    "   - DQN is deterministic (after training)\n",
    "   - Mixed strategy required (like rock-paper-scissors)\n",
    "   - Policy gradient naturally stochastic\n",
    "\n",
    "4. **Self-driving car ‚Üí Actor-Critic (PPO)**\n",
    "   - Continuous control (steering, acceleration)\n",
    "   - High-dimensional observations\n",
    "   - PPO: stable, robust, proven\n",
    "   - Used by Waymo, Tesla (rumored)\n",
    "\n",
    "**General Rules:**\n",
    "- Discrete actions + images ‚Üí **DQN**\n",
    "- Continuous actions ‚Üí **Actor-Critic (PPO/SAC/TD3)**\n",
    "- Need stochastic policy ‚Üí **Policy Gradients**\n",
    "- Sample efficiency matters ‚Üí **DQN**\n",
    "- Stability crucial ‚Üí **PPO**\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**You just learned:**\n",
    "\n",
    "### 1. **Deep Q-Networks (DQN)**\n",
    "   - ‚úÖ Q-learning + neural networks\n",
    "   - ‚úÖ Experience replay breaks correlation\n",
    "   - ‚úÖ Target network stabilizes training\n",
    "   - ‚úÖ First to master Atari at human level (2013)\n",
    "   - **Used in:** Game AI, discrete control\n",
    "\n",
    "### 2. **Policy Gradients**\n",
    "   - ‚úÖ Directly optimize policy œÄ(a|s; Œ∏)\n",
    "   - ‚úÖ REINFORCE algorithm (Monte Carlo)\n",
    "   - ‚úÖ Handles continuous actions naturally\n",
    "   - ‚úÖ Stochastic policies (needed for some games)\n",
    "   - **Used in:** Robotics, continuous control\n",
    "\n",
    "### 3. **Actor-Critic Methods**\n",
    "   - ‚úÖ Combines value + policy approaches\n",
    "   - ‚úÖ Actor learns policy, Critic evaluates\n",
    "   - ‚úÖ Lower variance than pure policy gradients\n",
    "   - ‚úÖ Foundation of modern algorithms (PPO, SAC)\n",
    "   - **Used in:** ChatGPT (RLHF), robotics, games\n",
    "\n",
    "### 4. **Real-World Applications**\n",
    "   - ‚úÖ CartPole: Classic control benchmark\n",
    "   - ‚úÖ DQN learns to balance pole\n",
    "   - ‚úÖ Similar to humanoid robot balancing\n",
    "\n",
    "### üåü Real-World Impact (2024-2025):\n",
    "\n",
    "**What You Can Build:**\n",
    "- üéÆ **Game AI:** DQN for Atari, board games\n",
    "- ü§ñ **Robotics:** Actor-Critic for manipulation\n",
    "- üöó **Autonomous vehicles:** PPO for driving\n",
    "- üí¨ **LLM fine-tuning:** PPO for RLHF (ChatGPT)\n",
    "- ‚úàÔ∏è **Drone control:** Continuous control\n",
    "- üè≠ **Industrial automation:** Process optimization\n",
    "\n",
    "**Modern Algorithms:**\n",
    "- **PPO (2017):** ChatGPT fine-tuning, OpenAI Five\n",
    "- **SAC (2018):** Robotics, continuous control\n",
    "- **Rainbow DQN (2017):** Combines 6 DQN improvements\n",
    "- **TD3 (2018):** Twin delayed DDPG for continuous\n",
    "- **AlphaZero (2017):** Self-play + MCTS + policy networks\n",
    "\n",
    "### üìä Algorithm Comparison:\n",
    "\n",
    "| Feature | DQN | REINFORCE | Actor-Critic | PPO |\n",
    "|---------|-----|-----------|--------------|-----|\n",
    "| **Action space** | Discrete | Both | Both | Both |\n",
    "| **Sample efficiency** | ‚úÖ Good | ‚ùå Poor | ‚ö†Ô∏è Medium | ‚ö†Ô∏è Medium |\n",
    "| **Stability** | ‚ö†Ô∏è Medium | ‚ùå Poor | ‚úÖ Good | ‚úÖ Excellent |\n",
    "| **Variance** | ‚úÖ Low | ‚ùå High | ‚ö†Ô∏è Medium | ‚úÖ Low |\n",
    "| **Online learning** | ‚ùå No (replay) | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| **Best for** | Atari games | Simple problems | Robotics | Everything! |\n",
    "| **Used in** | DeepMind Atari | Research | AlphaGo | ChatGPT |\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now understand:\n",
    "- How DeepMind's DQN mastered Atari\n",
    "- The algorithms powering modern robotics\n",
    "- How ChatGPT uses PPO for RLHF\n",
    "- Deep RL foundations\n",
    "\n",
    "**Next:** Advanced RL - Multi-agent, AlphaGo, Real-world applications! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "**Practice Exercises:**\n",
    "1. Implement Double DQN (addresses overestimation bias)\n",
    "2. Add dueling architecture to DQN\n",
    "3. Try prioritized experience replay\n",
    "4. Implement PPO (used in ChatGPT!)\n",
    "5. Train on Gymnasium environments (try MountainCar, LunarLander)\n",
    "\n",
    "**Coming Next:**\n",
    "- **Day 3:** Advanced RL Applications - Multi-agent, AlphaGo, RL for robotics, game playing, and optimization\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Deep Dive Resources:**\n",
    "- DeepMind DQN paper (Nature 2015)\n",
    "- Spinning Up in Deep RL (OpenAI)\n",
    "- Gymnasium (OpenAI Gym successor)\n",
    "- Stable-Baselines3 (pre-implemented algorithms)\n",
    "- David Silver's RL Course (YouTube)\n",
    "\n",
    "**Try It Yourself:**\n",
    "```bash\n",
    "pip install gymnasium\n",
    "pip install stable-baselines3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: Deep RL powers game AI, robotics, and LLM fine-tuning. You now know the core algorithms!* üåü\n",
    "\n",
    "**üéØ You understand how DeepMind built agents that beat human experts!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
