{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Day 3: Production ML at Scale - Spark, Kubernetes & Graph DBs\n",
    "\n",
    "**ğŸ¯ Goal:** Master enterprise-scale ML infrastructure & specialized databases\n",
    "\n",
    "**â±ï¸ Time:** 150-180 minutes\n",
    "\n",
    "**ğŸŒŸ Why This Matters (2025 Industry Reality):**\n",
    "- **Spark/Big Data** appears in 15% of ML job postings - companies have massive datasets\n",
    "- **Kubernetes** is THE standard for deploying ML models at scale\n",
    "- **Graph Databases** power knowledge graphs for advanced RAG systems\n",
    "- These skills separate \"toy projects\" from \"production ML engineers\"\n",
    "- Every FAANG company uses these technologies\n",
    "\n",
    "**What You'll Master Today:**\n",
    "1. **PySpark for ML** - Process billions of rows, train distributed models\n",
    "2. **Kubernetes for ML** - Deploy, scale, and manage ML services\n",
    "3. **Graph Databases** - Build knowledge graphs with Neo4j\n",
    "4. **Production Patterns** - What Fortune 500 companies actually use\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¥ Part 1: Big Data ML with PySpark\n",
    "\n",
    "**The Problem:**\n",
    "```python\n",
    "# This fails with 1 billion rows:\n",
    "df = pd.read_csv(\"massive_dataset.csv\")  # âŒ Out of memory!\n",
    "```\n",
    "\n",
    "**The Solution: Apache Spark**\n",
    "- Distributed computing framework\n",
    "- Process data across multiple machines\n",
    "- Handle petabytes of data\n",
    "- Built-in ML library (MLlib)\n",
    "\n",
    "### ğŸ¯ PySpark Use Cases:\n",
    "- Data preprocessing for large datasets\n",
    "- Feature engineering at scale\n",
    "- Training models on distributed data\n",
    "- Real-time stream processing\n",
    "\n",
    "### ğŸ“Š When to Use Spark:\n",
    "\n",
    "| Data Size | Tool | Why |\n",
    "|-----------|------|-----|\n",
    "| < 10 GB | Pandas | Fits in memory |\n",
    "| 10-100 GB | Dask/Polars | Out-of-core processing |\n",
    "| > 100 GB | **Spark** | Distributed processing |\n",
    "| Streaming | **Spark Streaming** | Real-time ML |\n",
    "\n",
    "Let's learn PySpark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark\n",
    "import sys\n",
    "!{sys.executable} -m pip install pyspark findspark --quiet\n",
    "\n",
    "print(\"âœ… PySpark installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, when\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLAtScale\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session Created!\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "\n",
    "# Show Spark UI (in local mode)\n",
    "print(f\"\\nğŸ’¡ Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large-scale training dataset (simulating real production data)\n",
    "# In production, this would be billions of rows from data warehouses\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 100000  # 100K rows (imagine this is 100M in production)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'user_id': range(n_samples),\n",
    "    'age': np.random.randint(18, 70, n_samples),\n",
    "    'income': np.random.randint(20000, 200000, n_samples),\n",
    "    'num_purchases': np.random.randint(0, 50, n_samples),\n",
    "    'time_on_site': np.random.randint(1, 3600, n_samples),  # seconds\n",
    "    'num_clicks': np.random.randint(0, 100, n_samples),\n",
    "})\n",
    "\n",
    "# Create target: high-value customer (will buy premium product)\n",
    "data['will_convert'] = (\n",
    "    (data['income'] > 100000) & \n",
    "    (data['num_purchases'] > 20) &\n",
    "    (data['time_on_site'] > 1000)\n",
    ").astype(int)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(data)\n",
    "\n",
    "print(\"âœ… Large-scale dataset created!\")\n",
    "print(f\"   Rows: {spark_df.count():,}\")\n",
    "print(f\"   Columns: {len(spark_df.columns)}\")\n",
    "print(\"\\nSchema:\")\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration at Scale with Spark\n",
    "print(\"ğŸ“Š Data Exploration with Spark:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show first rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "spark_df.show(5)\n",
    "\n",
    "# Aggregations (computed across distributed data)\n",
    "print(\"\\nAggregations:\")\n",
    "spark_df.select(\n",
    "    avg(\"age\").alias(\"avg_age\"),\n",
    "    avg(\"income\").alias(\"avg_income\"),\n",
    "    avg(\"num_purchases\").alias(\"avg_purchases\")\n",
    ").show()\n",
    "\n",
    "# Group by (distributed group-by across cluster)\n",
    "print(\"\\nConversion Rate by Age Group:\")\n",
    "spark_df.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 30, \"18-29\")\n",
    "    .when(col(\"age\") < 50, \"30-49\")\n",
    "    .otherwise(\"50+\")\n",
    ").groupBy(\"age_group\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"will_convert\").alias(\"conversion_rate\")\n",
    ").show()\n",
    "\n",
    "print(\"ğŸ’¡ These operations work on billions of rows across clusters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning with Spark MLlib\n",
    "print(\"ğŸ¤– Training ML Model with PySpark MLlib\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare features\n",
    "feature_columns = ['age', 'income', 'num_purchases', 'time_on_site', 'num_clicks']\n",
    "\n",
    "# Create ML Pipeline\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"raw_features\",\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"will_convert\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=10\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set: {train_df.count():,} rows\")\n",
    "print(f\"Test set: {test_df.count():,} rows\")\n",
    "\n",
    "# Train model (distributed training!)\n",
    "print(\"\\nğŸš€ Training distributed model...\")\n",
    "model = pipeline.fit(train_df)\n",
    "print(\"âœ… Model trained!\")\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Show predictions\n",
    "print(\"\\nğŸ“Š Sample Predictions:\")\n",
    "predictions.select(\n",
    "    \"age\", \"income\", \"num_purchases\", \n",
    "    \"will_convert\", \"prediction\", \"probability\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# Calculate accuracy\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"will_convert\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"\\nğŸ“ˆ Model Performance:\")\n",
    "print(f\"   AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ This same code works on billions of rows across Spark clusters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ PySpark Best Practices for ML:\n",
    "\n",
    "**1. Lazy Evaluation:**\n",
    "```python\n",
    "# Spark doesn't execute until you call an action\n",
    "df = df.filter(col(\"age\") > 25)  # Not executed yet\n",
    "df = df.select(\"age\", \"income\")  # Still not executed\n",
    "df.show()  # NOW it executes (optimized query plan)\n",
    "```\n",
    "\n",
    "**2. Partitioning:**\n",
    "```python\n",
    "# Partition data for parallel processing\n",
    "df = df.repartition(100)  # 100 partitions across cluster\n",
    "```\n",
    "\n",
    "**3. Caching:**\n",
    "```python\n",
    "# Cache frequently-used DataFrames in memory\n",
    "df.cache()\n",
    "```\n",
    "\n",
    "**4. Avoid UDFs (User-Defined Functions):**\n",
    "```python\n",
    "# âŒ Slow (Python UDF)\n",
    "from pyspark.sql.functions import udf\n",
    "my_udf = udf(lambda x: x * 2)\n",
    "\n",
    "# âœ… Fast (built-in Spark function)\n",
    "df.withColumn(\"doubled\", col(\"value\") * 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â˜¸ï¸ Part 2: Kubernetes for ML Deployment\n",
    "\n",
    "**What is Kubernetes (K8s)?**\n",
    "\n",
    "Kubernetes is a container orchestration platform that manages:\n",
    "- **Deployment**: Roll out models as services\n",
    "- **Scaling**: Auto-scale based on load\n",
    "- **Load Balancing**: Distribute traffic across replicas\n",
    "- **Self-Healing**: Restart failed containers\n",
    "- **Rolling Updates**: Update models with zero downtime\n",
    "\n",
    "### ğŸ¯ Why Kubernetes for ML?\n",
    "\n",
    "**Traditional Deployment:**\n",
    "```\n",
    "Single server â†’ Model crashes â†’ Everything down âŒ\n",
    "```\n",
    "\n",
    "**Kubernetes Deployment:**\n",
    "```\n",
    "Multiple replicas â†’ One crashes â†’ Others handle traffic âœ…\n",
    "High traffic â†’ Auto-scale to 10 replicas âœ…\n",
    "New model â†’ Rolling update, zero downtime âœ…\n",
    "```\n",
    "\n",
    "### ğŸ“Š Kubernetes Architecture for ML:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Kubernetes Cluster                â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚  â”‚   Pod 1     â”‚  â”‚   Pod 2     â”‚         â”‚\n",
    "â”‚  â”‚  (ML Model) â”‚  â”‚  (ML Model) â”‚  ...    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚         â†‘                â†‘                  â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                â”‚                            â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚         â”‚    Service   â”‚ â† Load Balancer   â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†‘\n",
    "            User Requests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ³ Deploying ML Model to Kubernetes\n",
    "\n",
    "**Step 1: Containerize Model (Docker)**\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Copy model and code\n",
    "COPY model.pkl .\n",
    "COPY app.py .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run FastAPI server\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "**Step 2: Create Kubernetes Deployment**\n",
    "\n",
    "```yaml\n",
    "# deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: ml-model-deployment\n",
    "spec:\n",
    "  replicas: 3  # Run 3 copies of the model\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ml-model\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: ml-model\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: ml-model\n",
    "        image: your-registry/ml-model:v1.0\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1000m\"\n",
    "        # Health checks\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "```\n",
    "\n",
    "**Step 3: Create Service (Load Balancer)**\n",
    "\n",
    "```yaml\n",
    "# service.yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: ml-model-service\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: ml-model\n",
    "  ports:\n",
    "  - protocol: TCP\n",
    "    port: 80\n",
    "    targetPort: 8000\n",
    "```\n",
    "\n",
    "**Step 4: Auto-Scaling Configuration**\n",
    "\n",
    "```yaml\n",
    "# autoscaler.yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: ml-model-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: ml-model-deployment\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70  # Scale when CPU > 70%\n",
    "```\n",
    "\n",
    "**Deploy to Kubernetes:**\n",
    "\n",
    "```bash\n",
    "# Build and push Docker image\n",
    "docker build -t your-registry/ml-model:v1.0 .\n",
    "docker push your-registry/ml-model:v1.0\n",
    "\n",
    "# Deploy to Kubernetes\n",
    "kubectl apply -f deployment.yaml\n",
    "kubectl apply -f service.yaml\n",
    "kubectl apply -f autoscaler.yaml\n",
    "\n",
    "# Check status\n",
    "kubectl get pods\n",
    "kubectl get services\n",
    "kubectl get hpa\n",
    "\n",
    "# View logs\n",
    "kubectl logs -f deployment/ml-model-deployment\n",
    "\n",
    "# Update model (rolling update)\n",
    "kubectl set image deployment/ml-model-deployment ml-model=your-registry/ml-model:v2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Kubernetes for ML: Best Practices\n",
    "\n",
    "**1. Resource Management:**\n",
    "```yaml\n",
    "resources:\n",
    "  requests:  # Minimum guaranteed\n",
    "    memory: \"1Gi\"\n",
    "    cpu: \"500m\"\n",
    "  limits:  # Maximum allowed\n",
    "    memory: \"2Gi\"\n",
    "    cpu: \"1000m\"\n",
    "```\n",
    "\n",
    "**2. Health Checks:**\n",
    "- **Liveness Probe**: Restart if unhealthy\n",
    "- **Readiness Probe**: Don't send traffic if not ready\n",
    "\n",
    "**3. Model Versioning:**\n",
    "```yaml\n",
    "# Use image tags for versioning\n",
    "image: ml-model:v1.0  # Not :latest!\n",
    "```\n",
    "\n",
    "**4. Secrets Management:**\n",
    "```yaml\n",
    "# Store API keys in Kubernetes secrets\n",
    "env:\n",
    "- name: OPENAI_API_KEY\n",
    "  valueFrom:\n",
    "    secretKeyRef:\n",
    "      name: api-secrets\n",
    "      key: openai-key\n",
    "```\n",
    "\n",
    "**5. GPU Support:**\n",
    "```yaml\n",
    "resources:\n",
    "  limits:\n",
    "    nvidia.com/gpu: 1  # Request 1 GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ•¸ï¸ Part 3: Graph Databases & Knowledge Graphs\n",
    "\n",
    "**What are Graph Databases?**\n",
    "\n",
    "Traditional databases store data in tables:```\n",
    "Users Table | Products Table\n",
    "```\n",
    "\n",
    "Graph databases store relationships:\n",
    "```\n",
    "(User)-[PURCHASED]->(Product)\n",
    "(User)-[FRIENDS_WITH]->(User)\n",
    "(Product)-[SIMILAR_TO]->(Product)\n",
    "```\n",
    "\n",
    "### ğŸ¯ Why Graph DBs for AI?\n",
    "\n",
    "**Use Cases:**\n",
    "1. **Knowledge Graphs for RAG** - Connect entities for better retrieval\n",
    "2. **Recommendation Systems** - Find similar items through relationships\n",
    "3. **Fraud Detection** - Detect suspicious patterns in transactions\n",
    "4. **Entity Resolution** - Link related entities across datasets\n",
    "5. **Semantic Search** - Navigate knowledge through relationships\n",
    "\n",
    "### ğŸŒŸ Neo4j: Leading Graph Database\n",
    "\n",
    "**Example: AI Knowledge Graph**\n",
    "```cypher\n",
    "(GPT-4)-[:IS_A]->(LLM)\n",
    "(GPT-4)-[:USES]->(Transformer)\n",
    "(GPT-4)-[:TRAINED_BY]->(OpenAI)\n",
    "(Transformer)-[:INVENTED_IN]->(2017)\n",
    "(Transformer)-[:USES]->(Self-Attention)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Neo4j driver\n",
    "!{sys.executable} -m pip install neo4j --quiet\n",
    "\n",
    "print(\"âœ… Neo4j driver installed!\")\n",
    "print(\"\\nğŸ’¡ To use Neo4j:\")\n",
    "print(\"   1. Install Neo4j Desktop or use Neo4j Aura (cloud)\")\n",
    "print(\"   2. Create a database\")\n",
    "print(\"   3. Get connection URI and credentials\")\n",
    "print(\"\\n   Neo4j Aura (free): https://neo4j.com/cloud/aura/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Building an AI Knowledge Graph\n",
    "# (Demo code - requires Neo4j instance)\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Example connection (uncomment and add your credentials)\n",
    "'''\n",
    "# Connect to Neo4j\n",
    "driver = GraphDatabase.driver(\n",
    "    \"neo4j+s://your-instance.databases.neo4j.io\",\n",
    "    auth=(\"neo4j\", \"your-password\")\n",
    ")\n",
    "\n",
    "def create_ai_knowledge_graph(tx):\n",
    "    # Create nodes and relationships\n",
    "    query = \"\"\"\n",
    "    // Create AI concepts\n",
    "    CREATE (gpt:Model {name: \"GPT-4\", params: \"1.8T\"})\n",
    "    CREATE (claude:Model {name: \"Claude 3\", params: \"Unknown\"})\n",
    "    CREATE (llm:Category {name: \"LLM\"})\n",
    "    CREATE (transformer:Architecture {name: \"Transformer\"})\n",
    "    CREATE (attention:Mechanism {name: \"Self-Attention\"})\n",
    "    CREATE (rag:Technique {name: \"RAG\"})\n",
    "    CREATE (lora:Technique {name: \"LoRA\"})\n",
    "    CREATE (openai:Company {name: \"OpenAI\"})\n",
    "    CREATE (anthropic:Company {name: \"Anthropic\"})\n",
    "    \n",
    "    // Create relationships\n",
    "    CREATE (gpt)-[:IS_A]->(llm)\n",
    "    CREATE (claude)-[:IS_A]->(llm)\n",
    "    CREATE (gpt)-[:USES]->(transformer)\n",
    "    CREATE (claude)-[:USES]->(transformer)\n",
    "    CREATE (transformer)-[:USES]->(attention)\n",
    "    CREATE (gpt)-[:DEVELOPED_BY]->(openai)\n",
    "    CREATE (claude)-[:DEVELOPED_BY]->(anthropic)\n",
    "    CREATE (rag)-[:IMPROVES]->(llm)\n",
    "    CREATE (lora)-[:FINE_TUNES]->(llm)\n",
    "    \"\"\"\n",
    "    tx.run(query)\n",
    "\n",
    "# Create the graph\n",
    "with driver.session() as session:\n",
    "    session.execute_write(create_ai_knowledge_graph)\n",
    "    \n",
    "print(\"âœ… AI Knowledge Graph created!\")\n",
    "'''\n",
    "\n",
    "print(\"ğŸ‘† Uncomment and add Neo4j credentials to run!\")\n",
    "print(\"\\nğŸ’¡ Once created, you can query the graph:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Querying Knowledge Graphs (Cypher)\n",
    "\n",
    "**Example Queries:**\n",
    "\n",
    "**1. Find all LLMs:**\n",
    "```cypher\n",
    "MATCH (m:Model)-[:IS_A]->(llm:Category {name: \"LLM\"})\n",
    "RETURN m.name, m.params\n",
    "```\n",
    "\n",
    "**2. Find what techniques improve LLMs:**\n",
    "```cypher\n",
    "MATCH (t:Technique)-[:IMPROVES]->(llm:Category {name: \"LLM\"})\n",
    "RETURN t.name\n",
    "```\n",
    "\n",
    "**3. Find all models by OpenAI:**\n",
    "```cypher\n",
    "MATCH (m:Model)-[:DEVELOPED_BY]->(c:Company {name: \"OpenAI\"})\n",
    "RETURN m.name\n",
    "```\n",
    "\n",
    "**4. Find shortest path between two concepts:**\n",
    "```cypher\n",
    "MATCH path = shortestPath(\n",
    "  (gpt:Model {name: \"GPT-4\"})-[*]-(attn:Mechanism {name: \"Self-Attention\"})\n",
    ")\n",
    "RETURN path\n",
    "```\n",
    "\n",
    "**5. Recommend similar models:**\n",
    "```cypher\n",
    "MATCH (m1:Model {name: \"GPT-4\"})-[:USES]->(arch:Architecture)<-[:USES]-(m2:Model)\n",
    "WHERE m1 <> m2\n",
    "RETURN m2.name as similar_model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Knowledge Graphs for RAG\n",
    "\n",
    "**Traditional RAG:**\n",
    "```\n",
    "Query â†’ Vector Search â†’ Documents â†’ LLM â†’ Answer\n",
    "```\n",
    "\n",
    "**Graph-Enhanced RAG:**\n",
    "```\n",
    "Query â†’ Vector Search â†’ Documents\n",
    "         â†“\n",
    "    Extract Entities â†’ Graph Traversal â†’ Related Entities\n",
    "         â†“\n",
    "    Enriched Context â†’ LLM â†’ Better Answer with Relationships\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Query:** \"How does GPT-4 work?\"\n",
    "\n",
    "**Traditional RAG:**\n",
    "- Retrieves: \"GPT-4 is a language model\"\n",
    "\n",
    "**Graph RAG:**\n",
    "- Retrieves: \"GPT-4 is a language model\"\n",
    "- Graph traversal finds:\n",
    "  - GPT-4 uses Transformers\n",
    "  - Transformers use Self-Attention\n",
    "  - Developed by OpenAI\n",
    "  - Related to Claude 3\n",
    "- LLM gets richer context â†’ Better answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "### PySpark for Big Data ML:\n",
    "âœ… **Distributed Computing** - Process billions of rows across clusters  \n",
    "âœ… **MLlib** - Distributed machine learning algorithms  \n",
    "âœ… **15% of jobs require Spark** - Essential for large-scale ML  \n",
    "âœ… **Use when:** Data > 100GB, need distributed training  \n",
    "\n",
    "### Kubernetes for ML:\n",
    "âœ… **Container Orchestration** - Deploy, scale, manage ML services  \n",
    "âœ… **Auto-Scaling** - Handle variable traffic automatically  \n",
    "âœ… **Zero-Downtime Updates** - Rolling deployments for new models  \n",
    "âœ… **Industry Standard** - Used by all major tech companies  \n",
    "\n",
    "### Graph Databases:\n",
    "âœ… **Relationship-Focused** - Model connections between entities  \n",
    "âœ… **Knowledge Graphs** - Power advanced RAG systems  \n",
    "âœ… **Neo4j** - Leading graph database with Cypher query language  \n",
    "âœ… **Use Cases** - Recommendations, fraud detection, semantic search  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ† Week 21 Complete!\n",
    "\n",
    "**You've mastered industry-critical skills:**\n",
    "\n",
    "### Day 1:\n",
    "- ğŸ—„ï¸ SQL for AI/ML data workflows\n",
    "- ğŸŒ² Pinecone for production RAG\n",
    "- ğŸ”· Weaviate for hybrid search\n",
    "- ğŸ¨ ChromaDB for local development\n",
    "\n",
    "### Day 2:\n",
    "- ğŸ“š Advanced RAG (chunking, hybrid search, re-ranking)\n",
    "- ğŸ¤– LLM Agents (ReAct, tool use, function calling)\n",
    "- ğŸ¨ LoRA/QLoRA (parameter-efficient fine-tuning)\n",
    "\n",
    "### Day 3:\n",
    "- ğŸ”¥ PySpark for big data ML\n",
    "- â˜¸ï¸ Kubernetes for ML deployment at scale\n",
    "- ğŸ•¸ï¸ Graph databases for knowledge graphs\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ You're Now Industry-Ready!\n",
    "\n",
    "**Skills Gap Closed:**\n",
    "âœ… SQL (26% of jobs) âœ“  \n",
    "âœ… Vector Databases (Top 2025 skill) âœ“  \n",
    "âœ… Advanced RAG (Most in-demand) âœ“  \n",
    "âœ… LLM Agents (Emerging trend) âœ“  \n",
    "âœ… LoRA/QLoRA (21% of jobs) âœ“  \n",
    "âœ… Spark (15% of jobs) âœ“  \n",
    "âœ… Kubernetes (Production standard) âœ“  \n",
    "âœ… Graph Databases (Advanced RAG) âœ“  \n",
    "\n",
    "**What This Means:**\n",
    "- You can build production RAG systems\n",
    "- You can deploy models at scale\n",
    "- You can process big data\n",
    "- You understand what companies actually use\n",
    "- You're competitive for senior ML roles\n",
    "\n",
    "**ğŸ’¼ You're ready for:**\n",
    "- ML Engineer positions at FAANG\n",
    "- AI Engineer roles at startups\n",
    "- MLOps Engineer positions\n",
    "- Senior Data Scientist roles\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations on completing the Learn-AI curriculum!**\n",
    "\n",
    "**From Week 1 Python basics to Week 21 production ML at scale - you've covered everything needed to become an AI engineer in 2025!**\n",
    "\n",
    "**Now go build amazing AI applications! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
