{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Day 2: Advanced RAG + LLM Agents + LoRA/QLoRA\n",
    "\n",
    "**üéØ Goal:** Master cutting-edge LLM techniques demanded by industry in 2025\n",
    "\n",
    "**‚è±Ô∏è Time:** 150-180 minutes\n",
    "\n",
    "**üåü Why This Matters (2025 Job Market):**\n",
    "- **RAG Engineers** are the hottest new role - companies hiring like crazy!\n",
    "- **LLM Agents** are the future - from ChatGPT plugins to AutoGPT\n",
    "- **LoRA/QLoRA** mentioned in 21% of ML job postings - essential for efficient fine-tuning\n",
    "- These skills separate junior from senior AI engineers\n",
    "- Real companies need: Advanced chunking, hybrid search, re-ranking, agent frameworks\n",
    "\n",
    "**What You'll Master Today:**\n",
    "1. **Advanced RAG:** Chunking strategies, hybrid search, re-ranking, evaluation\n",
    "2. **LLM Agents:** ReAct pattern, tool use, function calling, LangChain agents\n",
    "3. **LoRA/QLoRA:** Parameter-efficient fine-tuning, practical implementation\n",
    "4. **Production Patterns:** What companies actually use in 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Part 1: Advanced RAG Techniques\n",
    "\n",
    "**Basic RAG (What we covered before):**\n",
    "```\n",
    "Documents ‚Üí Embeddings ‚Üí Vector DB ‚Üí Retrieval ‚Üí LLM ‚Üí Answer\n",
    "```\n",
    "\n",
    "**Advanced RAG (What companies use in 2025):**\n",
    "```\n",
    "Documents ‚Üí Smart Chunking ‚Üí Multi-Representation Embeddings\n",
    "    ‚Üì\n",
    "Vector DB + Keyword Index (Hybrid Search)\n",
    "    ‚Üì\n",
    "Retrieval (Top-20) ‚Üí Re-Ranking (Top-3) ‚Üí Context Compression\n",
    "    ‚Üì\n",
    "LLM with Chain-of-Thought ‚Üí Answer + Citations\n",
    "    ‚Üì\n",
    "Evaluation (Faithfulness, Relevance)\n",
    "```\n",
    "\n",
    "### üéØ Advanced RAG Components:\n",
    "\n",
    "**1. Smart Chunking Strategies:**\n",
    "- Sentence-based chunking (semantic boundaries)\n",
    "- Sliding window with overlap\n",
    "- Recursive character splitting\n",
    "- Markdown/code-aware splitting\n",
    "\n",
    "**2. Hybrid Search:**\n",
    "- Semantic search (vector similarity)\n",
    "- Keyword search (BM25, TF-IDF)\n",
    "- Combine scores with weighted fusion\n",
    "\n",
    "**3. Re-Ranking:**\n",
    "- Retrieve top-K candidates (e.g., 20)\n",
    "- Re-rank with cross-encoder\n",
    "- Return top-N most relevant (e.g., 3)\n",
    "\n",
    "**4. Evaluation:**\n",
    "- Faithfulness: Does answer come from context?\n",
    "- Relevance: Is context relevant to query?\n",
    "- Answer correctness: Is answer accurate?\n",
    "\n",
    "Let's build each component!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install advanced RAG libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install langchain langchain-community rank-bm25 sentence-transformers chromadb --quiet\n",
    "\n",
    "print(\"‚úÖ Advanced RAG libraries installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Smart Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter\n",
    ")\n",
    "\n",
    "# Sample document (typical blog post or documentation)\n",
    "document = \"\"\"\n",
    "# Introduction to Large Language Models\n",
    "\n",
    "Large Language Models (LLMs) have revolutionized AI. Models like GPT-4, Claude, and Gemini can understand and generate human-like text.\n",
    "\n",
    "## How LLMs Work\n",
    "\n",
    "LLMs are trained on massive amounts of text data using the transformer architecture. They learn patterns in language by predicting the next word in a sequence.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "1. Pre-training: Models learn from internet-scale text data\n",
    "2. Fine-tuning: Models are adapted to specific tasks\n",
    "3. RLHF: Reinforcement Learning from Human Feedback improves quality\n",
    "\n",
    "## Applications\n",
    "\n",
    "LLMs power chatbots, code assistants, search engines, and more. Companies use them for customer support, content generation, and data analysis.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "When using LLMs, always validate outputs, use prompt engineering, and implement RAG systems for accurate information retrieval.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìÑ Original Document:\")\n",
    "print(f\"   Length: {len(document)} characters\")\n",
    "print(f\"   Length: {len(document.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Recursive Character Splitting (Markdown-Aware)\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,  # Target chunk size\n",
    "    chunk_overlap=50,  # Overlap between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Try these in order\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks_recursive = recursive_splitter.split_text(document)\n",
    "\n",
    "print(\"\\nüî™ Recursive Character Splitting:\")\n",
    "print(f\"   Created {len(chunks_recursive)} chunks\\n\")\n",
    "for i, chunk in enumerate(chunks_recursive, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars): {chunk[:100]}...\\n\")\n",
    "\n",
    "print(\"üí° This respects document structure (headers, paragraphs)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Sentence-Based Chunking with Token Limit\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=20,\n",
    "    tokens_per_chunk=50  # Based on model's token limit\n",
    ")\n",
    "\n",
    "chunks_tokens = token_splitter.split_text(document)\n",
    "\n",
    "print(\"\\n‚úÇÔ∏è Token-Based Splitting:\")\n",
    "print(f\"   Created {len(chunks_tokens)} chunks\\n\")\n",
    "for i, chunk in enumerate(chunks_tokens[:3], 1):  # Show first 3\n",
    "    print(f\"Chunk {i}: {chunk}\\n\")\n",
    "\n",
    "print(\"üí° This ensures chunks fit within embedding model's token limit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Hybrid Search (Semantic + Keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Sample knowledge base\n",
    "documents = [\n",
    "    \"GPT-4 is a large language model by OpenAI with advanced reasoning capabilities.\",\n",
    "    \"RAG systems combine retrieval with generation to reduce hallucinations.\",\n",
    "    \"LoRA enables efficient fine-tuning by updating only small adapter layers.\",\n",
    "    \"Transformers use self-attention mechanisms to process sequences in parallel.\",\n",
    "    \"Vector databases like Pinecone store embeddings for fast semantic search.\",\n",
    "    \"Prompt engineering involves crafting effective prompts for better LLM outputs.\",\n",
    "    \"BERT is an encoder-only transformer model for understanding tasks.\",\n",
    "    \"Fine-tuning adapts pre-trained models to specific domains and tasks.\"\n",
    "]\n",
    "\n",
    "# Initialize embedding model for semantic search\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "doc_embeddings = embedding_model.encode(documents)\n",
    "\n",
    "# Initialize BM25 for keyword search\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "print(\"‚úÖ Hybrid search system initialized!\")\n",
    "print(f\"   {len(documents)} documents indexed\")\n",
    "print(f\"   Semantic search: {doc_embeddings.shape[1]}-dim embeddings\")\n",
    "print(f\"   Keyword search: BM25 algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, top_k=3, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Hybrid search combining semantic and keyword search\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        top_k: Number of results\n",
    "        alpha: Weight for semantic search (0=keyword only, 1=semantic only)\n",
    "    \"\"\"\n",
    "    # Semantic search\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    semantic_scores = np.dot(doc_embeddings, query_embedding)\n",
    "    semantic_scores = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min())\n",
    "    \n",
    "    # Keyword search (BM25)\n",
    "    tokenized_query = query.lower().split()\n",
    "    keyword_scores = bm25.get_scores(tokenized_query)\n",
    "    keyword_scores = (keyword_scores - keyword_scores.min()) / (keyword_scores.max() - keyword_scores.min() + 1e-10)\n",
    "    \n",
    "    # Combine scores\n",
    "    hybrid_scores = alpha * semantic_scores + (1 - alpha) * keyword_scores\n",
    "    \n",
    "    # Get top-k results\n",
    "    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'document': documents[idx],\n",
    "            'score': hybrid_scores[idx],\n",
    "            'semantic_score': semantic_scores[idx],\n",
    "            'keyword_score': keyword_scores[idx]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test hybrid search\n",
    "query = \"How can I fine-tune models efficiently?\"\n",
    "\n",
    "print(f\"üîç Query: '{query}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare: Semantic only vs Keyword only vs Hybrid\n",
    "print(\"\\nüé® Semantic Search Only (alpha=1.0):\")\n",
    "semantic_results = hybrid_search(query, alpha=1.0)\n",
    "for i, r in enumerate(semantic_results, 1):\n",
    "    print(f\"  {i}. [Score: {r['score']:.3f}] {r['document']}\")\n",
    "\n",
    "print(\"\\nüìù Keyword Search Only (alpha=0.0):\")\n",
    "keyword_results = hybrid_search(query, alpha=0.0)\n",
    "for i, r in enumerate(keyword_results, 1):\n",
    "    print(f\"  {i}. [Score: {r['score']:.3f}] {r['document']}\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è Hybrid Search (alpha=0.5):\")\n",
    "hybrid_results = hybrid_search(query, alpha=0.5)\n",
    "for i, r in enumerate(hybrid_results, 1):\n",
    "    print(f\"  {i}. [Score: {r['score']:.3f}] {r['document']}\")\n",
    "    print(f\"      Semantic: {r['semantic_score']:.3f}, Keyword: {r['keyword_score']:.3f}\")\n",
    "\n",
    "print(\"\\nüí° Hybrid search combines best of both worlds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Re-Ranking with Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install sentence-transformers --quiet\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load cross-encoder for re-ranking\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "print(\"‚úÖ Cross-encoder loaded for re-ranking!\")\n",
    "print(\"\\nüí° Cross-encoders are more accurate than bi-encoders for ranking\")\n",
    "print(\"   but slower (can't pre-compute). Use them for re-ranking!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_rag_search(query, initial_k=5, final_k=2):\n",
    "    \"\"\"\n",
    "    Advanced RAG: Hybrid search + Re-ranking\n",
    "    \"\"\"\n",
    "    # Step 1: Hybrid search (get more candidates)\n",
    "    candidates = hybrid_search(query, top_k=initial_k, alpha=0.5)\n",
    "    \n",
    "    # Step 2: Re-rank with cross-encoder\n",
    "    pairs = [[query, c['document']] for c in candidates]\n",
    "    rerank_scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Add rerank scores\n",
    "    for i, score in enumerate(rerank_scores):\n",
    "        candidates[i]['rerank_score'] = score\n",
    "    \n",
    "    # Sort by rerank score\n",
    "    reranked = sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)[:final_k]\n",
    "    \n",
    "    return reranked\n",
    "\n",
    "# Test advanced RAG\n",
    "query = \"What's the most efficient way to adapt large models?\"\n",
    "\n",
    "print(f\"üîç Query: '{query}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = advanced_rag_search(query, initial_k=5, final_k=2)\n",
    "\n",
    "print(\"\\nüèÜ Final Re-Ranked Results:\\n\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [Re-rank Score: {r['rerank_score']:.4f}]\")\n",
    "    print(f\"   Document: {r['document']}\")\n",
    "    print(f\"   Original Hybrid Score: {r['score']:.3f}\\n\")\n",
    "\n",
    "print(\"üí° Re-ranking found LoRA as most relevant - more accurate than hybrid alone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Part 2: LLM Agents & Tool Use\n",
    "\n",
    "**What are LLM Agents?**\n",
    "\n",
    "Agents are LLMs that can:\n",
    "- ‚úÖ Use external tools (calculators, search engines, APIs)\n",
    "- ‚úÖ Make multi-step decisions\n",
    "- ‚úÖ Take actions in environments\n",
    "- ‚úÖ Learn from feedback\n",
    "\n",
    "**Examples:**\n",
    "- ChatGPT Plugins\n",
    "- AutoGPT\n",
    "- LangChain Agents\n",
    "- Microsoft Copilot\n",
    "\n",
    "### üéØ ReAct Pattern (Reasoning + Acting)\n",
    "\n",
    "**ReAct Framework:**\n",
    "```\n",
    "Thought: I need to find the current weather\n",
    "Action: search[\"weather in San Francisco\"]\n",
    "Observation: Currently 68¬∞F and sunny\n",
    "Thought: I have the info, I can answer now\n",
    "Answer: It's 68¬∞F and sunny in San Francisco\n",
    "```\n",
    "\n",
    "Let's build agents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain for agents\n",
    "!{sys.executable} -m pip install langchain langchain-community --quiet\n",
    "\n",
    "print(\"‚úÖ LangChain installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ReAct Agent Implementation\n",
    "import re\n",
    "\n",
    "# Define tools the agent can use\n",
    "def calculator(expression):\n",
    "    \"\"\"Evaluates mathematical expressions\"\"\"\n",
    "    try:\n",
    "        return eval(expression)\n",
    "    except:\n",
    "        return \"Error: Invalid expression\"\n",
    "\n",
    "def search(query):\n",
    "    \"\"\"Simulates searching a knowledge base\"\"\"\n",
    "    knowledge = {\n",
    "        \"capital of france\": \"Paris\",\n",
    "        \"population of tokyo\": \"14 million\",\n",
    "        \"largest ocean\": \"Pacific Ocean\",\n",
    "        \"speed of light\": \"299,792,458 meters per second\"\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    for key, value in knowledge.items():\n",
    "        if key in query_lower:\n",
    "            return value\n",
    "    return \"No information found\"\n",
    "\n",
    "# Tools registry\n",
    "TOOLS = {\n",
    "    \"calculator\": calculator,\n",
    "    \"search\": search\n",
    "}\n",
    "\n",
    "def react_agent(question, max_steps=5):\n",
    "    \"\"\"\n",
    "    Simple ReAct agent that can use tools\n",
    "    \"\"\"\n",
    "    print(f\"ü§ñ Agent Question: {question}\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        print(f\"\\nStep {step + 1}:\")\n",
    "        \n",
    "        # Thought (simplified - in production use LLM)\n",
    "        if \"calculate\" in question.lower() or \"+\" in question or \"*\" in question:\n",
    "            print(\"üí≠ Thought: I need to use the calculator\")\n",
    "            \n",
    "            # Extract expression\n",
    "            numbers = re.findall(r'\\d+', question)\n",
    "            if \"plus\" in question or \"+\" in question:\n",
    "                expr = f\"{numbers[0]} + {numbers[1]}\"\n",
    "            elif \"times\" in question or \"*\" in question:\n",
    "                expr = f\"{numbers[0]} * {numbers[1]}\"\n",
    "            else:\n",
    "                expr = \" + \".join(numbers)\n",
    "            \n",
    "            print(f\"üîß Action: calculator[{expr}]\")\n",
    "            result = calculator(expr)\n",
    "            print(f\"üëÄ Observation: {result}\")\n",
    "            print(f\"\\n‚úÖ Answer: {result}\")\n",
    "            return result\n",
    "        \n",
    "        else:\n",
    "            print(\"üí≠ Thought: I need to search for information\")\n",
    "            print(f\"üîß Action: search[{question}]\")\n",
    "            result = search(question)\n",
    "            print(f\"üëÄ Observation: {result}\")\n",
    "            print(f\"\\n‚úÖ Answer: {result}\")\n",
    "            return result\n",
    "    \n",
    "    return \"Could not answer within max steps\"\n",
    "\n",
    "# Test the agent\n",
    "print(\"üß™ Testing ReAct Agent:\\n\")\n",
    "\n",
    "# Question 1: Search\n",
    "react_agent(\"What is the capital of France?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Question 2: Calculator\n",
    "react_agent(\"What is 25 plus 17?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Function Calling (OpenAI-Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling enables LLMs to use structured tools\n",
    "\n",
    "# Example: Define tools for the LLM\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_documents\",\n",
    "        \"description\": \"Search internal company documents\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The search query\"\n",
    "                },\n",
    "                \"category\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"hr\", \"engineering\", \"sales\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üõ†Ô∏è Function Calling Tools Defined:\\n\")\n",
    "for tool in tools:\n",
    "    print(f\"  ‚Ä¢ {tool['name']}: {tool['description']}\")\n",
    "\n",
    "print(\"\\nüí° In production, you'd send these to OpenAI/Claude:\")\n",
    "print(\"\"\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What's the weather in SF?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "# LLM decides to call: get_current_weather(location=\"San Francisco, CA\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ This is how ChatGPT plugins work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Part 3: LoRA & QLoRA - Efficient Fine-Tuning\n",
    "\n",
    "**The Problem:**\n",
    "- Fine-tuning GPT-4 (1.8T params) = Impossible for most\n",
    "- Even Llama 70B = expensive, slow\n",
    "\n",
    "**The Solution: LoRA (Low-Rank Adaptation)**\n",
    "\n",
    "**How LoRA Works:**\n",
    "```\n",
    "Traditional Fine-Tuning:\n",
    "  Update ALL 70 billion parameters ‚ùå Expensive!\n",
    "\n",
    "LoRA:\n",
    "  Freeze base model (70B params)\n",
    "  Add small trainable adapters (10M params) ‚úÖ 100x cheaper!\n",
    "  Merge after training\n",
    "```\n",
    "\n",
    "**QLoRA = LoRA + 4-bit Quantization**\n",
    "- Load model in 4-bit (75% less memory)\n",
    "- Fine-tune with LoRA\n",
    "- Fine-tune 70B model on 1 GPU! üöÄ\n",
    "\n",
    "### üìä LoRA vs Full Fine-Tuning:\n",
    "\n",
    "| Metric | Full Fine-Tuning | LoRA | QLoRA |\n",
    "|--------|------------------|------|-------|\n",
    "| **Trainable Params** | 70B | 10M | 10M |\n",
    "| **GPU Memory** | 280GB | 80GB | 24GB |\n",
    "| **Training Time** | Days | Hours | Hours |\n",
    "| **Cost** | $$$$ | $ | $ |\n",
    "| **Performance** | 100% | ~95-99% | ~95-99% |\n",
    "\n",
    "Let's implement LoRA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PEFT (Parameter-Efficient Fine-Tuning) library\n",
    "!{sys.executable} -m pip install peft transformers datasets accelerate --quiet\n",
    "\n",
    "print(\"‚úÖ PEFT library installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Load base model (using GPT-2 for demo - same principles apply to Llama, Mistral, etc.)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Loaded base model: {model_name}\")\n",
    "print(f\"   Total parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,  # LoRA rank (higher = more capacity, but more params)\n",
    "    lora_alpha=32,  # LoRA scaling factor\n",
    "    lora_dropout=0.1,  # Dropout for regularization\n",
    "    target_modules=[\"c_attn\"],  # Which layers to apply LoRA (attention layers)\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nüí° See the difference?\")\n",
    "print(\"   Full fine-tuning: ~124M parameters\")\n",
    "print(\"   LoRA: Only ~300K trainable parameters!\")\n",
    "print(\"   That's 400x fewer parameters to train! üöÄ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset (custom domain-specific data)\n",
    "# Example: Fine-tune for technical AI explanations\n",
    "training_data = [\n",
    "    \"Q: What is LoRA? A: LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that adds small trainable adapters to a frozen pre-trained model.\",\n",
    "    \"Q: Why use RAG? A: RAG (Retrieval-Augmented Generation) combines information retrieval with LLMs to provide accurate, grounded answers with source citations.\",\n",
    "    \"Q: How do transformers work? A: Transformers use self-attention mechanisms to process sequences in parallel, enabling them to capture long-range dependencies efficiently.\",\n",
    "    \"Q: What is prompt engineering? A: Prompt engineering is the practice of designing effective prompts to guide LLMs toward desired outputs, using techniques like few-shot learning and chain-of-thought.\",\n",
    "    \"Q: Explain vector databases? A: Vector databases store high-dimensional embeddings and enable fast similarity search, which is essential for semantic search and RAG systems.\"\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": training_data})\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f\"‚úÖ Training dataset prepared: {len(tokenized_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-gpt2-ai-tuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=3e-4,  # Higher LR for LoRA\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",  # Don't save checkpoints for demo\n",
    "    report_to=\"none\"  # Disable wandb\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting LoRA fine-tuning...\\n\")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ LoRA fine-tuning complete!\")\n",
    "print(\"\\nüí° The model is now specialized for AI technical explanations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create generator with LoRA model\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    \"Q: What is LoRA? A:\",\n",
    "    \"Q: Why use RAG? A:\",\n",
    "    \"Q: What are vector databases? A:\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing LoRA Fine-Tuned Model:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{prompt}\")\n",
    "    output = generator(prompt, num_return_sequences=1, do_sample=False)[0]['generated_text']\n",
    "    answer = output[len(prompt):].strip()\n",
    "    print(f\"{answer[:200]}...\")  # Truncate for display\n",
    "    print(\"-\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ Model generates domain-specific AI explanations!\")\n",
    "print(\"\\nüí° This is how companies fine-tune Llama/Mistral for their specific use cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### Advanced RAG:\n",
    "‚úÖ **Smart Chunking** - Respect document structure, use overlap  \n",
    "‚úÖ **Hybrid Search** - Combine semantic (meaning) + keyword (exact match)  \n",
    "‚úÖ **Re-Ranking** - Retrieve many, re-rank with cross-encoder, return few  \n",
    "‚úÖ **This is what production RAG looks like** in 2025  \n",
    "\n",
    "### LLM Agents:\n",
    "‚úÖ **ReAct Pattern** - Reasoning + Acting in loops  \n",
    "‚úÖ **Tool Use** - LLMs can call functions, APIs, search engines  \n",
    "‚úÖ **Function Calling** - Structured way to give LLMs tools  \n",
    "‚úÖ **Agents are the future** - ChatGPT plugins, AutoGPT, Copilot  \n",
    "\n",
    "### LoRA/QLoRA:\n",
    "‚úÖ **100x more efficient** than full fine-tuning  \n",
    "‚úÖ **Same performance** - 95-99% of full fine-tuning quality  \n",
    "‚úÖ **PEFT library** - Production-ready implementation  \n",
    "‚úÖ **Mentioned in 21% of jobs** - Critical skill for 2025  \n",
    "\n",
    "---\n",
    "\n",
    "**You now have industry-ready skills for:**\n",
    "- üîç Building production RAG systems with hybrid search and re-ranking\n",
    "- ü§ñ Creating LLM agents that use tools and take actions\n",
    "- üé® Efficiently fine-tuning large models with LoRA/QLoRA\n",
    "- üìä Understanding what companies actually use in 2025\n",
    "\n",
    "**Next:** Day 3 - Big Data, Kubernetes, Graph Databases! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
