{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÑÔ∏è Day 1: SQL for AI + Production Vector Databases\n",
    "\n",
    "**üéØ Goal:** Master SQL for data workflows and production-grade vector databases\n",
    "\n",
    "**‚è±Ô∏è Time:** 120-150 minutes\n",
    "\n",
    "**üåü Why This Matters for AI (2025):**\n",
    "- **SQL appears in 26% of AI/ML job postings** - critical for data engineering\n",
    "- Vector databases are THE foundation of RAG systems (ChatGPT, Claude, etc.)\n",
    "- Every AI company uses: Pinecone, Weaviate, or Chroma in production\n",
    "- RAG is the #1 AI application pattern - you MUST know vector DBs\n",
    "- SQL + Vector DBs = Complete data stack for modern AI\n",
    "\n",
    "**What You'll Build Today:**\n",
    "1. Master SQL queries for AI data workflows\n",
    "2. Build production RAG with Pinecone (cloud vector DB)\n",
    "3. Use Weaviate for semantic search with hybrid search\n",
    "4. Deploy local RAG with ChromaDB\n",
    "5. Compare vector DB performance and choose the right one\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 1: SQL for AI/ML Engineers\n",
    "\n",
    "**Why AI Engineers Need SQL:**\n",
    "\n",
    "‚úÖ **Data Collection**: Query training data from databases  \n",
    "‚úÖ **Feature Engineering**: JOIN tables to create features  \n",
    "‚úÖ **Model Monitoring**: Query prediction logs, track metrics  \n",
    "‚úÖ **A/B Testing**: Analyze experiment results  \n",
    "‚úÖ **Production**: Most companies store data in SQL databases  \n",
    "\n",
    "### üéØ Essential SQL for AI:\n",
    "\n",
    "**1. SELECT - Retrieve Data**\n",
    "```sql\n",
    "SELECT user_id, prediction, confidence\n",
    "FROM ml_predictions\n",
    "WHERE confidence > 0.8;\n",
    "```\n",
    "\n",
    "**2. JOIN - Combine Tables**\n",
    "```sql\n",
    "SELECT u.user_id, u.age, p.prediction\n",
    "FROM users u\n",
    "JOIN predictions p ON u.user_id = p.user_id;\n",
    "```\n",
    "\n",
    "**3. GROUP BY - Aggregate Metrics**\n",
    "```sql\n",
    "SELECT model_version, AVG(accuracy) as avg_accuracy\n",
    "FROM model_metrics\n",
    "GROUP BY model_version;\n",
    "```\n",
    "\n",
    "**4. Window Functions - Running Metrics**\n",
    "```sql\n",
    "SELECT date, accuracy,\n",
    "       AVG(accuracy) OVER (ORDER BY date ROWS 7 PRECEDING) as rolling_avg\n",
    "FROM daily_metrics;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SQL libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install sqlite3 pandas sqlalchemy --quiet\n",
    "\n",
    "print(\"‚úÖ SQL libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample AI/ML database\n",
    "conn = sqlite3.connect(':memory:')  # In-memory database for demo\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables for ML workflow\n",
    "cursor.execute('''\n",
    "CREATE TABLE users (\n",
    "    user_id INTEGER PRIMARY KEY,\n",
    "    age INTEGER,\n",
    "    country TEXT,\n",
    "    signup_date DATE\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE model_predictions (\n",
    "    prediction_id INTEGER PRIMARY KEY,\n",
    "    user_id INTEGER,\n",
    "    model_version TEXT,\n",
    "    prediction TEXT,\n",
    "    confidence REAL,\n",
    "    timestamp DATETIME,\n",
    "    FOREIGN KEY (user_id) REFERENCES users(user_id)\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE model_metrics (\n",
    "    metric_id INTEGER PRIMARY KEY,\n",
    "    model_version TEXT,\n",
    "    date DATE,\n",
    "    accuracy REAL,\n",
    "    precision_score REAL,\n",
    "    recall REAL\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert sample data\n",
    "users_data = [\n",
    "    (1, 25, 'USA', '2024-01-15'),\n",
    "    (2, 34, 'UK', '2024-01-20'),\n",
    "    (3, 28, 'Canada', '2024-02-01'),\n",
    "    (4, 45, 'USA', '2024-02-10'),\n",
    "    (5, 31, 'Germany', '2024-02-15')\n",
    "]\n",
    "cursor.executemany('INSERT INTO users VALUES (?,?,?,?)', users_data)\n",
    "\n",
    "predictions_data = [\n",
    "    (1, 1, 'v1.0', 'high_value', 0.92, '2024-03-01 10:00:00'),\n",
    "    (2, 1, 'v1.0', 'high_value', 0.88, '2024-03-02 11:00:00'),\n",
    "    (3, 2, 'v1.0', 'low_value', 0.76, '2024-03-01 12:00:00'),\n",
    "    (4, 3, 'v2.0', 'high_value', 0.95, '2024-03-03 09:00:00'),\n",
    "    (5, 4, 'v2.0', 'medium_value', 0.82, '2024-03-04 14:00:00'),\n",
    "    (6, 5, 'v2.0', 'high_value', 0.91, '2024-03-05 10:30:00')\n",
    "]\n",
    "cursor.executemany('INSERT INTO model_predictions VALUES (?,?,?,?,?,?)', predictions_data)\n",
    "\n",
    "metrics_data = [\n",
    "    (1, 'v1.0', '2024-03-01', 0.85, 0.82, 0.88),\n",
    "    (2, 'v1.0', '2024-03-02', 0.86, 0.83, 0.89),\n",
    "    (3, 'v2.0', '2024-03-03', 0.91, 0.89, 0.93),\n",
    "    (4, 'v2.0', '2024-03-04', 0.92, 0.90, 0.94),\n",
    "    (5, 'v2.0', '2024-03-05', 0.93, 0.91, 0.95)\n",
    "]\n",
    "cursor.executemany('INSERT INTO model_metrics VALUES (?,?,?,?,?,?)', metrics_data)\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Sample ML database created with users, predictions, and metrics!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple SELECT - Get high-confidence predictions\n",
    "query = '''\n",
    "SELECT prediction_id, user_id, prediction, confidence\n",
    "FROM model_predictions\n",
    "WHERE confidence > 0.9\n",
    "ORDER BY confidence DESC;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(\"üéØ High-Confidence Predictions (>90%):\")\n",
    "print(df)\n",
    "print(f\"\\nüí° Found {len(df)} predictions with >90% confidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: JOIN - Combine user data with predictions\n",
    "query = '''\n",
    "SELECT \n",
    "    u.user_id,\n",
    "    u.age,\n",
    "    u.country,\n",
    "    p.prediction,\n",
    "    p.confidence,\n",
    "    p.model_version\n",
    "FROM users u\n",
    "JOIN model_predictions p ON u.user_id = p.user_id\n",
    "WHERE p.confidence > 0.85;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(\"üîó User Demographics + Predictions (JOINed):\")\n",
    "print(df)\n",
    "print(f\"\\nüí° This is how you create features by combining tables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: GROUP BY - Compare model versions\n",
    "query = '''\n",
    "SELECT \n",
    "    model_version,\n",
    "    COUNT(*) as num_predictions,\n",
    "    AVG(confidence) as avg_confidence,\n",
    "    MIN(confidence) as min_confidence,\n",
    "    MAX(confidence) as max_confidence\n",
    "FROM model_predictions\n",
    "GROUP BY model_version;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(\"üìä Model Version Comparison:\")\n",
    "print(df)\n",
    "print(f\"\\nüí° v2.0 has higher average confidence - it's performing better!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Window Functions - Rolling metrics\n",
    "query = '''\n",
    "SELECT \n",
    "    date,\n",
    "    model_version,\n",
    "    accuracy,\n",
    "    AVG(accuracy) OVER (\n",
    "        PARTITION BY model_version \n",
    "        ORDER BY date \n",
    "        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "    ) as rolling_avg_accuracy\n",
    "FROM model_metrics\n",
    "ORDER BY model_version, date;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(\"üìà Rolling Accuracy (3-day window):\")\n",
    "print(df)\n",
    "print(f\"\\nüí° Window functions track metrics over time - critical for monitoring!\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Part 2: Production Vector Databases\n",
    "\n",
    "**Vector Database Landscape (2025):**\n",
    "\n",
    "| Database | Type | Best For | Pricing |\n",
    "|----------|------|----------|----------|\n",
    "| **Pinecone** | Cloud | Production RAG, scalability | Paid (free tier) |\n",
    "| **Weaviate** | Self-hosted/Cloud | Hybrid search, flexibility | Open-source |\n",
    "| **ChromaDB** | Local/Embedded | Development, small projects | Free |\n",
    "| **Qdrant** | Self-hosted/Cloud | High performance, filtering | Open-source |\n",
    "| **Milvus** | Self-hosted | Enterprise scale | Open-source |\n",
    "| **FAISS** | Library | Research, prototyping | Free |\n",
    "\n",
    "**Today's Focus: Pinecone + Weaviate + ChromaDB** (most popular in industry)\n",
    "\n",
    "---\n",
    "\n",
    "### üå≤ Pinecone: Cloud-Native Vector Database\n",
    "\n",
    "**Why Pinecone:**\n",
    "- Fully managed (no infrastructure)\n",
    "- Scales to billions of vectors\n",
    "- Fast similarity search (<100ms)\n",
    "- Used by: OpenAI, Notion, Zapier\n",
    "\n",
    "**Use Case:** Production RAG for customer support chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pinecone\n",
    "!{sys.executable} -m pip install pinecone-client sentence-transformers --quiet\n",
    "\n",
    "print(\"‚úÖ Pinecone installed!\")\n",
    "print(\"\\n‚ö†Ô∏è To use Pinecone:\")\n",
    "print(\"   1. Sign up at https://www.pinecone.io (free tier available)\")\n",
    "print(\"   2. Get your API key\")\n",
    "print(\"   3. Uncomment the code below and add your key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone RAG System (Production-Ready)\n",
    "\n",
    "# UNCOMMENT AND ADD YOUR API KEY TO RUN:\n",
    "'''\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"YOUR_API_KEY_HERE\")\n",
    "\n",
    "# Create index (vector database)\n",
    "index_name = \"ai-docs-rag\"\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # Matches our embedding model\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "    print(f\"‚úÖ Created index: {index_name}\")\n",
    "\n",
    "# Connect to index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sample documents (company knowledge base)\n",
    "documents = [\n",
    "    {\"id\": \"doc1\", \"text\": \"Our AI chatbot supports 24/7 customer service with 95% accuracy.\"},\n",
    "    {\"id\": \"doc2\", \"text\": \"To reset your password, click 'Forgot Password' on the login page.\"},\n",
    "    {\"id\": \"doc3\", \"text\": \"Our premium plan includes unlimited API calls and priority support.\"},\n",
    "    {\"id\": \"doc4\", \"text\": \"Shipping typically takes 3-5 business days for domestic orders.\"},\n",
    "    {\"id\": \"doc5\", \"text\": \"We offer a 30-day money-back guarantee on all purchases.\"},\n",
    "]\n",
    "\n",
    "# Embed and upload documents\n",
    "vectors = []\n",
    "for doc in documents:\n",
    "    embedding = embedding_model.encode(doc[\"text\"]).tolist()\n",
    "    vectors.append({\n",
    "        \"id\": doc[\"id\"],\n",
    "        \"values\": embedding,\n",
    "        \"metadata\": {\"text\": doc[\"text\"]}\n",
    "    })\n",
    "\n",
    "# Upsert to Pinecone\n",
    "index.upsert(vectors=vectors)\n",
    "print(f\"‚úÖ Uploaded {len(vectors)} documents to Pinecone\")\n",
    "\n",
    "# Wait for index to be ready\n",
    "time.sleep(2)\n",
    "\n",
    "# Query the system\n",
    "query = \"How do I reset my password?\"\n",
    "query_embedding = embedding_model.encode(query).tolist()\n",
    "\n",
    "# Search\n",
    "results = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=2,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüîç Query: '{query}'\")\n",
    "print(\"\\nüìÑ Retrieved Documents:\")\n",
    "for match in results['matches']:\n",
    "    print(f\"  [Score: {match['score']:.4f}] {match['metadata']['text']}\")\n",
    "\n",
    "print(\"\\nüéâ Pinecone RAG system working! This is production-ready.\")\n",
    "'''\n",
    "\n",
    "print(\"üëÜ Uncomment the code above and add your Pinecone API key to run!\")\n",
    "print(\"\\nüí° Pinecone is used by companies for production RAG at scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∑ Weaviate: Hybrid Search Vector Database\n",
    "\n",
    "**Why Weaviate:**\n",
    "- Combines vector search + keyword search (hybrid)\n",
    "- Built-in ML models (no separate embedding step)\n",
    "- GraphQL API\n",
    "- Self-hostable (you own your data)\n",
    "\n",
    "**Use Case:** Semantic search with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Weaviate\n",
    "!{sys.executable} -m pip install weaviate-client --quiet\n",
    "\n",
    "print(\"‚úÖ Weaviate installed!\")\n",
    "print(\"\\nüí° For this demo, we'll use Weaviate Cloud (free tier)\")\n",
    "print(\"   Or run locally with: docker run -p 8080:8080 semitechnologies/weaviate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weaviate Hybrid Search Demo\n",
    "\n",
    "# UNCOMMENT TO RUN (requires Weaviate instance):\n",
    "'''\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "# Connect to Weaviate (local or cloud)\n",
    "client = weaviate.connect_to_local()  # For local Docker instance\n",
    "# OR for cloud: client = weaviate.connect_to_wcs(cluster_url=\"...\", auth_credentials=Auth.api_key(\"...\"))\n",
    "\n",
    "# Create collection (schema)\n",
    "if not client.collections.exists(\"Article\"):\n",
    "    client.collections.create(\n",
    "        name=\"Article\",\n",
    "        vectorizer_config=weaviate.Configure.Vectorizer.text2vec_transformers(),\n",
    "        properties=[\n",
    "            weaviate.Property(name=\"title\", data_type=weaviate.DataType.TEXT),\n",
    "            weaviate.Property(name=\"content\", data_type=weaviate.DataType.TEXT),\n",
    "            weaviate.Property(name=\"category\", data_type=weaviate.DataType.TEXT),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Get collection\n",
    "articles = client.collections.get(\"Article\")\n",
    "\n",
    "# Add documents\n",
    "articles.data.insert_many([\n",
    "    {\n",
    "        \"title\": \"Understanding Transformers\",\n",
    "        \"content\": \"Transformers use self-attention to process sequences in parallel.\",\n",
    "        \"category\": \"AI\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"RAG Systems Guide\",\n",
    "        \"content\": \"Retrieval-Augmented Generation combines search with LLMs.\",\n",
    "        \"category\": \"AI\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Python Best Practices\",\n",
    "        \"content\": \"Use type hints and docstrings for better code quality.\",\n",
    "        \"category\": \"Programming\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Documents added to Weaviate\")\n",
    "\n",
    "# Hybrid search (combines vector + keyword search)\n",
    "response = articles.query.hybrid(\n",
    "    query=\"how do transformers work?\",\n",
    "    limit=2\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Hybrid Search Results:\")\n",
    "for item in response.objects:\n",
    "    print(f\"  Title: {item.properties['title']}\")\n",
    "    print(f\"  Content: {item.properties['content']}\")\n",
    "    print(f\"  Category: {item.properties['category']}\")\n",
    "    print()\n",
    "\n",
    "client.close()\n",
    "'''\n",
    "\n",
    "print(\"üëÜ Uncomment to run with Weaviate!\")\n",
    "print(\"\\nüí° Hybrid search = semantic (meaning) + keyword (exact match)\")\n",
    "print(\"   Best for: Production search where you need both precision and recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé® ChromaDB: Local Vector Database\n",
    "\n",
    "**Why ChromaDB:**\n",
    "- Runs locally (no API keys needed!)\n",
    "- Simple Python API\n",
    "- Perfect for development and prototyping\n",
    "- Persistent storage\n",
    "\n",
    "**Use Case:** Local RAG system for personal documents\n",
    "\n",
    "**This one we can run right now!** ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ChromaDB\n",
    "!{sys.executable} -m pip install chromadb --quiet\n",
    "\n",
    "print(\"‚úÖ ChromaDB installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize ChromaDB (persistent)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create or get collection\n",
    "collection = client.create_collection(\n",
    "    name=\"ai_knowledge_base\",\n",
    "    metadata={\"description\": \"AI/ML knowledge articles\"}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ChromaDB collection created!\")\n",
    "print(f\"   Collection: {collection.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to ChromaDB\n",
    "documents = [\n",
    "    \"GPT-4 is a large language model developed by OpenAI with over 1 trillion parameters.\",\n",
    "    \"RAG systems retrieve relevant documents before generating answers, reducing hallucinations.\",\n",
    "    \"LoRA (Low-Rank Adaptation) enables efficient fine-tuning by updating only small adapter layers.\",\n",
    "    \"Vector databases like Pinecone and Weaviate are essential for semantic search at scale.\",\n",
    "    \"Prompt engineering is the art of crafting effective prompts to get better LLM outputs.\",\n",
    "    \"Fine-tuning adapts pre-trained models to specific domains using task-specific data.\",\n",
    "    \"Transformers use self-attention mechanisms to process sequences in parallel, unlike RNNs.\",\n",
    "    \"BERT is an encoder-only transformer designed for understanding tasks like classification.\",\n",
    "    \"LangChain provides a framework for building LLM applications with chains and agents.\",\n",
    "    \"Embeddings convert text into dense vectors that capture semantic meaning.\"\n",
    "]\n",
    "\n",
    "# ChromaDB auto-generates embeddings!\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    ids=[f\"doc{i}\" for i in range(len(documents))],\n",
    "    metadatas=[{\"topic\": \"AI\", \"index\": i} for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Added {len(documents)} documents to ChromaDB\")\n",
    "print(\"\\nüí° ChromaDB automatically created embeddings using its default model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query ChromaDB\n",
    "query = \"How can I make LLMs more accurate?\"\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(f\"üîç Query: '{query}'\\n\")\n",
    "print(\"üìÑ Retrieved Documents:\\n\")\n",
    "\n",
    "for i, (doc, distance, metadata) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['distances'][0],\n",
    "    results['metadatas'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. [Similarity: {1-distance:.3f}] {doc}\")\n",
    "    print(f\"   Metadata: {metadata}\\n\")\n",
    "\n",
    "print(\"üí° ChromaDB found relevant docs about RAG and fine-tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Filtering with metadata\n",
    "results_filtered = collection.query(\n",
    "    query_texts=[\"Tell me about transformers\"],\n",
    "    n_results=2,\n",
    "    where={\"topic\": \"AI\"}  # Metadata filter\n",
    ")\n",
    "\n",
    "print(\"üîç Query with Filter: 'Tell me about transformers' (topic=AI)\\n\")\n",
    "print(\"üìÑ Results:\\n\")\n",
    "\n",
    "for i, doc in enumerate(results_filtered['documents'][0], 1):\n",
    "    print(f\"{i}. {doc}\\n\")\n",
    "\n",
    "print(\"üí° Metadata filtering lets you search within specific subsets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Vector Database Comparison\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "**üå≤ Pinecone:**\n",
    "- ‚úÖ Production applications at scale\n",
    "- ‚úÖ Need managed service (no infrastructure)\n",
    "- ‚úÖ Willing to pay for reliability\n",
    "- **Example:** Customer support chatbot for 10,000+ users\n",
    "\n",
    "**üî∑ Weaviate:**\n",
    "- ‚úÖ Need hybrid search (semantic + keyword)\n",
    "- ‚úÖ Want self-hosted option (data privacy)\n",
    "- ‚úÖ Complex filtering requirements\n",
    "- **Example:** Enterprise search across internal documents\n",
    "\n",
    "**üé® ChromaDB:**\n",
    "- ‚úÖ Development and prototyping\n",
    "- ‚úÖ Small-scale projects (<100K vectors)\n",
    "- ‚úÖ Need simple, local setup\n",
    "- **Example:** Personal knowledge base, learning projects\n",
    "\n",
    "### Performance Comparison:\n",
    "\n",
    "| Feature | Pinecone | Weaviate | ChromaDB |\n",
    "|---------|----------|----------|----------|\n",
    "| **Scale** | Billions | Millions | Thousands-Millions |\n",
    "| **Speed** | <100ms | <200ms | Varies |\n",
    "| **Setup** | API key only | Docker/Cloud | `pip install` |\n",
    "| **Cost** | $70+/mo | Free (self-host) | Free |\n",
    "| **Hybrid Search** | ‚ùå | ‚úÖ | ‚ùå |\n",
    "| **GraphQL** | ‚ùå | ‚úÖ | ‚ùå |\n",
    "| **Learning Curve** | Easy | Medium | Easy |\n",
    "\n",
    "**üí° Pro Tip:** Start with ChromaDB for development, then move to Pinecone/Weaviate for production!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Complete RAG System with ChromaDB\n",
    "\n",
    "Let's build a production-quality RAG system using everything we learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize LLM for generation (using GPT-2 for demo)\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "def rag_query(question, top_k=3):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with ChromaDB\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = results['documents'][0]\n",
    "    \n",
    "    # Step 2: Build context\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[{i+1}] {doc}\" \n",
    "        for i, doc in enumerate(retrieved_docs)\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Create prompt\n",
    "    prompt = f\"\"\"Answer the question using the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 4: Generate answer\n",
    "    response = generator(\n",
    "        prompt,\n",
    "        max_length=len(prompt.split()) + 50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    answer = response[0]['generated_text'][len(prompt):].strip()\n",
    "    \n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'sources': retrieved_docs,\n",
    "        'context': context\n",
    "    }\n",
    "\n",
    "# Test the RAG system\n",
    "questions = [\n",
    "    \"What is RAG and how does it help?\",\n",
    "    \"How can I fine-tune models efficiently?\",\n",
    "    \"What are transformers?\"\n",
    "]\n",
    "\n",
    "print(\"ü§ñ Production RAG System with ChromaDB\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\\n\")\n",
    "    \n",
    "    result = rag_query(question)\n",
    "    \n",
    "    print(\"üìö Retrieved Sources:\")\n",
    "    for i, source in enumerate(result['sources'], 1):\n",
    "        print(f\"   [{i}] {source}\")\n",
    "    \n",
    "    print(f\"\\nüí¨ Generated Answer:\")\n",
    "    print(f\"   {result['answer'][:200]}...\")  # Truncate for display\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "print(\"\\nüéâ Complete RAG system working!\")\n",
    "print(\"\\nüí° In production, you'd use:\")\n",
    "print(\"   - GPT-4 or Claude for better generation\")\n",
    "print(\"   - Pinecone/Weaviate for scale\")\n",
    "print(\"   - Re-ranking for better retrieval\")\n",
    "print(\"   - Streaming for better UX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### SQL for AI:\n",
    "‚úÖ Essential for data collection and feature engineering  \n",
    "‚úÖ JOINs combine data from multiple tables  \n",
    "‚úÖ GROUP BY aggregates metrics for model evaluation  \n",
    "‚úÖ Window functions track metrics over time  \n",
    "‚úÖ 26% of AI jobs require SQL - master it!  \n",
    "\n",
    "### Vector Databases:\n",
    "‚úÖ Foundation of RAG systems (the #1 AI pattern)  \n",
    "‚úÖ **Pinecone:** Production-ready, fully managed, scales to billions  \n",
    "‚úÖ **Weaviate:** Hybrid search, self-hostable, GraphQL API  \n",
    "‚úÖ **ChromaDB:** Local development, simple API, perfect for learning  \n",
    "‚úÖ All major AI companies use vector DBs in production  \n",
    "\n",
    "### Best Practices:\n",
    "‚úÖ Start with ChromaDB for development  \n",
    "‚úÖ Use Pinecone/Weaviate for production at scale  \n",
    "‚úÖ Implement metadata filtering for better search  \n",
    "‚úÖ Combine with LLMs for complete RAG systems  \n",
    "‚úÖ Monitor and optimize embedding quality  \n",
    "\n",
    "---\n",
    "\n",
    "**You now have production-ready skills for:**\n",
    "- üóÑÔ∏è Querying AI/ML data with SQL\n",
    "- üîç Building semantic search with vector databases\n",
    "- ü§ñ Deploying production RAG systems\n",
    "- üìä Choosing the right vector DB for your use case\n",
    "\n",
    "**Next:** Day 2 - Advanced RAG Techniques + LLM Agents + LoRA/QLoRA! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
