{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 2: MLOps Best Practices\n",
    "\n",
    "**üéØ Goal:** Master MLOps workflows for production ML systems\n",
    "\n",
    "**‚è±Ô∏è Time:** 120-150 minutes\n",
    "\n",
    "**üåü Why This Matters for AI (2024-2025):**\n",
    "- MLOps is THE #1 skill gap in AI - companies desperately need MLOps engineers\n",
    "- 90% of ML models fail in production due to poor MLOps practices\n",
    "- Model versioning prevents disasters and enables rollbacks\n",
    "- Experiment tracking saves months of wasted work\n",
    "- Model monitoring catches problems before users do\n",
    "- A/B testing proves your model actually works in production\n",
    "- Every successful AI company has robust MLOps pipelines\n",
    "\n",
    "**What You'll Build Today:**\n",
    "1. **Model versioning** with MLflow and Weights & Biases\n",
    "2. **Experiment tracking** to compare 100s of model runs\n",
    "3. **Model monitoring** to detect drift and degradation\n",
    "4. **A/B testing** to validate models in production\n",
    "5. **Complete MLOps pipeline** from training to monitoring\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlops-landscape",
   "metadata": {},
   "source": [
    "## üåç MLOps Landscape (2024-2025)\n",
    "\n",
    "**MLOps = DevOps for Machine Learning**\n",
    "\n",
    "### üéØ What is MLOps?\n",
    "\n",
    "**The practice of deploying and maintaining ML models in production reliably and efficiently.**\n",
    "\n",
    "```\n",
    "Traditional Software:        Machine Learning:\n",
    "Code ‚Üí Test ‚Üí Deploy        Code + Data + Model ‚Üí Test ‚Üí Deploy ‚Üí Monitor\n",
    "       ‚Üì                                                            ‚Üì\n",
    "    DevOps                                                       MLOps\n",
    "```\n",
    "\n",
    "### üèóÔ∏è MLOps Components:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Experiment Tracking**\n",
    "**Problem:** \"Which hyperparameters gave best results 2 weeks ago?\"  \n",
    "**Solution:** Track every experiment automatically\n",
    "\n",
    "**Tools:**\n",
    "- **MLflow** (open-source, industry standard)\n",
    "- **Weights & Biases** (W&B) - best visualizations\n",
    "- **Neptune.ai** - enterprise features\n",
    "- **TensorBoard** - TensorFlow focused\n",
    "\n",
    "#### 2Ô∏è‚É£ **Model Versioning**\n",
    "**Problem:** \"Which model is in production? Can we rollback?\"  \n",
    "**Solution:** Version models like code (Git for models)\n",
    "\n",
    "**Tools:**\n",
    "- **MLflow Model Registry**\n",
    "- **DVC** (Data Version Control)\n",
    "- **Pachyderm**\n",
    "\n",
    "#### 3Ô∏è‚É£ **Model Monitoring**\n",
    "**Problem:** \"Model accuracy dropped from 95% to 60%!\"  \n",
    "**Solution:** Monitor performance, data drift, predictions\n",
    "\n",
    "**Tools:**\n",
    "- **Evidently AI**\n",
    "- **WhyLabs**\n",
    "- **Arize AI**\n",
    "- **Fiddler**\n",
    "\n",
    "#### 4Ô∏è‚É£ **CI/CD for ML**\n",
    "**Problem:** \"Manual deployment is slow and error-prone\"  \n",
    "**Solution:** Automate training, testing, deployment\n",
    "\n",
    "**Tools:**\n",
    "- **GitHub Actions** + ML\n",
    "- **Jenkins** + ML pipelines\n",
    "- **Argo Workflows**\n",
    "- **Kubeflow**\n",
    "\n",
    "#### 5Ô∏è‚É£ **Feature Stores**\n",
    "**Problem:** \"Different features in training vs production\"  \n",
    "**Solution:** Centralized feature management\n",
    "\n",
    "**Tools:**\n",
    "- **Feast** (open-source)\n",
    "- **Tecton**\n",
    "- **Hopsworks**\n",
    "\n",
    "### üìä MLOps Maturity Levels:\n",
    "\n",
    "| Level | Description | Characteristics |\n",
    "|-------|-------------|------------------|\n",
    "| **0 - Manual** | No automation | Jupyter notebooks, manual deployment |\n",
    "| **1 - DevOps** | Code automation | Automated testing, CI/CD for code |\n",
    "| **2 - Automated Training** | ML automation | Automated retraining, pipelines |\n",
    "| **3 - Full MLOps** | Complete automation | Auto deploy, monitor, retrain |\n",
    "\n",
    "**Most companies in 2024-2025: Level 1-2**  \n",
    "**Goal: Reach Level 3**\n",
    "\n",
    "### üåü Why MLOps Matters:\n",
    "\n",
    "**Without MLOps:**\n",
    "- ‚ùå Lost experiments (can't reproduce results)\n",
    "- ‚ùå Model rot (accuracy degrades over time)\n",
    "- ‚ùå Slow iteration (manual everything)\n",
    "- ‚ùå Production failures (untested deployments)\n",
    "- ‚ùå No accountability (who deployed what?)\n",
    "\n",
    "**With MLOps:**\n",
    "- ‚úÖ Reproducible experiments\n",
    "- ‚úÖ Automated monitoring and alerts\n",
    "- ‚úÖ Fast iteration cycles\n",
    "- ‚úÖ Reliable deployments\n",
    "- ‚úÖ Complete audit trail\n",
    "\n",
    "Let's build MLOps pipelines!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLOps libraries\n",
    "import sys\n",
    "\n",
    "# Core ML libraries\n",
    "!{sys.executable} -m pip install scikit-learn numpy pandas --quiet\n",
    "\n",
    "# MLOps tools\n",
    "!{sys.executable} -m pip install mlflow wandb --quiet\n",
    "\n",
    "# Monitoring\n",
    "!{sys.executable} -m pip install evidently --quiet\n",
    "\n",
    "# Visualization\n",
    "!{sys.executable} -m pip install matplotlib seaborn plotly --quiet\n",
    "\n",
    "print(\"‚úÖ MLOps libraries installed successfully!\")\n",
    "print(\"\\nüì¶ Installed:\")\n",
    "print(\"   - MLflow (experiment tracking & model registry)\")\n",
    "print(\"   - Weights & Biases (W&B)\")\n",
    "print(\"   - Evidently (model monitoring)\")\n",
    "print(\"\\nüöÄ Ready for MLOps!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# MLOps libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")\n",
    "print(\"üéØ Ready to build MLOps pipelines!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlflow-intro",
   "metadata": {},
   "source": [
    "## üìä Step 1: Experiment Tracking with MLflow\n",
    "\n",
    "**MLflow = THE industry standard for ML experiment tracking**\n",
    "\n",
    "### üéØ What is MLflow?\n",
    "\n",
    "**Open-source platform for the complete ML lifecycle:**\n",
    "1. **Tracking**: Log params, metrics, artifacts\n",
    "2. **Projects**: Package ML code\n",
    "3. **Models**: Manage and deploy models\n",
    "4. **Registry**: Version and stage models\n",
    "\n",
    "### üèóÔ∏è MLflow Tracking:\n",
    "\n",
    "```\n",
    "Experiment\n",
    "    ‚Üì\n",
    "‚îú‚îÄ Run 1 (model=RandomForest, n_estimators=100)\n",
    "‚îÇ  ‚îú‚îÄ Parameters: {n_estimators: 100, max_depth: 10}\n",
    "‚îÇ  ‚îú‚îÄ Metrics: {accuracy: 0.85, f1: 0.83}\n",
    "‚îÇ  ‚îî‚îÄ Artifacts: model.pkl, feature_importance.png\n",
    "‚îÇ\n",
    "‚îú‚îÄ Run 2 (model=RandomForest, n_estimators=200)\n",
    "‚îÇ  ‚îú‚îÄ Parameters: {n_estimators: 200, max_depth: 15}\n",
    "‚îÇ  ‚îú‚îÄ Metrics: {accuracy: 0.87, f1: 0.86}\n",
    "‚îÇ  ‚îî‚îÄ Artifacts: model.pkl, feature_importance.png\n",
    "‚îÇ\n",
    "‚îî‚îÄ Run 3 (model=GradientBoosting)\n",
    "   ‚îî‚îÄ ...\n",
    "```\n",
    "\n",
    "### üåü Why Use MLflow?\n",
    "\n",
    "‚úÖ **Never lose experiments**: All runs saved automatically  \n",
    "‚úÖ **Easy comparison**: Compare 100s of runs visually  \n",
    "‚úÖ **Reproducible**: Track everything needed to reproduce  \n",
    "‚úÖ **Team collaboration**: Share results with team  \n",
    "‚úÖ **Production ready**: Deploy best models directly  \n",
    "\n",
    "Let's track experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset for our experiments\n",
    "\n",
    "print(\"üìä Creating Sample Dataset\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate synthetic classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset created:\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "print(f\"   Classes: {len(np.unique(y))}\")\n",
    "\n",
    "# Class distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"\\nüìà Class distribution:\")\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"   Class {cls}: {count} samples ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlflow-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic MLflow experiment tracking\n",
    "\n",
    "print(\"üî¨ Running MLflow Experiment\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set experiment name\n",
    "mlflow.set_experiment(\"binary_classification\")\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"random_forest_v1\"):\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 2,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    print(\"üìù Logged parameters:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nüéØ Training model...\")\n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    print(\"\\nüìä Logged metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Model logged to MLflow!\")\n",
    "    \n",
    "    # Get run info\n",
    "    run = mlflow.active_run()\n",
    "    print(f\"\\nüîó Run ID: {run.info.run_id}\")\n",
    "    print(f\"üìÅ Artifact URI: {run.info.artifact_uri}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° View results: mlflow ui\")\n",
    "print(\"   Then open: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlflow-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple experiments with different models\n",
    "\n",
    "print(\"üî¨ Running Multiple Experiments\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define different models and their hyperparameters\n",
    "experiments = [\n",
    "    {\n",
    "        'name': 'random_forest_small',\n",
    "        'model': RandomForestClassifier,\n",
    "        'params': {'n_estimators': 50, 'max_depth': 5, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'random_forest_medium',\n",
    "        'model': RandomForestClassifier,\n",
    "        'params': {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'random_forest_large',\n",
    "        'model': RandomForestClassifier,\n",
    "        'params': {'n_estimators': 200, 'max_depth': 15, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'gradient_boosting',\n",
    "        'model': GradientBoostingClassifier,\n",
    "        'params': {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'logistic_regression',\n",
    "        'model': LogisticRegression,\n",
    "        'params': {'max_iter': 1000, 'random_state': 42}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Store results for comparison\n",
    "results = []\n",
    "\n",
    "# Run all experiments\n",
    "for i, exp in enumerate(experiments, 1):\n",
    "    print(f\"\\nüî¨ Experiment {i}/{len(experiments)}: {exp['name']}\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=exp['name']):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(exp['params'])\n",
    "        mlflow.log_param('model_type', exp['model'].__name__)\n",
    "        \n",
    "        # Train model\n",
    "        model = exp['model'](**exp['params'])\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Store for comparison\n",
    "        results.append({\n",
    "            'name': exp['name'],\n",
    "            'model': exp['model'].__name__,\n",
    "            **metrics\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úÖ Accuracy: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nüìä Experiment Results:\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_idx = results_df['accuracy'].idxmax()\n",
    "best_model = results_df.iloc[best_idx]\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model['name']}\")\n",
    "print(f\"   Model Type: {best_model['model']}\")\n",
    "print(f\"   Accuracy: {best_model['accuracy']:.4f}\")\n",
    "print(f\"   F1 Score: {best_model['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\nüí° All experiments tracked in MLflow!\")\n",
    "print(\"   Run 'mlflow ui' to compare visually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize experiment results\n",
    "\n",
    "print(\"üìä Visualizing Experiment Results\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "axes[0, 0].barh(results_df['name'], results_df['accuracy'], color='skyblue')\n",
    "axes[0, 0].set_xlabel('Accuracy', fontsize=12)\n",
    "axes[0, 0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "\n",
    "# Plot 2: F1 Score comparison\n",
    "axes[0, 1].barh(results_df['name'], results_df['f1_score'], color='lightcoral')\n",
    "axes[0, 1].set_xlabel('F1 Score', fontsize=12)\n",
    "axes[0, 1].set_title('Model F1 Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlim(0, 1)\n",
    "\n",
    "# Plot 3: Precision vs Recall\n",
    "axes[1, 0].scatter(results_df['precision'], results_df['recall'], s=200, alpha=0.6, c=range(len(results_df)), cmap='viridis')\n",
    "for i, name in enumerate(results_df['name']):\n",
    "    axes[1, 0].annotate(name, (results_df['precision'][i], results_df['recall'][i]), \n",
    "                        fontsize=8, ha='center')\n",
    "axes[1, 0].set_xlabel('Precision', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Recall', fontsize=12)\n",
    "axes[1, 0].set_title('Precision vs Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: All metrics comparison (heatmap)\n",
    "metrics_only = results_df[['accuracy', 'precision', 'recall', 'f1_score']]\n",
    "im = axes[1, 1].imshow(metrics_only.values, cmap='YlGnBu', aspect='auto', vmin=0, vmax=1)\n",
    "axes[1, 1].set_xticks(range(len(metrics_only.columns)))\n",
    "axes[1, 1].set_xticklabels(metrics_only.columns, rotation=45, ha='right')\n",
    "axes[1, 1].set_yticks(range(len(results_df)))\n",
    "axes[1, 1].set_yticklabels(results_df['name'])\n",
    "axes[1, 1].set_title('All Metrics Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add values to heatmap\n",
    "for i in range(len(results_df)):\n",
    "    for j in range(len(metrics_only.columns)):\n",
    "        text = axes[1, 1].text(j, i, f'{metrics_only.values[i, j]:.3f}',\n",
    "                              ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiment_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualizations created!\")\n",
    "print(\"   üìÑ Saved to: experiment_comparison.png\")\n",
    "print(\"\\nüí° This is how you compare 100s of experiments visually!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-registry",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Step 2: Model Versioning & Registry\n",
    "\n",
    "**Model Registry = Git for ML Models**\n",
    "\n",
    "### üéØ Why Model Versioning?\n",
    "\n",
    "**Problems without versioning:**\n",
    "- ‚ùå \"Which model is in production?\"\n",
    "- ‚ùå \"Can we rollback to last week's model?\"\n",
    "- ‚ùå \"Who deployed this model?\"\n",
    "- ‚ùå \"What data was it trained on?\"\n",
    "\n",
    "**Solutions with versioning:**\n",
    "- ‚úÖ Track all model versions\n",
    "- ‚úÖ Easy rollback to any version\n",
    "- ‚úÖ Complete audit trail\n",
    "- ‚úÖ Stage-based deployment (staging ‚Üí production)\n",
    "\n",
    "### üèóÔ∏è MLflow Model Registry:\n",
    "\n",
    "```\n",
    "Model: sentiment_classifier\n",
    "‚îÇ\n",
    "‚îú‚îÄ Version 1 [Production]\n",
    "‚îÇ  ‚îú‚îÄ Accuracy: 0.85\n",
    "‚îÇ  ‚îú‚îÄ Created: 2024-01-15\n",
    "‚îÇ  ‚îî‚îÄ Status: Production\n",
    "‚îÇ\n",
    "‚îú‚îÄ Version 2 [Staging]\n",
    "‚îÇ  ‚îú‚îÄ Accuracy: 0.87\n",
    "‚îÇ  ‚îú‚îÄ Created: 2024-01-20\n",
    "‚îÇ  ‚îî‚îÄ Status: Staging (testing)\n",
    "‚îÇ\n",
    "‚îî‚îÄ Version 3 [Archived]\n",
    "   ‚îú‚îÄ Accuracy: 0.82\n",
    "   ‚îú‚îÄ Created: 2024-01-10\n",
    "   ‚îî‚îÄ Status: Archived (old)\n",
    "```\n",
    "\n",
    "### üåü Model Stages:\n",
    "\n",
    "1. **None**: Just registered\n",
    "2. **Staging**: Being tested\n",
    "3. **Production**: Serving users\n",
    "4. **Archived**: Deprecated\n",
    "\n",
    "### üìä Versioning Best Practices:\n",
    "\n",
    "‚úÖ **Version everything**: Model, code, data, config  \n",
    "‚úÖ **Tag versions**: Add metadata (accuracy, dataset, etc.)  \n",
    "‚úÖ **Stage gradually**: Staging ‚Üí Canary ‚Üí Production  \n",
    "‚úÖ **Never delete**: Archive instead of delete  \n",
    "‚úÖ **Audit trail**: Log who did what when  \n",
    "\n",
    "Let's use the model registry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-registry-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow Model Registry demo\n",
    "\n",
    "print(\"üóÑÔ∏è  MLflow Model Registry Demo\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set tracking URI (use local file store)\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "\n",
    "# Register a model\n",
    "model_name = \"binary_classifier\"\n",
    "\n",
    "print(f\"üìù Registering model: {model_name}\")\n",
    "\n",
    "# Train and register model\n",
    "with mlflow.start_run(run_name=\"registry_demo\") as run:\n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Log and register model\n",
    "    mlflow.sklearn.log_model(\n",
    "        model,\n",
    "        \"model\",\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model registered!\")\n",
    "    print(f\"   Name: {model_name}\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° Model Registry Features:\")\n",
    "print(\"   ‚úÖ Version control for models\")\n",
    "print(\"   ‚úÖ Stage-based deployment (Staging ‚Üí Production)\")\n",
    "print(\"   ‚úÖ Model lineage (track training data, code)\")\n",
    "print(\"   ‚úÖ Easy rollback to previous versions\")\n",
    "print(\"\\nüåê View in UI: mlflow ui --backend-store-uri sqlite:///mlflow.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring-intro",
   "metadata": {},
   "source": [
    "## üìà Step 3: Model Monitoring\n",
    "\n",
    "**Monitor models in production to catch issues early**\n",
    "\n",
    "### üéØ What to Monitor?\n",
    "\n",
    "#### 1Ô∏è‚É£ **Model Performance**\n",
    "- Accuracy, precision, recall\n",
    "- Latency (prediction time)\n",
    "- Throughput (requests/second)\n",
    "\n",
    "#### 2Ô∏è‚É£ **Data Drift**\n",
    "**Problem:** Input data distribution changes over time\n",
    "\n",
    "```\n",
    "Training data:        Production data (6 months later):\n",
    "Age: 25-35           Age: 45-55  ‚Üê DRIFT!\n",
    "Income: $50k         Income: $80k ‚Üê DRIFT!\n",
    "```\n",
    "\n",
    "**Detection:**\n",
    "- Statistical tests (KS test, Chi-square)\n",
    "- Distribution comparison\n",
    "- Feature drift monitoring\n",
    "\n",
    "#### 3Ô∏è‚É£ **Concept Drift**\n",
    "**Problem:** Relationship between features and target changes\n",
    "\n",
    "```\n",
    "Before: \"Buy\" button clicks ‚Üí High conversion\n",
    "After: \"Buy\" button clicks ‚Üí Low conversion  ‚Üê CONCEPT DRIFT!\n",
    "(User behavior changed)\n",
    "```\n",
    "\n",
    "#### 4Ô∏è‚É£ **Prediction Drift**\n",
    "**Problem:** Model outputs change distribution\n",
    "\n",
    "```\n",
    "Week 1: 50% positive predictions\n",
    "Week 2: 90% positive predictions ‚Üê DRIFT!\n",
    "```\n",
    "\n",
    "### üö® When to Alert?\n",
    "\n",
    "**Trigger alerts when:**\n",
    "- ‚ùå Accuracy drops > 5%\n",
    "- ‚ùå Data drift detected (p-value < 0.05)\n",
    "- ‚ùå Latency increases > 2x\n",
    "- ‚ùå Error rate spikes\n",
    "- ‚ùå Prediction distribution shifts significantly\n",
    "\n",
    "### üõ†Ô∏è Monitoring Tools:\n",
    "\n",
    "**Open Source:**\n",
    "- **Evidently AI** - drift detection\n",
    "- **Great Expectations** - data validation\n",
    "- **Prometheus + Grafana** - metrics\n",
    "\n",
    "**Commercial:**\n",
    "- **WhyLabs** - data logging\n",
    "- **Arize AI** - observability\n",
    "- **Fiddler** - explainability\n",
    "\n",
    "Let's implement monitoring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitoring-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model monitoring with Evidently\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset, DataQualityPreset\n",
    "\n",
    "print(\"üìà Model Monitoring Demo\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create reference and current datasets\n",
    "# Reference: Original training data\n",
    "# Current: Simulated production data (with drift)\n",
    "\n",
    "reference_data = pd.DataFrame(X_train, columns=[f'feature_{i}' for i in range(X_train.shape[1])])\n",
    "reference_data['target'] = y_train\n",
    "\n",
    "# Simulate production data with drift\n",
    "# Add some noise to simulate distribution shift\n",
    "X_prod = X_test + np.random.normal(0, 0.5, X_test.shape)\n",
    "current_data = pd.DataFrame(X_prod, columns=[f'feature_{i}' for i in range(X_prod.shape[1])])\n",
    "current_data['target'] = y_test\n",
    "\n",
    "print(\"üìä Creating Drift Report...\\n\")\n",
    "\n",
    "# Create drift report\n",
    "report = Report(metrics=[\n",
    "    DataDriftPreset(),\n",
    "    DataQualityPreset()\n",
    "])\n",
    "\n",
    "report.run(\n",
    "    reference_data=reference_data,\n",
    "    current_data=current_data,\n",
    "    column_mapping=None\n",
    ")\n",
    "\n",
    "# Save report\n",
    "report.save_html('drift_report.html')\n",
    "\n",
    "print(\"‚úÖ Drift report generated!\")\n",
    "print(\"   üìÑ Saved to: drift_report.html\")\n",
    "print(\"\\nüí° Open the HTML file to see:\")\n",
    "print(\"   - Data drift detection\")\n",
    "print(\"   - Feature-by-feature analysis\")\n",
    "print(\"   - Distribution comparisons\")\n",
    "print(\"   - Data quality metrics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom monitoring: Track performance over time\n",
    "\n",
    "print(\"üìä Performance Monitoring Over Time\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulate model performance over time\n",
    "import datetime\n",
    "\n",
    "# Generate time series of model performance\n",
    "dates = pd.date_range(start='2024-01-01', periods=30, freq='D')\n",
    "\n",
    "# Simulate degrading performance (model rot)\n",
    "np.random.seed(42)\n",
    "base_accuracy = 0.85\n",
    "degradation = np.linspace(0, -0.15, 30)  # Gradual degradation\n",
    "noise = np.random.normal(0, 0.02, 30)  # Daily variance\n",
    "accuracy_over_time = base_accuracy + degradation + noise\n",
    "\n",
    "# Create monitoring dataframe\n",
    "monitoring_df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'accuracy': accuracy_over_time,\n",
    "    'predictions_count': np.random.randint(100, 1000, 30),\n",
    "    'avg_latency_ms': np.random.normal(50, 10, 30)\n",
    "})\n",
    "\n",
    "# Plot monitoring dashboard\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Accuracy over time\n",
    "axes[0].plot(monitoring_df['date'], monitoring_df['accuracy'], marker='o', linewidth=2, color='blue')\n",
    "axes[0].axhline(y=0.80, color='red', linestyle='--', label='Alert Threshold (80%)')\n",
    "axes[0].axhline(y=0.85, color='green', linestyle='--', label='Target (85%)')\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight degradation period\n",
    "degraded_dates = monitoring_df[monitoring_df['accuracy'] < 0.80]['date']\n",
    "if len(degraded_dates) > 0:\n",
    "    axes[0].axvspan(degraded_dates.min(), degraded_dates.max(), \n",
    "                    alpha=0.2, color='red', label='Degraded Performance')\n",
    "\n",
    "# Plot 2: Prediction volume\n",
    "axes[1].bar(monitoring_df['date'], monitoring_df['predictions_count'], \n",
    "            color='skyblue', alpha=0.7)\n",
    "axes[1].set_ylabel('Predictions Count', fontsize=12)\n",
    "axes[1].set_title('Daily Prediction Volume', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Latency\n",
    "axes[2].plot(monitoring_df['date'], monitoring_df['avg_latency_ms'], \n",
    "            marker='s', linewidth=2, color='orange')\n",
    "axes[2].axhline(y=100, color='red', linestyle='--', label='SLA Limit (100ms)')\n",
    "axes[2].set_ylabel('Latency (ms)', fontsize=12)\n",
    "axes[2].set_xlabel('Date', fontsize=12)\n",
    "axes[2].set_title('Average Prediction Latency', fontsize=14, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('monitoring_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print alerts\n",
    "print(\"\\nüö® Monitoring Alerts:\\n\")\n",
    "\n",
    "# Check for accuracy degradation\n",
    "low_accuracy_days = monitoring_df[monitoring_df['accuracy'] < 0.80]\n",
    "if len(low_accuracy_days) > 0:\n",
    "    print(f\"‚ö†Ô∏è  ALERT: Accuracy below 80% on {len(low_accuracy_days)} days\")\n",
    "    print(f\"   First occurrence: {low_accuracy_days['date'].min().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Lowest accuracy: {low_accuracy_days['accuracy'].min():.2%}\")\n",
    "    print(f\"   üîß Action: Retrain model with recent data\")\n",
    "else:\n",
    "    print(\"‚úÖ Accuracy within acceptable range\")\n",
    "\n",
    "# Check for latency issues\n",
    "high_latency_days = monitoring_df[monitoring_df['avg_latency_ms'] > 100]\n",
    "if len(high_latency_days) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  ALERT: High latency on {len(high_latency_days)} days\")\n",
    "    print(f\"   Max latency: {monitoring_df['avg_latency_ms'].max():.1f}ms\")\n",
    "    print(f\"   üîß Action: Optimize model or scale infrastructure\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Latency within SLA\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° In production, set up automated alerts:\")\n",
    "print(\"   - Email/Slack when accuracy drops\")\n",
    "print(\"   - PagerDuty for critical failures\")\n",
    "print(\"   - Auto-trigger retraining pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab-testing",
   "metadata": {},
   "source": [
    "## üî¨ Step 4: A/B Testing ML Models\n",
    "\n",
    "**Validate model improvements in production with real users**\n",
    "\n",
    "### üéØ What is A/B Testing?\n",
    "\n",
    "**Compare two models with real traffic:**\n",
    "\n",
    "```\n",
    "Users\n",
    "  ‚Üì\n",
    "  50% ‚Üí Model A (current)\n",
    "  50% ‚Üí Model B (new)\n",
    "  ‚Üì\n",
    "Measure: Conversion, engagement, accuracy\n",
    "  ‚Üì\n",
    "Winner ‚Üí 100% traffic\n",
    "```\n",
    "\n",
    "### üèóÔ∏è A/B Testing Strategies:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Random Split**\n",
    "- 50/50 split\n",
    "- Simple and fair\n",
    "- Fast results\n",
    "\n",
    "#### 2Ô∏è‚É£ **Canary Deployment**\n",
    "```\n",
    "5% ‚Üí New model (canary)\n",
    "95% ‚Üí Old model (stable)\n",
    "```\n",
    "- Lower risk\n",
    "- Gradual rollout\n",
    "\n",
    "#### 3Ô∏è‚É£ **Multi-Armed Bandit**\n",
    "- Dynamic allocation\n",
    "- More traffic to better model\n",
    "- Faster convergence\n",
    "\n",
    "### üìä What to Measure?\n",
    "\n",
    "**Technical Metrics:**\n",
    "- Accuracy, precision, recall\n",
    "- Latency, throughput\n",
    "- Error rate\n",
    "\n",
    "**Business Metrics:**\n",
    "- Click-through rate (CTR)\n",
    "- Conversion rate\n",
    "- Revenue per user\n",
    "- User engagement\n",
    "\n",
    "### ‚úÖ Statistical Significance:\n",
    "\n",
    "**Don't declare winner too early!**\n",
    "\n",
    "```python\n",
    "# Need enough samples\n",
    "p_value < 0.05  # 95% confidence\n",
    "sample_size > 1000  # Minimum\n",
    "```\n",
    "\n",
    "### üéØ A/B Testing Best Practices:\n",
    "\n",
    "‚úÖ **Define success metrics** before testing  \n",
    "‚úÖ **Calculate required sample size**  \n",
    "‚úÖ **Run for sufficient time** (1-2 weeks)  \n",
    "‚úÖ **Check for statistical significance**  \n",
    "‚úÖ **Monitor both groups** for differences  \n",
    "‚úÖ **Have rollback plan** ready  \n",
    "\n",
    "Let's implement A/B testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab-test-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Testing simulation\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "print(\"üî¨ A/B Testing Simulation\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulate two models\n",
    "print(\"Training two models...\\n\")\n",
    "\n",
    "# Model A: Current (baseline)\n",
    "model_a = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model_a.fit(X_train, y_train)\n",
    "\n",
    "# Model B: New (challenger)\n",
    "model_b = GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "model_b.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Models trained!\")\n",
    "\n",
    "# Simulate A/B test with production data\n",
    "print(\"\\nüé≤ Simulating A/B Test...\\n\")\n",
    "\n",
    "# Randomly assign users to groups\n",
    "n_samples = len(X_test)\n",
    "np.random.seed(42)\n",
    "assignments = np.random.choice(['A', 'B'], size=n_samples, p=[0.5, 0.5])\n",
    "\n",
    "# Make predictions\n",
    "results_a = []\n",
    "results_b = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    if assignments[i] == 'A':\n",
    "        pred = model_a.predict([X_test[i]])[0]\n",
    "        correct = (pred == y_test[i])\n",
    "        results_a.append(correct)\n",
    "    else:\n",
    "        pred = model_b.predict([X_test[i]])[0]\n",
    "        correct = (pred == y_test[i])\n",
    "        results_b.append(correct)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_a = np.mean(results_a)\n",
    "accuracy_b = np.mean(results_b)\n",
    "\n",
    "print(\"üìä A/B Test Results:\\n\")\n",
    "print(f\"Model A (Baseline):\")\n",
    "print(f\"   Samples: {len(results_a)}\")\n",
    "print(f\"   Accuracy: {accuracy_a:.4f} ({accuracy_a:.2%})\")\n",
    "\n",
    "print(f\"\\nModel B (Challenger):\")\n",
    "print(f\"   Samples: {len(results_b)}\")\n",
    "print(f\"   Accuracy: {accuracy_b:.4f} ({accuracy_b:.2%})\")\n",
    "\n",
    "# Statistical significance test\n",
    "print(\"\\nüî¨ Statistical Significance Test:\\n\")\n",
    "\n",
    "# Chi-square test\n",
    "contingency_table = [\n",
    "    [sum(results_a), len(results_a) - sum(results_a)],  # A: correct, incorrect\n",
    "    [sum(results_b), len(results_b) - sum(results_b)]   # B: correct, incorrect\n",
    "]\n",
    "\n",
    "chi2, p_value = stats.chi2_contingency(contingency_table)[:2]\n",
    "\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Determine winner\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "if p_value < alpha:\n",
    "    if accuracy_b > accuracy_a:\n",
    "        improvement = (accuracy_b - accuracy_a) / accuracy_a * 100\n",
    "        print(f\"\\nüèÜ WINNER: Model B!\")\n",
    "        print(f\"   ‚úÖ Statistically significant (p < {alpha})\")\n",
    "        print(f\"   üìà Improvement: {improvement:.2f}%\")\n",
    "        print(f\"\\nüöÄ Recommendation: Deploy Model B to production\")\n",
    "    else:\n",
    "        print(f\"\\nüèÜ WINNER: Model A (current)\")\n",
    "        print(f\"   ‚úÖ Statistically significant (p < {alpha})\")\n",
    "        print(f\"\\n‚ö†Ô∏è  Recommendation: Keep Model A, don't deploy B\")\n",
    "else:\n",
    "    print(f\"\\nü§î INCONCLUSIVE\")\n",
    "    print(f\"   ‚ùå Not statistically significant (p >= {alpha})\")\n",
    "    print(f\"\\nüìä Recommendation:\")\n",
    "    print(f\"      - Collect more data\")\n",
    "    print(f\"      - Run test longer\")\n",
    "    print(f\"      - Increase sample size\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab-test-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize A/B test results\n",
    "\n",
    "print(\"üìä Visualizing A/B Test Results\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "models = ['Model A\\n(Baseline)', 'Model B\\n(Challenger)']\n",
    "accuracies = [accuracy_a, accuracy_b]\n",
    "colors = ['lightblue', 'lightgreen']\n",
    "\n",
    "bars = axes[0].bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('A/B Test: Model Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{acc:.2%}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add winner indicator\n",
    "if p_value < alpha:\n",
    "    winner_idx = 1 if accuracy_b > accuracy_a else 0\n",
    "    bars[winner_idx].set_edgecolor('gold')\n",
    "    bars[winner_idx].set_linewidth(3)\n",
    "    axes[0].text(winner_idx, accuracies[winner_idx] + 0.05, 'üëë WINNER',\n",
    "                ha='center', fontsize=14, fontweight='bold', color='gold')\n",
    "\n",
    "# Plot 2: Sample distribution\n",
    "sizes = [len(results_a), len(results_b)]\n",
    "axes[1].pie(sizes, labels=models, autopct='%1.1f%%',\n",
    "           colors=colors, startangle=90, textprops={'fontsize': 12})\n",
    "axes[1].set_title('Traffic Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ab_test_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ A/B test visualization created!\")\n",
    "print(\"   üìÑ Saved to: ab_test_results.png\")\n",
    "print(\"\\nüí° In production:\")\n",
    "print(\"   - Run for 1-2 weeks minimum\")\n",
    "print(\"   - Monitor both technical AND business metrics\")\n",
    "print(\"   - Have automatic rollback if B performs worse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-pipeline",
   "metadata": {},
   "source": [
    "## üè≠ Real AI Example: Complete MLOps Pipeline\n",
    "\n",
    "**End-to-end MLOps workflow from training to production**\n",
    "\n",
    "This example demonstrates a production-ready MLOps pipeline with:\n",
    "- Automated experiment tracking\n",
    "- Model versioning and registry\n",
    "- Performance monitoring\n",
    "- Automated retraining triggers\n",
    "- A/B testing framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlops-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete MLOps Pipeline\n",
    "\n",
    "class MLOpsPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready MLOps pipeline\n",
    "    \n",
    "    Features:\n",
    "    - Experiment tracking with MLflow\n",
    "    - Model versioning\n",
    "    - Performance monitoring\n",
    "    - Automated retraining\n",
    "    - A/B testing support\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        self.performance_threshold = 0.80\n",
    "        self.production_model = None\n",
    "        \n",
    "    def train_and_track(self, model, params, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train model with full MLflow tracking\"\"\"\n",
    "        \n",
    "        with mlflow.start_run() as run:\n",
    "            # Log parameters\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_param('model_type', type(model).__name__)\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "                'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "                'f1_score': f1_score(y_test, y_pred, average='weighted')\n",
    "            }\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metrics(metrics)\n",
    "            \n",
    "            # Log model\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "            \n",
    "            return run.info.run_id, metrics\n",
    "    \n",
    "    def check_performance(self, metrics):\n",
    "        \"\"\"Check if model meets performance threshold\"\"\"\n",
    "        \n",
    "        if metrics['accuracy'] < self.performance_threshold:\n",
    "            print(f\"‚ö†Ô∏è  ALERT: Accuracy {metrics['accuracy']:.2%} below threshold {self.performance_threshold:.2%}\")\n",
    "            print(\"üîß Triggering retraining...\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def deploy_with_ab_test(self, new_model, current_model, X_test, y_test, traffic_split=0.5):\n",
    "        \"\"\"Deploy new model with A/B testing\"\"\"\n",
    "        \n",
    "        print(\"\\nüî¨ Starting A/B Test Deployment...\\n\")\n",
    "        \n",
    "        # Random assignment\n",
    "        assignments = np.random.choice(['A', 'B'], size=len(X_test), \n",
    "                                      p=[traffic_split, 1-traffic_split])\n",
    "        \n",
    "        # Track performance\n",
    "        results_a = []\n",
    "        results_b = []\n",
    "        \n",
    "        for i in range(len(X_test)):\n",
    "            if assignments[i] == 'A':\n",
    "                pred = current_model.predict([X_test[i]])[0]\n",
    "                results_a.append(pred == y_test[i])\n",
    "            else:\n",
    "                pred = new_model.predict([X_test[i]])[0]\n",
    "                results_b.append(pred == y_test[i])\n",
    "        \n",
    "        acc_a = np.mean(results_a)\n",
    "        acc_b = np.mean(results_b)\n",
    "        \n",
    "        # Statistical test\n",
    "        contingency = [\n",
    "            [sum(results_a), len(results_a) - sum(results_a)],\n",
    "            [sum(results_b), len(results_b) - sum(results_b)]\n",
    "        ]\n",
    "        _, p_value = stats.chi2_contingency(contingency)[:2]\n",
    "        \n",
    "        # Decision\n",
    "        if p_value < 0.05 and acc_b > acc_a:\n",
    "            print(f\"‚úÖ New model wins!\")\n",
    "            print(f\"   Current: {acc_a:.2%}\")\n",
    "            print(f\"   New: {acc_b:.2%}\")\n",
    "            print(f\"   Improvement: {(acc_b-acc_a)/acc_a*100:.2f}%\")\n",
    "            print(f\"\\nüöÄ Deploying new model to production...\")\n",
    "            self.production_model = new_model\n",
    "            return 'new'\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Keeping current model\")\n",
    "            print(f\"   Current: {acc_a:.2%}\")\n",
    "            print(f\"   New: {acc_b:.2%}\")\n",
    "            return 'current'\n",
    "\n",
    "# Demo the pipeline\n",
    "print(\"üè≠ Complete MLOps Pipeline Demo\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = MLOpsPipeline(\"production_pipeline\")\n",
    "\n",
    "# Train initial model\n",
    "print(\"\\n1Ô∏è‚É£ Training initial model...\\n\")\n",
    "model_v1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "run_id_v1, metrics_v1 = pipeline.train_and_track(\n",
    "    model_v1, \n",
    "    {'n_estimators': 100}, \n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "print(f\"‚úÖ Model v1 trained: Accuracy = {metrics_v1['accuracy']:.2%}\")\n",
    "\n",
    "# Check performance\n",
    "print(\"\\n2Ô∏è‚É£ Monitoring performance...\\n\")\n",
    "pipeline.check_performance(metrics_v1)\n",
    "\n",
    "# Train improved model\n",
    "print(\"\\n3Ô∏è‚É£ Training improved model...\\n\")\n",
    "model_v2 = GradientBoostingClassifier(n_estimators=150, random_state=42)\n",
    "run_id_v2, metrics_v2 = pipeline.train_and_track(\n",
    "    model_v2,\n",
    "    {'n_estimators': 150},\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "print(f\"‚úÖ Model v2 trained: Accuracy = {metrics_v2['accuracy']:.2%}\")\n",
    "\n",
    "# A/B test deployment\n",
    "print(\"\\n4Ô∏è‚É£ A/B Testing Deployment...\")\n",
    "winner = pipeline.deploy_with_ab_test(model_v2, model_v1, X_test, y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüéâ MLOps Pipeline Complete!\")\n",
    "print(\"\\nüí° In production, this pipeline would:\")\n",
    "print(\"   ‚úÖ Run automatically on schedule\")\n",
    "print(\"   ‚úÖ Monitor performance continuously\")\n",
    "print(\"   ‚úÖ Trigger retraining when needed\")\n",
    "print(\"   ‚úÖ A/B test before full deployment\")\n",
    "print(\"   ‚úÖ Rollback automatically if issues detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercises\n",
    "\n",
    "**Practice your MLOps skills!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### Exercise 1: Build Your MLOps Pipeline\n",
    "\n",
    "**Task:** Create a complete MLOps pipeline for a classification task\n",
    "\n",
    "**Requirements:**\n",
    "1. Use MLflow to track 5+ experiments\n",
    "2. Try different models and hyperparameters\n",
    "3. Register the best model\n",
    "4. Create a monitoring dashboard\n",
    "5. Implement A/B testing\n",
    "\n",
    "**Dataset suggestions:**\n",
    "- Iris classification\n",
    "- Breast cancer detection\n",
    "- Wine quality prediction\n",
    "\n",
    "**Bonus:** Add automated retraining triggers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# TODO: Load dataset\n",
    "# from sklearn.datasets import load_iris\n",
    "\n",
    "# TODO: Set up MLflow experiment\n",
    "# mlflow.set_experiment(\"my_mlops_pipeline\")\n",
    "\n",
    "# TODO: Train multiple models\n",
    "# models = [RandomForestClassifier, GradientBoostingClassifier, ...]\n",
    "\n",
    "# TODO: Track all experiments\n",
    "# for model in models:\n",
    "#     with mlflow.start_run():\n",
    "#         ...\n",
    "\n",
    "# TODO: Register best model\n",
    "# mlflow.sklearn.log_model(..., registered_model_name=\"my_model\")\n",
    "\n",
    "# TODO: Create monitoring dashboard\n",
    "\n",
    "# TODO: Implement A/B testing\n",
    "\n",
    "print(\"Complete the exercise above!\")\n",
    "print(\"\\nHints:\")\n",
    "print(\"1. Use sklearn.datasets for quick datasets\")\n",
    "print(\"2. Track at least: params, metrics, model\")\n",
    "print(\"3. Compare models using MLflow UI\")\n",
    "print(\"4. Use matplotlib for monitoring visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Drift Detection\n",
    "\n",
    "**Task:** Build a drift detection system\n",
    "\n",
    "**Requirements:**\n",
    "1. Create reference dataset (training data)\n",
    "2. Generate current dataset (with artificial drift)\n",
    "3. Use Evidently to detect drift\n",
    "4. Create custom drift metrics\n",
    "5. Set up automated alerts\n",
    "\n",
    "**Bonus:** Implement automatic retraining when drift detected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-2-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# TODO: Create reference data\n",
    "# reference_data = ...\n",
    "\n",
    "# TODO: Simulate drift (add noise, shift distribution)\n",
    "# current_data = reference_data + noise\n",
    "\n",
    "# TODO: Use Evidently for drift detection\n",
    "# from evidently.report import Report\n",
    "# from evidently.metric_preset import DataDriftPreset\n",
    "\n",
    "# TODO: Create custom drift metrics\n",
    "# def calculate_ks_statistic(ref, curr):\n",
    "#     ...\n",
    "\n",
    "# TODO: Set alert thresholds\n",
    "# if drift_detected:\n",
    "#     send_alert()\n",
    "#     trigger_retraining()\n",
    "\n",
    "print(\"Complete the exercise above!\")\n",
    "print(\"\\nLibraries to explore:\")\n",
    "print(\"- evidently (drift detection)\")\n",
    "print(\"- scipy.stats (statistical tests)\")\n",
    "print(\"- alibi-detect (advanced drift detection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéâ Key Takeaways\n",
    "\n",
    "**Congratulations! You've mastered MLOps best practices!**\n",
    "\n",
    "### 1Ô∏è‚É£ **Experiment Tracking (MLflow)**\n",
    "   - ‚úÖ Track parameters, metrics, artifacts\n",
    "   - ‚úÖ Compare 100s of experiments easily\n",
    "   - ‚úÖ Reproduce any experiment\n",
    "   - **Use when:** Training any ML model (always!)\n",
    "\n",
    "### 2Ô∏è‚É£ **Model Versioning**\n",
    "   - ‚úÖ Version control for models\n",
    "   - ‚úÖ Stage-based deployment (staging ‚Üí production)\n",
    "   - ‚úÖ Easy rollback to previous versions\n",
    "   - **Use when:** Deploying to production (essential!)\n",
    "\n",
    "### 3Ô∏è‚É£ **Model Monitoring**\n",
    "   - ‚úÖ Track performance over time\n",
    "   - ‚úÖ Detect data and concept drift\n",
    "   - ‚úÖ Automated alerts for issues\n",
    "   - **Use when:** Models in production (mandatory!)\n",
    "\n",
    "### 4Ô∏è‚É£ **A/B Testing**\n",
    "   - ‚úÖ Validate improvements with real traffic\n",
    "   - ‚úÖ Statistical significance testing\n",
    "   - ‚úÖ Safe, gradual rollouts\n",
    "   - **Use when:** Deploying model updates\n",
    "\n",
    "---\n",
    "\n",
    "## üåü MLOps Maturity Progression\n",
    "\n",
    "**Your journey:**\n",
    "\n",
    "```\n",
    "Level 0: Manual\n",
    "  ‚Üì\n",
    "Level 1: Basic tracking (‚úÖ You are here after this lesson!)\n",
    "  ‚Üì\n",
    "Level 2: Automated pipelines\n",
    "  ‚Üì\n",
    "Level 3: Full MLOps (production-ready)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Production MLOps Stack (2024-2025)\n",
    "\n",
    "**Recommended tools:**\n",
    "\n",
    "| Component | Tool | Why |\n",
    "|-----------|------|-----|\n",
    "| **Tracking** | MLflow | Industry standard, open-source |\n",
    "| **Versioning** | MLflow Registry | Integrated with tracking |\n",
    "| **Monitoring** | Evidently + Grafana | Open-source, powerful |\n",
    "| **Orchestration** | Airflow / Prefect | Workflow automation |\n",
    "| **Feature Store** | Feast | Open-source |\n",
    "| **Model Serving** | FastAPI + Docker | Fast, scalable |\n",
    "| **CI/CD** | GitHub Actions | Free, integrated |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ MLOps Checklist\n",
    "\n",
    "**Before deploying to production:**\n",
    "\n",
    "- [ ] All experiments tracked in MLflow\n",
    "- [ ] Model versioned in registry\n",
    "- [ ] Performance monitoring in place\n",
    "- [ ] Drift detection configured\n",
    "- [ ] Alerts set up (email/Slack/PagerDuty)\n",
    "- [ ] A/B testing framework ready\n",
    "- [ ] Rollback procedure documented\n",
    "- [ ] Retraining pipeline automated\n",
    "- [ ] Data versioning (DVC)\n",
    "- [ ] Model documentation complete\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**Continue your MLOps journey:**\n",
    "\n",
    "1. **Day 3: Cloud Deployment**\n",
    "   - Deploy to AWS, GCP, Azure\n",
    "   - Serverless ML\n",
    "   - Hugging Face Spaces\n",
    "   - Streamlit apps\n",
    "\n",
    "2. **Advanced MLOps Topics:**\n",
    "   - Feature stores (Feast)\n",
    "   - Model serving at scale (KServe)\n",
    "   - ML pipelines (Kubeflow, Vertex AI)\n",
    "   - Data versioning (DVC)\n",
    "\n",
    "3. **Practice:**\n",
    "   - Build end-to-end MLOps pipeline\n",
    "   - Contribute to open-source MLOps tools\n",
    "   - Set up CI/CD for ML project\n",
    "\n",
    "---\n",
    "\n",
    "**üí¨ Final Thoughts:**\n",
    "\n",
    "*\"MLOps is not optional - it's the difference between a demo and a production system. You now have the skills to build reliable, maintainable ML systems that companies actually use. MLflow + monitoring + A/B testing is the minimum viable MLOps stack for 2024-2025. Master these, and you're ready for ML engineering roles!\"*\n",
    "\n",
    "**üéâ Day 2 Complete! Tomorrow: Cloud Deployment! üöÄ**\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Additional Resources:**\n",
    "- MLflow Docs: https://mlflow.org\n",
    "- Evidently AI: https://evidentlyai.com\n",
    "- Made With ML (MLOps): https://madewithml.com\n",
    "- ML-Ops.org: https://ml-ops.org\n",
    "- Google's MLOps Guide: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning\n",
    "\n",
    "**Keep building! üåü**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
