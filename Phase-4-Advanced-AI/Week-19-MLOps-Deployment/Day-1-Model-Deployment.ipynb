{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 1: ML Model Deployment\n",
    "\n",
    "**üéØ Goal:** Master deploying machine learning models to production\n",
    "\n",
    "**‚è±Ô∏è Time:** 120-150 minutes\n",
    "\n",
    "**üåü Why This Matters for AI (2024-2025):**\n",
    "- A model is USELESS if it can't be deployed - deployment is where value is created\n",
    "- 87% of ML projects never make it to production - learn deployment to be in the 13%!\n",
    "- Flask and FastAPI are THE industry standards for ML APIs\n",
    "- Docker is essential for reproducible, scalable deployments\n",
    "- Every AI company deploys models as REST APIs for real-time predictions\n",
    "- Deployment skills separate hobbyists from professional ML engineers\n",
    "\n",
    "**What You'll Build Today:**\n",
    "1. **Save and load ML models** using joblib and pickle\n",
    "2. **Build a Flask API** for ML model serving\n",
    "3. **Create a FastAPI service** for production-grade deployment\n",
    "4. **Containerize with Docker** for reproducibility\n",
    "5. **Deploy a real sentiment analysis API** from scratch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deployment-landscape",
   "metadata": {},
   "source": [
    "## üåç ML Deployment Landscape (2024-2025)\n",
    "\n",
    "**From Jupyter Notebook to Production!**\n",
    "\n",
    "### üéØ Deployment Strategies:\n",
    "\n",
    "#### üîß **1. Real-Time (Online) Serving**\n",
    "\n",
    "**What:** Instant predictions on-demand via API\n",
    "\n",
    "**How:**\n",
    "- REST API (Flask, FastAPI)\n",
    "- gRPC for high performance\n",
    "- WebSocket for streaming\n",
    "\n",
    "**Use Cases:**\n",
    "- Chatbots (immediate responses)\n",
    "- Recommendation systems (real-time suggestions)\n",
    "- Fraud detection (instant decision)\n",
    "- Image classification (upload and classify)\n",
    "\n",
    "**Pros:** Immediate results, interactive  \n",
    "**Cons:** Higher latency, more expensive\n",
    "\n",
    "#### üì¶ **2. Batch Prediction**\n",
    "\n",
    "**What:** Process large datasets periodically\n",
    "\n",
    "**How:**\n",
    "- Scheduled jobs (Airflow, cron)\n",
    "- Spark for big data\n",
    "- Cloud batch services\n",
    "\n",
    "**Use Cases:**\n",
    "- Email campaigns (score all users nightly)\n",
    "- Risk assessment (monthly credit scoring)\n",
    "- Report generation (weekly forecasts)\n",
    "\n",
    "**Pros:** Efficient, cheaper  \n",
    "**Cons:** Not real-time\n",
    "\n",
    "#### ‚ö° **3. Edge Deployment**\n",
    "\n",
    "**What:** Run models on devices (phones, IoT)\n",
    "\n",
    "**How:**\n",
    "- TensorFlow Lite\n",
    "- ONNX Runtime\n",
    "- CoreML (iOS)\n",
    "\n",
    "**Use Cases:**\n",
    "- Mobile apps (face recognition)\n",
    "- IoT sensors (anomaly detection)\n",
    "- Autonomous vehicles\n",
    "\n",
    "**Pros:** Ultra-fast, privacy  \n",
    "**Cons:** Limited compute\n",
    "\n",
    "#### üåê **4. Serverless**\n",
    "\n",
    "**What:** Pay-per-request, auto-scaling\n",
    "\n",
    "**How:**\n",
    "- AWS Lambda\n",
    "- Google Cloud Functions\n",
    "- Azure Functions\n",
    "\n",
    "**Use Cases:**\n",
    "- Sporadic requests\n",
    "- Microservices\n",
    "- Event-driven ML\n",
    "\n",
    "**Pros:** No servers, auto-scale  \n",
    "**Cons:** Cold starts, limits\n",
    "\n",
    "### üìä Choosing a Strategy:\n",
    "\n",
    "| Need | Strategy | Why |\n",
    "|------|----------|-----|\n",
    "| **Instant results** | Real-time API | User waiting |\n",
    "| **Million predictions** | Batch | Efficient |\n",
    "| **Low latency** | Edge | On-device |\n",
    "| **Variable traffic** | Serverless | Auto-scale |\n",
    "| **Chat/interactive** | Real-time | Conversational |\n",
    "| **Cost-sensitive** | Batch | Cheaper |\n",
    "\n",
    "**Today's focus: Real-time APIs** (most common deployment pattern)\n",
    "\n",
    "Let's build!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup & Installation\n",
    "\n",
    "**Install required libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "\n",
    "# Core ML libraries\n",
    "!{sys.executable} -m pip install scikit-learn numpy pandas joblib --quiet\n",
    "\n",
    "# Web frameworks\n",
    "!{sys.executable} -m pip install flask fastapi uvicorn[standard] requests --quiet\n",
    "\n",
    "# For text processing\n",
    "!{sys.executable} -m pip install nltk transformers torch --quiet\n",
    "\n",
    "# Visualization\n",
    "!{sys.executable} -m pip install matplotlib seaborn --quiet\n",
    "\n",
    "print(\"‚úÖ Libraries installed successfully!\")\n",
    "print(\"\\nüí° Docker installation: https://docs.docker.com/get-docker/\")\n",
    "print(\"   (Docker is optional for this notebook but recommended for production)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")\n",
    "print(\"üöÄ Ready to deploy ML models!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-saving",
   "metadata": {},
   "source": [
    "## üíæ Step 1: Model Serialization (Saving & Loading)\n",
    "\n",
    "**Before deployment, you need to SAVE your trained model!**\n",
    "\n",
    "### üéØ Why Save Models?\n",
    "\n",
    "- ‚úÖ **No re-training**: Train once, deploy many times\n",
    "- ‚úÖ **Version control**: Save different model versions\n",
    "- ‚úÖ **Reproducibility**: Exact same predictions\n",
    "- ‚úÖ **Sharing**: Share models with team/production\n",
    "\n",
    "### üìä Saving Methods:\n",
    "\n",
    "| Method | Library | Use Case |\n",
    "|--------|---------|----------|\n",
    "| **Joblib** | scikit-learn | Sklearn models (recommended) |\n",
    "| **Pickle** | Python built-in | Any Python object |\n",
    "| **SavedModel** | TensorFlow | TF/Keras models |\n",
    "| **torch.save** | PyTorch | PyTorch models |\n",
    "| **ONNX** | Cross-platform | Framework-agnostic |\n",
    "\n",
    "**Best practice: Use joblib for sklearn (faster for large numpy arrays)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a simple sentiment analysis model\n",
    "\n",
    "print(\"üéØ Training Sentiment Analysis Model\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample dataset (in production, use real datasets like IMDB)\n",
    "texts = [\n",
    "    \"I love this product! It's amazing!\",\n",
    "    \"Terrible experience, very disappointed.\",\n",
    "    \"Great quality and fast shipping!\",\n",
    "    \"Worst purchase ever, don't buy.\",\n",
    "    \"Absolutely fantastic, highly recommend!\",\n",
    "    \"Poor quality, broke after one day.\",\n",
    "    \"Exceeded my expectations, wonderful!\",\n",
    "    \"Waste of money, completely useless.\",\n",
    "    \"Best decision ever, love it!\",\n",
    "    \"Horrible customer service and product.\",\n",
    "    \"Outstanding quality, will buy again!\",\n",
    "    \"Not worth the price, very bad.\",\n",
    "    \"Perfect! Exactly what I needed.\",\n",
    "    \"Disappointing and overpriced.\",\n",
    "    \"Amazing product, works perfectly!\",\n",
    "    \"Awful quality, returned immediately.\",\n",
    "    \"Superb! Better than expected.\",\n",
    "    \"Garbage product, total scam.\",\n",
    "    \"Incredible value, very satisfied!\",\n",
    "    \"Terrible quality, broke quickly.\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset:\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Vectorizer created:\")\n",
    "print(f\"   Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ Model trained!\")\n",
    "print(f\"   Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Test predictions\n",
    "print(f\"\\nüß™ Test Predictions:\\n\")\n",
    "test_samples = [\n",
    "    \"This is awesome!\",\n",
    "    \"I hate this product\",\n",
    "    \"Pretty good, I like it\"\n",
    "]\n",
    "\n",
    "for text in test_samples:\n",
    "    vec = vectorizer.transform([text])\n",
    "    pred = model.predict(vec)[0]\n",
    "    prob = model.predict_proba(vec)[0]\n",
    "    sentiment = \"Positive üòä\" if pred == 1 else \"Negative üòû\"\n",
    "    print(f\"{sentiment} ({prob[pred]:.2%} confidence)\")\n",
    "    print(f\"   Text: \\\"{text}\\\"\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° Now let's SAVE this model for deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and vectorizer using joblib\n",
    "\n",
    "print(\"üíæ Saving Model & Vectorizer\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create models directory\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save with joblib (recommended for sklearn)\n",
    "joblib.dump(model, 'models/sentiment_model.joblib')\n",
    "joblib.dump(vectorizer, 'models/vectorizer.joblib')\n",
    "\n",
    "print(\"‚úÖ Saved with joblib:\")\n",
    "print(\"   üìÑ models/sentiment_model.joblib\")\n",
    "print(\"   üìÑ models/vectorizer.joblib\")\n",
    "\n",
    "# Save with pickle (alternative method)\n",
    "with open('models/sentiment_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open('models/vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "print(\"\\n‚úÖ Also saved with pickle:\")\n",
    "print(\"   üìÑ models/sentiment_model.pkl\")\n",
    "print(\"   üìÑ models/vectorizer.pkl\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_type': 'LogisticRegression',\n",
    "    'accuracy': float(accuracy),\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'num_features': len(vectorizer.vocabulary_),\n",
    "    'classes': ['negative', 'positive']\n",
    "}\n",
    "\n",
    "with open('models/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Metadata saved:\")\n",
    "print(\"   üìÑ models/metadata.json\")\n",
    "\n",
    "# Check file sizes\n",
    "model_size = os.path.getsize('models/sentiment_model.joblib') / 1024\n",
    "vec_size = os.path.getsize('models/vectorizer.joblib') / 1024\n",
    "\n",
    "print(f\"\\nüìä File Sizes:\")\n",
    "print(f\"   Model: {model_size:.2f} KB\")\n",
    "print(f\"   Vectorizer: {vec_size:.2f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° Models saved! Now they can be loaded in production without re-training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and test (simulating production environment)\n",
    "\n",
    "print(\"üìÇ Loading Saved Model\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load model and vectorizer\n",
    "loaded_model = joblib.load('models/sentiment_model.joblib')\n",
    "loaded_vectorizer = joblib.load('models/vectorizer.joblib')\n",
    "\n",
    "print(\"‚úÖ Model and vectorizer loaded!\")\n",
    "\n",
    "# Load metadata\n",
    "with open('models/metadata.json', 'r') as f:\n",
    "    loaded_metadata = json.load(f)\n",
    "\n",
    "print(\"\\nüìã Model Metadata:\")\n",
    "for key, value in loaded_metadata.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Test loaded model\n",
    "print(\"\\nüß™ Testing Loaded Model:\\n\")\n",
    "\n",
    "test_texts = [\n",
    "    \"This is the best thing ever!\",\n",
    "    \"Absolutely terrible, very bad\",\n",
    "    \"It's okay, nothing special\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    vec = loaded_vectorizer.transform([text])\n",
    "    pred = loaded_model.predict(vec)[0]\n",
    "    prob = loaded_model.predict_proba(vec)[0]\n",
    "    \n",
    "    sentiment = loaded_metadata['classes'][pred]\n",
    "    emoji = \"üòä\" if pred == 1 else \"üòû\"\n",
    "    \n",
    "    print(f\"{emoji} {sentiment.upper()} ({prob[pred]:.2%})\")\n",
    "    print(f\"   Text: \\\"{text}\\\"\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Loaded model works perfectly!\")\n",
    "print(\"\\nüí° This is EXACTLY how models are loaded in production APIs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flask-intro",
   "metadata": {},
   "source": [
    "## üåê Step 2: Flask API for ML Models\n",
    "\n",
    "**Flask = Simple, lightweight web framework for Python**\n",
    "\n",
    "### üéØ Why Flask?\n",
    "\n",
    "‚úÖ **Easy to learn**: Minimal boilerplate  \n",
    "‚úÖ **Lightweight**: Perfect for simple APIs  \n",
    "‚úÖ **Widely used**: Huge community  \n",
    "‚úÖ **Flexible**: Add only what you need  \n",
    "\n",
    "### üèóÔ∏è Flask ML API Architecture:\n",
    "\n",
    "```\n",
    "Client (Browser/App)\n",
    "       ‚Üì\n",
    "   POST /predict\n",
    "   {\"text\": \"I love this!\"}\n",
    "       ‚Üì\n",
    "   Flask API\n",
    "       ‚Üì\n",
    "   1. Load model\n",
    "   2. Preprocess input\n",
    "   3. Make prediction\n",
    "   4. Return JSON response\n",
    "       ‚Üì\n",
    "   {\"sentiment\": \"positive\", \"confidence\": 0.95}\n",
    "```\n",
    "\n",
    "### üìù Key Components:\n",
    "\n",
    "**1. Routes:** Define API endpoints  \n",
    "**2. Request handling:** Parse incoming data  \n",
    "**3. Model inference:** Load model, predict  \n",
    "**4. Response:** Return JSON results  \n",
    "\n",
    "Let's build a Flask API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flask-app",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Flask app for sentiment analysis\n",
    "# Save this as 'app.py' to run: python app.py\n",
    "\n",
    "flask_app_code = '''\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model and vectorizer at startup (once)\n",
    "print(\"Loading model...\")\n",
    "model = joblib.load('models/sentiment_model.joblib')\n",
    "vectorizer = joblib.load('models/vectorizer.joblib')\n",
    "with open('models/metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Health check endpoint\n",
    "@app.route('/', methods=['GET'])\n",
    "def home():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'online',\n",
    "        'model': metadata['model_type'],\n",
    "        'version': '1.0',\n",
    "        'message': 'Sentiment Analysis API is running!'\n",
    "    })\n",
    "\n",
    "# Prediction endpoint\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"Predict sentiment from text\"\"\"\n",
    "    try:\n",
    "        # Get input data\n",
    "        data = request.get_json()\n",
    "        \n",
    "        # Validate input\n",
    "        if 'text' not in data:\n",
    "            return jsonify({'error': 'Missing \"text\" field'}), 400\n",
    "        \n",
    "        text = data['text']\n",
    "        \n",
    "        if not text or not isinstance(text, str):\n",
    "            return jsonify({'error': 'Invalid text input'}), 400\n",
    "        \n",
    "        # Preprocess and predict\n",
    "        text_vec = vectorizer.transform([text])\n",
    "        prediction = model.predict(text_vec)[0]\n",
    "        probabilities = model.predict_proba(text_vec)[0]\n",
    "        \n",
    "        # Format response\n",
    "        sentiment = metadata['classes'][prediction]\n",
    "        confidence = float(probabilities[prediction])\n",
    "        \n",
    "        return jsonify({\n",
    "            'text': text,\n",
    "            'sentiment': sentiment,\n",
    "            'confidence': confidence,\n",
    "            'probabilities': {\n",
    "                'negative': float(probabilities[0]),\n",
    "                'positive': float(probabilities[1])\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "# Batch prediction endpoint\n",
    "@app.route('/predict_batch', methods=['POST'])\n",
    "def predict_batch():\n",
    "    \"\"\"Predict sentiment for multiple texts\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        if 'texts' not in data:\n",
    "            return jsonify({'error': 'Missing \"texts\" field'}), 400\n",
    "        \n",
    "        texts = data['texts']\n",
    "        \n",
    "        if not isinstance(texts, list):\n",
    "            return jsonify({'error': '\"texts\" must be a list'}), 400\n",
    "        \n",
    "        # Process all texts\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            text_vec = vectorizer.transform([text])\n",
    "            prediction = model.predict(text_vec)[0]\n",
    "            probabilities = model.predict_proba(text_vec)[0]\n",
    "            \n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'sentiment': metadata['classes'][prediction],\n",
    "                'confidence': float(probabilities[prediction])\n",
    "            })\n",
    "        \n",
    "        return jsonify({'predictions': results})\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run app\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True)\n",
    "'''\n",
    "\n",
    "# Save Flask app to file\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(flask_app_code)\n",
    "\n",
    "print(\"üìù Flask App Created!\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Saved to: app.py\")\n",
    "print(\"\\nüöÄ To run the Flask API:\")\n",
    "print(\"   1. python app.py\")\n",
    "print(\"   2. API will run on http://localhost:5000\")\n",
    "print(\"\\nüì° API Endpoints:\")\n",
    "print(\"   GET  /           - Health check\")\n",
    "print(\"   POST /predict    - Single prediction\")\n",
    "print(\"   POST /predict_batch - Batch predictions\")\n",
    "print(\"\\nüí° Example request:\")\n",
    "print('''   curl -X POST http://localhost:5000/predict \\\\''')\n",
    "print('''        -H \"Content-Type: application/json\" \\\\''')\n",
    "print('''        -d '{\"text\": \"I love this product!\"}\\'\\n''')\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-flask",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Flask API (using requests library)\n",
    "# Note: This assumes Flask app is running on localhost:5000\n",
    "\n",
    "import requests\n",
    "\n",
    "print(\"üß™ Testing Flask API\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ö†Ô∏è  Make sure Flask app is running: python app.py\")\n",
    "print(\"\\nIf the app is running, uncomment the code below to test:\\n\")\n",
    "\n",
    "test_code = '''\n",
    "# Test health check\n",
    "response = requests.get('http://localhost:5000/')\n",
    "print(\"‚úÖ Health Check:\")\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n",
    "# Test single prediction\n",
    "response = requests.post(\n",
    "    'http://localhost:5000/predict',\n",
    "    json={'text': 'This product is amazing!'}\n",
    ")\n",
    "print(\"\\n‚úÖ Single Prediction:\")\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n",
    "# Test batch prediction\n",
    "response = requests.post(\n",
    "    'http://localhost:5000/predict_batch',\n",
    "    json={'texts': [\n",
    "        'I love this!',\n",
    "        'This is terrible',\n",
    "        'Pretty good product'\n",
    "    ]}\n",
    ")\n",
    "print(\"\\n‚úÖ Batch Prediction:\")\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "'''\n",
    "\n",
    "print(\"# \" + \"\\n# \".join(test_code.split(\"\\n\")))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° Flask is great for prototypes, but FastAPI is better for production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fastapi-intro",
   "metadata": {},
   "source": [
    "## ‚ö° Step 3: FastAPI for Production\n",
    "\n",
    "**FastAPI = Modern, high-performance web framework**\n",
    "\n",
    "### üéØ Why FastAPI? (The 2024-2025 Standard)\n",
    "\n",
    "‚úÖ **FAST**: 3-4x faster than Flask  \n",
    "‚úÖ **Auto docs**: Swagger UI built-in  \n",
    "‚úÖ **Type hints**: Automatic validation  \n",
    "‚úÖ **Async**: Handle concurrent requests  \n",
    "‚úÖ **Modern**: Based on Python 3.7+ features  \n",
    "\n",
    "### üìä Flask vs FastAPI:\n",
    "\n",
    "| Feature | Flask | FastAPI |\n",
    "|---------|-------|----------|\n",
    "| **Speed** | Moderate | Very Fast |\n",
    "| **Documentation** | Manual | Auto-generated |\n",
    "| **Validation** | Manual | Automatic |\n",
    "| **Async** | Limited | Full support |\n",
    "| **Learning Curve** | Easy | Moderate |\n",
    "| **Best For** | Simple APIs | Production APIs |\n",
    "\n",
    "**In 2024-2025: FastAPI is the industry standard for ML APIs!**\n",
    "\n",
    "### üåü FastAPI Features:\n",
    "\n",
    "**1. Automatic Documentation**\n",
    "- Swagger UI at `/docs`\n",
    "- ReDoc at `/redoc`\n",
    "- Interactive testing\n",
    "\n",
    "**2. Type Validation**\n",
    "- Pydantic models\n",
    "- Automatic type checking\n",
    "- Better error messages\n",
    "\n",
    "**3. Performance**\n",
    "- Async/await support\n",
    "- Fast JSON serialization\n",
    "- Production-ready\n",
    "\n",
    "Let's build a FastAPI service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fastapi-app",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FastAPI app for sentiment analysis\n",
    "# Save this as 'main.py' to run: uvicorn main:app --reload\n",
    "\n",
    "fastapi_app_code = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Sentiment Analysis API\",\n",
    "    description=\"Real-time sentiment analysis using ML\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Load model at startup\n",
    "print(\"üöÄ Loading ML model...\")\n",
    "model = joblib.load('models/sentiment_model.joblib')\n",
    "vectorizer = joblib.load('models/vectorizer.joblib')\n",
    "with open('models/metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Pydantic models for request/response validation\n",
    "class PredictionRequest(BaseModel):\n",
    "    text: str = Field(..., min_length=1, description=\"Text to analyze\")\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"text\": \"I love this product!\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    text: str\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "    probabilities: dict\n",
    "    timestamp: str\n",
    "\n",
    "class BatchRequest(BaseModel):\n",
    "    texts: List[str] = Field(..., min_items=1, description=\"List of texts\")\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"texts\": [\"Great product!\", \"Terrible quality\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "class BatchResponse(BaseModel):\n",
    "    predictions: List[PredictionResponse]\n",
    "    count: int\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Health check and API information\"\"\"\n",
    "    return {\n",
    "        \"status\": \"online\",\n",
    "        \"model\": metadata['model_type'],\n",
    "        \"accuracy\": metadata['accuracy'],\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"endpoints\": {\n",
    "            \"docs\": \"/docs\",\n",
    "            \"predict\": \"/predict\",\n",
    "            \"batch\": \"/predict_batch\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Single prediction endpoint\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    \"\"\"Predict sentiment for a single text\"\"\"\n",
    "    try:\n",
    "        # Vectorize input\n",
    "        text_vec = vectorizer.transform([request.text])\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(text_vec)[0]\n",
    "        probabilities = model.predict_proba(text_vec)[0]\n",
    "        \n",
    "        # Format response\n",
    "        return PredictionResponse(\n",
    "            text=request.text,\n",
    "            sentiment=metadata['classes'][prediction],\n",
    "            confidence=float(probabilities[prediction]),\n",
    "            probabilities={\n",
    "                'negative': float(probabilities[0]),\n",
    "                'positive': float(probabilities[1])\n",
    "            },\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Batch prediction endpoint\n",
    "@app.post(\"/predict_batch\", response_model=BatchResponse)\n",
    "async def predict_batch(request: BatchRequest):\n",
    "    \"\"\"Predict sentiment for multiple texts\"\"\"\n",
    "    try:\n",
    "        predictions = []\n",
    "        \n",
    "        for text in request.texts:\n",
    "            text_vec = vectorizer.transform([text])\n",
    "            prediction = model.predict(text_vec)[0]\n",
    "            probabilities = model.predict_proba(text_vec)[0]\n",
    "            \n",
    "            predictions.append(\n",
    "                PredictionResponse(\n",
    "                    text=text,\n",
    "                    sentiment=metadata['classes'][prediction],\n",
    "                    confidence=float(probabilities[prediction]),\n",
    "                    probabilities={\n",
    "                        'negative': float(probabilities[0]),\n",
    "                        'positive': float(probabilities[1])\n",
    "                    },\n",
    "                    timestamp=datetime.now().isoformat()\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return BatchResponse(\n",
    "            predictions=predictions,\n",
    "            count=len(predictions)\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Model info endpoint\n",
    "@app.get(\"/model_info\")\n",
    "async def model_info():\n",
    "    \"\"\"Get model metadata and statistics\"\"\"\n",
    "    return metadata\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "# Save FastAPI app to file\n",
    "with open('main.py', 'w') as f:\n",
    "    f.write(fastapi_app_code)\n",
    "\n",
    "print(\"üìù FastAPI App Created!\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Saved to: main.py\")\n",
    "print(\"\\nüöÄ To run the FastAPI service:\")\n",
    "print(\"   uvicorn main:app --reload\")\n",
    "print(\"\\nüì° API will run on: http://localhost:8000\")\n",
    "print(\"üìö Auto-generated docs: http://localhost:8000/docs\")\n",
    "print(\"üìñ Alternative docs: http://localhost:8000/redoc\")\n",
    "print(\"\\nüåü FastAPI Features:\")\n",
    "print(\"   ‚úÖ Interactive Swagger UI at /docs\")\n",
    "print(\"   ‚úÖ Automatic request validation\")\n",
    "print(\"   ‚úÖ Type hints and error handling\")\n",
    "print(\"   ‚úÖ Production-ready performance\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "docker-intro",
   "metadata": {},
   "source": [
    "## üê≥ Step 4: Docker Containerization\n",
    "\n",
    "**Docker = Package your app + dependencies into a container**\n",
    "\n",
    "### üéØ Why Docker?\n",
    "\n",
    "‚úÖ **\"It works on my machine\"** ‚Üí \"It works everywhere\"  \n",
    "‚úÖ **Reproducible**: Same environment every time  \n",
    "‚úÖ **Portable**: Run anywhere (local, cloud, edge)  \n",
    "‚úÖ **Isolated**: No dependency conflicts  \n",
    "‚úÖ **Scalable**: Easy to replicate and scale  \n",
    "\n",
    "### üèóÔ∏è Docker Concepts:\n",
    "\n",
    "**1. Image**: Blueprint for container  \n",
    "**2. Container**: Running instance of image  \n",
    "**3. Dockerfile**: Instructions to build image  \n",
    "**4. Registry**: Store and share images (Docker Hub)  \n",
    "\n",
    "### üì¶ ML Deployment with Docker:\n",
    "\n",
    "```\n",
    "Dockerfile\n",
    "    ‚Üì\n",
    "Docker Build\n",
    "    ‚Üì\n",
    "Docker Image (app + model + dependencies)\n",
    "    ‚Üì\n",
    "Docker Run\n",
    "    ‚Üì\n",
    "Container (isolated, reproducible environment)\n",
    "```\n",
    "\n",
    "### üåü Production Benefits:\n",
    "\n",
    "- **Development**: Same as production\n",
    "- **Testing**: Isolated test environment\n",
    "- **Deployment**: Push to cloud (AWS, GCP, Azure)\n",
    "- **Scaling**: Spin up multiple containers\n",
    "- **Updates**: Replace containers, zero downtime\n",
    "\n",
    "Let's dockerize our FastAPI app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dockerfile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile for FastAPI app\n",
    "\n",
    "dockerfile_content = '''\n",
    "# Use official Python runtime as base image\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory in container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements file\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY main.py .\n",
    "COPY models/ ./models/\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\\\n",
    "    CMD curl -f http://localhost:8000/ || exit 1\n",
    "\n",
    "# Run FastAPI with uvicorn\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "with open('Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content.strip())\n",
    "\n",
    "print(\"üìù Dockerfile Created!\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create requirements.txt\n",
    "requirements = '''\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "pydantic==2.5.0\n",
    "scikit-learn==1.3.2\n",
    "joblib==1.3.2\n",
    "numpy==1.24.3\n",
    "'''\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements.strip())\n",
    "\n",
    "print(\"\\n‚úÖ Files created:\")\n",
    "print(\"   üìÑ Dockerfile\")\n",
    "print(\"   üìÑ requirements.txt\")\n",
    "\n",
    "# Create .dockerignore\n",
    "dockerignore = '''\n",
    "__pycache__\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    ".Python\n",
    "env/\n",
    "venv/\n",
    ".git\n",
    ".gitignore\n",
    "*.ipynb\n",
    ".ipynb_checkpoints\n",
    "*.md\n",
    "'''\n",
    "\n",
    "with open('.dockerignore', 'w') as f:\n",
    "    f.write(dockerignore.strip())\n",
    "\n",
    "print(\"   üìÑ .dockerignore\")\n",
    "\n",
    "# Create docker-compose.yml for easy deployment\n",
    "docker_compose = '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  sentiment-api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - PYTHONUNBUFFERED=1\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/\"]\n",
    "      interval: 30s\n",
    "      timeout: 3s\n",
    "      retries: 3\n",
    "'''\n",
    "\n",
    "with open('docker-compose.yml', 'w') as f:\n",
    "    f.write(docker_compose.strip())\n",
    "\n",
    "print(\"   üìÑ docker-compose.yml\")\n",
    "\n",
    "print(\"\\nüê≥ Docker Setup Complete!\")\n",
    "print(\"\\nüìã Docker Commands:\")\n",
    "print(\"\\n1Ô∏è‚É£ Build Docker image:\")\n",
    "print(\"   docker build -t sentiment-api .\")\n",
    "print(\"\\n2Ô∏è‚É£ Run container:\")\n",
    "print(\"   docker run -p 8000:8000 sentiment-api\")\n",
    "print(\"\\n3Ô∏è‚É£ Or use docker-compose:\")\n",
    "print(\"   docker-compose up\")\n",
    "print(\"\\n4Ô∏è‚É£ Stop container:\")\n",
    "print(\"   docker-compose down\")\n",
    "print(\"\\n5Ô∏è‚É£ Push to Docker Hub:\")\n",
    "print(\"   docker tag sentiment-api username/sentiment-api:v1\")\n",
    "print(\"   docker push username/sentiment-api:v1\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° Now your API can run ANYWHERE that supports Docker!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-example",
   "metadata": {},
   "source": [
    "## üéØ Real AI Example: Complete Deployment Pipeline\n",
    "\n",
    "**Let's deploy a real sentiment analysis model using transformers!**\n",
    "\n",
    "This example shows a production-ready deployment using:\n",
    "- Pre-trained BERT model from HuggingFace\n",
    "- FastAPI for the API\n",
    "- Docker for containerization\n",
    "- Proper error handling and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production-api",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready FastAPI app with HuggingFace model\n",
    "\n",
    "production_app = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import pipeline\n",
    "from typing import List, Optional\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Production Sentiment Analysis API\",\n",
    "    description=\"Real-time sentiment analysis using BERT\",\n",
    "    version=\"2.0.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\"\n",
    ")\n",
    "\n",
    "# Load model at startup (cached)\n",
    "logger.info(\"Loading transformer model...\")\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=-1  # Use CPU (-1) or GPU (0)\n",
    ")\n",
    "logger.info(\"Model loaded successfully!\")\n",
    "\n",
    "# Request/Response models\n",
    "class SentimentRequest(BaseModel):\n",
    "    text: str = Field(..., min_length=1, max_length=512)\n",
    "    \n",
    "class SentimentResponse(BaseModel):\n",
    "    text: str\n",
    "    label: str\n",
    "    score: float\n",
    "    processing_time: float\n",
    "    timestamp: str\n",
    "\n",
    "class BatchRequest(BaseModel):\n",
    "    texts: List[str] = Field(..., min_items=1, max_items=100)\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str\n",
    "    model: str\n",
    "    version: str\n",
    "    uptime: str\n",
    "\n",
    "# Startup time\n",
    "START_TIME = time.time()\n",
    "\n",
    "@app.get(\"/\", response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    uptime = time.time() - START_TIME\n",
    "    return HealthResponse(\n",
    "        status=\"healthy\",\n",
    "        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        version=\"2.0.0\",\n",
    "        uptime=f\"{uptime:.2f}s\"\n",
    "    )\n",
    "\n",
    "@app.post(\"/analyze\", response_model=SentimentResponse)\n",
    "async def analyze_sentiment(request: SentimentRequest):\n",
    "    \"\"\"Analyze sentiment of text\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run inference\n",
    "        result = sentiment_pipeline(request.text)[0]\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        logger.info(f\"Processed: {request.text[:50]}... ({processing_time:.3f}s)\")\n",
    "        \n",
    "        return SentimentResponse(\n",
    "            text=request.text,\n",
    "            label=result['label'],\n",
    "            score=result['score'],\n",
    "            processing_time=processing_time,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/analyze_batch\")\n",
    "async def analyze_batch(request: BatchRequest):\n",
    "    \"\"\"Analyze multiple texts\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Batch inference\n",
    "        results = sentiment_pipeline(request.texts)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        responses = [\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"label\": result['label'],\n",
    "                \"score\": result['score']\n",
    "            }\n",
    "            for text, result in zip(request.texts, results)\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"results\": responses,\n",
    "            \"count\": len(responses),\n",
    "            \"processing_time\": processing_time,\n",
    "            \"avg_time_per_text\": processing_time / len(request.texts)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Batch error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def get_metrics():\n",
    "    \"\"\"Get API metrics\"\"\"\n",
    "    return {\n",
    "        \"uptime_seconds\": time.time() - START_TIME,\n",
    "        \"model_loaded\": True,\n",
    "        \"status\": \"operational\"\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "'''\n",
    "\n",
    "with open('production_app.py', 'w') as f:\n",
    "    f.write(production_app.strip())\n",
    "\n",
    "print(\"üè≠ Production-Ready API Created!\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Saved to: production_app.py\")\n",
    "print(\"\\nüåü Features:\")\n",
    "print(\"   ‚úÖ Uses pre-trained BERT model\")\n",
    "print(\"   ‚úÖ Comprehensive logging\")\n",
    "print(\"   ‚úÖ Error handling\")\n",
    "print(\"   ‚úÖ Performance metrics\")\n",
    "print(\"   ‚úÖ Batch processing\")\n",
    "print(\"   ‚úÖ Health checks\")\n",
    "print(\"\\nüöÄ Run with: uvicorn production_app:app --reload\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercises\n",
    "\n",
    "**Practice your deployment skills!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### Exercise 1: Deploy Your Own Model\n",
    "\n",
    "**Task:** Train and deploy a classification model\n",
    "\n",
    "**Requirements:**\n",
    "1. Train a model (any sklearn classifier)\n",
    "2. Save it using joblib\n",
    "3. Create a FastAPI endpoint\n",
    "4. Add input validation\n",
    "5. Test with sample data\n",
    "\n",
    "**Model ideas:**\n",
    "- Iris flower classification\n",
    "- Spam detection\n",
    "- Price prediction\n",
    "\n",
    "**Bonus:** Containerize with Docker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# TODO: Train your model\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# TODO: Save your model\n",
    "# joblib.dump(model, 'my_model.joblib')\n",
    "\n",
    "# TODO: Create FastAPI app\n",
    "# (Create a new .py file with FastAPI code)\n",
    "\n",
    "print(\"Complete the exercise above!\")\n",
    "print(\"\\nHints:\")\n",
    "print(\"1. Use a dataset from sklearn.datasets\")\n",
    "print(\"2. Train any classifier (RandomForest, SVM, etc.)\")\n",
    "print(\"3. Follow the FastAPI pattern from earlier examples\")\n",
    "print(\"4. Test your API with curl or requests library\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### Exercise 2: Add Features to API\n",
    "\n",
    "**Task:** Enhance the sentiment API with additional features\n",
    "\n",
    "**Add these features:**\n",
    "1. **Rate limiting**: Limit requests per minute\n",
    "2. **Caching**: Cache recent predictions\n",
    "3. **Logging**: Log all predictions to file\n",
    "4. **Authentication**: Add API key validation\n",
    "5. **Metrics**: Track request count, avg latency\n",
    "\n",
    "**Bonus:** Add CORS for frontend integration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-2-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Example: Add rate limiting\n",
    "'''\n",
    "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
    "from slowapi.util import get_remote_address\n",
    "from slowapi.errors import RateLimitExceeded\n",
    "\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "app.state.limiter = limiter\n",
    "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "@limiter.limit(\"10/minute\")\n",
    "async def predict(request: Request, data: PredictionRequest):\n",
    "    # Your code here\n",
    "    pass\n",
    "'''\n",
    "\n",
    "print(\"Complete the exercise above!\")\n",
    "print(\"\\nLibraries to explore:\")\n",
    "print(\"- slowapi (rate limiting)\")\n",
    "print(\"- cachetools (caching)\")\n",
    "print(\"- python-multipart (file uploads)\")\n",
    "print(\"- fastapi.middleware.cors (CORS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéâ Key Takeaways\n",
    "\n",
    "**Congratulations! You've mastered ML model deployment!**\n",
    "\n",
    "### 1Ô∏è‚É£ **Model Serialization**\n",
    "   - ‚úÖ Save models with joblib/pickle\n",
    "   - ‚úÖ Version control for models\n",
    "   - ‚úÖ Save metadata for reproducibility\n",
    "   - **Use when:** Moving models from training to production\n",
    "\n",
    "### 2Ô∏è‚É£ **Flask APIs**\n",
    "   - ‚úÖ Simple and lightweight\n",
    "   - ‚úÖ Great for prototypes\n",
    "   - ‚úÖ Easy to learn\n",
    "   - **Use when:** Building MVP or simple APIs\n",
    "\n",
    "### 3Ô∏è‚É£ **FastAPI (Recommended)**\n",
    "   - ‚úÖ Production-ready performance\n",
    "   - ‚úÖ Auto-generated documentation\n",
    "   - ‚úÖ Type validation with Pydantic\n",
    "   - ‚úÖ Async support\n",
    "   - **Use when:** Building production APIs (always!)\n",
    "\n",
    "### 4Ô∏è‚É£ **Docker Containers**\n",
    "   - ‚úÖ Reproducible environments\n",
    "   - ‚úÖ Easy deployment anywhere\n",
    "   - ‚úÖ Isolation and security\n",
    "   - **Use when:** Deploying to production (essential!)\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Real-World Impact\n",
    "\n",
    "**Skills you can apply immediately:**\n",
    "\n",
    "### üíº **Career Skills**\n",
    "- Deploy ML models as REST APIs\n",
    "- Build production-ready FastAPI services\n",
    "- Containerize applications with Docker\n",
    "- Design scalable ML architectures\n",
    "\n",
    "### üèóÔ∏è **Deployment Patterns**\n",
    "\n",
    "**1. Simple Deployment**\n",
    "```\n",
    "Train Model ‚Üí Save ‚Üí FastAPI ‚Üí Heroku/Railway\n",
    "```\n",
    "\n",
    "**2. Production Deployment**\n",
    "```\n",
    "Train ‚Üí Save ‚Üí Docker ‚Üí AWS ECS/K8s ‚Üí Load Balancer\n",
    "```\n",
    "\n",
    "**3. Serverless**\n",
    "```\n",
    "Train ‚Üí Save ‚Üí AWS Lambda ‚Üí API Gateway\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Best Practices\n",
    "\n",
    "### ‚úÖ **DO:**\n",
    "- Use FastAPI for new projects\n",
    "- Containerize with Docker\n",
    "- Add health check endpoints\n",
    "- Log predictions for monitoring\n",
    "- Version your models\n",
    "- Validate inputs thoroughly\n",
    "- Handle errors gracefully\n",
    "- Test before deploying\n",
    "\n",
    "### ‚ùå **DON'T:**\n",
    "- Deploy without testing\n",
    "- Ignore error handling\n",
    "- Skip input validation\n",
    "- Use Flask for large-scale production\n",
    "- Hard-code configurations\n",
    "- Forget about monitoring\n",
    "- Deploy without versioning\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**Continue your deployment journey:**\n",
    "\n",
    "1. **Day 2: MLOps Best Practices**\n",
    "   - Model versioning with MLflow\n",
    "   - Experiment tracking\n",
    "   - Model monitoring\n",
    "   - A/B testing\n",
    "\n",
    "2. **Day 3: Cloud Deployment**\n",
    "   - AWS, GCP, Azure deployment\n",
    "   - Serverless ML\n",
    "   - Hugging Face Spaces\n",
    "   - Streamlit apps\n",
    "\n",
    "3. **Practice Projects:**\n",
    "   - Deploy image classifier API\n",
    "   - Build recommendation system API\n",
    "   - Create chatbot API\n",
    "   - Deploy NLP models\n",
    "\n",
    "---\n",
    "\n",
    "**üí¨ Final Thoughts:**\n",
    "\n",
    "*\"A model that isn't deployed creates ZERO value. You now have the skills to take ML models from Jupyter notebooks to production APIs that can serve millions of users. FastAPI + Docker is the modern standard for ML deployment in 2024-2025. Master these tools, and you'll be job-ready for ML engineering roles!\"*\n",
    "\n",
    "**üéâ Day 1 Complete! Tomorrow: MLOps Best Practices! üöÄ**\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Additional Resources:**\n",
    "- FastAPI Docs: https://fastapi.tiangolo.com\n",
    "- Docker Docs: https://docs.docker.com\n",
    "- HuggingFace Model Hub: https://huggingface.co/models\n",
    "- Deployment Guide: https://ml-ops.org\n",
    "\n",
    "**Keep deploying! üåü**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
