{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 3: Cloud Deployment & Production ML\n",
    "\n",
    "**üéØ Goal:** Deploy ML models to the cloud and build production applications\n",
    "\n",
    "**‚è±Ô∏è Time:** 120-150 minutes\n",
    "\n",
    "**üåü Why This Matters for AI (2024-2025):**\n",
    "- Cloud deployment is THE standard - 95% of ML models run in the cloud\n",
    "- AWS, GCP, and Azure are the platforms companies actually use\n",
    "- Serverless ML (Lambda, Cloud Functions) enables pay-per-use scaling\n",
    "- Hugging Face Spaces lets you deploy for FREE with millions of users\n",
    "- Streamlit is THE fastest way to build ML demos that impress employers\n",
    "- Model serving at scale separates hobbyists from production engineers\n",
    "- Every AI job posting mentions cloud deployment experience\n",
    "\n",
    "**What You'll Build Today:**\n",
    "1. **Cloud deployment concepts** for AWS, GCP, and Azure\n",
    "2. **Serverless ML** with AWS Lambda and Cloud Functions\n",
    "3. **Hugging Face Spaces** deployment (real, live deployment!)\n",
    "4. **Streamlit ML app** with interactive UI\n",
    "5. **Production-ready model serving** at scale\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloud-landscape",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Cloud ML Deployment Landscape (2024-2025)\n",
    "\n",
    "**The cloud is where ML lives in production**\n",
    "\n",
    "### üéØ Major Cloud Providers:\n",
    "\n",
    "#### 1Ô∏è‚É£ **AWS (Amazon Web Services)**\n",
    "**Market leader: 32% of cloud market**\n",
    "\n",
    "**Key Services:**\n",
    "- **SageMaker**: Full ML platform (training + deployment)\n",
    "- **Lambda**: Serverless compute (pay per request)\n",
    "- **EC2**: Virtual machines (full control)\n",
    "- **ECS/EKS**: Container orchestration\n",
    "- **S3**: Model and data storage\n",
    "\n",
    "**Best for:** Enterprise, mature ML teams  \n",
    "**Pros:** Most features, largest ecosystem  \n",
    "**Cons:** Complex, expensive, steep learning curve  \n",
    "\n",
    "#### 2Ô∏è‚É£ **GCP (Google Cloud Platform)**\n",
    "**Google's cloud: 11% market share, best for ML**\n",
    "\n",
    "**Key Services:**\n",
    "- **Vertex AI**: Unified ML platform (TensorFlow native)\n",
    "- **Cloud Functions**: Serverless (like Lambda)\n",
    "- **Cloud Run**: Serverless containers\n",
    "- **GKE**: Kubernetes (easiest K8s)\n",
    "- **BigQuery ML**: SQL-based ML\n",
    "\n",
    "**Best for:** ML/AI workloads, startups  \n",
    "**Pros:** Best ML tools, cheaper, easier  \n",
    "**Cons:** Smaller than AWS  \n",
    "\n",
    "#### 3Ô∏è‚É£ **Azure (Microsoft)**\n",
    "**Microsoft's cloud: 23% market share**\n",
    "\n",
    "**Key Services:**\n",
    "- **Azure ML**: ML platform (like SageMaker)\n",
    "- **Azure Functions**: Serverless\n",
    "- **Azure Container Instances**: Easy containers\n",
    "- **AKS**: Kubernetes\n",
    "- **Cognitive Services**: Pre-built AI APIs\n",
    "\n",
    "**Best for:** Enterprises (especially .NET/Microsoft shops)  \n",
    "**Pros:** Great integration with Microsoft tools  \n",
    "**Cons:** Less ML-focused than GCP  \n",
    "\n",
    "### üìä Comparison:\n",
    "\n",
    "| Feature | AWS | GCP | Azure |\n",
    "|---------|-----|-----|-------|\n",
    "| **Market Share** | 32% | 11% | 23% |\n",
    "| **ML Platform** | SageMaker | Vertex AI | Azure ML |\n",
    "| **Serverless** | Lambda | Cloud Functions | Functions |\n",
    "| **Best For** | Enterprise | ML/AI | Microsoft shops |\n",
    "| **Free Tier** | ‚úÖ 12 months | ‚úÖ Always free | ‚úÖ 12 months |\n",
    "| **Ease of Use** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **ML Features** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "### üåü Alternative Platforms:\n",
    "\n",
    "**Specialized ML Platforms:**\n",
    "- **Hugging Face Spaces**: FREE ML app hosting (we'll use this!)\n",
    "- **Replicate**: Easy model deployment\n",
    "- **Modal**: Serverless for ML\n",
    "- **Gradio + Hugging Face**: Interactive ML demos\n",
    "\n",
    "**General Platforms:**\n",
    "- **Heroku**: Easy deployment (acquired by Salesforce)\n",
    "- **Railway**: Modern Heroku alternative\n",
    "- **Fly.io**: Global edge deployment\n",
    "- **Render**: Simple cloud platform\n",
    "\n",
    "### üèóÔ∏è Deployment Architectures:\n",
    "\n",
    "#### **Simple:**\n",
    "```\n",
    "User ‚Üí FastAPI (Railway/Render) ‚Üí Model\n",
    "```\n",
    "\n",
    "#### **Serverless:**\n",
    "```\n",
    "User ‚Üí API Gateway ‚Üí Lambda ‚Üí Model (S3)\n",
    "```\n",
    "\n",
    "#### **Production:**\n",
    "```\n",
    "User ‚Üí Load Balancer ‚Üí [Container 1, Container 2, ...] ‚Üí Model\n",
    "      ‚Üì\n",
    "   Auto-scaling, monitoring, logging\n",
    "```\n",
    "\n",
    "Let's deploy to the cloud!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "\n",
    "# Core ML libraries\n",
    "!{sys.executable} -m pip install scikit-learn numpy pandas joblib --quiet\n",
    "\n",
    "# Web frameworks\n",
    "!{sys.executable} -m pip install streamlit gradio --quiet\n",
    "\n",
    "# Hugging Face\n",
    "!{sys.executable} -m pip install transformers torch huggingface-hub --quiet\n",
    "\n",
    "# Cloud SDKs (optional - install only if you have accounts)\n",
    "# !{sys.executable} -m pip install boto3 google-cloud-aiplatform azure-ai-ml --quiet\n",
    "\n",
    "# Deployment helpers\n",
    "!{sys.executable} -m pip install requests plotly --quiet\n",
    "\n",
    "print(\"‚úÖ Libraries installed successfully!\")\n",
    "print(\"\\nüì¶ Installed:\")\n",
    "print(\"   - Streamlit (interactive apps)\")\n",
    "print(\"   - Gradio (ML interfaces)\")\n",
    "print(\"   - Hugging Face Hub\")\n",
    "print(\"\\nüöÄ Ready for cloud deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.datasets import load_iris, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")\n",
    "print(\"‚òÅÔ∏è  Ready to deploy to the cloud!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serverless-intro",
   "metadata": {},
   "source": [
    "## ‚ö° Step 1: Serverless ML Deployment\n",
    "\n",
    "**Serverless = Pay only for what you use, auto-scale infinitely**\n",
    "\n",
    "### üéØ What is Serverless?\n",
    "\n",
    "**Traditional:**\n",
    "```\n",
    "You manage: Servers, scaling, patching, monitoring\n",
    "Cost: $50-500/month (even with 0 users)\n",
    "```\n",
    "\n",
    "**Serverless:**\n",
    "```\n",
    "Cloud manages: Everything!\n",
    "You write: Just the code\n",
    "Cost: $0 with 0 users, scales automatically\n",
    "```\n",
    "\n",
    "### üèóÔ∏è Serverless Architecture:\n",
    "\n",
    "```\n",
    "User Request\n",
    "    ‚Üì\n",
    "API Gateway (routes requests)\n",
    "    ‚Üì\n",
    "Lambda Function (your code)\n",
    "    ‚Üì\n",
    "Load model from S3/Cloud Storage\n",
    "    ‚Üì\n",
    "Make prediction\n",
    "    ‚Üì\n",
    "Return JSON response\n",
    "```\n",
    "\n",
    "### ‚ö° Serverless Platforms:\n",
    "\n",
    "| Platform | Provider | Free Tier | Best For |\n",
    "|----------|----------|-----------|----------|\n",
    "| **AWS Lambda** | Amazon | 1M requests/mo | Enterprise, full control |\n",
    "| **Cloud Functions** | Google | 2M requests/mo | Easier, better for ML |\n",
    "| **Azure Functions** | Microsoft | 1M requests/mo | Microsoft ecosystem |\n",
    "| **Modal** | Modal Labs | Generous | ML-specific |\n",
    "| **Replicate** | Replicate | Pay-as-go | Pre-built models |\n",
    "\n",
    "### üìä Serverless Pros & Cons:\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Zero cost at zero usage\n",
    "- ‚úÖ Infinite auto-scaling\n",
    "- ‚úÖ No server management\n",
    "- ‚úÖ Pay per request\n",
    "- ‚úÖ Built-in redundancy\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Cold starts (first request slow)\n",
    "- ‚ùå Execution time limits (15 min max)\n",
    "- ‚ùå Memory limits (10GB max)\n",
    "- ‚ùå Vendor lock-in\n",
    "\n",
    "### üéØ When to Use Serverless:\n",
    "\n",
    "‚úÖ **Good for:**\n",
    "- Sporadic traffic\n",
    "- Small to medium models\n",
    "- API endpoints\n",
    "- Event-driven predictions\n",
    "- Cost optimization\n",
    "\n",
    "‚ùå **Not ideal for:**\n",
    "- Constant high traffic\n",
    "- Large models (>1GB)\n",
    "- Long-running tasks\n",
    "- Real-time streaming\n",
    "\n",
    "Let's create a serverless function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lambda-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Lambda function for ML prediction\n",
    "# This is the code you would deploy to Lambda\n",
    "\n",
    "lambda_function_code = '''\n",
    "import json\n",
    "import boto3\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Global variable for model (loaded once, reused)\n",
    "model = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load model from S3 (lazy loading)\"\"\"\n",
    "    global model\n",
    "    if model is None:\n",
    "        # Download model from S3\n",
    "        s3.download_file(\n",
    "            Bucket='my-ml-models',\n",
    "            Key='models/classifier.joblib',\n",
    "            Filename='/tmp/model.joblib'\n",
    "        )\n",
    "        model = joblib.load('/tmp/model.joblib')\n",
    "    return model\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    AWS Lambda handler function\n",
    "    \n",
    "    Input:\n",
    "        event: {\n",
    "            \"body\": \"{\\\"features\\\": [5.1, 3.5, 1.4, 0.2]}\"\n",
    "        }\n",
    "    \n",
    "    Output:\n",
    "        {\n",
    "            \"statusCode\": 200,\n",
    "            \"body\": \"{\\\"prediction\\\": 0, \\\"probability\\\": 0.95}\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse input\n",
    "        body = json.loads(event['body'])\n",
    "        features = np.array(body['features']).reshape(1, -1)\n",
    "        \n",
    "        # Load model\n",
    "        model = load_model()\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = int(model.predict(features)[0])\n",
    "        probability = float(model.predict_proba(features).max())\n",
    "        \n",
    "        # Return response\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'headers': {\n",
    "                'Content-Type': 'application/json',\n",
    "                'Access-Control-Allow-Origin': '*'\n",
    "            },\n",
    "            'body': json.dumps({\n",
    "                'prediction': prediction,\n",
    "                'probability': probability\n",
    "            })\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({\n",
    "                'error': str(e)\n",
    "            })\n",
    "        }\n",
    "'''\n",
    "\n",
    "# Save Lambda function\n",
    "with open('lambda_function.py', 'w') as f:\n",
    "    f.write(lambda_function_code)\n",
    "\n",
    "print(\"‚ö° AWS Lambda Function Created!\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Saved to: lambda_function.py\")\n",
    "print(\"\\nüöÄ Deployment Steps:\\n\")\n",
    "print(\"1Ô∏è‚É£ Package dependencies:\")\n",
    "print(\"   pip install -t package/ scikit-learn joblib numpy\")\n",
    "print(\"   cd package && zip -r ../deployment.zip .\")\n",
    "print(\"   cd .. && zip -g deployment.zip lambda_function.py\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Upload model to S3:\")\n",
    "print(\"   aws s3 cp model.joblib s3://my-ml-models/models/\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Create Lambda function:\")\n",
    "print(\"   aws lambda create-function \\\\\")\n",
    "print(\"     --function-name ml-predictor \\\\\")\n",
    "print(\"     --runtime python3.9 \\\\\")\n",
    "print(\"     --handler lambda_function.lambda_handler \\\\\")\n",
    "print(\"     --zip-file fileb://deployment.zip\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Create API Gateway:\")\n",
    "print(\"   - Create REST API\")\n",
    "print(\"   - Add POST method\")\n",
    "print(\"   - Link to Lambda function\")\n",
    "print(\"   - Deploy API\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° Result: https://your-api-id.execute-api.region.amazonaws.com/predict\")\n",
    "print(\"\\nüí∞ Cost: ~$0.20 per 1M requests (practically free!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcp-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud Function for ML prediction\n",
    "# Similar to Lambda but often easier for ML\n",
    "\n",
    "gcp_function_code = '''\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "\n",
    "# Global model variable\n",
    "model = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load model from Cloud Storage\"\"\"\n",
    "    global model\n",
    "    if model is None:\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket('my-ml-models')\n",
    "        blob = bucket.blob('models/classifier.joblib')\n",
    "        blob.download_to_filename('/tmp/model.joblib')\n",
    "        model = joblib.load('/tmp/model.joblib')\n",
    "    return model\n",
    "\n",
    "def predict(request):\n",
    "    \"\"\"\n",
    "    Google Cloud Function handler\n",
    "    \n",
    "    HTTP trigger endpoint\n",
    "    \"\"\"\n",
    "    # Set CORS headers\n",
    "    headers = {\n",
    "        'Access-Control-Allow-Origin': '*',\n",
    "        'Access-Control-Allow-Methods': 'POST',\n",
    "        'Access-Control-Allow-Headers': 'Content-Type',\n",
    "    }\n",
    "    \n",
    "    # Handle preflight\n",
    "    if request.method == 'OPTIONS':\n",
    "        return ('', 204, headers)\n",
    "    \n",
    "    try:\n",
    "        # Parse request\n",
    "        request_json = request.get_json()\n",
    "        features = np.array(request_json['features']).reshape(1, -1)\n",
    "        \n",
    "        # Load model and predict\n",
    "        model = load_model()\n",
    "        prediction = int(model.predict(features)[0])\n",
    "        probability = float(model.predict_proba(features).max())\n",
    "        \n",
    "        # Return response\n",
    "        return (json.dumps({\n",
    "            'prediction': prediction,\n",
    "            'probability': probability\n",
    "        }), 200, headers)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (json.dumps({'error': str(e)}), 500, headers)\n",
    "'''\n",
    "\n",
    "# Save Cloud Function\n",
    "with open('main.py', 'w') as f:\n",
    "    f.write(gcp_function_code)\n",
    "\n",
    "# Create requirements.txt\n",
    "requirements = '''\n",
    "scikit-learn==1.3.2\n",
    "joblib==1.3.2\n",
    "numpy==1.24.3\n",
    "google-cloud-storage==2.10.0\n",
    "'''\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements.strip())\n",
    "\n",
    "print(\"‚òÅÔ∏è  Google Cloud Function Created!\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Files created:\")\n",
    "print(\"   üìÑ main.py (function code)\")\n",
    "print(\"   üìÑ requirements.txt (dependencies)\")\n",
    "\n",
    "print(\"\\nüöÄ Deployment Steps:\\n\")\n",
    "print(\"1Ô∏è‚É£ Upload model to Cloud Storage:\")\n",
    "print(\"   gsutil cp model.joblib gs://my-ml-models/models/\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Deploy function:\")\n",
    "print(\"   gcloud functions deploy ml-predictor \\\\\")\n",
    "print(\"     --runtime python39 \\\\\")\n",
    "print(\"     --trigger-http \\\\\")\n",
    "print(\"     --allow-unauthenticated \\\\\")\n",
    "print(\"     --entry-point predict\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° Result: https://region-project-id.cloudfunctions.net/ml-predictor\")\n",
    "print(\"\\nüí∞ Cost: Free tier includes 2M requests/month!\")\n",
    "print(\"\\nüåü GCP is often easier than AWS for ML workloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf-spaces-intro",
   "metadata": {},
   "source": [
    "## ü§ó Step 2: Hugging Face Spaces Deployment\n",
    "\n",
    "**Deploy ML apps for FREE with Hugging Face Spaces!**\n",
    "\n",
    "### üéØ What are Hugging Face Spaces?\n",
    "\n",
    "**FREE hosting for ML applications:**\n",
    "- ‚úÖ 100% FREE (no credit card needed)\n",
    "- ‚úÖ Unlimited apps\n",
    "- ‚úÖ Custom domains\n",
    "- ‚úÖ Gradio or Streamlit apps\n",
    "- ‚úÖ GPU support (paid tier)\n",
    "- ‚úÖ Millions of potential users\n",
    "\n",
    "### üèóÔ∏è How Spaces Work:\n",
    "\n",
    "```\n",
    "Your Code (app.py)\n",
    "    ‚Üì\n",
    "Push to Hugging Face\n",
    "    ‚Üì\n",
    "Auto-build & Deploy\n",
    "    ‚Üì\n",
    "Live at: huggingface.co/spaces/username/app-name\n",
    "    ‚Üì\n",
    "Share with the world! üåç\n",
    "```\n",
    "\n",
    "### üìä Spaces Options:\n",
    "\n",
    "| Framework | Best For | Pros |\n",
    "|-----------|----------|------|\n",
    "| **Gradio** | ML demos | Fastest setup, auto UI |\n",
    "| **Streamlit** | Data apps | More control, beautiful |\n",
    "| **Docker** | Custom | Full flexibility |\n",
    "| **Static** | Simple HTML | No backend needed |\n",
    "\n",
    "### üåü Why Use Spaces?\n",
    "\n",
    "**For Portfolio:**\n",
    "- ‚úÖ Impress employers with live demos\n",
    "- ‚úÖ Share projects easily (just a URL)\n",
    "- ‚úÖ No DevOps knowledge needed\n",
    "- ‚úÖ Professional-looking apps\n",
    "\n",
    "**For Learning:**\n",
    "- ‚úÖ Deploy in minutes\n",
    "- ‚úÖ Free hosting forever\n",
    "- ‚úÖ Great documentation\n",
    "- ‚úÖ Active community\n",
    "\n",
    "**For Projects:**\n",
    "- ‚úÖ Rapid prototyping\n",
    "- ‚úÖ User testing\n",
    "- ‚úÖ Demo to stakeholders\n",
    "- ‚úÖ MVP deployment\n",
    "\n",
    "Let's deploy to Spaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio-app",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradio app for Hugging Face Spaces\n",
    "# This creates an interactive ML interface\n",
    "\n",
    "gradio_app = '''\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis model\n",
    "print(\"Loading model...\")\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of input text\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with label and score\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return {\"Error\": \"Please enter some text\"}\n",
    "    \n",
    "    # Get prediction\n",
    "    result = classifier(text)[0]\n",
    "    \n",
    "    # Format output\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    \n",
    "    # Create confidence breakdown\n",
    "    if label == \"POSITIVE\":\n",
    "        return {\n",
    "            \"Positive\": score,\n",
    "            \"Negative\": 1 - score\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"Positive\": 1 - score,\n",
    "            \"Negative\": score\n",
    "        }\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=analyze_sentiment,\n",
    "    inputs=gr.Textbox(\n",
    "        lines=5,\n",
    "        placeholder=\"Enter text to analyze sentiment...\",\n",
    "        label=\"Input Text\"\n",
    "    ),\n",
    "    outputs=gr.Label(\n",
    "        num_top_classes=2,\n",
    "        label=\"Sentiment Analysis Results\"\n",
    "    ),\n",
    "    title=\"üé≠ Sentiment Analysis with BERT\",\n",
    "    description=\"Analyze the sentiment of any text using a fine-tuned BERT model. Try it with movie reviews, product feedback, or tweets!\",\n",
    "    examples=[\n",
    "        [\"I absolutely love this product! It exceeded all my expectations.\"],\n",
    "        [\"This is the worst experience I've ever had. Very disappointed.\"],\n",
    "        [\"It's okay, nothing special but does the job.\"],\n",
    "        [\"Amazing quality! Highly recommend to everyone!\"],\n",
    "        [\"Terrible customer service and poor quality product.\"]\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    "    analytics_enabled=True\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "'''\n",
    "\n",
    "# Save Gradio app\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(gradio_app)\n",
    "\n",
    "# Create requirements for Spaces\n",
    "spaces_requirements = '''\n",
    "gradio==4.7.1\n",
    "transformers==4.35.2\n",
    "torch==2.1.1\n",
    "'''\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(spaces_requirements.strip())\n",
    "\n",
    "# Create README for Spaces\n",
    "readme = '''\n",
    "---\n",
    "title: Sentiment Analysis\n",
    "emoji: üé≠\n",
    "colorFrom: blue\n",
    "colorTo: purple\n",
    "sdk: gradio\n",
    "sdk_version: 4.7.1\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "---\n",
    "\n",
    "# Sentiment Analysis with BERT\n",
    "\n",
    "This app uses a fine-tuned BERT model to analyze the sentiment of text.\n",
    "\n",
    "## Features\n",
    "- Real-time sentiment analysis\n",
    "- Confidence scores for positive/negative\n",
    "- Pre-loaded examples\n",
    "- Clean, intuitive interface\n",
    "\n",
    "## Model\n",
    "- **Model**: DistilBERT fine-tuned on SST-2\n",
    "- **Task**: Binary sentiment classification\n",
    "- **Accuracy**: ~91% on test set\n",
    "\n",
    "## Usage\n",
    "1. Enter your text in the input box\n",
    "2. Click \"Submit\" or press Enter\n",
    "3. View sentiment scores\n",
    "\n",
    "Try the example texts or input your own!\n",
    "'''\n",
    "\n",
    "with open('README.md', 'w') as f:\n",
    "    f.write(readme.strip())\n",
    "\n",
    "print(\"ü§ó Hugging Face Space Created!\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Files created:\")\n",
    "print(\"   üìÑ app.py (Gradio application)\")\n",
    "print(\"   üìÑ requirements.txt (dependencies)\")\n",
    "print(\"   üìÑ README.md (space description)\")\n",
    "\n",
    "print(\"\\nüöÄ Deployment Steps:\\n\")\n",
    "print(\"1Ô∏è‚É£ Create account at huggingface.co\")\n",
    "print(\"\\n2Ô∏è‚É£ Create new Space:\")\n",
    "print(\"   - Go to huggingface.co/new-space\")\n",
    "print(\"   - Choose 'Gradio' as SDK\")\n",
    "print(\"   - Choose 'Public' visibility\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Upload files:\")\n",
    "print(\"   - Upload app.py, requirements.txt, README.md\")\n",
    "print(\"   - Or use Git: git push to the Space repository\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Wait for build (2-3 minutes)\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ Share your app!\")\n",
    "print(\"   URL: https://huggingface.co/spaces/USERNAME/APP-NAME\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° Your app is now LIVE and can handle millions of users!\")\n",
    "print(\"üí∞ Cost: $0 (completely FREE!)\")\n",
    "print(\"üåü Perfect for portfolio projects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streamlit-intro",
   "metadata": {},
   "source": [
    "## üé® Step 3: Streamlit ML Applications\n",
    "\n",
    "**Build beautiful ML apps in minutes with Streamlit**\n",
    "\n",
    "### üéØ What is Streamlit?\n",
    "\n",
    "**The fastest way to build data/ML apps:**\n",
    "- ‚úÖ Pure Python (no HTML/CSS/JS)\n",
    "- ‚úÖ Reactive updates (auto-refresh)\n",
    "- ‚úÖ Beautiful UI out of the box\n",
    "- ‚úÖ 100+ components\n",
    "- ‚úÖ FREE deployment (Streamlit Cloud)\n",
    "\n",
    "### üèóÔ∏è Streamlit Architecture:\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "\n",
    "# That's it! No routing, no templates\n",
    "st.title(\"My ML App\")\n",
    "text = st.text_input(\"Enter text\")\n",
    "if st.button(\"Predict\"):\n",
    "    result = model.predict(text)\n",
    "    st.write(f\"Result: {result}\")\n",
    "```\n",
    "\n",
    "### üìä Streamlit vs Gradio:\n",
    "\n",
    "| Feature | Streamlit | Gradio |\n",
    "|---------|-----------|--------|\n",
    "| **Setup Speed** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Customization** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **ML Focus** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Best For** | Data apps, dashboards | ML demos |\n",
    "| **Learning Curve** | Easy | Easiest |\n",
    "\n",
    "### üåü Streamlit Features:\n",
    "\n",
    "**Inputs:**\n",
    "- Text input, number input, sliders\n",
    "- File uploads, camera input\n",
    "- Select boxes, multi-select\n",
    "- Date/time pickers\n",
    "\n",
    "**Outputs:**\n",
    "- Text, markdown, code\n",
    "- Charts (matplotlib, plotly, altair)\n",
    "- Tables, dataframes\n",
    "- Images, audio, video\n",
    "- Maps, 3D plots\n",
    "\n",
    "**Layout:**\n",
    "- Columns, tabs, expandable sections\n",
    "- Sidebars, containers\n",
    "- Progress bars, spinners\n",
    "- Custom themes\n",
    "\n",
    "Let's build a Streamlit app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streamlit-app",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive Streamlit ML app\n",
    "\n",
    "streamlit_app = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"ML Model Predictor\",\n",
    "    page_icon=\"ü§ñ\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .main-header {\n",
    "        font-size: 3rem;\n",
    "        font-weight: bold;\n",
    "        color: #1f77b4;\n",
    "        text-align: center;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "    .metric-card {\n",
    "        background-color: #f0f2f6;\n",
    "        padding: 1rem;\n",
    "        border-radius: 0.5rem;\n",
    "        margin: 0.5rem 0;\n",
    "    }\n",
    "    </style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Title\n",
    "st.markdown('<h1 class=\"main-header\">ü§ñ ML Model Predictor</h1>', unsafe_allow_html=True)\n",
    "st.markdown(\"### Interactive Machine Learning Application\")\n",
    "\n",
    "# Sidebar\n",
    "with st.sidebar:\n",
    "    st.header(\"‚öôÔ∏è Configuration\")\n",
    "    \n",
    "    # Model selection\n",
    "    st.subheader(\"Model Settings\")\n",
    "    n_estimators = st.slider(\n",
    "        \"Number of trees\",\n",
    "        min_value=10,\n",
    "        max_value=200,\n",
    "        value=100,\n",
    "        step=10\n",
    "    )\n",
    "    \n",
    "    max_depth = st.slider(\n",
    "        \"Max depth\",\n",
    "        min_value=1,\n",
    "        max_value=20,\n",
    "        value=10\n",
    "    )\n",
    "    \n",
    "    # Train button\n",
    "    train_button = st.button(\"üöÄ Train Model\", type=\"primary\")\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### üìä About\")\n",
    "    st.info(\n",
    "        \"This app demonstrates ML model training and prediction \"\n",
    "        \"using the Iris dataset and Random Forest classifier.\"\n",
    "    )\n",
    "\n",
    "# Main content\n",
    "tab1, tab2, tab3 = st.tabs([\"üìà Training\", \"üéØ Prediction\", \"üìä Visualization\"])\n",
    "\n",
    "# Load data\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    iris = load_iris()\n",
    "    X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    y = iris.target\n",
    "    return X, y, iris.target_names\n",
    "\n",
    "X, y, target_names = load_data()\n",
    "\n",
    "# Training tab\n",
    "with tab1:\n",
    "    st.header(\"Model Training\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"üìä Dataset Overview\")\n",
    "        st.write(f\"**Total Samples:** {len(X)}\")\n",
    "        st.write(f\"**Features:** {len(X.columns)}\")\n",
    "        st.write(f\"**Classes:** {len(target_names)}\")\n",
    "        \n",
    "        # Show data\n",
    "        if st.checkbox(\"Show dataset\"):\n",
    "            st.dataframe(X.head(10))\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"‚öôÔ∏è Model Configuration\")\n",
    "        st.write(f\"**Algorithm:** Random Forest\")\n",
    "        st.write(f\"**Trees:** {n_estimators}\")\n",
    "        st.write(f\"**Max Depth:** {max_depth}\")\n",
    "    \n",
    "    # Train model\n",
    "    if train_button or 'model' not in st.session_state:\n",
    "        with st.spinner(\"Training model...\"):\n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Train\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                random_state=42\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            # Store in session\n",
    "            st.session_state.model = model\n",
    "            st.session_state.accuracy = accuracy\n",
    "            st.session_state.cm = cm\n",
    "            st.session_state.feature_importance = model.feature_importances_\n",
    "        \n",
    "        st.success(\"‚úÖ Model trained successfully!\")\n",
    "    \n",
    "    # Show results\n",
    "    if 'model' in st.session_state:\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"üìä Training Results\")\n",
    "        \n",
    "        # Metrics\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        col1.metric(\"Accuracy\", f\"{st.session_state.accuracy:.2%}\")\n",
    "        col2.metric(\"Train Samples\", f\"{int(len(X) * 0.8)}\")\n",
    "        col3.metric(\"Test Samples\", f\"{int(len(X) * 0.2)}\")\n",
    "\n",
    "# Prediction tab\n",
    "with tab2:\n",
    "    st.header(\"Make Predictions\")\n",
    "    \n",
    "    if 'model' not in st.session_state:\n",
    "        st.warning(\"‚ö†Ô∏è Please train the model first (Training tab)\")\n",
    "    else:\n",
    "        st.subheader(\"Enter Feature Values\")\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            sepal_length = st.number_input(\n",
    "                \"Sepal Length (cm)\",\n",
    "                min_value=0.0,\n",
    "                max_value=10.0,\n",
    "                value=5.1,\n",
    "                step=0.1\n",
    "            )\n",
    "            sepal_width = st.number_input(\n",
    "                \"Sepal Width (cm)\",\n",
    "                min_value=0.0,\n",
    "                max_value=10.0,\n",
    "                value=3.5,\n",
    "                step=0.1\n",
    "            )\n",
    "        \n",
    "        with col2:\n",
    "            petal_length = st.number_input(\n",
    "                \"Petal Length (cm)\",\n",
    "                min_value=0.0,\n",
    "                max_value=10.0,\n",
    "                value=1.4,\n",
    "                step=0.1\n",
    "            )\n",
    "            petal_width = st.number_input(\n",
    "                \"Petal Width (cm)\",\n",
    "                min_value=0.0,\n",
    "                max_value=10.0,\n",
    "                value=0.2,\n",
    "                step=0.1\n",
    "            )\n",
    "        \n",
    "        if st.button(\"üéØ Predict\", type=\"primary\"):\n",
    "            # Make prediction\n",
    "            features = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
    "            prediction = st.session_state.model.predict(features)[0]\n",
    "            probabilities = st.session_state.model.predict_proba(features)[0]\n",
    "            \n",
    "            # Display results\n",
    "            st.markdown(\"---\")\n",
    "            st.subheader(\"üéØ Prediction Results\")\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                st.success(f\"**Predicted Class:** {target_names[prediction]}\")\n",
    "                st.info(f\"**Confidence:** {probabilities[prediction]:.2%}\")\n",
    "            \n",
    "            with col2:\n",
    "                # Probability chart\n",
    "                prob_df = pd.DataFrame({\n",
    "                    'Class': target_names,\n",
    "                    'Probability': probabilities\n",
    "                })\n",
    "                fig = px.bar(\n",
    "                    prob_df,\n",
    "                    x='Class',\n",
    "                    y='Probability',\n",
    "                    title='Class Probabilities',\n",
    "                    color='Probability',\n",
    "                    color_continuous_scale='viridis'\n",
    "                )\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# Visualization tab\n",
    "with tab3:\n",
    "    st.header(\"Data Visualization\")\n",
    "    \n",
    "    if 'model' not in st.session_state:\n",
    "        st.warning(\"‚ö†Ô∏è Please train the model first (Training tab)\")\n",
    "    else:\n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            # Feature importance\n",
    "            st.subheader(\"üìä Feature Importance\")\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': X.columns,\n",
    "                'Importance': st.session_state.feature_importance\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            fig = px.bar(\n",
    "                importance_df,\n",
    "                x='Importance',\n",
    "                y='Feature',\n",
    "                orientation='h',\n",
    "                title='Feature Importance',\n",
    "                color='Importance',\n",
    "                color_continuous_scale='blues'\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        with col2:\n",
    "            # Confusion matrix\n",
    "            st.subheader(\"üéØ Confusion Matrix\")\n",
    "            fig = px.imshow(\n",
    "                st.session_state.cm,\n",
    "                labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "                x=target_names,\n",
    "                y=target_names,\n",
    "                title='Confusion Matrix',\n",
    "                color_continuous_scale='blues',\n",
    "                text_auto=True\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        # Feature distributions\n",
    "        st.subheader(\"üìà Feature Distributions\")\n",
    "        feature = st.selectbox(\"Select feature\", X.columns)\n",
    "        \n",
    "        fig = px.histogram(\n",
    "            X,\n",
    "            x=feature,\n",
    "            nbins=30,\n",
    "            title=f'Distribution of {feature}',\n",
    "            color_discrete_sequence=['#636EFA']\n",
    "        )\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\n",
    "    \"<div style='text-align: center; color: #666;'>\\n\"\n",
    "    \"Built with Streamlit üéà | Powered by scikit-learn ü§ñ\\n\"\n",
    "    \"</div>\",\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "'''\n",
    "\n",
    "# Save Streamlit app\n",
    "with open('streamlit_app.py', 'w') as f:\n",
    "    f.write(streamlit_app)\n",
    "\n",
    "print(\"üé® Streamlit App Created!\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Saved to: streamlit_app.py\")\n",
    "\n",
    "print(\"\\nüöÄ Run locally:\")\n",
    "print(\"   streamlit run streamlit_app.py\")\n",
    "\n",
    "print(\"\\n‚òÅÔ∏è  Deploy to Streamlit Cloud (FREE):\")\n",
    "print(\"\\n1Ô∏è‚É£ Push code to GitHub\")\n",
    "print(\"2Ô∏è‚É£ Go to share.streamlit.io\")\n",
    "print(\"3Ô∏è‚É£ Connect GitHub repo\")\n",
    "print(\"4Ô∏è‚É£ Select streamlit_app.py\")\n",
    "print(\"5Ô∏è‚É£ Click Deploy!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° Your app will be live at: yourapp.streamlit.app\")\n",
    "print(\"üí∞ Cost: $0 (FREE forever!)\")\n",
    "print(\"üåü Perfect for impressive portfolio projects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production-serving",
   "metadata": {},
   "source": [
    "## üè≠ Step 4: Production Model Serving\n",
    "\n",
    "**Scale ML models to handle millions of requests**\n",
    "\n",
    "### üéØ Model Serving Solutions:\n",
    "\n",
    "#### 1Ô∏è‚É£ **TensorFlow Serving**\n",
    "**Google's production ML serving system**\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Optimized for TensorFlow/Keras\n",
    "- ‚úÖ REST and gRPC APIs\n",
    "- ‚úÖ Model versioning\n",
    "- ‚úÖ A/B testing support\n",
    "- ‚úÖ Batching for efficiency\n",
    "\n",
    "**Best for:** TensorFlow models at scale\n",
    "\n",
    "#### 2Ô∏è‚É£ **TorchServe**\n",
    "**PyTorch's official serving tool**\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ PyTorch native\n",
    "- ‚úÖ Multi-model serving\n",
    "- ‚úÖ Auto-scaling\n",
    "- ‚úÖ Metrics and logging\n",
    "\n",
    "**Best for:** PyTorch models\n",
    "\n",
    "#### 3Ô∏è‚É£ **KServe (formerly KFServing)**\n",
    "**Kubernetes-native ML serving**\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Framework-agnostic\n",
    "- ‚úÖ Auto-scaling on K8s\n",
    "- ‚úÖ Canary deployments\n",
    "- ‚úÖ GPU support\n",
    "\n",
    "**Best for:** Large-scale, multi-model deployments\n",
    "\n",
    "#### 4Ô∏è‚É£ **BentoML**\n",
    "**Modern ML serving framework**\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Package models + dependencies\n",
    "- ‚úÖ Deploy anywhere (Docker, K8s, cloud)\n",
    "- ‚úÖ Built-in monitoring\n",
    "- ‚úÖ Adaptive batching\n",
    "\n",
    "**Best for:** Production deployments, any framework\n",
    "\n",
    "#### 5Ô∏è‚É£ **FastAPI + Uvicorn**\n",
    "**Custom solution (what we've been using!)**\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Full control\n",
    "- ‚úÖ Fast and modern\n",
    "- ‚úÖ Easy to customize\n",
    "- ‚úÖ Great documentation\n",
    "\n",
    "**Best for:** Custom requirements, smaller scale\n",
    "\n",
    "### üìä Comparison:\n",
    "\n",
    "| Solution | Complexity | Performance | Flexibility | Best For |\n",
    "|----------|------------|-------------|-------------|-----------|\n",
    "| **TF Serving** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | TensorFlow models |\n",
    "| **TorchServe** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | PyTorch models |\n",
    "| **KServe** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Enterprise K8s |\n",
    "| **BentoML** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Production |\n",
    "| **FastAPI** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Custom needs |\n",
    "\n",
    "### üèóÔ∏è Production Architecture:\n",
    "\n",
    "```\n",
    "Internet\n",
    "    ‚Üì\n",
    "Load Balancer (AWS ALB / GCP LB)\n",
    "    ‚Üì\n",
    "API Gateway (rate limiting, auth)\n",
    "    ‚Üì\n",
    "Model Servers [Auto-scaling]\n",
    "‚îú‚îÄ Server 1 (GPU)\n",
    "‚îú‚îÄ Server 2 (GPU)\n",
    "‚îî‚îÄ Server 3 (GPU)\n",
    "    ‚Üì\n",
    "Model Cache (Redis)\n",
    "    ‚Üì\n",
    "Model Storage (S3 / GCS)\n",
    "```\n",
    "\n",
    "### üåü Production Best Practices:\n",
    "\n",
    "**Performance:**\n",
    "- ‚úÖ Use batching for throughput\n",
    "- ‚úÖ Cache predictions when possible\n",
    "- ‚úÖ Use GPUs for deep learning\n",
    "- ‚úÖ Optimize model size (quantization, pruning)\n",
    "\n",
    "**Reliability:**\n",
    "- ‚úÖ Health checks and auto-restart\n",
    "- ‚úÖ Graceful degradation\n",
    "- ‚úÖ Circuit breakers\n",
    "- ‚úÖ Retry logic\n",
    "\n",
    "**Monitoring:**\n",
    "- ‚úÖ Latency metrics (p50, p95, p99)\n",
    "- ‚úÖ Throughput tracking\n",
    "- ‚úÖ Error rate monitoring\n",
    "- ‚úÖ Model performance tracking\n",
    "\n",
    "**Security:**\n",
    "- ‚úÖ API authentication (API keys, OAuth)\n",
    "- ‚úÖ Rate limiting\n",
    "- ‚úÖ Input validation\n",
    "- ‚úÖ HTTPS only\n",
    "\n",
    "Let's see production deployment code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready model serving with monitoring\n",
    "\n",
    "production_code = '''\n",
    "from fastapi import FastAPI, HTTPException, Depends, status\n",
    "from fastapi.security import APIKeyHeader\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional\n",
    "import joblib\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from prometheus_client import Counter, Histogram, generate_latest\n",
    "from starlette.responses import Response\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Production ML API\",\n",
    "    description=\"Production-ready model serving with monitoring\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "REQUEST_COUNT = Counter(\n",
    "    'prediction_requests_total',\n",
    "    'Total prediction requests',\n",
    "    ['status']\n",
    ")\n",
    "REQUEST_LATENCY = Histogram(\n",
    "    'prediction_latency_seconds',\n",
    "    'Prediction latency in seconds'\n",
    ")\n",
    "\n",
    "# API Key authentication\n",
    "API_KEY_HEADER = APIKeyHeader(name=\"X-API-Key\")\n",
    "\n",
    "def verify_api_key(api_key: str = Depends(API_KEY_HEADER)):\n",
    "    \"\"\"Verify API key\"\"\"\n",
    "    # In production, check against database/secret manager\n",
    "    valid_keys = [\"your-secret-api-key-here\"]\n",
    "    if api_key not in valid_keys:\n",
    "        logger.warning(f\"Invalid API key attempt: {api_key[:10]}...\")\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Invalid API key\"\n",
    "        )\n",
    "    return api_key\n",
    "\n",
    "# Load model at startup\n",
    "logger.info(\"Loading ML model...\")\n",
    "model = joblib.load('model.joblib')\n",
    "logger.info(\"Model loaded successfully\")\n",
    "\n",
    "# Request/Response models\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: List[float] = Field(..., min_items=4, max_items=4)\n",
    "    \n",
    "    @validator('features')\n",
    "    def validate_features(cls, v):\n",
    "        if any(x < 0 for x in v):\n",
    "            raise ValueError('Features must be non-negative')\n",
    "        return v\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"features\": [5.1, 3.5, 1.4, 0.2]\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: int\n",
    "    probability: float\n",
    "    latency_ms: float\n",
    "    timestamp: str\n",
    "\n",
    "# Health check\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Readiness check\n",
    "@app.get(\"/ready\")\n",
    "async def readiness_check():\n",
    "    \"\"\"Readiness check for load balancer\"\"\"\n",
    "    if model is None:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"Model not loaded\"\n",
    "        )\n",
    "    return {\"status\": \"ready\"}\n",
    "\n",
    "# Metrics endpoint\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    return Response(\n",
    "        content=generate_latest(),\n",
    "        media_type=\"text/plain\"\n",
    "    )\n",
    "\n",
    "# Prediction endpoint\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(\n",
    "    request: PredictionRequest,\n",
    "    api_key: str = Depends(verify_api_key)\n",
    "):\n",
    "    \"\"\"Make prediction with authentication and monitoring\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Convert to numpy array\n",
    "        features = np.array(request.features).reshape(1, -1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = int(model.predict(features)[0])\n",
    "        probability = float(model.predict_proba(features).max())\n",
    "        \n",
    "        # Calculate latency\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Log prediction\n",
    "        logger.info(\n",
    "            f\"Prediction: {prediction}, \"\n",
    "            f\"Probability: {probability:.4f}, \"\n",
    "            f\"Latency: {latency_ms:.2f}ms\"\n",
    "        )\n",
    "        \n",
    "        # Update metrics\n",
    "        REQUEST_COUNT.labels(status='success').inc()\n",
    "        REQUEST_LATENCY.observe(time.time() - start_time)\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            prediction=prediction,\n",
    "            probability=probability,\n",
    "            latency_ms=latency_ms,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {str(e)}\")\n",
    "        REQUEST_COUNT.labels(status='error').inc()\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=str(e)\n",
    "        )\n",
    "\n",
    "# Batch prediction\n",
    "@app.post(\"/predict_batch\")\n",
    "async def predict_batch(\n",
    "    requests: List[PredictionRequest],\n",
    "    api_key: str = Depends(verify_api_key)\n",
    "):\n",
    "    \"\"\"Batch predictions for efficiency\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Collect all features\n",
    "        features_batch = np.array([req.features for req in requests])\n",
    "        \n",
    "        # Batch prediction\n",
    "        predictions = model.predict(features_batch)\n",
    "        probabilities = model.predict_proba(features_batch)\n",
    "        \n",
    "        # Format results\n",
    "        results = [\n",
    "            {\n",
    "                'prediction': int(pred),\n",
    "                'probability': float(prob.max())\n",
    "            }\n",
    "            for pred, prob in zip(predictions, probabilities)\n",
    "        ]\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Batch prediction: {len(requests)} samples, \"\n",
    "            f\"Latency: {latency_ms:.2f}ms\"\n",
    "        )\n",
    "        \n",
    "        REQUEST_COUNT.labels(status='success').inc()\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'count': len(results),\n",
    "            'latency_ms': latency_ms\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Batch prediction error: {str(e)}\")\n",
    "        REQUEST_COUNT.labels(status='error').inc()\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=str(e)\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        log_level=\"info\",\n",
    "        access_log=True\n",
    "    )\n",
    "'''\n",
    "\n",
    "with open('production_api.py', 'w') as f:\n",
    "    f.write(production_code)\n",
    "\n",
    "print(\"üè≠ Production API Created!\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Saved to: production_api.py\")\n",
    "\n",
    "print(\"\\nüåü Production Features:\")\n",
    "print(\"   ‚úÖ API key authentication\")\n",
    "print(\"   ‚úÖ Request validation (Pydantic)\")\n",
    "print(\"   ‚úÖ Comprehensive logging\")\n",
    "print(\"   ‚úÖ Prometheus metrics\")\n",
    "print(\"   ‚úÖ Health & readiness checks\")\n",
    "print(\"   ‚úÖ Batch prediction support\")\n",
    "print(\"   ‚úÖ Error handling\")\n",
    "print(\"   ‚úÖ Performance tracking\")\n",
    "\n",
    "print(\"\\nüöÄ Deploy to production:\")\n",
    "print(\"   1. Docker: docker build -t ml-api .\")\n",
    "print(\"   2. K8s: kubectl apply -f deployment.yaml\")\n",
    "print(\"   3. Cloud: gcloud run deploy ml-api\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercises\n",
    "\n",
    "**Practice your cloud deployment skills!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### Exercise 1: Deploy to Hugging Face Spaces\n",
    "\n",
    "**Task:** Create and deploy a real ML app to Hugging Face Spaces\n",
    "\n",
    "**Requirements:**\n",
    "1. Choose a task (sentiment analysis, image classification, etc.)\n",
    "2. Create a Gradio or Streamlit app\n",
    "3. Add example inputs\n",
    "4. Deploy to Spaces\n",
    "5. Share the live URL!\n",
    "\n",
    "**Bonus:** Add your own trained model instead of using pre-trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# TODO: Create your Gradio/Streamlit app\n",
    "# import gradio as gr\n",
    "\n",
    "# TODO: Define your prediction function\n",
    "# def predict(input):\n",
    "#     ...\n",
    "\n",
    "# TODO: Create interface\n",
    "# demo = gr.Interface(...)\n",
    "\n",
    "# TODO: Deploy to Spaces\n",
    "# 1. Create account at huggingface.co\n",
    "# 2. Create new Space\n",
    "# 3. Upload app.py and requirements.txt\n",
    "\n",
    "print(\"Complete the exercise above!\")\n",
    "print(\"\\nYour app URL will be:\")\n",
    "print(\"https://huggingface.co/spaces/YOUR-USERNAME/YOUR-APP-NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### Exercise 2: Build Production-Ready API\n",
    "\n",
    "**Task:** Enhance the basic FastAPI with production features\n",
    "\n",
    "**Add these features:**\n",
    "1. Rate limiting (max 100 requests/minute)\n",
    "2. Caching (cache predictions for 5 minutes)\n",
    "3. Comprehensive error handling\n",
    "4. API documentation with examples\n",
    "5. Request/response logging\n",
    "\n",
    "**Bonus:** Add Prometheus metrics and deploy to Railway or Render!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-2-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# TODO: Add rate limiting\n",
    "# from slowapi import Limiter\n",
    "\n",
    "# TODO: Add caching\n",
    "# from cachetools import TTLCache\n",
    "\n",
    "# TODO: Enhance error handling\n",
    "# @app.exception_handler(...)\n",
    "\n",
    "# TODO: Add comprehensive logging\n",
    "# import structlog\n",
    "\n",
    "# TODO: Add metrics\n",
    "# from prometheus_client import ...\n",
    "\n",
    "print(\"Complete the exercise above!\")\n",
    "print(\"\\nLibraries to explore:\")\n",
    "print(\"- slowapi (rate limiting)\")\n",
    "print(\"- cachetools (caching)\")\n",
    "print(\"- structlog (structured logging)\")\n",
    "print(\"- prometheus_client (metrics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéâ Key Takeaways\n",
    "\n",
    "**Congratulations! You've mastered cloud ML deployment!**\n",
    "\n",
    "### 1Ô∏è‚É£ **Cloud Platforms**\n",
    "   - ‚úÖ AWS, GCP, Azure - know the major providers\n",
    "   - ‚úÖ Each has ML-specific services\n",
    "   - ‚úÖ Choose based on your needs and budget\n",
    "   - **Use when:** Building production systems\n",
    "\n",
    "### 2Ô∏è‚É£ **Serverless ML**\n",
    "   - ‚úÖ Pay only for what you use\n",
    "   - ‚úÖ Auto-scaling built-in\n",
    "   - ‚úÖ No server management\n",
    "   - **Use when:** Variable traffic, cost optimization\n",
    "\n",
    "### 3Ô∏è‚É£ **Hugging Face Spaces**\n",
    "   - ‚úÖ FREE ML app hosting\n",
    "   - ‚úÖ Deploy in minutes\n",
    "   - ‚úÖ Perfect for portfolio\n",
    "   - **Use when:** Demos, MVPs, learning (always!)\n",
    "\n",
    "### 4Ô∏è‚É£ **Streamlit/Gradio**\n",
    "   - ‚úÖ Build ML apps without frontend skills\n",
    "   - ‚úÖ Beautiful UIs out of the box\n",
    "   - ‚úÖ Rapid prototyping\n",
    "   - **Use when:** Need quick, impressive demos\n",
    "\n",
    "### 5Ô∏è‚É£ **Production Serving**\n",
    "   - ‚úÖ Monitoring, logging, metrics essential\n",
    "   - ‚úÖ Authentication and security\n",
    "   - ‚úÖ Auto-scaling and redundancy\n",
    "   - **Use when:** Serving real users at scale\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Deployment Decision Tree\n",
    "\n",
    "**Choose your deployment strategy:**\n",
    "\n",
    "```\n",
    "Need deployment?\n",
    "    |\n",
    "    ‚îú‚îÄ Quick demo/portfolio? ‚Üí Hugging Face Spaces (FREE!)\n",
    "    |\n",
    "    ‚îú‚îÄ Internal tool? ‚Üí Streamlit Cloud (FREE!)\n",
    "    |\n",
    "    ‚îú‚îÄ Low traffic API? ‚Üí Railway/Render ($5-20/mo)\n",
    "    |\n",
    "    ‚îú‚îÄ Variable traffic? ‚Üí Serverless (AWS Lambda/Cloud Functions)\n",
    "    |\n",
    "    ‚îî‚îÄ Production scale? ‚Üí Cloud (AWS/GCP/Azure) with auto-scaling\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Cost Comparison\n",
    "\n",
    "**Monthly costs for different strategies:**\n",
    "\n",
    "| Solution | Traffic | Cost | Best For |\n",
    "|----------|---------|------|----------|\n",
    "| **HF Spaces** | Any | $0 | Demos, portfolio |\n",
    "| **Streamlit Cloud** | Low | $0 | Internal tools |\n",
    "| **Railway** | Medium | $5-20 | Startups, MVPs |\n",
    "| **Serverless** | Variable | $0-100 | Sporadic traffic |\n",
    "| **Cloud VMs** | Constant | $50-500 | Production |\n",
    "| **K8s Cluster** | High | $500+ | Enterprise |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Deployment Checklist\n",
    "\n",
    "**Before deploying to production:**\n",
    "\n",
    "**Code:**\n",
    "- [ ] Input validation\n",
    "- [ ] Error handling\n",
    "- [ ] Logging configured\n",
    "- [ ] API documentation\n",
    "- [ ] Tests written\n",
    "\n",
    "**Infrastructure:**\n",
    "- [ ] Health checks\n",
    "- [ ] Auto-scaling configured\n",
    "- [ ] Load balancer set up\n",
    "- [ ] SSL/HTTPS enabled\n",
    "- [ ] Monitoring alerts\n",
    "\n",
    "**Security:**\n",
    "- [ ] API authentication\n",
    "- [ ] Rate limiting\n",
    "- [ ] Input sanitization\n",
    "- [ ] Secrets in environment vars\n",
    "- [ ] CORS configured\n",
    "\n",
    "**Operations:**\n",
    "- [ ] CI/CD pipeline\n",
    "- [ ] Rollback procedure\n",
    "- [ ] Monitoring dashboard\n",
    "- [ ] Documentation complete\n",
    "- [ ] On-call rotation\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**Continue your deployment journey:**\n",
    "\n",
    "1. **Deploy Real Projects:**\n",
    "   - Create 3 Spaces apps for portfolio\n",
    "   - Build Streamlit dashboard\n",
    "   - Deploy to cloud (use free tiers)\n",
    "\n",
    "2. **Learn Advanced Topics:**\n",
    "   - Kubernetes for ML (KServe, Seldon)\n",
    "   - Multi-model serving\n",
    "   - Edge deployment (TensorFlow Lite)\n",
    "   - Model optimization (ONNX, TensorRT)\n",
    "\n",
    "3. **Build Portfolio:**\n",
    "   - 3-5 deployed ML apps\n",
    "   - GitHub repos with deployment docs\n",
    "   - Blog posts about deployment\n",
    "   - Contribute to open-source\n",
    "\n",
    "---\n",
    "\n",
    "**üí¨ Final Thoughts:**\n",
    "\n",
    "*\"You now have the complete skill set to deploy ML models from local prototypes to cloud-scale production systems. Hugging Face Spaces gives you FREE hosting to build an impressive portfolio. Streamlit lets you create beautiful apps without frontend skills. And production deployment knowledge makes you job-ready for ML engineering roles. The gap between learning ML and deploying ML is now closed - you can do both!\"*\n",
    "\n",
    "**üéâ Week 19 Complete! You've mastered MLOps & Deployment! üöÄ**\n",
    "\n",
    "**What you've learned:**\n",
    "- Day 1: Model deployment (Flask, FastAPI, Docker)\n",
    "- Day 2: MLOps best practices (MLflow, monitoring, A/B testing)\n",
    "- Day 3: Cloud deployment (AWS, GCP, Spaces, Streamlit)\n",
    "\n",
    "**You can now:**\n",
    "- Deploy models as REST APIs\n",
    "- Build MLOps pipelines\n",
    "- Deploy to cloud platforms\n",
    "- Create production-ready ML systems\n",
    "\n",
    "**üåü You're ready for ML engineering roles! üåü**\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Additional Resources:**\n",
    "- Hugging Face Spaces: https://huggingface.co/spaces\n",
    "- Streamlit Docs: https://docs.streamlit.io\n",
    "- AWS SageMaker: https://aws.amazon.com/sagemaker\n",
    "- GCP Vertex AI: https://cloud.google.com/vertex-ai\n",
    "- Full Stack Deep Learning: https://fullstackdeeplearning.com\n",
    "- MLOps Community: https://mlops.community\n",
    "\n",
    "**Keep deploying! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
