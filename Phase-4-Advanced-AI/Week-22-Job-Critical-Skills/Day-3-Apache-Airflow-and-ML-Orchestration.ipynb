{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Week 22, Day 3: Apache Airflow & ML Orchestration\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/Learn-AI/blob/main/Phase-4-Advanced-AI/Week-22-Job-Critical-Skills/Day-3-Apache-Airflow-and-ML-Orchestration.ipynb)\n",
    "\n",
    "## üöÄ Why This Matters\n",
    "\n",
    "ML in production is **NOT** just training models in notebooks!\n",
    "\n",
    "**Real production ML pipelines need**:\n",
    "- üìä **Data ingestion** from multiple sources\n",
    "- üîÑ **Data transformation** and validation\n",
    "- ü§ñ **Model training** (potentially hours/days)\n",
    "- ‚úÖ **Model evaluation** and comparison\n",
    "- üöÄ **Model deployment** if performance is good\n",
    "- üîÅ **Retraining** on schedule (daily, weekly)\n",
    "- üìß **Alerting** when things fail\n",
    "\n",
    "**Enter: ML Orchestration!**\n",
    "\n",
    "### Job Market Reality\n",
    "\n",
    "**Woolworths job listing**: ‚úÖ\n",
    "> \"Experience with Airflow or other orchestration tools for ML pipelines\"\n",
    "\n",
    "**Industry Stats**:\n",
    "- 40% of ML Engineer jobs mention orchestration\n",
    "- Airflow used by: Airbnb, Netflix, Spotify, Uber, Twitter\n",
    "- Kubeflow used by: Google, Cisco, Bloomberg\n",
    "\n",
    "**Real Impact**:\n",
    "- Airbnb: 1000+ daily ML pipelines in Airflow\n",
    "- Uber: Automated model retraining every 6 hours\n",
    "- Netflix: Recommendation models retrain automatically\n",
    "\n",
    "## üìã What You'll Learn Today\n",
    "\n",
    "1. **Apache Airflow Fundamentals** - DAGs, operators, scheduling\n",
    "2. **ML Pipeline Orchestration** - End-to-end workflow\n",
    "3. **Advanced Concepts** - Dynamic DAGs, XCom, branching\n",
    "4. **Other Tools** - Kubeflow, Argo Workflows, Prefect\n",
    "5. **Best Practices** - Error handling, monitoring, testing\n",
    "6. **üèÜ Project: Complete Automated ML Pipeline**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Apache Airflow Fundamentals\n",
    "\n",
    "### What is Apache Airflow?\n",
    "\n",
    "**Airflow** = Platform to programmatically author, schedule, and monitor workflows\n",
    "\n",
    "**Key Concepts**:\n",
    "\n",
    "1. **DAG (Directed Acyclic Graph)**\n",
    "   - Defines workflow structure\n",
    "   - Nodes = Tasks\n",
    "   - Edges = Dependencies\n",
    "   - Acyclic = No loops!\n",
    "\n",
    "2. **Operators**\n",
    "   - PythonOperator: Run Python functions\n",
    "   - BashOperator: Run bash commands\n",
    "   - EmailOperator: Send emails\n",
    "   - Many more (Docker, Kubernetes, Spark, etc.)\n",
    "\n",
    "3. **Tasks**\n",
    "   - Instance of an operator\n",
    "   - Can have dependencies: task_a >> task_b\n",
    "\n",
    "4. **Schedule**\n",
    "   - Cron expressions: `@daily`, `@hourly`, `0 0 * * *`\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Scheduler  ‚îÇ ‚Üê Reads DAGs, schedules tasks\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "      ‚îÇ\n",
    "      v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Executor   ‚îÇ ‚Üê Runs tasks (Local/Celery/Kubernetes)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "      ‚îÇ\n",
    "      v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Workers   ‚îÇ ‚Üê Execute task code\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Airflow (Note: In production, use Docker or proper installation)\n",
    "# For Colab, we'll demonstrate concepts without full installation\n",
    "\n",
    "!pip install apache-airflow==2.7.0 -q\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries installed!\")\n",
    "print(\"\\nNote: Full Airflow requires separate installation.\")\n",
    "print(\"We'll demonstrate DAG code that you can deploy in production.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your First Airflow DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Airflow DAG (save as: dags/hello_world_dag.py)\n",
    "\n",
    "hello_world_dag = '''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Default arguments\n",
    "default_args = {\n",
    "    'owner': 'ml_engineer',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'email': ['alerts@company.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Define Python tasks\n",
    "def print_hello():\n",
    "    print(\"Hello from Airflow!\")\n",
    "    return \"Success!\"\n",
    "\n",
    "def print_date():\n",
    "    print(f\"Current date: {datetime.now()}\")\n",
    "\n",
    "# Create DAG\n",
    "with DAG(\n",
    "    dag_id='hello_world',\n",
    "    default_args=default_args,\n",
    "    description='Simple hello world DAG',\n",
    "    schedule_interval='@daily',  # Run daily\n",
    "    catchup=False,  # Don't backfill\n",
    ") as dag:\n",
    "    \n",
    "    # Task 1: Print hello\n",
    "    task_hello = PythonOperator(\n",
    "        task_id='print_hello',\n",
    "        python_callable=print_hello,\n",
    "    )\n",
    "    \n",
    "    # Task 2: Print date\n",
    "    task_date = PythonOperator(\n",
    "        task_id='print_date',\n",
    "        python_callable=print_date,\n",
    "    )\n",
    "    \n",
    "    # Task 3: Bash command\n",
    "    task_bash = BashOperator(\n",
    "        task_id='run_bash',\n",
    "        bash_command='echo \"Running Bash task!\"',\n",
    "    )\n",
    "    \n",
    "    # Define dependencies (task flow)\n",
    "    task_hello >> task_date >> task_bash  # Sequential execution\n",
    "'''\n",
    "\n",
    "print(\"üìù Example Airflow DAG:\")\n",
    "print(\"=\"*70)\n",
    "print(hello_world_dag)\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° This DAG will:\")\n",
    "print(\"  1. Run daily (@daily schedule)\")\n",
    "print(\"  2. Execute 3 tasks sequentially\")\n",
    "print(\"  3. Retry failed tasks 2 times\")\n",
    "print(\"  4. Send email alerts on failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Dependencies\n",
    "\n",
    "**Multiple ways to define dependencies**:\n",
    "\n",
    "```python\n",
    "# Method 1: Bitshift operators\n",
    "task_a >> task_b >> task_c  # Sequential: A ‚Üí B ‚Üí C\n",
    "\n",
    "# Method 2: Parallel then merge\n",
    "task_a >> [task_b, task_c] >> task_d\n",
    "# A ‚Üí B ‚Üí D\n",
    "#   ‚Üí C ‚Üí D\n",
    "\n",
    "# Method 3: set_upstream/set_downstream\n",
    "task_b.set_upstream(task_a)\n",
    "task_c.set_downstream(task_d)\n",
    "\n",
    "# Method 4: Complex dependencies\n",
    "[task_a, task_b] >> task_c >> [task_d, task_e]\n",
    "# A ‚Üí C ‚Üí D\n",
    "# B ‚Üí C ‚Üí E\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ML Pipeline with Airflow\n",
    "\n",
    "### Complete ML Workflow\n",
    "\n",
    "```\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Extract Data  ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             v\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Validate Data  ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             v\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Transform Data ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ                 ‚îÇ\n",
    "    v                 v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Train   ‚îÇ    ‚îÇ Train       ‚îÇ\n",
    "‚îÇ Model A ‚îÇ    ‚îÇ Model B     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "     ‚îÇ                ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ\n",
    "              v\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Evaluate       ‚îÇ\n",
    "    ‚îÇ Best Model     ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             v\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Deploy Model   ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             v\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Send Alert     ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete ML Pipeline DAG\n",
    "\n",
    "ml_pipeline_dag = '''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = '/data/customer_data.csv'\n",
    "MODEL_PATH = '/models/'\n",
    "METRICS_PATH = '/metrics/'\n",
    "MIN_ACCURACY = 0.85\n",
    "\n",
    "# Task 1: Extract data from database/API\n",
    "def extract_data(**context):\n",
    "    \"\"\"\n",
    "    Extract data from source (database, API, files)\n",
    "    \"\"\"\n",
    "    print(\"Extracting data from source...\")\n",
    "    \n",
    "    # Simulated data extraction\n",
    "    # In production: pd.read_sql(), requests.get(), etc.\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    \n",
    "    print(f\"Extracted {len(df)} rows\")\n",
    "    \n",
    "    # Push data to XCom (Airflow's inter-task communication)\n",
    "    context['ti'].xcom_push(key='row_count', value=len(df))\n",
    "    \n",
    "    # Save to temp location\n",
    "    df.to_csv('/tmp/raw_data.csv', index=False)\n",
    "    \n",
    "    return 'Data extracted successfully'\n",
    "\n",
    "# Task 2: Validate data quality\n",
    "def validate_data(**context):\n",
    "    \"\"\"\n",
    "    Check data quality: missing values, outliers, schema\n",
    "    \"\"\"\n",
    "    print(\"Validating data quality...\")\n",
    "    \n",
    "    df = pd.read_csv('/tmp/raw_data.csv')\n",
    "    \n",
    "    # Check for issues\n",
    "    issues = []\n",
    "    \n",
    "    # Missing values\n",
    "    missing_pct = df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100\n",
    "    if missing_pct > 10:\n",
    "        issues.append(f\"High missing values: {missing_pct:.2f}%\")\n",
    "    \n",
    "    # Duplicates\n",
    "    dup_count = df.duplicated().sum()\n",
    "    if dup_count > 0:\n",
    "        issues.append(f\"Found {dup_count} duplicates\")\n",
    "    \n",
    "    if issues:\n",
    "        raise ValueError(f\"Data quality issues: {issues}\")\n",
    "    \n",
    "    print(\"‚úÖ Data validation passed!\")\n",
    "    return 'Data is valid'\n",
    "\n",
    "# Task 3: Transform data\n",
    "def transform_data(**context):\n",
    "    \"\"\"\n",
    "    Feature engineering, encoding, scaling\n",
    "    \"\"\"\n",
    "    print(\"Transforming data...\")\n",
    "    \n",
    "    df = pd.read_csv('/tmp/raw_data.csv')\n",
    "    \n",
    "    # Feature engineering (example)\n",
    "    # df['new_feature'] = df['a'] * df['b']\n",
    "    \n",
    "    # Handle missing values\n",
    "    df = df.fillna(df.mean())\n",
    "    \n",
    "    # Encoding categorical variables\n",
    "    # df = pd.get_dummies(df, columns=['category'])\n",
    "    \n",
    "    # Save processed data\n",
    "    df.to_csv('/tmp/processed_data.csv', index=False)\n",
    "    \n",
    "    print(\"‚úÖ Data transformation complete!\")\n",
    "    return 'Data transformed'\n",
    "\n",
    "# Task 4a: Train Random Forest\n",
    "def train_random_forest(**context):\n",
    "    \"\"\"\n",
    "    Train Random Forest model\n",
    "    \"\"\"\n",
    "    print(\"Training Random Forest...\")\n",
    "    \n",
    "    df = pd.read_csv('/tmp/processed_data.csv')\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Random Forest - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    # Save model and metrics\n",
    "    with open('/tmp/rf_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    context['ti'].xcom_push(key='rf_accuracy', value=accuracy)\n",
    "    context['ti'].xcom_push(key='rf_f1', value=f1)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Task 4b: Train Logistic Regression\n",
    "def train_logistic_regression(**context):\n",
    "    \"\"\"\n",
    "    Train Logistic Regression model\n",
    "    \"\"\"\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    \n",
    "    df = pd.read_csv('/tmp/processed_data.csv')\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Logistic Regression - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    # Save model and metrics\n",
    "    with open('/tmp/lr_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    context['ti'].xcom_push(key='lr_accuracy', value=accuracy)\n",
    "    context['ti'].xcom_push(key='lr_f1', value=f1)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Task 5: Select best model\n",
    "def select_best_model(**context):\n",
    "    \"\"\"\n",
    "    Compare models and select best one\n",
    "    \"\"\"\n",
    "    ti = context['ti']\n",
    "    \n",
    "    rf_acc = ti.xcom_pull(key='rf_accuracy', task_ids='train_random_forest')\n",
    "    lr_acc = ti.xcom_pull(key='lr_accuracy', task_ids='train_logistic_regression')\n",
    "    \n",
    "    print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n",
    "    print(f\"Logistic Regression Accuracy: {lr_acc:.4f}\")\n",
    "    \n",
    "    if rf_acc > lr_acc:\n",
    "        best_model = 'random_forest'\n",
    "        best_acc = rf_acc\n",
    "    else:\n",
    "        best_model = 'logistic_regression'\n",
    "        best_acc = lr_acc\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Model: {best_model} (Accuracy: {best_acc:.4f})\")\n",
    "    \n",
    "    # Check if meets threshold\n",
    "    ti.xcom_push(key='best_model', value=best_model)\n",
    "    ti.xcom_push(key='best_accuracy', value=best_acc)\n",
    "    \n",
    "    if best_acc >= MIN_ACCURACY:\n",
    "        return 'deploy_model'  # Branch to deployment\n",
    "    else:\n",
    "        return 'send_failure_alert'  # Branch to alert\n",
    "\n",
    "# Task 6a: Deploy model\n",
    "def deploy_model(**context):\n",
    "    \"\"\"\n",
    "    Deploy best model to production\n",
    "    \"\"\"\n",
    "    ti = context['ti']\n",
    "    best_model = ti.xcom_pull(key='best_model', task_ids='select_best_model')\n",
    "    \n",
    "    print(f\"Deploying {best_model} to production...\")\n",
    "    \n",
    "    # Copy model to production path\n",
    "    # shutil.copy(f'/tmp/{best_model}.pkl', f'{MODEL_PATH}/production_model.pkl')\n",
    "    \n",
    "    # Update model registry\n",
    "    # Update API endpoint\n",
    "    # Restart service\n",
    "    \n",
    "    print(\"‚úÖ Model deployed successfully!\")\n",
    "    return 'Deployment successful'\n",
    "\n",
    "# Task 6b: Send failure alert\n",
    "def send_failure_alert(**context):\n",
    "    \"\"\"\n",
    "    Alert when model doesn't meet threshold\n",
    "    \"\"\"\n",
    "    ti = context['ti']\n",
    "    best_acc = ti.xcom_pull(key='best_accuracy', task_ids='select_best_model')\n",
    "    \n",
    "    message = f\"\"\"\n",
    "    ‚ö†Ô∏è ML Pipeline Alert\n",
    "    \n",
    "    Model accuracy ({best_acc:.4f}) is below threshold ({MIN_ACCURACY}).\n",
    "    Manual review required.\n",
    "    \n",
    "    Pipeline run: {context['execution_date']}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(message)\n",
    "    # Send email/Slack notification\n",
    "    \n",
    "    return 'Alert sent'\n",
    "\n",
    "# Define DAG\n",
    "default_args = {\n",
    "    'owner': 'ml_team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'email': ['ml-team@company.com'],\n",
    "    'email_on_failure': True,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='ml_training_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Automated ML training and deployment',\n",
    "    schedule_interval='0 2 * * *',  # Daily at 2 AM\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    \n",
    "    # Define tasks\n",
    "    extract = PythonOperator(\n",
    "        task_id='extract_data',\n",
    "        python_callable=extract_data,\n",
    "    )\n",
    "    \n",
    "    validate = PythonOperator(\n",
    "        task_id='validate_data',\n",
    "        python_callable=validate_data,\n",
    "    )\n",
    "    \n",
    "    transform = PythonOperator(\n",
    "        task_id='transform_data',\n",
    "        python_callable=transform_data,\n",
    "    )\n",
    "    \n",
    "    train_rf = PythonOperator(\n",
    "        task_id='train_random_forest',\n",
    "        python_callable=train_random_forest,\n",
    "    )\n",
    "    \n",
    "    train_lr = PythonOperator(\n",
    "        task_id='train_logistic_regression',\n",
    "        python_callable=train_logistic_regression,\n",
    "    )\n",
    "    \n",
    "    select_model = BranchPythonOperator(\n",
    "        task_id='select_best_model',\n",
    "        python_callable=select_best_model,\n",
    "    )\n",
    "    \n",
    "    deploy = PythonOperator(\n",
    "        task_id='deploy_model',\n",
    "        python_callable=deploy_model,\n",
    "    )\n",
    "    \n",
    "    alert = PythonOperator(\n",
    "        task_id='send_failure_alert',\n",
    "        python_callable=send_failure_alert,\n",
    "    )\n",
    "    \n",
    "    # Define pipeline flow\n",
    "    extract >> validate >> transform >> [train_rf, train_lr] >> select_model\n",
    "    select_model >> deploy\n",
    "    select_model >> alert\n",
    "'''\n",
    "\n",
    "print(\"üìù Complete ML Pipeline DAG:\")\n",
    "print(\"=\"*70)\n",
    "print(\"This is a production-ready ML pipeline that:\")\n",
    "print(\"\")\n",
    "print(\"1. ‚úÖ Extracts data from source\")\n",
    "print(\"2. ‚úÖ Validates data quality\")\n",
    "print(\"3. ‚úÖ Transforms/engineers features\")\n",
    "print(\"4. ‚úÖ Trains multiple models in parallel\")\n",
    "print(\"5. ‚úÖ Selects best model\")\n",
    "print(\"6. ‚úÖ Deploys if accuracy threshold met\")\n",
    "print(\"7. ‚úÖ Sends alerts if threshold not met\")\n",
    "print(\"8. ‚úÖ Runs daily at 2 AM\")\n",
    "print(\"9. ‚úÖ Retries failed tasks 3 times\")\n",
    "print(\"10. ‚úÖ Sends email on failures\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Airflow Concepts\n",
    "\n",
    "### XCom (Cross-Communication)\n",
    "\n",
    "**Share data between tasks**:\n",
    "\n",
    "```python\n",
    "# Push data to XCom\n",
    "def task_a(**context):\n",
    "    context['ti'].xcom_push(key='my_key', value='my_value')\n",
    "\n",
    "# Pull data from XCom\n",
    "def task_b(**context):\n",
    "    value = context['ti'].xcom_pull(key='my_key', task_ids='task_a')\n",
    "    print(f\"Received: {value}\")\n",
    "```\n",
    "\n",
    "**Note**: XCom limited to small data (<1MB). For large data, use files/databases.\n",
    "\n",
    "### Dynamic DAGs\n",
    "\n",
    "**Generate tasks dynamically**:\n",
    "\n",
    "```python\n",
    "# Train models for multiple products\n",
    "products = ['electronics', 'clothing', 'food']\n",
    "\n",
    "for product in products:\n",
    "    task = PythonOperator(\n",
    "        task_id=f'train_{product}_model',\n",
    "        python_callable=train_model,\n",
    "        op_kwargs={'product': product},\n",
    "    )\n",
    "```\n",
    "\n",
    "### Sensors\n",
    "\n",
    "**Wait for conditions**:\n",
    "\n",
    "```python\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "\n",
    "wait_for_file = FileSensor(\n",
    "    task_id='wait_for_data',\n",
    "    filepath='/data/new_data.csv',\n",
    "    poke_interval=60,  # Check every 60 seconds\n",
    "    timeout=3600,  # Timeout after 1 hour\n",
    ")\n",
    "```\n",
    "\n",
    "### TaskGroups\n",
    "\n",
    "**Organize related tasks**:\n",
    "\n",
    "```python\n",
    "from airflow.utils.task_group import TaskGroup\n",
    "\n",
    "with TaskGroup('data_prep') as data_prep:\n",
    "    extract = PythonOperator(...)\n",
    "    validate = PythonOperator(...)\n",
    "    transform = PythonOperator(...)\n",
    "    extract >> validate >> transform\n",
    "\n",
    "with TaskGroup('model_training') as model_training:\n",
    "    train_rf = PythonOperator(...)\n",
    "    train_lr = PythonOperator(...)\n",
    "\n",
    "data_prep >> model_training\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Other Orchestration Tools\n",
    "\n",
    "### Kubeflow Pipelines\n",
    "\n",
    "**Kubernetes-native ML orchestration**\n",
    "\n",
    "**Advantages**:\n",
    "- ‚úÖ Designed specifically for ML\n",
    "- ‚úÖ Native Kubernetes integration\n",
    "- ‚úÖ Experiment tracking built-in\n",
    "- ‚úÖ Used by Google, Cisco, Bloomberg\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubeflow_example = '''\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import create_component_from_func\n",
    "\n",
    "# Define pipeline components\n",
    "@create_component_from_func\n",
    "def load_data_op() -> str:\n",
    "    \"\"\"Load data component\"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('/data/dataset.csv')\n",
    "    return '/tmp/data.csv'\n",
    "\n",
    "@create_component_from_func\n",
    "def train_model_op(data_path: str) -> str:\n",
    "    \"\"\"Train model component\"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import pickle\n",
    "    \n",
    "    # Load data, train model\n",
    "    model = RandomForestClassifier()\n",
    "    # model.fit(X, y)\n",
    "    \n",
    "    # Save model\n",
    "    with open('/tmp/model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    return '/tmp/model.pkl'\n",
    "\n",
    "@create_component_from_func\n",
    "def deploy_model_op(model_path: str):\n",
    "    \"\"\"Deploy model component\"\"\"\n",
    "    print(f\"Deploying model from {model_path}\")\n",
    "    # Deployment logic\n",
    "\n",
    "# Define pipeline\n",
    "@dsl.pipeline(\n",
    "    name='ML Training Pipeline',\n",
    "    description='Train and deploy ML model'\n",
    ")\n",
    "def ml_pipeline():\n",
    "    # Create pipeline steps\n",
    "    load_data_task = load_data_op()\n",
    "    train_model_task = train_model_op(load_data_task.output)\n",
    "    deploy_model_task = deploy_model_op(train_model_task.output)\n",
    "\n",
    "# Compile pipeline\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=ml_pipeline,\n",
    "    package_path='ml_pipeline.yaml'\n",
    ")\n",
    "\n",
    "# Deploy to Kubeflow\n",
    "client = kfp.Client()\n",
    "client.create_run_from_pipeline_func(\n",
    "    ml_pipeline,\n",
    "    arguments={},\n",
    "    experiment_name='ml_experiment'\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"üìù Kubeflow Pipeline Example:\")\n",
    "print(\"=\"*70)\n",
    "print(kubeflow_example)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefect\n",
    "\n",
    "**Modern workflow orchestration**\n",
    "\n",
    "**Advantages**:\n",
    "- ‚úÖ Python-first (more Pythonic than Airflow)\n",
    "- ‚úÖ Better error handling\n",
    "- ‚úÖ Dynamic workflows\n",
    "- ‚úÖ Cloud-hosted option\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect_example = '''\n",
    "from prefect import task, Flow\n",
    "from prefect.schedules import CronSchedule\n",
    "import pandas as pd\n",
    "\n",
    "@task\n",
    "def extract_data():\n",
    "    df = pd.read_csv('/data/dataset.csv')\n",
    "    return df\n",
    "\n",
    "@task\n",
    "def transform_data(df):\n",
    "    # Feature engineering\n",
    "    df_transformed = df.fillna(0)\n",
    "    return df_transformed\n",
    "\n",
    "@task\n",
    "def train_model(df):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "@task\n",
    "def deploy_model(model):\n",
    "    # Deployment logic\n",
    "    print(\"Model deployed!\")\n",
    "\n",
    "# Define flow\n",
    "schedule = CronSchedule(\"0 2 * * *\")  # Daily at 2 AM\n",
    "\n",
    "with Flow(\"ml_pipeline\", schedule=schedule) as flow:\n",
    "    data = extract_data()\n",
    "    transformed = transform_data(data)\n",
    "    model = train_model(transformed)\n",
    "    deploy_model(model)\n",
    "\n",
    "# Register flow\n",
    "flow.register(project_name=\"ml_project\")\n",
    "\n",
    "# Run flow\n",
    "flow.run()\n",
    "'''\n",
    "\n",
    "print(\"üìù Prefect Pipeline Example:\")\n",
    "print(\"=\"*70)\n",
    "print(prefect_example)\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° Prefect advantages:\")\n",
    "print(\"  - More Pythonic syntax\")\n",
    "print(\"  - Better error handling\")\n",
    "print(\"  - Native Python dataflow\")\n",
    "print(\"  - Easier to test locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Comparison\n",
    "\n",
    "| Feature | Airflow | Kubeflow | Prefect | Argo Workflows |\n",
    "|---------|---------|----------|---------|----------------|\n",
    "| **Best For** | General workflows | ML on Kubernetes | Python workflows | Kubernetes workflows |\n",
    "| **Learning Curve** | Medium | High | Low | Medium |\n",
    "| **ML-Specific** | ‚ùå | ‚úÖ | ‚ùå | ‚ùå |\n",
    "| **Kubernetes** | Optional | Native | Optional | Native |\n",
    "| **UI** | ‚úÖ Good | ‚úÖ Good | ‚úÖ Excellent | ‚úÖ Good |\n",
    "| **Community** | ‚úÖ Large | Medium | Growing | Medium |\n",
    "| **Used By** | Airbnb, Netflix | Google, Cisco | Prefect Cloud | Intuit, Adobe |\n",
    "\n",
    "**When to use each**:\n",
    "- **Airflow**: General data engineering + ML (most common)\n",
    "- **Kubeflow**: ML-focused, already on Kubernetes\n",
    "- **Prefect**: New projects, Python-heavy teams\n",
    "- **Argo**: Kubernetes-native, CI/CD + ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Best Practices\n",
    "\n",
    "### 1. Idempotency\n",
    "\n",
    "**Tasks should produce same result when run multiple times**\n",
    "\n",
    "```python\n",
    "# ‚ùå BAD: Appends every time\n",
    "def bad_task():\n",
    "    df = pd.read_csv('data.csv')\n",
    "    df.to_csv('output.csv', mode='a')  # Appends!\n",
    "\n",
    "# ‚úÖ GOOD: Overwrites\n",
    "def good_task():\n",
    "    df = pd.read_csv('data.csv')\n",
    "    df.to_csv('output.csv', mode='w')  # Overwrites\n",
    "```\n",
    "\n",
    "### 2. Error Handling\n",
    "\n",
    "```python\n",
    "@task(retries=3, retry_delay=timedelta(minutes=5))\n",
    "def robust_task():\n",
    "    try:\n",
    "        # Task logic\n",
    "        result = risky_operation()\n",
    "        return result\n",
    "    except SpecificError as e:\n",
    "        logger.error(f\"Task failed: {e}\")\n",
    "        raise  # Re-raise for Airflow to retry\n",
    "```\n",
    "\n",
    "### 3. Monitoring & Alerting\n",
    "\n",
    "```python\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "send_alert = EmailOperator(\n",
    "    task_id='send_alert',\n",
    "    to='ml-team@company.com',\n",
    "    subject='Pipeline Failed',\n",
    "    html_content='Pipeline {{ dag.dag_id }} failed.',\n",
    "    trigger_rule='one_failed',  # Trigger if any upstream task fails\n",
    ")\n",
    "```\n",
    "\n",
    "### 4. Testing DAGs\n",
    "\n",
    "```python\n",
    "# Test DAG structure\n",
    "def test_dag_loaded():\n",
    "    from airflow.models import DagBag\n",
    "    dagbag = DagBag()\n",
    "    assert 'ml_pipeline' in dagbag.dags\n",
    "    assert len(dagbag.import_errors) == 0\n",
    "\n",
    "# Test task logic\n",
    "def test_transform_data():\n",
    "    df = pd.DataFrame({'a': [1, 2, 3]})\n",
    "    result = transform_data(df)\n",
    "    assert result is not None\n",
    "    assert len(result) == 3\n",
    "```\n",
    "\n",
    "### 5. Secrets Management\n",
    "\n",
    "```python\n",
    "from airflow.hooks.base import BaseHook\n",
    "\n",
    "# ‚ùå BAD: Hardcoded credentials\n",
    "db_conn = psycopg2.connect(\n",
    "    host='db.example.com',\n",
    "    password='mysecret123'\n",
    ")\n",
    "\n",
    "# ‚úÖ GOOD: Use Airflow connections\n",
    "connection = BaseHook.get_connection('my_postgres')\n",
    "db_conn = psycopg2.connect(\n",
    "    host=connection.host,\n",
    "    password=connection.password\n",
    ")\n",
    "```\n",
    "\n",
    "### 6. Resource Management\n",
    "\n",
    "```python\n",
    "# Set resource limits\n",
    "heavy_task = PythonOperator(\n",
    "    task_id='train_large_model',\n",
    "    python_callable=train_model,\n",
    "    pool='gpu_pool',  # Use dedicated resource pool\n",
    "    queue='gpu_queue',  # Use specific queue\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ CAPSTONE PROJECT: Production ML Pipeline\n",
    "\n",
    "### Project Goal\n",
    "Build a complete production ML pipeline with:\n",
    "- Data ingestion from multiple sources\n",
    "- Data validation and quality checks\n",
    "- Feature engineering\n",
    "- Multiple model training\n",
    "- Model evaluation and selection\n",
    "- Automated deployment\n",
    "- Monitoring and alerting\n",
    "- Scheduled retraining\n",
    "\n",
    "### System Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    AIRFLOW SCHEDULER                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                  ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ             ‚îÇ             ‚îÇ\n",
    "    v             v             v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Worker 1‚îÇ  ‚îÇ Worker 2‚îÇ  ‚îÇ Worker 3‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îÇ             ‚îÇ             ‚îÇ\n",
    "    v             v             v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ        DATA SOURCES                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Database  ‚Ä¢ API  ‚Ä¢ S3  ‚Ä¢ CSV      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ      FEATURE STORE (Redis)           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ      MODEL REGISTRY (MLflow)         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ      DEPLOYMENT (Kubernetes)         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Production ML Pipeline\n",
    "\n",
    "production_pipeline = '''\n",
    "\"\"\"\n",
    "Production ML Pipeline - Customer Churn Prediction\n",
    "\n",
    "Schedule: Daily at 2 AM\n",
    "Purpose: Retrain churn model with latest data\n",
    "Deployment: Automatic if accuracy > 85%\n",
    "\"\"\"\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pickle\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'db_conn_id': 'postgres_production',\n",
    "    's3_bucket': 'ml-models-prod',\n",
    "    'model_registry': 'mlflow_server',\n",
    "    'min_accuracy': 0.85,\n",
    "    'min_rows': 1000,\n",
    "    'alert_email': 'ml-team@company.com',\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Task 1: Extract data from multiple sources\n",
    "def extract_data(**context):\n",
    "    \"\"\"\n",
    "    Extract customer data from:\n",
    "    - PostgreSQL (customer info)\n",
    "    - S3 (behavioral data)\n",
    "    - API (realtime features)\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting data extraction...\")\n",
    "    \n",
    "    # Extract from PostgreSQL\n",
    "    pg_hook = PostgresHook(postgres_conn_id=CONFIG['db_conn_id'])\n",
    "    sql = \"\"\"\n",
    "        SELECT customer_id, age, tenure, monthly_charges, total_charges,\n",
    "               contract_type, payment_method, churn\n",
    "        FROM customers\n",
    "        WHERE last_updated >= CURRENT_DATE - INTERVAL '7 days'\n",
    "    \"\"\"\n",
    "    df_customers = pg_hook.get_pandas_df(sql)\n",
    "    \n",
    "    # Extract from S3 (behavioral data)\n",
    "    s3_hook = S3Hook(aws_conn_id='aws_default')\n",
    "    obj = s3_hook.get_key(\n",
    "        key='behavioral_features/latest.csv',\n",
    "        bucket_name=CONFIG['s3_bucket']\n",
    "    )\n",
    "    df_behavior = pd.read_csv(obj.get()['Body'])\n",
    "    \n",
    "    # Merge datasets\n",
    "    df = df_customers.merge(df_behavior, on='customer_id', how='left')\n",
    "    \n",
    "    # Save to temp\n",
    "    df.to_csv('/tmp/raw_data.csv', index=False)\n",
    "    \n",
    "    # Push metadata\n",
    "    context['ti'].xcom_push(key='row_count', value=len(df))\n",
    "    context['ti'].xcom_push(key='extraction_time', value=datetime.now().isoformat())\n",
    "    \n",
    "    logger.info(f\"Extracted {len(df)} rows\")\n",
    "    return 'Data extracted'\n",
    "\n",
    "# Task 2: Data validation\n",
    "def validate_data(**context):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality checks\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting data validation...\")\n",
    "    \n",
    "    df = pd.read_csv('/tmp/raw_data.csv')\n",
    "    \n",
    "    validation_results = {\n",
    "        'checks_passed': [],\n",
    "        'checks_failed': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    # Check 1: Minimum row count\n",
    "    if len(df) < CONFIG['min_rows']:\n",
    "        validation_results['checks_failed'].append(\n",
    "            f\"Insufficient data: {len(df)} < {CONFIG['min_rows']}\"\n",
    "        )\n",
    "    else:\n",
    "        validation_results['checks_passed'].append('Row count check')\n",
    "    \n",
    "    # Check 2: Missing values\n",
    "    missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "    if missing_pct > 20:\n",
    "        validation_results['checks_failed'].append(\n",
    "            f\"High missing values: {missing_pct:.2f}%\"\n",
    "        )\n",
    "    elif missing_pct > 10:\n",
    "        validation_results['warnings'].append(\n",
    "            f\"Moderate missing values: {missing_pct:.2f}%\"\n",
    "        )\n",
    "    else:\n",
    "        validation_results['checks_passed'].append('Missing values check')\n",
    "    \n",
    "    # Check 3: Duplicates\n",
    "    dup_count = df.duplicated(subset=['customer_id']).sum()\n",
    "    if dup_count > 0:\n",
    "        validation_results['warnings'].append(f\"Found {dup_count} duplicates\")\n",
    "    else:\n",
    "        validation_results['checks_passed'].append('Duplicate check')\n",
    "    \n",
    "    # Check 4: Data types\n",
    "    if df['age'].dtype not in [np.int64, np.float64]:\n",
    "        validation_results['checks_failed'].append(\"Invalid age data type\")\n",
    "    else:\n",
    "        validation_results['checks_passed'].append('Data type check')\n",
    "    \n",
    "    # Check 5: Target distribution\n",
    "    churn_rate = df['churn'].mean()\n",
    "    if churn_rate < 0.05 or churn_rate > 0.5:\n",
    "        validation_results['warnings'].append(\n",
    "            f\"Unusual churn rate: {churn_rate:.2%}\"\n",
    "        )\n",
    "    else:\n",
    "        validation_results['checks_passed'].append('Target distribution check')\n",
    "    \n",
    "    # Save validation report\n",
    "    context['ti'].xcom_push(key='validation_results', value=validation_results)\n",
    "    \n",
    "    # Fail if critical checks failed\n",
    "    if validation_results['checks_failed']:\n",
    "        raise ValueError(f\"Data validation failed: {validation_results['checks_failed']}\")\n",
    "    \n",
    "    logger.info(f\"‚úÖ Validation passed! {len(validation_results['checks_passed'])} checks\")\n",
    "    \n",
    "    if validation_results['warnings']:\n",
    "        logger.warning(f\"Warnings: {validation_results['warnings']}\")\n",
    "    \n",
    "    return 'Data validated'\n",
    "\n",
    "# Task 3: Feature engineering\n",
    "def engineer_features(**context):\n",
    "    \"\"\"\n",
    "    Advanced feature engineering\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting feature engineering...\")\n",
    "    \n",
    "    df = pd.read_csv('/tmp/raw_data.csv')\n",
    "    \n",
    "    # Handle missing values\n",
    "    df = df.fillna(df.median(numeric_only=True))\n",
    "    \n",
    "    # Create new features\n",
    "    df['avg_monthly_charges'] = df['total_charges'] / (df['tenure'] + 1)\n",
    "    df['tenure_years'] = df['tenure'] / 12\n",
    "    df['is_new_customer'] = (df['tenure'] < 12).astype(int)\n",
    "    df['is_high_value'] = (df['monthly_charges'] > df['monthly_charges'].quantile(0.75)).astype(int)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    df = pd.get_dummies(df, columns=['contract_type', 'payment_method'], drop_first=True)\n",
    "    \n",
    "    # Save processed data\n",
    "    df.to_csv('/tmp/processed_data.csv', index=False)\n",
    "    \n",
    "    # Push feature metadata\n",
    "    context['ti'].xcom_push(key='feature_count', value=len(df.columns) - 1)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Feature engineering complete! {len(df.columns) - 1} features\")\n",
    "    return 'Features engineered'\n",
    "\n",
    "# Task 4a: Train Random Forest\n",
    "def train_random_forest(**context):\n",
    "    logger.info(\"Training Random Forest...\")\n",
    "    \n",
    "    df = pd.read_csv('/tmp/processed_data.csv')\n",
    "    X = df.drop(['customer_id', 'churn'], axis=1)\n",
    "    y = df['churn']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train with MLflow tracking\n",
    "    with mlflow.start_run(run_name='random_forest'):\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=20,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "        }\n",
    "        \n",
    "        # Log to MLflow\n",
    "        mlflow.log_params(model.get_params())\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # Save model\n",
    "    with open('/tmp/rf_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # Push metrics\n",
    "    for key, value in metrics.items():\n",
    "        context['ti'].xcom_push(key=f'rf_{key}', value=value)\n",
    "    \n",
    "    logger.info(f\"‚úÖ RF trained! Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    return metrics['accuracy']\n",
    "\n",
    "# Similar tasks for other models (GradientBoosting, LogisticRegression)\n",
    "# ...\n",
    "\n",
    "# Task 5: Select best model\n",
    "def select_best_model(**context):\n",
    "    ti = context['ti']\n",
    "    \n",
    "    # Pull metrics from all models\n",
    "    models = ['rf', 'gb', 'lr']\n",
    "    results = {}\n",
    "    \n",
    "    for model in models:\n",
    "        results[model] = {\n",
    "            'accuracy': ti.xcom_pull(key=f'{model}_accuracy', task_ids=f'train_{model}'),\n",
    "            'f1': ti.xcom_pull(key=f'{model}_f1', task_ids=f'train_{model}'),\n",
    "            'roc_auc': ti.xcom_pull(key=f'{model}_roc_auc', task_ids=f'train_{model}')\n",
    "        }\n",
    "    \n",
    "    # Select based on F1 score (balanced metric)\n",
    "    best_model = max(results, key=lambda x: results[x]['f1'])\n",
    "    best_metrics = results[best_model]\n",
    "    \n",
    "    logger.info(f\"üèÜ Best model: {best_model}\")\n",
    "    logger.info(f\"Metrics: {best_metrics}\")\n",
    "    \n",
    "    # Push results\n",
    "    ti.xcom_push(key='best_model', value=best_model)\n",
    "    ti.xcom_push(key='best_metrics', value=best_metrics)\n",
    "    \n",
    "    # Decision: deploy or alert\n",
    "    if best_metrics['accuracy'] >= CONFIG['min_accuracy']:\n",
    "        return 'deploy_model'\n",
    "    else:\n",
    "        return 'send_alert'\n",
    "\n",
    "# Task 6a: Deploy model\n",
    "def deploy_model(**context):\n",
    "    ti = context['ti']\n",
    "    best_model = ti.xcom_pull(key='best_model', task_ids='select_best_model')\n",
    "    \n",
    "    logger.info(f\"Deploying {best_model} to production...\")\n",
    "    \n",
    "    # 1. Upload to S3\n",
    "    s3_hook = S3Hook(aws_conn_id='aws_default')\n",
    "    s3_hook.load_file(\n",
    "        filename=f'/tmp/{best_model}_model.pkl',\n",
    "        key=f'models/churn_model_latest.pkl',\n",
    "        bucket_name=CONFIG['s3_bucket'],\n",
    "        replace=True\n",
    "    )\n",
    "    \n",
    "    # 2. Update model registry\n",
    "    mlflow.register_model(\n",
    "        f\"runs:/{context['run_id']}/model\",\n",
    "        \"churn_prediction_model\"\n",
    "    )\n",
    "    \n",
    "    # 3. Update Kubernetes deployment\n",
    "    # kubectl set image deployment/ml-api ml-api=ml-api:${NEW_VERSION}\n",
    "    \n",
    "    logger.info(\"‚úÖ Model deployed successfully!\")\n",
    "    return 'Deployment complete'\n",
    "\n",
    "# Task 6b: Send alert\n",
    "def send_alert(**context):\n",
    "    ti = context['ti']\n",
    "    best_metrics = ti.xcom_pull(key='best_metrics', task_ids='select_best_model')\n",
    "    \n",
    "    message = f\"\"\"\n",
    "    ‚ö†Ô∏è ML Pipeline Alert - Manual Review Required\n",
    "    \n",
    "    Model accuracy ({best_metrics['accuracy']:.4f}) is below threshold ({CONFIG['min_accuracy']}).\n",
    "    \n",
    "    Best Model Metrics:\n",
    "    - Accuracy: {best_metrics['accuracy']:.4f}\n",
    "    - F1 Score: {best_metrics['f1']:.4f}\n",
    "    - ROC AUC: {best_metrics['roc_auc']:.4f}\n",
    "    \n",
    "    Pipeline Run: {context['execution_date']}\n",
    "    DAG: {context['dag'].dag_id}\n",
    "    \n",
    "    Action Required: Review model performance and data quality.\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.warning(message)\n",
    "    # Send to Slack/Email/PagerDuty\n",
    "    \n",
    "    return 'Alert sent'\n",
    "\n",
    "# Define DAG\n",
    "default_args = {\n",
    "    'owner': 'ml_platform_team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'email': [CONFIG['alert_email']],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'retry_exponential_backoff': True,\n",
    "    'max_retry_delay': timedelta(minutes=30),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='production_churn_prediction_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Production ML pipeline for customer churn prediction',\n",
    "    schedule_interval='0 2 * * *',  # Daily at 2 AM\n",
    "    catchup=False,\n",
    "    tags=['ml', 'production', 'churn'],\n",
    ") as dag:\n",
    "    \n",
    "    extract = PythonOperator(task_id='extract_data', python_callable=extract_data)\n",
    "    validate = PythonOperator(task_id='validate_data', python_callable=validate_data)\n",
    "    engineer = PythonOperator(task_id='engineer_features', python_callable=engineer_features)\n",
    "    \n",
    "    train_rf = PythonOperator(task_id='train_rf', python_callable=train_random_forest)\n",
    "    train_gb = PythonOperator(task_id='train_gb', python_callable=train_gradient_boosting)\n",
    "    train_lr = PythonOperator(task_id='train_lr', python_callable=train_logistic_regression)\n",
    "    \n",
    "    select = BranchPythonOperator(task_id='select_best_model', python_callable=select_best_model)\n",
    "    \n",
    "    deploy = PythonOperator(task_id='deploy_model', python_callable=deploy_model)\n",
    "    alert = PythonOperator(task_id='send_alert', python_callable=send_alert)\n",
    "    \n",
    "    # Pipeline flow\n",
    "    extract >> validate >> engineer >> [train_rf, train_gb, train_lr] >> select\n",
    "    select >> deploy\n",
    "    select >> alert\n",
    "'''\n",
    "\n",
    "print(\"üìù Complete Production ML Pipeline:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ This is a PRODUCTION-READY pipeline with:\")\n",
    "print(\"\")\n",
    "print(\"1. Multi-source data extraction (PostgreSQL + S3 + API)\")\n",
    "print(\"2. Comprehensive data validation (5+ checks)\")\n",
    "print(\"3. Advanced feature engineering\")\n",
    "print(\"4. Parallel model training (3 models)\")\n",
    "print(\"5. MLflow experiment tracking\")\n",
    "print(\"6. Automated model selection\")\n",
    "print(\"7. Conditional deployment (accuracy threshold)\")\n",
    "print(\"8. S3 + MLflow + Kubernetes deployment\")\n",
    "print(\"9. Comprehensive error handling & retries\")\n",
    "print(\"10. Monitoring & alerting\")\n",
    "print(\"11. Scheduled retraining (daily)\")\n",
    "print(\"\")\n",
    "print(\"This is what ML Engineers build in real companies! üöÄ\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **Airflow Fundamentals**\n",
    "   - DAGs, operators, tasks, dependencies\n",
    "   - Scheduling with cron expressions\n",
    "   - XCom for inter-task communication\n",
    "\n",
    "2. **ML Pipeline Design**\n",
    "   - Extract ‚Üí Validate ‚Üí Transform ‚Üí Train ‚Üí Deploy\n",
    "   - Parallel model training\n",
    "   - Conditional branching\n",
    "\n",
    "3. **Advanced Concepts**\n",
    "   - Dynamic DAGs\n",
    "   - Sensors for waiting\n",
    "   - TaskGroups for organization\n",
    "   - BranchOperator for conditional logic\n",
    "\n",
    "4. **Other Tools**\n",
    "   - Kubeflow for ML on Kubernetes\n",
    "   - Prefect for Python-first workflows\n",
    "   - Argo for cloud-native workflows\n",
    "\n",
    "5. **Best Practices**\n",
    "   - Idempotency\n",
    "   - Error handling & retries\n",
    "   - Monitoring & alerting\n",
    "   - Testing DAGs\n",
    "   - Secrets management\n",
    "\n",
    "6. **Production Skills**\n",
    "   - Multi-source data integration\n",
    "   - Data quality validation\n",
    "   - MLflow experiment tracking\n",
    "   - Automated deployment\n",
    "   - Scheduled retraining\n",
    "\n",
    "### Interview-Ready Skills ‚úÖ\n",
    "\n",
    "**You can now answer:**\n",
    "- \"Explain how you would orchestrate an ML pipeline\"\n",
    "- \"What is Airflow and when would you use it?\"\n",
    "- \"How do you handle failures in ML pipelines?\"\n",
    "- \"Describe a production ML pipeline you've built\"\n",
    "- \"What is the difference between Airflow and Kubeflow?\"\n",
    "\n",
    "**You can now build:**\n",
    "- Automated ML training pipelines\n",
    "- Multi-model evaluation systems\n",
    "- Scheduled model retraining\n",
    "- Production deployment workflows\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Tech Companies**:\n",
    "- Airbnb: 1000+ Airflow DAGs for ML\n",
    "- Uber: Automated model retraining\n",
    "- Netflix: Recommendation model pipelines\n",
    "- Spotify: Music recommendation orchestration\n",
    "\n",
    "**Financial Services**:\n",
    "- Fraud detection model updates\n",
    "- Credit scoring automation\n",
    "- Risk model retraining\n",
    "\n",
    "**E-commerce** (Woolworths, Amazon):\n",
    "- Demand forecasting pipelines\n",
    "- Price optimization automation\n",
    "- Recommendation system updates\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Set up Airflow locally**: Use Docker Compose\n",
    "2. **Build your own pipeline**: Automate a personal ML project\n",
    "3. **Learn Kubernetes**: For scalable orchestration\n",
    "4. **Explore Kubeflow**: For ML-specific workflows\n",
    "5. **Add to portfolio**: Show end-to-end automation\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "**Documentation**:\n",
    "- [Apache Airflow Docs](https://airflow.apache.org/docs/)\n",
    "- [Kubeflow Docs](https://www.kubeflow.org/docs/)\n",
    "- [Prefect Docs](https://docs.prefect.io/)\n",
    "\n",
    "**Courses**:\n",
    "- \"Apache Airflow: The Hands-On Guide\" (Udemy)\n",
    "- \"Building Production ML Pipelines\" (Google Cloud)\n",
    "\n",
    "**Books**:\n",
    "- \"Data Pipelines with Apache Airflow\"\n",
    "- \"ML Engineering\" by Andriy Burkov\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Job Market Relevance\n",
    "\n",
    "**Woolworths ML Engineer**: ‚úÖ\n",
    "> \"Experience with Airflow or other orchestration tools for ML pipelines\"\n",
    "\n",
    "**Industry Stats**: ‚úÖ\n",
    "- 40% of ML Engineer jobs mention orchestration\n",
    "- Airflow is #1 orchestration tool\n",
    "- Production ML = Automation!\n",
    "\n",
    "**You are now job-ready for ML orchestration roles!** üéâ\n",
    "\n",
    "---\n",
    "\n",
    "## üéä Congratulations!\n",
    "\n",
    "**You've completed Week 22: Job-Critical Skills!**\n",
    "\n",
    "You now have:\n",
    "- ‚úÖ Recommender Systems (Day 1)\n",
    "- ‚úÖ Time Series Forecasting (Day 2)\n",
    "- ‚úÖ ML Orchestration (Day 3)\n",
    "\n",
    "**These are the TOP 3 missing skills from job postings.**\n",
    "\n",
    "**Combined with Week 21, you now have 100% coverage of 2025 job market requirements!**\n",
    "\n",
    "---\n",
    "\n",
    "**You are ready to interview for Senior ML Engineer roles!** üöÄ\n",
    "\n",
    "Go build amazing things! üí™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
