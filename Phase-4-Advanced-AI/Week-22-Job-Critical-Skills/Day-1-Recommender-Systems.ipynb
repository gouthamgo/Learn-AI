{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Week 22, Day 1: Recommender Systems\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/Learn-AI/blob/main/Phase-4-Advanced-AI/Week-22-Job-Critical-Skills/Day-1-Recommender-Systems.ipynb)\n",
    "\n",
    "## ðŸš€ Why This Matters\n",
    "\n",
    "Recommender systems are **EVERYWHERE** in tech:\n",
    "- ðŸ“º **Netflix**: 80% of watched content comes from recommendations\n",
    "- ðŸ›’ **Amazon**: 35% of revenue from recommendations\n",
    "- ðŸŽµ **Spotify**: Discover Weekly has 40M+ active users\n",
    "- ðŸ“± **TikTok**: Entire platform built on recommendation algorithm\n",
    "\n",
    "**Job Market Reality:**\n",
    "- REA Group specifically lists \"Recommender systems (Collaborative Filtering, Content-Based, Hybrid)\" âœ…\n",
    "- E-commerce companies (Woolworths, Amazon) heavily use RecSys\n",
    "- Social media, streaming, ad tech - all need RecSys engineers\n",
    "\n",
    "## ðŸ“‹ What You'll Learn Today\n",
    "\n",
    "1. **Collaborative Filtering** - Learn from user behavior\n",
    "   - User-User similarity\n",
    "   - Item-Item similarity\n",
    "2. **Matrix Factorization** - The Netflix Prize winner!\n",
    "   - SVD (Singular Value Decomposition)\n",
    "   - ALS (Alternating Least Squares)\n",
    "3. **Content-Based Filtering** - Recommend similar items\n",
    "4. **Hybrid Systems** - Combine the best of both worlds\n",
    "5. **Deep Learning RecSys** - Neural Collaborative Filtering\n",
    "6. **Learning to Rank** - Optimize for relevance\n",
    "7. **ðŸ† Project: E-Commerce Recommendation Engine**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Understanding Recommender Systems\n",
    "\n",
    "### Types of Recommender Systems\n",
    "\n",
    "1. **Collaborative Filtering**: \"Users who liked this also liked...\"\n",
    "   - User-based: Find similar users, recommend what they liked\n",
    "   - Item-based: Find similar items, recommend those\n",
    "\n",
    "2. **Content-Based**: \"Since you liked X with features Y, try Z with similar features\"\n",
    "   - Uses item features/metadata\n",
    "   - Independent of other users\n",
    "\n",
    "3. **Hybrid**: Combine collaborative + content-based\n",
    "   - Solves cold start problem\n",
    "   - Better accuracy\n",
    "\n",
    "4. **Deep Learning**: Neural networks for recommendations\n",
    "   - Can learn complex patterns\n",
    "   - Used by YouTube, TikTok\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "- **Precision@K**: Of K recommendations, how many are relevant?\n",
    "- **Recall@K**: Of all relevant items, how many are in top K?\n",
    "- **NDCG**: Normalized Discounted Cumulative Gain (considers ranking)\n",
    "- **MAE/RMSE**: For rating prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install surprise scikit-learn pandas numpy matplotlib seaborn -q\n",
    "!pip install implicit -q  # For ALS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All libraries installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Collaborative Filtering\n",
    "\n",
    "### 2.1 User-Based Collaborative Filtering\n",
    "\n",
    "**Idea**: Find users similar to you, recommend what they liked.\n",
    "\n",
    "**Example**:\n",
    "- You liked: \"Inception\", \"Interstellar\", \"The Matrix\"\n",
    "- User B (80% similar) also liked those + \"Blade Runner\"\n",
    "- Recommendation: \"Blade Runner\" âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample user-item rating matrix\n",
    "ratings_data = {\n",
    "    'user_id': [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5],\n",
    "    'item_id': [101, 102, 103, 101, 102, 104, 102, 103, 104, 101, 103, 105, 102, 104, 105],\n",
    "    'rating': [5, 4, 3, 5, 5, 4, 4, 5, 4, 5, 4, 3, 5, 5, 4]\n",
    "}\n",
    "\n",
    "ratings_df = pd.DataFrame(ratings_data)\n",
    "\n",
    "# Create user-item matrix\n",
    "user_item_matrix = ratings_df.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='item_id',\n",
    "    values='rating',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"User-Item Rating Matrix:\")\n",
    "print(user_item_matrix)\n",
    "print(f\"\\nShape: {user_item_matrix.shape} (users x items)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate user-user similarity\n",
    "user_similarity = cosine_similarity(user_item_matrix)\n",
    "user_similarity_df = pd.DataFrame(\n",
    "    user_similarity,\n",
    "    index=user_item_matrix.index,\n",
    "    columns=user_item_matrix.index\n",
    ")\n",
    "\n",
    "print(\"User-User Similarity Matrix:\")\n",
    "print(user_similarity_df.round(3))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(user_similarity_df, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('User-User Similarity (Cosine)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-based recommendation function\n",
    "def recommend_user_based(user_id, user_item_matrix, user_similarity_df, top_n=3):\n",
    "    \"\"\"\n",
    "    Recommend items based on similar users' preferences.\n",
    "    \"\"\"\n",
    "    # Get similarity scores for this user\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False)[1:]  # Exclude self\n",
    "    \n",
    "    # Get items this user hasn't rated\n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    unrated_items = user_ratings[user_ratings == 0].index\n",
    "    \n",
    "    # Calculate predicted ratings\n",
    "    predictions = {}\n",
    "    for item in unrated_items:\n",
    "        # Weighted average of similar users' ratings\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        \n",
    "        for similar_user in similar_users.index:\n",
    "            rating = user_item_matrix.loc[similar_user, item]\n",
    "            similarity = similar_users[similar_user]\n",
    "            \n",
    "            if rating > 0:  # User rated this item\n",
    "                numerator += rating * similarity\n",
    "                denominator += similarity\n",
    "        \n",
    "        if denominator > 0:\n",
    "            predictions[item] = numerator / denominator\n",
    "    \n",
    "    # Sort and return top N\n",
    "    sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_predictions[:top_n]\n",
    "\n",
    "# Test recommendations\n",
    "user_id = 1\n",
    "recommendations = recommend_user_based(user_id, user_item_matrix, user_similarity_df)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Top Recommendations for User {user_id}:\")\n",
    "for item, score in recommendations:\n",
    "    print(f\"  Item {item}: Predicted Rating = {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Item-Based Collaborative Filtering\n",
    "\n",
    "**Idea**: \"You liked item A, here's item B which is similar to A\"\n",
    "\n",
    "**Advantages over user-based**:\n",
    "- Items change less frequently than user preferences\n",
    "- More stable recommendations\n",
    "- Easier to explain: \"Because you bought X, you might like Y\"\n",
    "\n",
    "**Used by**: Amazon, YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate item-item similarity\n",
    "item_similarity = cosine_similarity(user_item_matrix.T)  # Transpose!\n",
    "item_similarity_df = pd.DataFrame(\n",
    "    item_similarity,\n",
    "    index=user_item_matrix.columns,\n",
    "    columns=user_item_matrix.columns\n",
    ")\n",
    "\n",
    "print(\"Item-Item Similarity Matrix:\")\n",
    "print(item_similarity_df.round(3))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(item_similarity_df, annot=True, fmt='.2f', cmap='viridis')\n",
    "plt.title('Item-Item Similarity (Cosine)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-based recommendation function\n",
    "def recommend_item_based(user_id, user_item_matrix, item_similarity_df, top_n=3):\n",
    "    \"\"\"\n",
    "    Recommend items similar to what user already liked.\n",
    "    \"\"\"\n",
    "    # Get items this user rated\n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    rated_items = user_ratings[user_ratings > 0]\n",
    "    unrated_items = user_ratings[user_ratings == 0].index\n",
    "    \n",
    "    # Calculate predicted ratings\n",
    "    predictions = {}\n",
    "    for item in unrated_items:\n",
    "        # Weighted average based on similar items\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        \n",
    "        for rated_item, rating in rated_items.items():\n",
    "            similarity = item_similarity_df.loc[item, rated_item]\n",
    "            numerator += rating * similarity\n",
    "            denominator += similarity\n",
    "        \n",
    "        if denominator > 0:\n",
    "            predictions[item] = numerator / denominator\n",
    "    \n",
    "    # Sort and return top N\n",
    "    sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_predictions[:top_n]\n",
    "\n",
    "# Test recommendations\n",
    "recommendations = recommend_item_based(user_id, user_item_matrix, item_similarity_df)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Item-Based Recommendations for User {user_id}:\")\n",
    "for item, score in recommendations:\n",
    "    print(f\"  Item {item}: Predicted Rating = {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Matrix Factorization (The Netflix Prize Winner!)\n",
    "\n",
    "### Why Matrix Factorization?\n",
    "\n",
    "**Problem with CF**: User-item matrix is HUGE and SPARSE\n",
    "- Netflix: 500K users Ã— 20K movies = 10 billion entries\n",
    "- Most users rate <1% of items\n",
    "- 99%+ sparsity!\n",
    "\n",
    "**Solution**: Decompose into smaller matrices\n",
    "- User-Item Matrix (m Ã— n) â†’ User-Factor (m Ã— k) Ã— Factor-Item (k Ã— n)\n",
    "- k = 10-100 latent factors (vs thousands of items)\n",
    "\n",
    "### SVD (Singular Value Decomposition)\n",
    "\n",
    "**Math**: R â‰ˆ U Ã— Î£ Ã— V^T\n",
    "- U: User embeddings in latent space\n",
    "- Î£: Importance of each factor\n",
    "- V: Item embeddings in latent space\n",
    "\n",
    "**Surprise library**: Production-ready SVD implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Create dataset in Surprise format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings_df[['user_id', 'item_id', 'rating']], reader)\n",
    "\n",
    "# Split train/test\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train SVD model\n",
    "svd_model = SVD(\n",
    "    n_factors=10,      # Number of latent factors (k)\n",
    "    n_epochs=20,       # Training iterations\n",
    "    lr_all=0.005,      # Learning rate\n",
    "    reg_all=0.02,      # Regularization\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svd_model.fit(trainset)\n",
    "\n",
    "# Evaluate\n",
    "predictions = svd_model.test(testset)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "mae = accuracy.mae(predictions)\n",
    "\n",
    "print(f\"\\nâœ… SVD Model Trained!\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with SVD\n",
    "user_id = 1\n",
    "all_items = ratings_df['item_id'].unique()\n",
    "\n",
    "# Get items user hasn't rated\n",
    "rated_items = ratings_df[ratings_df['user_id'] == user_id]['item_id'].values\n",
    "unrated_items = [item for item in all_items if item not in rated_items]\n",
    "\n",
    "# Predict ratings for unrated items\n",
    "svd_predictions = []\n",
    "for item in unrated_items:\n",
    "    pred = svd_model.predict(user_id, item)\n",
    "    svd_predictions.append((item, pred.est))\n",
    "\n",
    "# Sort by predicted rating\n",
    "svd_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ SVD Recommendations for User {user_id}:\")\n",
    "for item, rating in svd_predictions[:3]:\n",
    "    print(f\"  Item {item}: Predicted Rating = {rating:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation for Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different algorithms\n",
    "from surprise import SVD, NMF, KNNBasic\n",
    "\n",
    "algorithms = {\n",
    "    'SVD': SVD(n_factors=10, random_state=42),\n",
    "    'NMF': NMF(n_factors=10, random_state=42),\n",
    "    'KNN-Basic': KNNBasic(k=3)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, algo in algorithms.items():\n",
    "    cv_results = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=3, verbose=False)\n",
    "    results[name] = {\n",
    "        'RMSE': cv_results['test_rmse'].mean(),\n",
    "        'MAE': cv_results['test_mae'].mean()\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nðŸ“Š Algorithm Comparison (3-Fold CV):\")\n",
    "print(results_df.round(3))\n",
    "\n",
    "# Visualize\n",
    "results_df.plot(kind='bar', figsize=(10, 5))\n",
    "plt.title('Recommender Algorithm Performance')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Content-Based Filtering\n",
    "\n",
    "### When to Use Content-Based?\n",
    "\n",
    "**Advantages**:\n",
    "- âœ… No cold start problem for new items (if you have metadata)\n",
    "- âœ… Can explain recommendations: \"Because this movie is Action/Sci-Fi like Inception\"\n",
    "- âœ… Works with limited user data\n",
    "\n",
    "**Disadvantages**:\n",
    "- âŒ Limited serendipity (only recommends similar items)\n",
    "- âŒ Requires good item features/metadata\n",
    "\n",
    "**Example**: Recommend movies based on genre, director, actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create movie dataset with features\n",
    "movies_data = {\n",
    "    'movie_id': [101, 102, 103, 104, 105],\n",
    "    'title': ['Inception', 'Interstellar', 'The Matrix', 'Blade Runner', 'Avatar'],\n",
    "    'genre': [\n",
    "        'Action Sci-Fi Thriller',\n",
    "        'Adventure Drama Sci-Fi',\n",
    "        'Action Sci-Fi',\n",
    "        'Sci-Fi Thriller',\n",
    "        'Action Adventure Sci-Fi'\n",
    "    ],\n",
    "    'director': [\n",
    "        'Christopher Nolan',\n",
    "        'Christopher Nolan',\n",
    "        'Wachowskis',\n",
    "        'Ridley Scott',\n",
    "        'James Cameron'\n",
    "    ]\n",
    "}\n",
    "\n",
    "movies_df = pd.DataFrame(movies_data)\n",
    "movies_df['features'] = movies_df['genre'] + ' ' + movies_df['director']\n",
    "\n",
    "print(\"Movies Dataset:\")\n",
    "print(movies_df[['title', 'features']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectors from features\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(movies_df['features'])\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Features: {tfidf.get_feature_names_out()}\")\n",
    "\n",
    "# Calculate movie similarity\n",
    "movie_similarity = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "movie_similarity_df = pd.DataFrame(\n",
    "    movie_similarity,\n",
    "    index=movies_df['title'],\n",
    "    columns=movies_df['title']\n",
    ")\n",
    "\n",
    "print(\"\\nMovie Similarity Matrix:\")\n",
    "print(movie_similarity_df.round(3))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(movie_similarity_df, annot=True, fmt='.2f', cmap='YlOrRd')\n",
    "plt.title('Movie Similarity (Content-Based)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-based recommendation function\n",
    "def recommend_content_based(movie_title, movies_df, similarity_df, top_n=3):\n",
    "    \"\"\"\n",
    "    Recommend movies similar to the given movie.\n",
    "    \"\"\"\n",
    "    # Get similarity scores\n",
    "    similar_scores = similarity_df[movie_title].sort_values(ascending=False)[1:]  # Exclude self\n",
    "    \n",
    "    # Get top N\n",
    "    top_movies = similar_scores.head(top_n)\n",
    "    \n",
    "    # Get movie details\n",
    "    recommendations = []\n",
    "    for movie, score in top_movies.items():\n",
    "        movie_info = movies_df[movies_df['title'] == movie].iloc[0]\n",
    "        recommendations.append({\n",
    "            'title': movie,\n",
    "            'similarity': score,\n",
    "            'genre': movie_info['genre'],\n",
    "            'director': movie_info['director']\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test recommendations\n",
    "movie = 'Inception'\n",
    "recommendations = recommend_content_based(movie, movies_df, movie_similarity_df)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Because you liked '{movie}', you might also like:\\n\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec['title']} (Similarity: {rec['similarity']:.2f})\")\n",
    "    print(f\"   Genre: {rec['genre']}\")\n",
    "    print(f\"   Director: {rec['director']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Hybrid Recommender Systems\n",
    "\n",
    "### Why Hybrid?\n",
    "\n",
    "**Problem**: Each approach has weaknesses\n",
    "- Collaborative: Cold start, sparsity\n",
    "- Content-based: No serendipity, over-specialization\n",
    "\n",
    "**Solution**: Combine them!\n",
    "\n",
    "### Hybrid Approaches\n",
    "\n",
    "1. **Weighted**: Score = Î± Ã— CF_score + (1-Î±) Ã— CB_score\n",
    "2. **Switching**: Use CF if enough data, else CB\n",
    "3. **Mixed**: Show recommendations from both\n",
    "4. **Feature Combination**: Use CF scores as features in CB\n",
    "5. **Cascade**: CF filters, then CB re-ranks\n",
    "6. **Meta-level**: Use CB to build user profile, then CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid recommender: Weighted approach\n",
    "def recommend_hybrid(user_id, item_similarity_df, user_item_matrix, \n",
    "                     movie_similarity_df, movies_df, alpha=0.6, top_n=3):\n",
    "    \"\"\"\n",
    "    Hybrid recommender combining collaborative and content-based.\n",
    "    alpha: weight for collaborative filtering (1-alpha for content-based)\n",
    "    \"\"\"\n",
    "    # Get user's rated items\n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    rated_items = user_ratings[user_ratings > 0]\n",
    "    unrated_items = user_ratings[user_ratings == 0].index\n",
    "    \n",
    "    hybrid_scores = {}\n",
    "    \n",
    "    for item in unrated_items:\n",
    "        # Collaborative filtering score\n",
    "        cf_score = 0\n",
    "        cf_denominator = 0\n",
    "        for rated_item, rating in rated_items.items():\n",
    "            similarity = item_similarity_df.loc[item, rated_item]\n",
    "            cf_score += rating * similarity\n",
    "            cf_denominator += similarity\n",
    "        cf_score = cf_score / cf_denominator if cf_denominator > 0 else 0\n",
    "        \n",
    "        # Content-based score\n",
    "        cb_score = 0\n",
    "        cb_count = 0\n",
    "        item_title = movies_df[movies_df['movie_id'] == item]['title'].iloc[0]\n",
    "        for rated_item in rated_items.index:\n",
    "            rated_title = movies_df[movies_df['movie_id'] == rated_item]['title'].iloc[0]\n",
    "            if rated_title in movie_similarity_df.columns:\n",
    "                cb_score += movie_similarity_df.loc[item_title, rated_title]\n",
    "                cb_count += 1\n",
    "        cb_score = cb_score / cb_count if cb_count > 0 else 0\n",
    "        \n",
    "        # Combine scores\n",
    "        hybrid_scores[item] = alpha * cf_score + (1 - alpha) * cb_score\n",
    "    \n",
    "    # Sort and return top N\n",
    "    sorted_scores = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_scores[:top_n]\n",
    "\n",
    "# Test hybrid recommendations\n",
    "user_id = 1\n",
    "recommendations = recommend_hybrid(\n",
    "    user_id, item_similarity_df, user_item_matrix,\n",
    "    movie_similarity_df, movies_df, alpha=0.6\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Hybrid Recommendations for User {user_id}:\")\n",
    "print(\"(60% Collaborative + 40% Content-Based)\\n\")\n",
    "for item, score in recommendations:\n",
    "    movie_title = movies_df[movies_df['movie_id'] == item]['title'].iloc[0]\n",
    "    print(f\"  {movie_title} (Item {item}): Score = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Deep Learning for Recommender Systems\n",
    "\n",
    "### Neural Collaborative Filtering (NCF)\n",
    "\n",
    "**Idea**: Replace dot product with neural network\n",
    "- Traditional CF: rating = user_vector Â· item_vector\n",
    "- NCF: rating = Neural_Network(user_vector, item_vector)\n",
    "\n",
    "**Benefits**:\n",
    "- Can learn complex, non-linear patterns\n",
    "- Better than matrix factorization on large datasets\n",
    "- Used by YouTube, TikTok, Instagram\n",
    "\n",
    "**Architecture**:\n",
    "1. User & Item embeddings\n",
    "2. Concatenate embeddings\n",
    "3. Deep neural network\n",
    "4. Output: Predicted rating/probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch\n",
    "!pip install torch -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"âœ… PyTorch installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Collaborative Filtering Model\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=16, hidden_layers=[64, 32, 16]):\n",
    "        super(NCF, self).__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # MLP layers\n",
    "        layers = []\n",
    "        input_dim = embedding_dim * 2\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(hidden_layers[-1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        # Get embeddings\n",
    "        user_embed = self.user_embedding(user_ids)\n",
    "        item_embed = self.item_embedding(item_ids)\n",
    "        \n",
    "        # Concatenate\n",
    "        x = torch.cat([user_embed, item_embed], dim=1)\n",
    "        \n",
    "        # MLP\n",
    "        x = self.mlp(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x) * 5  # Scale to 1-5 rating\n",
    "        \n",
    "        return x.squeeze()\n",
    "\n",
    "print(\"âœ… NCF Model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch\n",
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, ratings_df):\n",
    "        # Convert to 0-indexed\n",
    "        self.users = torch.LongTensor(ratings_df['user_id'].values - 1)\n",
    "        self.items = torch.LongTensor(ratings_df['item_id'].values - 101)\n",
    "        self.ratings = torch.FloatTensor(ratings_df['rating'].values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = RatingsDataset(ratings_df)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "num_users = ratings_df['user_id'].nunique()\n",
    "num_items = ratings_df['item_id'].nunique()\n",
    "\n",
    "model = NCF(num_users, num_items, embedding_dim=16, hidden_layers=[32, 16, 8])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"âœ… Model initialized!\")\n",
    "print(f\"Users: {num_users}, Items: {num_items}\")\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NCF model\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for user_ids, item_ids, ratings in dataloader:\n",
    "        # Forward pass\n",
    "        predictions = model(user_ids, item_ids)\n",
    "        loss = criterion(predictions, ratings)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('NCF Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with NCF\n",
    "model.eval()\n",
    "\n",
    "user_id = 0  # 0-indexed (User 1)\n",
    "all_items = torch.LongTensor(range(num_items))\n",
    "user_ids = torch.LongTensor([user_id] * num_items)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(user_ids, all_items)\n",
    "\n",
    "# Get top recommendations\n",
    "top_items = torch.argsort(predictions, descending=True)\n",
    "\n",
    "# Filter out already rated items\n",
    "rated_items = ratings_df[ratings_df['user_id'] == user_id + 1]['item_id'].values - 101\n",
    "\n",
    "print(f\"\\nðŸŽ¯ NCF Recommendations for User {user_id + 1}:\\n\")\n",
    "count = 0\n",
    "for item_idx in top_items:\n",
    "    if item_idx.item() not in rated_items:\n",
    "        actual_item_id = item_idx.item() + 101\n",
    "        pred_rating = predictions[item_idx].item()\n",
    "        movie_title = movies_df[movies_df['movie_id'] == actual_item_id]['title'].iloc[0]\n",
    "        print(f\"  {movie_title} (Item {actual_item_id}): Predicted Rating = {pred_rating:.2f}\")\n",
    "        count += 1\n",
    "        if count >= 3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Evaluation Metrics\n",
    "\n",
    "### Ranking Metrics\n",
    "\n",
    "**Precision@K**: Of top K recommendations, how many are relevant?\n",
    "$$Precision@K = \\frac{\\text{Relevant items in top K}}{K}$$\n",
    "\n",
    "**Recall@K**: Of all relevant items, how many are in top K?\n",
    "$$Recall@K = \\frac{\\text{Relevant items in top K}}{\\text{Total relevant items}}$$\n",
    "\n",
    "**NDCG@K**: Normalized Discounted Cumulative Gain\n",
    "- Considers ranking position (top recommendations matter more)\n",
    "- Industry standard metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics implementation\n",
    "def precision_at_k(recommended_items, relevant_items, k):\n",
    "    \"\"\"Calculate Precision@K\"\"\"\n",
    "    recommended_k = recommended_items[:k]\n",
    "    relevant_in_k = len(set(recommended_k) & set(relevant_items))\n",
    "    return relevant_in_k / k if k > 0 else 0\n",
    "\n",
    "def recall_at_k(recommended_items, relevant_items, k):\n",
    "    \"\"\"Calculate Recall@K\"\"\"\n",
    "    recommended_k = recommended_items[:k]\n",
    "    relevant_in_k = len(set(recommended_k) & set(relevant_items))\n",
    "    return relevant_in_k / len(relevant_items) if len(relevant_items) > 0 else 0\n",
    "\n",
    "def ndcg_at_k(recommended_items, relevant_items, k):\n",
    "    \"\"\"Calculate NDCG@K\"\"\"\n",
    "    dcg = 0\n",
    "    for i, item in enumerate(recommended_items[:k]):\n",
    "        if item in relevant_items:\n",
    "            # Discounted gain (1/log2(position+2))\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "    \n",
    "    # Ideal DCG (best possible ranking)\n",
    "    idcg = sum([1 / np.log2(i + 2) for i in range(min(len(relevant_items), k))])\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# Example evaluation\n",
    "recommended = [102, 104, 105, 106, 107]  # Recommended items\n",
    "relevant = [102, 104, 108, 109]  # Actually relevant items\n",
    "\n",
    "k = 3\n",
    "print(f\"Recommended (top {k}): {recommended[:k]}\")\n",
    "print(f\"Relevant items: {relevant}\")\n",
    "print(f\"\\nMetrics @ K={k}:\")\n",
    "print(f\"  Precision@{k}: {precision_at_k(recommended, relevant, k):.3f}\")\n",
    "print(f\"  Recall@{k}: {recall_at_k(recommended, relevant, k):.3f}\")\n",
    "print(f\"  NDCG@{k}: {ndcg_at_k(recommended, relevant, k):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ† CAPSTONE PROJECT: E-Commerce Recommendation Engine\n",
    "\n",
    "### Project Goal\n",
    "Build a complete recommendation system for an e-commerce platform using real-world data.\n",
    "\n",
    "### Requirements\n",
    "1. Use collaborative filtering (SVD)\n",
    "2. Implement content-based filtering\n",
    "3. Create hybrid recommender\n",
    "4. Evaluate with Precision@K, Recall@K, NDCG\n",
    "5. Handle cold start for new users/items\n",
    "6. Bonus: Add deep learning model (NCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real e-commerce dataset\n",
    "# We'll create a synthetic but realistic dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "n_users = 100\n",
    "n_products = 50\n",
    "n_ratings = 500\n",
    "\n",
    "# Create ratings\n",
    "user_ids = np.random.randint(1, n_users + 1, n_ratings)\n",
    "product_ids = np.random.randint(1, n_products + 1, n_ratings)\n",
    "ratings = np.random.randint(1, 6, n_ratings)\n",
    "\n",
    "ecommerce_ratings = pd.DataFrame({\n",
    "    'user_id': user_ids,\n",
    "    'product_id': product_ids,\n",
    "    'rating': ratings\n",
    "})\n",
    "\n",
    "# Remove duplicates (user can't rate same product twice)\n",
    "ecommerce_ratings = ecommerce_ratings.drop_duplicates(subset=['user_id', 'product_id'])\n",
    "\n",
    "print(f\"âœ… E-Commerce Dataset Created!\")\n",
    "print(f\"Users: {ecommerce_ratings['user_id'].nunique()}\")\n",
    "print(f\"Products: {ecommerce_ratings['product_id'].nunique()}\")\n",
    "print(f\"Ratings: {len(ecommerce_ratings)}\")\n",
    "print(f\"Sparsity: {(1 - len(ecommerce_ratings) / (n_users * n_products)) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "print(ecommerce_ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product metadata\n",
    "categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Toys', 'Sports']\n",
    "brands = ['BrandA', 'BrandB', 'BrandC', 'BrandD', 'BrandE']\n",
    "\n",
    "products_data = []\n",
    "for i in range(1, n_products + 1):\n",
    "    products_data.append({\n",
    "        'product_id': i,\n",
    "        'name': f'Product_{i}',\n",
    "        'category': np.random.choice(categories),\n",
    "        'brand': np.random.choice(brands),\n",
    "        'price': np.random.uniform(10, 500)\n",
    "    })\n",
    "\n",
    "products_df = pd.DataFrame(products_data)\n",
    "products_df['features'] = products_df['category'] + ' ' + products_df['brand']\n",
    "\n",
    "print(\"Products Dataset:\")\n",
    "print(products_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collaborative Filtering with SVD\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ecommerce_ratings[['user_id', 'product_id', 'rating']], reader)\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVD\n",
    "svd = SVD(n_factors=20, n_epochs=30, lr_all=0.005, reg_all=0.02, random_state=42)\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Evaluate\n",
    "predictions = svd.test(testset)\n",
    "rmse = accuracy.rmse(predictions, verbose=False)\n",
    "mae = accuracy.mae(predictions, verbose=False)\n",
    "\n",
    "print(f\"âœ… Collaborative Filtering Model Trained!\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Content-Based Filtering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# TF-IDF on product features\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(products_df['features'])\n",
    "content_similarity = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "print(f\"âœ… Content-Based Model Created!\")\n",
    "print(f\"Similarity Matrix Shape: {content_similarity.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Hybrid Recommender System\n",
    "class HybridRecommender:\n",
    "    def __init__(self, svd_model, content_sim, products_df, ratings_df, alpha=0.7):\n",
    "        self.svd = svd_model\n",
    "        self.content_sim = content_sim\n",
    "        self.products_df = products_df\n",
    "        self.ratings_df = ratings_df\n",
    "        self.alpha = alpha  # Weight for collaborative filtering\n",
    "    \n",
    "    def recommend(self, user_id, top_n=5):\n",
    "        # Get all products\n",
    "        all_products = self.products_df['product_id'].values\n",
    "        \n",
    "        # Get products user already rated\n",
    "        rated_products = self.ratings_df[self.ratings_df['user_id'] == user_id]['product_id'].values\n",
    "        unrated_products = [p for p in all_products if p not in rated_products]\n",
    "        \n",
    "        scores = {}\n",
    "        for product in unrated_products:\n",
    "            # Collaborative filtering score\n",
    "            cf_pred = self.svd.predict(user_id, product)\n",
    "            cf_score = cf_pred.est / 5.0  # Normalize to 0-1\n",
    "            \n",
    "            # Content-based score\n",
    "            cb_score = 0\n",
    "            if len(rated_products) > 0:\n",
    "                # Average similarity to user's rated products\n",
    "                product_idx = product - 1\n",
    "                similarities = []\n",
    "                for rated_prod in rated_products:\n",
    "                    rated_idx = rated_prod - 1\n",
    "                    similarities.append(self.content_sim[product_idx, rated_idx])\n",
    "                cb_score = np.mean(similarities)\n",
    "            \n",
    "            # Hybrid score\n",
    "            scores[product] = self.alpha * cf_score + (1 - self.alpha) * cb_score\n",
    "        \n",
    "        # Sort and return top N\n",
    "        sorted_products = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_products[:top_n]\n",
    "    \n",
    "    def explain(self, user_id, product_id):\n",
    "        \"\"\"Explain why a product is recommended\"\"\"\n",
    "        # Get user's highly rated products\n",
    "        user_ratings = self.ratings_df[\n",
    "            (self.ratings_df['user_id'] == user_id) & \n",
    "            (self.ratings_df['rating'] >= 4)\n",
    "        ]\n",
    "        \n",
    "        if len(user_ratings) == 0:\n",
    "            return \"Based on your general preferences (cold start)\"\n",
    "        \n",
    "        # Find most similar product user liked\n",
    "        product_idx = product_id - 1\n",
    "        max_sim = 0\n",
    "        most_similar = None\n",
    "        \n",
    "        for _, row in user_ratings.iterrows():\n",
    "            rated_prod = row['product_id']\n",
    "            rated_idx = rated_prod - 1\n",
    "            sim = self.content_sim[product_idx, rated_idx]\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                most_similar = rated_prod\n",
    "        \n",
    "        if most_similar:\n",
    "            similar_prod = self.products_df[self.products_df['product_id'] == most_similar].iloc[0]\n",
    "            rec_prod = self.products_df[self.products_df['product_id'] == product_id].iloc[0]\n",
    "            return f\"Because you liked {similar_prod['name']} ({similar_prod['category']}), you might like this {rec_prod['category']} product!\"\n",
    "        \n",
    "        return \"Based on what similar users enjoyed\"\n",
    "\n",
    "# Create hybrid recommender\n",
    "hybrid_rec = HybridRecommender(svd, content_similarity, products_df, ecommerce_ratings, alpha=0.7)\n",
    "\n",
    "print(\"âœ… Hybrid Recommender System Created!\")\n",
    "print(\"Alpha = 0.7 (70% collaborative, 30% content-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Recommendations\n",
    "test_user = 5\n",
    "recommendations = hybrid_rec.recommend(test_user, top_n=5)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Top 5 Recommendations for User {test_user}:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (product_id, score) in enumerate(recommendations, 1):\n",
    "    product = products_df[products_df['product_id'] == product_id].iloc[0]\n",
    "    explanation = hybrid_rec.explain(test_user, product_id)\n",
    "    \n",
    "    print(f\"{i}. {product['name']}\")\n",
    "    print(f\"   Category: {product['category']} | Brand: {product['brand']} | Price: ${product['price']:.2f}\")\n",
    "    print(f\"   Score: {score:.3f}\")\n",
    "    print(f\"   Why: {explanation}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluation\n",
    "def evaluate_recommender(recommender, ratings_df, n_users=20, k=5):\n",
    "    \"\"\"Evaluate recommender system performance\"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    # Sample users who have enough ratings\n",
    "    user_counts = ratings_df['user_id'].value_counts()\n",
    "    test_users = user_counts[user_counts >= 5].index[:n_users]\n",
    "    \n",
    "    for user_id in test_users:\n",
    "        # Get user's highly rated products (4-5 stars) as ground truth\n",
    "        relevant_products = ratings_df[\n",
    "            (ratings_df['user_id'] == user_id) & \n",
    "            (ratings_df['rating'] >= 4)\n",
    "        ]['product_id'].values\n",
    "        \n",
    "        if len(relevant_products) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get recommendations\n",
    "        recommendations = recommender.recommend(user_id, top_n=k)\n",
    "        recommended_products = [prod_id for prod_id, _ in recommendations]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precisions.append(precision_at_k(recommended_products, relevant_products, k))\n",
    "        recalls.append(recall_at_k(recommended_products, relevant_products, k))\n",
    "        ndcgs.append(ndcg_at_k(recommended_products, relevant_products, k))\n",
    "    \n",
    "    return {\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls),\n",
    "        'ndcg': np.mean(ndcgs)\n",
    "    }\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluate_recommender(hybrid_rec, ecommerce_ratings, n_users=30, k=5)\n",
    "\n",
    "print(\"\\nðŸ“Š Hybrid Recommender Performance (K=5):\\n\")\n",
    "print(f\"Precision@5: {metrics['precision']:.3f}\")\n",
    "print(f\"Recall@5: {metrics['recall']:.3f}\")\n",
    "print(f\"NDCG@5: {metrics['ndcg']:.3f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "metrics_list = ['Precision@5', 'Recall@5', 'NDCG@5']\n",
    "values = [metrics['precision'], metrics['recall'], metrics['ndcg']]\n",
    "\n",
    "bars = plt.bar(metrics_list, values, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "plt.ylabel('Score')\n",
    "plt.title('Hybrid Recommender Performance')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **Collaborative Filtering**\n",
    "   - User-based and item-based approaches\n",
    "   - When to use each\n",
    "   \n",
    "2. **Matrix Factorization**\n",
    "   - SVD for sparse matrices\n",
    "   - The Netflix Prize technique\n",
    "   - Production-ready with Surprise library\n",
    "   \n",
    "3. **Content-Based Filtering**\n",
    "   - TF-IDF for feature extraction\n",
    "   - Cosine similarity for recommendations\n",
    "   - Explainable recommendations\n",
    "   \n",
    "4. **Hybrid Systems**\n",
    "   - Combining CF and CB\n",
    "   - Handling cold start\n",
    "   - Best of both worlds\n",
    "   \n",
    "5. **Deep Learning**\n",
    "   - Neural Collaborative Filtering\n",
    "   - Embeddings + MLP\n",
    "   - State-of-the-art performance\n",
    "   \n",
    "6. **Evaluation**\n",
    "   - Precision, Recall, NDCG\n",
    "   - Ranking metrics\n",
    "   - Real-world evaluation\n",
    "\n",
    "### Interview-Ready Skills âœ…\n",
    "\n",
    "**You can now answer:**\n",
    "- \"Explain collaborative filtering vs content-based filtering\"\n",
    "- \"How do you handle cold start problem?\"\n",
    "- \"What is matrix factorization? Why is it useful?\"\n",
    "- \"How do you evaluate a recommender system?\"\n",
    "- \"Describe a hybrid recommender system you've built\"\n",
    "\n",
    "**You can now build:**\n",
    "- E-commerce product recommendations (Amazon-style)\n",
    "- Content recommendations (Netflix-style)\n",
    "- Social media feeds (TikTok-style)\n",
    "- Ad targeting systems\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**E-Commerce** (Woolworths, Amazon):\n",
    "- Product recommendations\n",
    "- \"Frequently bought together\"\n",
    "- Personalized search results\n",
    "\n",
    "**Property/Real Estate** (REA Group):\n",
    "- Property recommendations based on user search history\n",
    "- Similar properties suggestion\n",
    "- Location-based recommendations\n",
    "\n",
    "**Streaming** (Netflix, Spotify):\n",
    "- Content recommendations\n",
    "- Personalized playlists/homepages\n",
    "- Continue watching suggestions\n",
    "\n",
    "**Social Media** (TikTok, Instagram):\n",
    "- Feed ranking\n",
    "- Content discovery\n",
    "- Friend suggestions\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "1. **Add to your portfolio**: Deploy this e-commerce recommender as a web app\n",
    "2. **Learn more**: Learning to Rank (LambdaMART), Multi-Armed Bandits\n",
    "3. **Practice**: Kaggle competitions (MovieLens, Amazon reviews)\n",
    "4. **Scale**: Learn about distributed recommenders (Spark MLlib)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "**Papers**:\n",
    "- Netflix Prize winning solution\n",
    "- Neural Collaborative Filtering (He et al., 2017)\n",
    "- Deep Learning based Recommender System (Zhang et al., 2019)\n",
    "\n",
    "**Libraries**:\n",
    "- Surprise: For collaborative filtering\n",
    "- LightFM: Hybrid recommender\n",
    "- Implicit: Fast ALS implementation\n",
    "- RecBole: Comprehensive RecSys library\n",
    "\n",
    "**Datasets**:\n",
    "- MovieLens: Movie ratings\n",
    "- Amazon Reviews: Product ratings\n",
    "- LastFM: Music listening history\n",
    "- Yelp: Restaurant reviews\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Job Market Relevance\n",
    "\n",
    "**REA Group Job Posting**: âœ…\n",
    "> \"Experience with recommender systems (Collaborative Filtering, Content-Based, Hybrid)\"\n",
    "\n",
    "**Woolworths ML Engineer**: âœ…\n",
    "> \"Build personalized customer experiences\"\n",
    "\n",
    "**Industry Usage**: âœ…\n",
    "- 35% of Amazon revenue from recommendations\n",
    "- 75% of Netflix viewing from recommendations\n",
    "- 60% of YouTube watch time from recommendations\n",
    "\n",
    "**You are now job-ready for RecSys roles!** ðŸŽ‰\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed Day 1 of Week 22. Tomorrow: Time Series Forecasting! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
