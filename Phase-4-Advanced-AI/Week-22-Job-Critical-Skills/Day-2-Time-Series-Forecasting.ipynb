{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Week 22, Day 2: Time Series Forecasting\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gouthamgo/Learn-AI/blob/main/Phase-4-Advanced-AI/Week-22-Job-Critical-Skills/Day-2-Time-Series-Forecasting.ipynb)\n",
    "\n",
    "## üöÄ Why This Matters\n",
    "\n",
    "Time series forecasting is **CRITICAL** for business operations:\n",
    "- üõí **Retail**: Demand forecasting prevents stockouts & overstocking (saves millions)\n",
    "- üí∞ **Finance**: Stock price prediction, risk management\n",
    "- üè≠ **Manufacturing**: Production planning, inventory optimization\n",
    "- ‚ö° **Energy**: Load forecasting for power grids\n",
    "- üå§Ô∏è **Weather**: Temperature, rainfall prediction\n",
    "\n",
    "**Job Market Reality:**\n",
    "- Woolworths job listing: \"Create time series models for demand forecasting\" ‚úÖ\n",
    "- Finance companies: Stock/crypto price prediction\n",
    "- Tech companies: User growth, server load prediction\n",
    "- Manufacturing: Equipment maintenance prediction\n",
    "\n",
    "**Real Impact**:\n",
    "- Walmart saves $billions with demand forecasting\n",
    "- Amazon adjusts prices 2.5M times/day using predictions\n",
    "- Airlines use forecasting for dynamic pricing\n",
    "\n",
    "## üìã What You'll Learn Today\n",
    "\n",
    "1. **Time Series Fundamentals** - Trend, seasonality, stationarity\n",
    "2. **Classical Methods** - ARIMA, SARIMA\n",
    "3. **Modern Methods** - Prophet (Facebook's library)\n",
    "4. **Deep Learning** - LSTMs, GRUs for time series\n",
    "5. **Evaluation Metrics** - MAE, RMSE, MAPE\n",
    "6. **üèÜ Project: Retail Demand Forecasting System**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Time Series Fundamentals\n",
    "\n",
    "### What is Time Series Data?\n",
    "\n",
    "Data points indexed in time order:\n",
    "- Stock prices (every minute)\n",
    "- Daily sales (every day)\n",
    "- Monthly revenue (every month)\n",
    "- Server metrics (every second)\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Trend** (T): Long-term increase/decrease\n",
    "2. **Seasonality** (S): Regular patterns (daily, weekly, yearly)\n",
    "3. **Cyclical** (C): Long-term oscillations (business cycles)\n",
    "4. **Residual/Noise** (R): Random variations\n",
    "\n",
    "$$Y(t) = T(t) + S(t) + C(t) + R(t)$$\n",
    "\n",
    "### Stationarity\n",
    "\n",
    "**Stationary series**: Statistical properties don't change over time\n",
    "- Constant mean\n",
    "- Constant variance\n",
    "- No seasonality\n",
    "\n",
    "**Why it matters**: Most statistical methods assume stationarity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install pandas numpy matplotlib seaborn statsmodels prophet pmdarima scikit-learn -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ All libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic retail sales data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 2 years of daily sales data\n",
    "dates = pd.date_range(start='2022-01-01', end='2023-12-31', freq='D')\n",
    "n_days = len(dates)\n",
    "\n",
    "# Components\n",
    "trend = np.linspace(100, 150, n_days)  # Upward trend\n",
    "seasonality = 20 * np.sin(2 * np.pi * np.arange(n_days) / 365)  # Yearly seasonality\n",
    "weekly = 10 * np.sin(2 * np.pi * np.arange(n_days) / 7)  # Weekly seasonality\n",
    "noise = np.random.normal(0, 5, n_days)  # Random noise\n",
    "\n",
    "# Combine components\n",
    "sales = trend + seasonality + weekly + noise\n",
    "sales = np.maximum(sales, 0)  # No negative sales\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'sales': sales\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Retail Sales Dataset Created!\")\n",
    "print(f\"Period: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"Total days: {len(df)}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the time series\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "\n",
    "# Original series\n",
    "axes[0].plot(df['date'], df['sales'], linewidth=1)\n",
    "axes[0].set_title('Original Sales Data', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Sales')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Trend\n",
    "axes[1].plot(df['date'], trend, color='red', linewidth=2)\n",
    "axes[1].set_title('Trend Component', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Trend')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Seasonality (yearly)\n",
    "axes[2].plot(df['date'], seasonality, color='green', linewidth=2)\n",
    "axes[2].set_title('Yearly Seasonality Component', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Seasonality')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "# Noise\n",
    "axes[3].plot(df['date'], noise, color='gray', linewidth=0.5, alpha=0.7)\n",
    "axes[3].set_title('Noise/Residual Component', fontsize=14, fontweight='bold')\n",
    "axes[3].set_ylabel('Noise')\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Time series decomposed into: Trend + Seasonality + Noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Stationarity\n",
    "\n",
    "**Augmented Dickey-Fuller (ADF) Test**:\n",
    "- Null hypothesis: Time series is non-stationary\n",
    "- p-value < 0.05 ‚Üí Reject null ‚Üí Series is stationary ‚úÖ\n",
    "- p-value > 0.05 ‚Üí Series is non-stationary ‚ùå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "def check_stationarity(series, name='Series'):\n",
    "    \"\"\"\n",
    "    Perform ADF test for stationarity.\n",
    "    \"\"\"\n",
    "    result = adfuller(series)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ADF Test Results for {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"ADF Statistic: {result[0]:.4f}\")\n",
    "    print(f\"p-value: {result[1]:.4f}\")\n",
    "    print(f\"Critical Values:\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    if result[1] < 0.05:\n",
    "        print(f\"\\n‚úÖ Series is STATIONARY (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Series is NON-STATIONARY (p > 0.05)\")\n",
    "        print(\"   ‚Üí Need to apply differencing or transformation\")\n",
    "\n",
    "# Test original series\n",
    "check_stationarity(df['sales'], 'Original Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make series stationary using differencing\n",
    "df['sales_diff'] = df['sales'].diff()  # First-order differencing\n",
    "\n",
    "# Remove NaN from differencing\n",
    "df_stationary = df.dropna()\n",
    "\n",
    "# Test differenced series\n",
    "check_stationarity(df_stationary['sales_diff'], 'Differenced Sales')\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "axes[0].plot(df['date'], df['sales'])\n",
    "axes[0].set_title('Original Sales (Non-Stationary)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Sales')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(df_stationary['date'], df_stationary['sales_diff'])\n",
    "axes[1].set_title('Differenced Sales (Stationary)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Sales Difference')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ARIMA Models\n",
    "\n",
    "### ARIMA: AutoRegressive Integrated Moving Average\n",
    "\n",
    "**ARIMA(p, d, q)**:\n",
    "- **p** = AR order (autoregressive): Past values influence current value\n",
    "- **d** = Differencing order: Make series stationary\n",
    "- **q** = MA order (moving average): Past errors influence current value\n",
    "\n",
    "### How to Choose p, d, q?\n",
    "\n",
    "1. **d**: Difference until stationary (usually 1 or 2)\n",
    "2. **p**: Look at PACF plot (partial autocorrelation)\n",
    "3. **q**: Look at ACF plot (autocorrelation)\n",
    "4. **Or**: Use auto_arima to find best parameters!\n",
    "\n",
    "### SARIMA: Seasonal ARIMA\n",
    "\n",
    "**SARIMA(p,d,q)(P,D,Q,s)**:\n",
    "- Additional seasonal parameters\n",
    "- s = seasonal period (7 for weekly, 12 for monthly, 365 for yearly)\n",
    "- Better for data with seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Split data into train/test\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df[:train_size].copy()\n",
    "test_df = df[train_size:].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_df)} days\")\n",
    "print(f\"Test set: {len(test_df)} days\")\n",
    "print(f\"Train period: {train_df['date'].min().date()} to {train_df['date'].max().date()}\")\n",
    "print(f\"Test period: {test_df['date'].min().date()} to {test_df['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ARIMA model\n",
    "print(\"Training ARIMA(2,1,2) model...\")\n",
    "\n",
    "arima_model = ARIMA(train_df['sales'], order=(2, 1, 2))\n",
    "arima_fitted = arima_model.fit()\n",
    "\n",
    "print(\"\\n‚úÖ ARIMA Model Trained!\")\n",
    "print(arima_fitted.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "n_forecast = len(test_df)\n",
    "arima_forecast = arima_fitted.forecast(steps=n_forecast)\n",
    "\n",
    "# Calculate errors\n",
    "mae = mean_absolute_error(test_df['sales'], arima_forecast)\n",
    "rmse = np.sqrt(mean_squared_error(test_df['sales'], arima_forecast))\n",
    "mape = np.mean(np.abs((test_df['sales'].values - arima_forecast) / test_df['sales'].values)) * 100\n",
    "\n",
    "print(f\"\\nüìä ARIMA Performance:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(train_df['date'], train_df['sales'], label='Training Data', linewidth=2)\n",
    "plt.plot(test_df['date'], test_df['sales'], label='Actual Test Data', linewidth=2)\n",
    "plt.plot(test_df['date'], arima_forecast, label='ARIMA Forecast', linewidth=2, linestyle='--')\n",
    "plt.axvline(x=train_df['date'].iloc[-1], color='red', linestyle=':', linewidth=2, label='Train/Test Split')\n",
    "plt.title('ARIMA Forecasting', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto ARIMA - automatically find best parameters\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "print(\"Finding best ARIMA parameters with auto_arima...\")\n",
    "print(\"(This may take a minute)\\n\")\n",
    "\n",
    "auto_model = auto_arima(\n",
    "    train_df['sales'],\n",
    "    start_p=0, start_q=0,\n",
    "    max_p=5, max_q=5,\n",
    "    d=None,  # Let auto_arima determine d\n",
    "    seasonal=False,\n",
    "    stepwise=True,\n",
    "    suppress_warnings=True,\n",
    "    error_action='ignore',\n",
    "    trace=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Best model: ARIMA{auto_model.order}\")\n",
    "print(f\"AIC: {auto_model.aic():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA for Seasonal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SARIMA model with weekly seasonality\n",
    "print(\"Training SARIMA model with weekly seasonality...\")\n",
    "\n",
    "sarima_model = SARIMAX(\n",
    "    train_df['sales'],\n",
    "    order=(1, 1, 1),  # ARIMA parameters\n",
    "    seasonal_order=(1, 1, 1, 7)  # Seasonal parameters (weekly)\n",
    ")\n",
    "sarima_fitted = sarima_model.fit(disp=False)\n",
    "\n",
    "print(\"‚úÖ SARIMA Model Trained!\")\n",
    "\n",
    "# Make predictions\n",
    "sarima_forecast = sarima_fitted.forecast(steps=n_forecast)\n",
    "\n",
    "# Calculate errors\n",
    "mae = mean_absolute_error(test_df['sales'], sarima_forecast)\n",
    "rmse = np.sqrt(mean_squared_error(test_df['sales'], sarima_forecast))\n",
    "mape = np.mean(np.abs((test_df['sales'].values - sarima_forecast) / test_df['sales'].values)) * 100\n",
    "\n",
    "print(f\"\\nüìä SARIMA Performance:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(train_df['date'], train_df['sales'], label='Training Data', linewidth=2)\n",
    "plt.plot(test_df['date'], test_df['sales'], label='Actual Test Data', linewidth=2)\n",
    "plt.plot(test_df['date'], sarima_forecast, label='SARIMA Forecast', linewidth=2, linestyle='--')\n",
    "plt.axvline(x=train_df['date'].iloc[-1], color='red', linestyle=':', linewidth=2, label='Train/Test Split')\n",
    "plt.title('SARIMA Forecasting (Weekly Seasonality)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prophet (Facebook's Time Series Library)\n",
    "\n",
    "### Why Prophet?\n",
    "\n",
    "**Advantages**:\n",
    "- ‚úÖ Handles seasonality automatically (daily, weekly, yearly)\n",
    "- ‚úÖ Robust to missing data\n",
    "- ‚úÖ Robust to outliers\n",
    "- ‚úÖ Includes holidays effects\n",
    "- ‚úÖ Easy to use (minimal parameter tuning)\n",
    "- ‚úÖ Interpretable components\n",
    "\n",
    "**Used by**: Facebook, Uber, Airbnb, many tech companies\n",
    "\n",
    "**When to use**:\n",
    "- Business forecasting (sales, revenue, users)\n",
    "- Data with strong seasonal patterns\n",
    "- Need quick, reliable forecasts\n",
    "\n",
    "### Prophet Model\n",
    "\n",
    "$$y(t) = g(t) + s(t) + h(t) + \\epsilon_t$$\n",
    "\n",
    "- **g(t)**: Trend (piecewise linear or logistic)\n",
    "- **s(t)**: Seasonality (Fourier series)\n",
    "- **h(t)**: Holidays effects\n",
    "- **Œµ_t**: Error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# Prepare data for Prophet (needs 'ds' and 'y' columns)\n",
    "prophet_train = train_df.rename(columns={'date': 'ds', 'sales': 'y'})[['ds', 'y']]\n",
    "prophet_test = test_df.rename(columns={'date': 'ds', 'sales': 'y'})[['ds', 'y']]\n",
    "\n",
    "# Create and fit Prophet model\n",
    "print(\"Training Prophet model...\")\n",
    "\n",
    "prophet_model = Prophet(\n",
    "    daily_seasonality=False,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True,\n",
    "    seasonality_mode='additive',\n",
    "    changepoint_prior_scale=0.05\n",
    ")\n",
    "\n",
    "prophet_model.fit(prophet_train)\n",
    "\n",
    "print(\"‚úÖ Prophet Model Trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "future = prophet_model.make_future_dataframe(periods=n_forecast)\n",
    "prophet_forecast = prophet_model.predict(future)\n",
    "\n",
    "# Extract test predictions\n",
    "prophet_test_pred = prophet_forecast.iloc[-n_forecast:]['yhat'].values\n",
    "\n",
    "# Calculate errors\n",
    "mae = mean_absolute_error(test_df['sales'], prophet_test_pred)\n",
    "rmse = np.sqrt(mean_squared_error(test_df['sales'], prophet_test_pred))\n",
    "mape = np.mean(np.abs((test_df['sales'].values - prophet_test_pred) / test_df['sales'].values)) * 100\n",
    "\n",
    "print(f\"\\nüìä Prophet Performance:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Visualize forecast\n",
    "fig = prophet_model.plot(prophet_forecast, figsize=(14, 6))\n",
    "plt.axvline(x=train_df['date'].iloc[-1], color='red', linestyle=':', linewidth=2, label='Train/Test Split')\n",
    "plt.title('Prophet Forecasting', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize components\n",
    "fig = prophet_model.plot_components(prophet_forecast, figsize=(14, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Prophet automatically decomposed the time series into:\")\n",
    "print(\"  1. Trend: Overall direction\")\n",
    "print(\"  2. Weekly Seasonality: Day-of-week patterns\")\n",
    "print(\"  3. Yearly Seasonality: Month/season patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Deep Learning for Time Series (LSTMs)\n",
    "\n",
    "### Why LSTMs for Time Series?\n",
    "\n",
    "**Long Short-Term Memory (LSTM)**:\n",
    "- Can learn long-term dependencies\n",
    "- Handles complex non-linear patterns\n",
    "- Great for multivariate time series\n",
    "- Used by: Google, Amazon, finance firms\n",
    "\n",
    "**When to use**:\n",
    "- Complex patterns ARIMA can't capture\n",
    "- Multiple input features\n",
    "- Large datasets (thousands of data points)\n",
    "- Non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"‚úÖ PyTorch installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_length=30):\n",
    "        self.seq_length = seq_length\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: seq_length past values\n",
    "        x = self.data[idx:idx+self.seq_length]\n",
    "        # Target: next value\n",
    "        y = self.data[idx+self.seq_length]\n",
    "        return torch.FloatTensor(x), torch.FloatTensor([y])\n",
    "\n",
    "# Scale data to 0-1\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[['sales']]).flatten()\n",
    "test_scaled = scaler.transform(test_df[['sales']]).flatten()\n",
    "\n",
    "# Create datasets\n",
    "seq_length = 30  # Use 30 days to predict next day\n",
    "train_dataset = TimeSeriesDataset(train_scaled, seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"‚úÖ Data prepared!\")\n",
    "print(f\"Sequence length: {seq_length} days\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_length, features)\n",
    "        x = x.unsqueeze(-1)  # Add feature dimension\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Take last output\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        prediction = self.fc(last_output)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMForecaster(input_size=1, hidden_size=64, num_layers=2, dropout=0.2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"‚úÖ LSTM Model Created!\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "num_epochs = 50\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        predictions = model(x_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ LSTM Training Complete!\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('LSTM Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with LSTM\n",
    "model.eval()\n",
    "\n",
    "# Use last seq_length points from training to start predictions\n",
    "lstm_predictions = []\n",
    "current_seq = train_scaled[-seq_length:].tolist()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(len(test_df)):\n",
    "        # Predict next value\n",
    "        x = torch.FloatTensor(current_seq).unsqueeze(0)\n",
    "        pred = model(x).item()\n",
    "        lstm_predictions.append(pred)\n",
    "        \n",
    "        # Update sequence (sliding window)\n",
    "        current_seq = current_seq[1:] + [pred]\n",
    "\n",
    "# Inverse transform to original scale\n",
    "lstm_predictions = scaler.inverse_transform(np.array(lstm_predictions).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate errors\n",
    "mae = mean_absolute_error(test_df['sales'], lstm_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(test_df['sales'], lstm_predictions))\n",
    "mape = np.mean(np.abs((test_df['sales'].values - lstm_predictions) / test_df['sales'].values)) * 100\n",
    "\n",
    "print(f\"\\nüìä LSTM Performance:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(train_df['date'], train_df['sales'], label='Training Data', linewidth=2)\n",
    "plt.plot(test_df['date'], test_df['sales'], label='Actual Test Data', linewidth=2)\n",
    "plt.plot(test_df['date'], lstm_predictions, label='LSTM Forecast', linewidth=2, linestyle='--')\n",
    "plt.axvline(x=train_df['date'].iloc[-1], color='red', linestyle=':', linewidth=2, label='Train/Test Split')\n",
    "plt.title('LSTM Forecasting', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "# (Re-run predictions if needed to get all forecasts)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['ARIMA', 'SARIMA', 'Prophet', 'LSTM'],\n",
    "    'MAE': [7.89, 6.45, 5.23, 4.87],  # Example values - replace with your actual values\n",
    "    'RMSE': [9.45, 8.12, 6.78, 6.34],\n",
    "    'MAPE': [6.2, 5.1, 4.3, 3.9]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Model Comparison:\\n\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "metrics = ['MAE', 'RMSE', 'MAPE']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].bar(results['Model'], results[metric], color=colors)\n",
    "    axes[i].set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(results[metric]):\n",
    "        axes[i].text(j, v + 0.2, f'{v:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüèÜ Best Model (Lowest Error): LSTM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ CAPSTONE PROJECT: Retail Demand Forecasting System\n",
    "\n",
    "### Project Goal\n",
    "Build a production-ready demand forecasting system for a retail chain.\n",
    "\n",
    "### Requirements\n",
    "1. Multiple products with different sales patterns\n",
    "2. Handle seasonality (weekly, yearly)\n",
    "3. Incorporate external factors (promotions, holidays)\n",
    "4. Compare multiple models\n",
    "5. Generate actionable insights\n",
    "6. Visualize forecasts with confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-product retail dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "products = ['Electronics', 'Clothing', 'Food', 'Home_Goods']\n",
    "dates = pd.date_range(start='2021-01-01', end='2023-12-31', freq='D')\n",
    "\n",
    "retail_data = []\n",
    "\n",
    "for product in products:\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    # Different patterns for each product\n",
    "    if product == 'Electronics':\n",
    "        base = 200\n",
    "        trend = np.linspace(0, 50, n_days)\n",
    "        # Strong yearly seasonality (Christmas spike)\n",
    "        seasonality = 30 * np.sin(2 * np.pi * (np.arange(n_days) % 365) / 365 - np.pi/2) + 30\n",
    "    elif product == 'Clothing':\n",
    "        base = 150\n",
    "        trend = np.linspace(0, 30, n_days)\n",
    "        # Seasonal (summer/winter)\n",
    "        seasonality = 20 * np.sin(2 * np.pi * np.arange(n_days) / 365)\n",
    "    elif product == 'Food':\n",
    "        base = 500\n",
    "        trend = np.linspace(0, 20, n_days)\n",
    "        # Weekly seasonality (weekend shopping)\n",
    "        seasonality = 50 * np.sin(2 * np.pi * np.arange(n_days) / 7)\n",
    "    else:  # Home_Goods\n",
    "        base = 100\n",
    "        trend = np.linspace(0, 40, n_days)\n",
    "        seasonality = 15 * np.sin(2 * np.pi * np.arange(n_days) / 365)\n",
    "    \n",
    "    # Add promotions (random spikes)\n",
    "    promotions = np.zeros(n_days)\n",
    "    promo_days = np.random.choice(n_days, size=20, replace=False)\n",
    "    promotions[promo_days] = np.random.uniform(50, 100, 20)\n",
    "    \n",
    "    # Combine\n",
    "    noise = np.random.normal(0, 10, n_days)\n",
    "    sales = base + trend + seasonality + promotions + noise\n",
    "    sales = np.maximum(sales, 0)\n",
    "    \n",
    "    # Create records\n",
    "    for i, date in enumerate(dates):\n",
    "        retail_data.append({\n",
    "            'date': date,\n",
    "            'product': product,\n",
    "            'sales': sales[i],\n",
    "            'promotion': 1 if promotions[i] > 0 else 0\n",
    "        })\n",
    "\n",
    "retail_df = pd.DataFrame(retail_data)\n",
    "\n",
    "print(\"‚úÖ Multi-Product Retail Dataset Created!\")\n",
    "print(f\"Products: {products}\")\n",
    "print(f\"Period: {retail_df['date'].min().date()} to {retail_df['date'].max().date()}\")\n",
    "print(f\"Total records: {len(retail_df)}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(retail_df.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sales by product\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, product in enumerate(products):\n",
    "    product_data = retail_df[retail_df['product'] == product]\n",
    "    axes[i].plot(product_data['date'], product_data['sales'], linewidth=1)\n",
    "    \n",
    "    # Highlight promotions\n",
    "    promo_data = product_data[product_data['promotion'] == 1]\n",
    "    axes[i].scatter(promo_data['date'], promo_data['sales'], \n",
    "                    color='red', s=30, alpha=0.6, label='Promotion')\n",
    "    \n",
    "    axes[i].set_title(f'{product} Sales', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_ylabel('Sales')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast for each product using Prophet\n",
    "def forecast_product(product_name, retail_df, forecast_days=90):\n",
    "    \"\"\"\n",
    "    Forecast sales for a specific product.\n",
    "    \"\"\"\n",
    "    # Filter product data\n",
    "    product_data = retail_df[retail_df['product'] == product_name].copy()\n",
    "    \n",
    "    # Train/test split\n",
    "    train_size = len(product_data) - forecast_days\n",
    "    train = product_data[:train_size]\n",
    "    test = product_data[train_size:]\n",
    "    \n",
    "    # Prepare for Prophet\n",
    "    prophet_data = train.rename(columns={'date': 'ds', 'sales': 'y'})[['ds', 'y', 'promotion']]\n",
    "    \n",
    "    # Create model with promotion as regressor\n",
    "    model = Prophet(\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        seasonality_mode='additive'\n",
    "    )\n",
    "    model.add_regressor('promotion')\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(prophet_data)\n",
    "    \n",
    "    # Create future dataframe\n",
    "    future = model.make_future_dataframe(periods=forecast_days)\n",
    "    future['promotion'] = 0  # No promotions in future (can be customized)\n",
    "    \n",
    "    # Merge actual promotions from test set\n",
    "    test_promo = test.rename(columns={'date': 'ds'})[['ds', 'promotion']]\n",
    "    future = future.merge(test_promo, on='ds', how='left', suffixes=('', '_actual'))\n",
    "    future['promotion'] = future['promotion_actual'].fillna(future['promotion'])\n",
    "    future = future.drop('promotion_actual', axis=1)\n",
    "    \n",
    "    # Forecast\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # Extract test predictions\n",
    "    test_pred = forecast.iloc[-forecast_days:]['yhat'].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(test['sales'], test_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(test['sales'], test_pred))\n",
    "    mape = np.mean(np.abs((test['sales'].values - test_pred) / test['sales'].values)) * 100\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'forecast': forecast,\n",
    "        'train': train,\n",
    "        'test': test,\n",
    "        'test_pred': test_pred,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'mape': mape\n",
    "    }\n",
    "\n",
    "# Forecast all products\n",
    "print(\"Forecasting sales for all products...\\n\")\n",
    "forecasts = {}\n",
    "\n",
    "for product in products:\n",
    "    print(f\"Processing {product}...\")\n",
    "    forecasts[product] = forecast_product(product, retail_df, forecast_days=90)\n",
    "    print(f\"  MAE: {forecasts[product]['mae']:.2f}\")\n",
    "    print(f\"  RMSE: {forecasts[product]['rmse']:.2f}\")\n",
    "    print(f\"  MAPE: {forecasts[product]['mape']:.2f}%\\n\")\n",
    "\n",
    "print(\"‚úÖ All forecasts completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecasts for all products\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, product in enumerate(products):\n",
    "    result = forecasts[product]\n",
    "    \n",
    "    # Plot training data\n",
    "    axes[i].plot(result['train']['date'], result['train']['sales'], \n",
    "                 label='Training Data', linewidth=2)\n",
    "    \n",
    "    # Plot actual test data\n",
    "    axes[i].plot(result['test']['date'], result['test']['sales'], \n",
    "                 label='Actual', linewidth=2, color='green')\n",
    "    \n",
    "    # Plot forecast\n",
    "    test_dates = result['test']['date'].values\n",
    "    axes[i].plot(test_dates, result['test_pred'], \n",
    "                 label='Forecast', linewidth=2, linestyle='--', color='red')\n",
    "    \n",
    "    # Plot confidence interval\n",
    "    forecast_test = result['forecast'].iloc[-90:]\n",
    "    axes[i].fill_between(\n",
    "        test_dates,\n",
    "        forecast_test['yhat_lower'].values,\n",
    "        forecast_test['yhat_upper'].values,\n",
    "        alpha=0.2, color='red'\n",
    "    )\n",
    "    \n",
    "    axes[i].axvline(x=result['train']['date'].iloc[-1], \n",
    "                    color='black', linestyle=':', linewidth=2)\n",
    "    \n",
    "    axes[i].set_title(f\"{product} - MAPE: {result['mape']:.2f}%\", \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[i].set_ylabel('Sales')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate actionable insights\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ RETAIL DEMAND FORECASTING - ACTIONABLE INSIGHTS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for product in products:\n",
    "    result = forecasts[product]\n",
    "    \n",
    "    # Calculate average forecast\n",
    "    avg_forecast = np.mean(result['test_pred'])\n",
    "    avg_actual = np.mean(result['test']['sales'])\n",
    "    \n",
    "    # Calculate total forecast\n",
    "    total_forecast = np.sum(result['test_pred'])\n",
    "    \n",
    "    # Forecast accuracy\n",
    "    accuracy = 100 - result['mape']\n",
    "    \n",
    "    print(f\"üì¶ {product.upper()}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  Forecast Accuracy: {accuracy:.1f}%\")\n",
    "    print(f\"  Average Daily Sales (Next 90 days): {avg_forecast:.0f} units\")\n",
    "    print(f\"  Total Forecast (Next 90 days): {total_forecast:.0f} units\")\n",
    "    \n",
    "    # Recommendations\n",
    "    if avg_forecast > avg_actual * 1.1:\n",
    "        print(f\"  üìà RECOMMENDATION: Increase inventory by 10-15%\")\n",
    "    elif avg_forecast < avg_actual * 0.9:\n",
    "        print(f\"  üìâ RECOMMENDATION: Reduce inventory by 10-15%\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ RECOMMENDATION: Maintain current inventory levels\")\n",
    "    \n",
    "    # Peak day\n",
    "    peak_day = result['test']['date'].iloc[np.argmax(result['test_pred'])]\n",
    "    peak_value = np.max(result['test_pred'])\n",
    "    print(f\"  üî• Peak Demand Day: {peak_day.date()} ({peak_value:.0f} units)\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **Time Series Fundamentals**\n",
    "   - Trend, seasonality, stationarity\n",
    "   - Decomposition of time series\n",
    "   - ADF test for stationarity\n",
    "\n",
    "2. **Classical Methods**\n",
    "   - ARIMA for non-seasonal data\n",
    "   - SARIMA for seasonal patterns\n",
    "   - auto_arima for parameter selection\n",
    "\n",
    "3. **Modern Methods**\n",
    "   - Prophet for business forecasting\n",
    "   - Automatic seasonality detection\n",
    "   - Holiday effects and external regressors\n",
    "\n",
    "4. **Deep Learning**\n",
    "   - LSTMs for complex patterns\n",
    "   - Sequence modeling\n",
    "   - When to use deep learning vs classical\n",
    "\n",
    "5. **Evaluation**\n",
    "   - MAE, RMSE, MAPE metrics\n",
    "   - Model comparison\n",
    "   - Business impact assessment\n",
    "\n",
    "6. **Production System**\n",
    "   - Multi-product forecasting\n",
    "   - Confidence intervals\n",
    "   - Actionable recommendations\n",
    "\n",
    "### Interview-Ready Skills ‚úÖ\n",
    "\n",
    "**You can now answer:**\n",
    "- \"Explain the difference between ARIMA and Prophet\"\n",
    "- \"How do you check if a time series is stationary?\"\n",
    "- \"When would you use LSTMs over ARIMA?\"\n",
    "- \"How do you handle seasonality in forecasting?\"\n",
    "- \"Explain a demand forecasting system you've built\"\n",
    "\n",
    "**You can now build:**\n",
    "- Retail demand forecasting (Woolworths-style)\n",
    "- Financial time series prediction\n",
    "- User growth forecasting\n",
    "- Server load prediction\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Retail** (Woolworths, Amazon):\n",
    "- Demand forecasting for inventory\n",
    "- Price optimization\n",
    "- Staffing requirements\n",
    "\n",
    "**Finance**:\n",
    "- Stock price prediction\n",
    "- Risk forecasting\n",
    "- Cash flow prediction\n",
    "\n",
    "**Tech**:\n",
    "- User growth forecasting\n",
    "- Resource planning (servers, bandwidth)\n",
    "- Anomaly detection\n",
    "\n",
    "**Manufacturing**:\n",
    "- Production planning\n",
    "- Equipment maintenance\n",
    "- Supply chain optimization\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Add to portfolio**: Deploy demand forecasting as web dashboard\n",
    "2. **Learn more**: VAR (multivariate), GARCH (volatility), Transformer models\n",
    "3. **Practice**: Kaggle time series competitions\n",
    "4. **Scale**: Learn Spark MLlib for distributed forecasting\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "**Libraries**:\n",
    "- Prophet: Facebook's forecasting library\n",
    "- pmdarima: Auto ARIMA\n",
    "- statsmodels: Classical time series models\n",
    "- darts: Modern time series library\n",
    "- GluonTS: Deep learning time series (Amazon)\n",
    "\n",
    "**Books**:\n",
    "- \"Forecasting: Principles and Practice\" by Hyndman & Athanasopoulos\n",
    "- \"Time Series Analysis\" by Hamilton\n",
    "\n",
    "**Papers**:\n",
    "- \"Forecasting at Scale\" (Prophet paper - Facebook)\n",
    "- \"N-BEATS\" (Neural basis expansion - state-of-the-art)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Job Market Relevance\n",
    "\n",
    "**Woolworths ML Engineer**: ‚úÖ\n",
    "> \"Create time series models for demand forecasting\"\n",
    "\n",
    "**Industry Impact**: ‚úÖ\n",
    "- Walmart saves $billions with forecasting\n",
    "- Amazon adjusts 2.5M prices daily\n",
    "- Every retail/finance company needs this\n",
    "\n",
    "**You are now job-ready for time series roles!** üéâ\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed Day 2 of Week 22. Tomorrow: Apache Airflow & ML Orchestration! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
