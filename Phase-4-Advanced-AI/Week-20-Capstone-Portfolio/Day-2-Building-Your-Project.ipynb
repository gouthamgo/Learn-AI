{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 2: Building Your Capstone Project\n",
    "\n",
    "**üéØ Goal:** Build a production-ready end-to-end AI project\n",
    "\n",
    "**‚è±Ô∏è Time:** 120-150 minutes\n",
    "\n",
    "**üåü Why This Matters for AI (2024-2025):**\n",
    "- Companies want BUILDERS, not just learners\n",
    "- End-to-end projects prove you can ship real AI products\n",
    "- Clean code and documentation separate professionals from hobbyists\n",
    "- Multi-modal projects (CV + NLP) showcase cutting-edge 2024-2025 skills\n",
    "- Deployment experience is the #1 gap between students and professionals\n",
    "- One deployed project is worth 100 tutorial completions\n",
    "\n",
    "**What You'll Build Today:**\n",
    "1. **Complete ML project workflow** from data to deployment\n",
    "2. **Multi-modal AI system** combining computer vision + NLP\n",
    "3. **Production-ready code** with proper organization\n",
    "4. **Professional documentation** that impresses recruiters\n",
    "5. **Deployment pipeline** using modern tools (Streamlit/Gradio)\n",
    "6. **Real-world best practices** used by AI companies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml-workflow",
   "metadata": {},
   "source": [
    "## üîÑ End-to-End ML Project Workflow\n",
    "\n",
    "**The complete pipeline from idea to deployment!**\n",
    "\n",
    "### üéØ The 7 Stages of ML Projects\n",
    "\n",
    "```\n",
    "1. PROBLEM DEFINITION\n",
    "   ‚Üì\n",
    "2. DATA COLLECTION & EXPLORATION\n",
    "   ‚Üì\n",
    "3. DATA PREPROCESSING & FEATURE ENGINEERING\n",
    "   ‚Üì\n",
    "4. MODEL SELECTION & TRAINING\n",
    "   ‚Üì\n",
    "5. EVALUATION & ITERATION\n",
    "   ‚Üì\n",
    "6. DEPLOYMENT & MONITORING\n",
    "   ‚Üì\n",
    "7. DOCUMENTATION & PRESENTATION\n",
    "```\n",
    "\n",
    "### 1Ô∏è‚É£ Problem Definition\n",
    "\n",
    "**Questions to answer:**\n",
    "- What EXACTLY are we predicting/classifying?\n",
    "- What does success look like? (metrics)\n",
    "- Who will use this? How?\n",
    "- What's the baseline to beat?\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Problem: Detect toxic comments on social media\n",
    "Success: >90% F1 score, <100ms latency\n",
    "Users: Content moderators\n",
    "Baseline: Keyword matching (~60% accuracy)\n",
    "```\n",
    "\n",
    "### 2Ô∏è‚É£ Data Collection & Exploration\n",
    "\n",
    "**Key tasks:**\n",
    "- Gather data from sources\n",
    "- Explore distributions, patterns\n",
    "- Check for missing values, outliers\n",
    "- Understand class balance\n",
    "- Visualize key insights\n",
    "\n",
    "**Tools:**\n",
    "- `pandas` - data manipulation\n",
    "- `matplotlib/seaborn` - visualization\n",
    "- `pandas-profiling` - auto EDA\n",
    "\n",
    "### 3Ô∏è‚É£ Data Preprocessing & Feature Engineering\n",
    "\n",
    "**For Tabular Data:**\n",
    "- Handle missing values\n",
    "- Encode categorical variables\n",
    "- Scale/normalize features\n",
    "- Create new features\n",
    "- Remove outliers\n",
    "\n",
    "**For Text (NLP):**\n",
    "- Tokenization\n",
    "- Lowercasing, removing punctuation\n",
    "- Stop word removal (sometimes)\n",
    "- Lemmatization/stemming\n",
    "- Convert to embeddings\n",
    "\n",
    "**For Images (CV):**\n",
    "- Resize to consistent dimensions\n",
    "- Normalize pixel values (0-1 or mean/std)\n",
    "- Data augmentation (rotation, flip, crop)\n",
    "- Convert to tensors\n",
    "\n",
    "### 4Ô∏è‚É£ Model Selection & Training\n",
    "\n",
    "**Model Selection Strategy:**\n",
    "\n",
    "| Task | Start With | If More Accuracy Needed |\n",
    "|------|------------|-------------------------|\n",
    "| **Classification (Tabular)** | Random Forest, XGBoost | Neural Network |\n",
    "| **Regression** | Linear Regression, XGBoost | Neural Network |\n",
    "| **NLP** | Pre-trained BERT | Fine-tune larger model |\n",
    "| **Computer Vision** | Pre-trained ResNet/EfficientNet | Fine-tune, ensemble |\n",
    "| **Time Series** | ARIMA, Prophet | LSTM, Transformer |\n",
    "\n",
    "**Training Best Practices:**\n",
    "- Start with simple baseline\n",
    "- Use train/validation/test split\n",
    "- Implement early stopping\n",
    "- Track experiments (MLflow, Weights & Biases)\n",
    "- Save checkpoints\n",
    "\n",
    "### 5Ô∏è‚É£ Evaluation & Iteration\n",
    "\n",
    "**Metrics by Task:**\n",
    "\n",
    "**Classification:**\n",
    "- Balanced data: Accuracy\n",
    "- Imbalanced: F1 Score, ROC-AUC\n",
    "- Multi-class: Macro/Micro F1\n",
    "\n",
    "**Regression:**\n",
    "- MAE (Mean Absolute Error)\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- R¬≤ Score\n",
    "\n",
    "**NLP/CV:**\n",
    "- Task-specific (BLEU, ROUGE, mAP, etc.)\n",
    "\n",
    "**Error Analysis:**\n",
    "1. Look at misclassified examples\n",
    "2. Find patterns in errors\n",
    "3. Fix data issues or adjust model\n",
    "4. Iterate!\n",
    "\n",
    "### 6Ô∏è‚É£ Deployment & Monitoring\n",
    "\n",
    "**Deployment Options:**\n",
    "\n",
    "**Quick Demo:**\n",
    "- Streamlit (easiest)\n",
    "- Gradio (great for ML)\n",
    "- Deploy to Streamlit Cloud (free!)\n",
    "\n",
    "**Production API:**\n",
    "- FastAPI (modern, fast)\n",
    "- Flask (simple)\n",
    "- Deploy to AWS, GCP, Azure\n",
    "\n",
    "**Model Serving:**\n",
    "- TorchServe (PyTorch)\n",
    "- TensorFlow Serving\n",
    "- ONNX Runtime\n",
    "\n",
    "**Monitoring:**\n",
    "- Track prediction latency\n",
    "- Monitor input distributions (drift)\n",
    "- Log predictions for retraining\n",
    "- Set up alerts\n",
    "\n",
    "### 7Ô∏è‚É£ Documentation & Presentation\n",
    "\n",
    "**Must-Have Documentation:**\n",
    "- README with usage instructions\n",
    "- Model card (architecture, metrics, limitations)\n",
    "- API documentation\n",
    "- Setup/installation guide\n",
    "- Demo video/GIF\n",
    "\n",
    "**Portfolio Presentation:**\n",
    "- Problem statement\n",
    "- Data overview\n",
    "- Approach & architecture\n",
    "- Results & metrics\n",
    "- Challenges & learnings\n",
    "- Future improvements\n",
    "\n",
    "### üéØ Workflow Best Practices (2024-2025)\n",
    "\n",
    "**DO:**\n",
    "- ‚úÖ Start simple, add complexity gradually\n",
    "- ‚úÖ Version control EVERYTHING (code + data)\n",
    "- ‚úÖ Document decisions and experiments\n",
    "- ‚úÖ Test on fresh data regularly\n",
    "- ‚úÖ Automate repetitive tasks\n",
    "- ‚úÖ Deploy early and often\n",
    "\n",
    "**DON'T:**\n",
    "- ‚ùå Jump to complex models without baseline\n",
    "- ‚ùå Spend weeks on 1% accuracy improvement\n",
    "- ‚ùå Ignore data quality issues\n",
    "- ‚ùå Train on test data (data leakage!)\n",
    "- ‚ùå Deploy without error handling\n",
    "- ‚ùå Forget to document your process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "code-organization",
   "metadata": {},
   "source": [
    "## üìÅ Code Organization Best Practices\n",
    "\n",
    "**Clean code = Professional developer**\n",
    "\n",
    "### üéØ Project Structure (Industry Standard)\n",
    "\n",
    "**Template for ANY ML project:**\n",
    "\n",
    "```\n",
    "project-name/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ README.md                  # Project overview\n",
    "‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies\n",
    "‚îú‚îÄ‚îÄ setup.py                   # Package installation\n",
    "‚îú‚îÄ‚îÄ .gitignore                 # Git ignore rules\n",
    "‚îú‚îÄ‚îÄ .env.example               # Environment variables template\n",
    "‚îú‚îÄ‚îÄ Makefile                   # Common commands\n",
    "‚îú‚îÄ‚îÄ Dockerfile                 # Container definition\n",
    "‚îú‚îÄ‚îÄ docker-compose.yml         # Multi-container setup\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ config/                    # Configuration files\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.yaml            # Main config\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ logging.yaml           # Logging config\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ data/                      # Data directory\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/                   # Original, immutable data\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ interim/               # Intermediate transformations\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed/             # Final, ready-to-use data\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ external/              # External data sources\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ notebooks/                 # Jupyter notebooks\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 01-eda.ipynb          # Exploratory analysis\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 02-preprocessing.ipynb # Data preprocessing\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 03-modeling.ipynb     # Model experiments\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 04-evaluation.ipynb   # Results analysis\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ src/                       # Source code\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data/                  # Data processing\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load_data.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ augmentation.py\n",
    "‚îÇ   ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ features/              # Feature engineering\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py\n",
    "‚îÇ   ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ models/                # Model definitions\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_model.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cv_model.py       # Computer vision\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nlp_model.py      # NLP\n",
    "‚îÇ   ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ training/              # Training scripts\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py\n",
    "‚îÇ   ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utils/                 # Utility functions\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ logger.py\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ helpers.py\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ models/                    # Saved models\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/          # Training checkpoints\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ production/           # Production models\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ tests/                     # Unit tests\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ test_data.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ test_models.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ scripts/                   # Standalone scripts\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ download_data.sh\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train_model.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ deploy.sh\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ app/                       # Application code\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ main.py               # Main app\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ api.py                # API endpoints\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py      # Streamlit demo\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ docs/                      # Documentation\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ architecture.md\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ api_reference.md\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ model_card.md\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ reports/                   # Analysis reports\n",
    "    ‚îú‚îÄ‚îÄ figures/              # Generated figures\n",
    "    ‚îî‚îÄ‚îÄ metrics/              # Model metrics\n",
    "```\n",
    "\n",
    "### üêç Python Code Style\n",
    "\n",
    "**Follow PEP 8 + AI Best Practices:**\n",
    "\n",
    "**1. Naming Conventions**\n",
    "```python\n",
    "# Good ‚úÖ\n",
    "class SentimentClassifier:          # PascalCase for classes\n",
    "    def predict_sentiment(self):    # snake_case for functions\n",
    "        max_length = 512            # snake_case for variables\n",
    "        BATCH_SIZE = 32             # UPPERCASE for constants\n",
    "\n",
    "# Bad ‚ùå\n",
    "class sentiment_classifier:         # Wrong case\n",
    "    def PredictSentiment(self):     # Wrong case\n",
    "        MaxLength = 512             # Wrong case\n",
    "```\n",
    "\n",
    "**2. Docstrings (Google Style)**\n",
    "```python\n",
    "def train_model(model, train_loader, epochs=10, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Train a neural network model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): PyTorch model to train\n",
    "        train_loader (DataLoader): Training data loader\n",
    "        epochs (int): Number of training epochs (default: 10)\n",
    "        lr (float): Learning rate (default: 1e-4)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training history with loss and accuracy\n",
    "    \n",
    "    Example:\n",
    "        >>> history = train_model(model, train_loader, epochs=5)\n",
    "        >>> print(f\"Final loss: {history['loss'][-1]}\")\n",
    "    \"\"\"\n",
    "    # Implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "**3. Type Hints (Python 3.9+)**\n",
    "```python\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def preprocess_text(\n",
    "    texts: List[str],\n",
    "    max_length: int = 512,\n",
    "    return_tensors: Optional[str] = None\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Preprocess text for model input.\"\"\"\n",
    "    # Implementation\n",
    "    pass\n",
    "```\n",
    "\n",
    "**4. Configuration Management**\n",
    "```python\n",
    "# config/config.yaml\n",
    "model:\n",
    "  name: \"bert-base-uncased\"\n",
    "  max_length: 512\n",
    "  num_labels: 3\n",
    "\n",
    "training:\n",
    "  batch_size: 32\n",
    "  epochs: 10\n",
    "  learning_rate: 2e-5\n",
    "  \n",
    "paths:\n",
    "  data: \"data/processed\"\n",
    "  models: \"models/\"\n",
    "\n",
    "# Load in Python\n",
    "import yaml\n",
    "\n",
    "with open('config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "```\n",
    "\n",
    "**5. Logging**\n",
    "```python\n",
    "import logging\n",
    "\n",
    "# Setup logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Use in code\n",
    "logger.info(f\"Training started with {len(train_data)} examples\")\n",
    "logger.warning(\"Low GPU memory detected\")\n",
    "logger.error(f\"Failed to load model: {e}\")\n",
    "```\n",
    "\n",
    "**6. Error Handling**\n",
    "```python\n",
    "def load_model(model_path: str):\n",
    "    \"\"\"\n",
    "    Load a trained model from disk.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If model file doesn't exist\n",
    "        RuntimeError: If model loading fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "        \n",
    "        model = torch.load(model_path)\n",
    "        logger.info(f\"Model loaded successfully from {model_path}\")\n",
    "        return model\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\")\n",
    "        raise RuntimeError(f\"Failed to load model: {e}\")\n",
    "```\n",
    "\n",
    "### üéØ Code Quality Tools\n",
    "\n",
    "**Essential Tools:**\n",
    "\n",
    "```bash\n",
    "# Install\n",
    "pip install black flake8 isort mypy pytest pytest-cov\n",
    "\n",
    "# Format code (auto-fix)\n",
    "black src/\n",
    "\n",
    "# Sort imports\n",
    "isort src/\n",
    "\n",
    "# Lint (find issues)\n",
    "flake8 src/\n",
    "\n",
    "# Type checking\n",
    "mypy src/\n",
    "\n",
    "# Run tests\n",
    "pytest tests/ --cov=src\n",
    "```\n",
    "\n",
    "**Pre-commit Hooks** (auto-run before commits):\n",
    "\n",
    "```yaml\n",
    "# .pre-commit-config.yaml\n",
    "repos:\n",
    "  - repo: https://github.com/psf/black\n",
    "    rev: 23.7.0\n",
    "    hooks:\n",
    "      - id: black\n",
    "  \n",
    "  - repo: https://github.com/pycqa/flake8\n",
    "    rev: 6.0.0\n",
    "    hooks:\n",
    "      - id: flake8\n",
    "```\n",
    "\n",
    "### üìù Makefile (Automation)\n",
    "\n",
    "```makefile\n",
    "# Makefile\n",
    ".PHONY: install format lint test train deploy clean\n",
    "\n",
    "install:\n",
    "\tpip install -r requirements.txt\n",
    "\n",
    "format:\n",
    "\tblack src/ tests/\n",
    "\tisort src/ tests/\n",
    "\n",
    "lint:\n",
    "\tflake8 src/ tests/\n",
    "\tmypy src/\n",
    "\n",
    "test:\n",
    "\tpytest tests/ --cov=src --cov-report=html\n",
    "\n",
    "train:\n",
    "\tpython scripts/train_model.py\n",
    "\n",
    "deploy:\n",
    "\tstreamlit run app/streamlit_app.py\n",
    "\n",
    "clean:\n",
    "\tfind . -type f -name '*.pyc' -delete\n",
    "\tfind . -type d -name '__pycache__' -delete\n",
    "```\n",
    "\n",
    "**Usage:**\n",
    "```bash\n",
    "make install    # Install dependencies\n",
    "make format     # Format code\n",
    "make test       # Run tests\n",
    "make train      # Train model\n",
    "make deploy     # Deploy app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-modal-project",
   "metadata": {},
   "source": [
    "## üé® Real AI Example: Multi-Modal Project (CV + NLP)\n",
    "\n",
    "**Building a cutting-edge 2024-2025 project!**\n",
    "\n",
    "### üéØ Project: Image Caption Sentiment Analyzer\n",
    "\n",
    "**What it does:**\n",
    "- Takes an image as input\n",
    "- Generates caption using CV model\n",
    "- Analyzes sentiment of caption using NLP model\n",
    "- Returns: caption + sentiment + confidence\n",
    "\n",
    "**Why it's impressive:**\n",
    "- ‚úÖ Multi-modal (combines CV + NLP)\n",
    "- ‚úÖ Modern architecture (2024-2025 techniques)\n",
    "- ‚úÖ End-to-end pipeline\n",
    "- ‚úÖ Deployed with Streamlit\n",
    "- ‚úÖ Production-ready code\n",
    "\n",
    "**Tech Stack:**\n",
    "- **CV**: BLIP (Bootstrapping Language-Image Pre-training)\n",
    "- **NLP**: DistilBERT for sentiment\n",
    "- **Framework**: PyTorch, Transformers\n",
    "- **Deployment**: Streamlit\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input Image\n",
    "     ‚Üì\n",
    "[BLIP Model] ‚Üí Generate Caption\n",
    "     ‚Üì\n",
    "\"A happy dog playing in the park\"\n",
    "     ‚Üì\n",
    "[BERT Model] ‚Üí Analyze Sentiment\n",
    "     ‚Üì\n",
    "Output: {caption, sentiment, confidence}\n",
    "```\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "import sys\n",
    "\n",
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "\n",
    "!{sys.executable} -m pip install transformers torch pillow requests --quiet\n",
    "!{sys.executable} -m pip install streamlit gradio --quiet\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    pipeline\n",
    ")\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ü§ñ Multi-Modal AI Project Setup\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(\"\\nüöÄ Ready to build!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "\n",
    "print(\"üì• Loading Models...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Model 1: Image Captioning (BLIP)\n",
    "print(\"\\n1Ô∏è‚É£ Loading BLIP for image captioning...\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "print(\"   ‚úÖ BLIP loaded successfully!\")\n",
    "\n",
    "# Model 2: Sentiment Analysis (DistilBERT)\n",
    "print(\"\\n2Ô∏è‚É£ Loading DistilBERT for sentiment analysis...\")\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "print(\"   ‚úÖ DistilBERT loaded successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nüéâ All models loaded! Ready for inference.\")\n",
    "print(\"\\nüí° This combines:\")\n",
    "print(\"   - Computer Vision (BLIP for image understanding)\")\n",
    "print(\"   - Natural Language Processing (BERT for sentiment)\")\n",
    "print(\"\\nüåü This is a true multi-modal AI system!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multimodal-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MultiModal AI Class (Production-Ready)\n",
    "\n",
    "class ImageCaptionSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Multi-modal AI system combining computer vision and NLP.\n",
    "    \n",
    "    Generates captions for images and analyzes their sentiment.\n",
    "    \n",
    "    Attributes:\n",
    "        blip_processor: BLIP image processor\n",
    "        blip_model: BLIP captioning model\n",
    "        sentiment_analyzer: Sentiment analysis pipeline\n",
    "    \n",
    "    Example:\n",
    "        >>> analyzer = ImageCaptionSentimentAnalyzer()\n",
    "        >>> result = analyzer.analyze_image(image_url)\n",
    "        >>> print(result['caption'])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize models.\"\"\"\n",
    "        print(\"üîß Initializing Multi-Modal AI System...\")\n",
    "        \n",
    "        # Load BLIP\n",
    "        self.blip_processor = BlipProcessor.from_pretrained(\n",
    "            \"Salesforce/blip-image-captioning-base\"\n",
    "        )\n",
    "        self.blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip-image-captioning-base\"\n",
    "        )\n",
    "        \n",
    "        # Load sentiment analyzer\n",
    "        self.sentiment_analyzer = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ System initialized!\\n\")\n",
    "    \n",
    "    def load_image(self, image_source):\n",
    "        \"\"\"\n",
    "        Load image from URL or file path.\n",
    "        \n",
    "        Args:\n",
    "            image_source (str): URL or file path\n",
    "        \n",
    "        Returns:\n",
    "            PIL.Image: Loaded image\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if image_source.startswith('http'):\n",
    "                # Load from URL\n",
    "                response = requests.get(image_source)\n",
    "                image = Image.open(BytesIO(response.content))\n",
    "            else:\n",
    "                # Load from file\n",
    "                image = Image.open(image_source)\n",
    "            \n",
    "            return image.convert('RGB')\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load image: {e}\")\n",
    "    \n",
    "    def generate_caption(self, image):\n",
    "        \"\"\"\n",
    "        Generate caption for image using BLIP.\n",
    "        \n",
    "        Args:\n",
    "            image (PIL.Image): Input image\n",
    "        \n",
    "        Returns:\n",
    "            str: Generated caption\n",
    "        \"\"\"\n",
    "        # Process image\n",
    "        inputs = self.blip_processor(image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate caption\n",
    "        outputs = self.blip_model.generate(**inputs, max_length=50)\n",
    "        caption = self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return caption\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "        \n",
    "        Returns:\n",
    "            dict: Sentiment label and confidence\n",
    "        \"\"\"\n",
    "        result = self.sentiment_analyzer(text)[0]\n",
    "        \n",
    "        return {\n",
    "            'sentiment': result['label'].lower(),\n",
    "            'confidence': round(result['score'], 4)\n",
    "        }\n",
    "    \n",
    "    def analyze_image(self, image_source, verbose=True):\n",
    "        \"\"\"\n",
    "        Complete pipeline: Image ‚Üí Caption ‚Üí Sentiment.\n",
    "        \n",
    "        Args:\n",
    "            image_source (str): URL or file path\n",
    "            verbose (bool): Print progress (default: True)\n",
    "        \n",
    "        Returns:\n",
    "            dict: Complete analysis results\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"üîç Analyzing image...\\n\")\n",
    "        \n",
    "        # Step 1: Load image\n",
    "        if verbose:\n",
    "            print(\"üì• Loading image...\")\n",
    "        image = self.load_image(image_source)\n",
    "        \n",
    "        # Step 2: Generate caption\n",
    "        if verbose:\n",
    "            print(\"üñºÔ∏è  Generating caption...\")\n",
    "        caption = self.generate_caption(image)\n",
    "        \n",
    "        # Step 3: Analyze sentiment\n",
    "        if verbose:\n",
    "            print(\"üòä Analyzing sentiment...\\n\")\n",
    "        sentiment = self.analyze_sentiment(caption)\n",
    "        \n",
    "        # Combine results\n",
    "        result = {\n",
    "            'caption': caption,\n",
    "            'sentiment': sentiment['sentiment'],\n",
    "            'confidence': sentiment['confidence'],\n",
    "            'image_source': image_source\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Initialize the analyzer\n",
    "analyzer = ImageCaptionSentimentAnalyzer()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüéâ Multi-Modal AI System Ready!\")\n",
    "print(\"\\nüí° This class demonstrates:\")\n",
    "print(\"   ‚úì Clean, professional code structure\")\n",
    "print(\"   ‚úì Comprehensive docstrings\")\n",
    "print(\"   ‚úì Error handling\")\n",
    "print(\"   ‚úì Modular design (easy to extend)\")\n",
    "print(\"   ‚úì Production-ready implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-multimodal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Multi-Modal System\n",
    "\n",
    "print(\"üß™ TESTING MULTI-MODAL AI SYSTEM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test images (various scenarios)\n",
    "test_images = [\n",
    "    {\n",
    "        'url': 'https://images.unsplash.com/photo-1548199973-03cce0bbc87b?w=500',\n",
    "        'description': 'Happy puppy'\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=500',\n",
    "        'description': 'Professional portrait'\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=500',\n",
    "        'description': 'Scenic mountain'\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, image_data in enumerate(test_images, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"\\nüñºÔ∏è  Test Image {i}: {image_data['description']}\")\n",
    "    print(f\"   URL: {image_data['url'][:50]}...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Analyze image\n",
    "        result = analyzer.analyze_image(image_data['url'], verbose=False)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"üìä RESULTS:\")\n",
    "        print(f\"   Caption: \\\"{result['caption']}\\\"\")\n",
    "        \n",
    "        sentiment_emoji = \"üòä\" if result['sentiment'] == 'positive' else \"üòû\"\n",
    "        print(f\"   Sentiment: {sentiment_emoji} {result['sentiment'].upper()}\")\n",
    "        print(f\"   Confidence: {result['confidence']:.2%}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"\\n‚úÖ Tested {len(test_images)} images successfully!\")\n",
    "print(\"\\nüí° This demonstrates:\")\n",
    "print(\"   ‚Ä¢ Computer Vision: Image understanding\")\n",
    "print(\"   ‚Ä¢ Natural Language Processing: Text analysis\")\n",
    "print(\"   ‚Ä¢ Multi-modal AI: Combining both!\")\n",
    "print(\"\\nüåü This is exactly what companies like OpenAI, Google, and Meta are building!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streamlit-app",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streamlit App for Deployment\n",
    "\n",
    "streamlit_code = '''\n",
    "# app/streamlit_app.py\n",
    "\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Multi-Modal AI Analyzer\",\n",
    "    page_icon=\"ü§ñ\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Title\n",
    "st.title(\"ü§ñ Multi-Modal AI: Image Caption + Sentiment\")\n",
    "st.markdown(\"\"\"\n",
    "Upload an image, and our AI will:\n",
    "1. Generate a descriptive caption (Computer Vision)\n",
    "2. Analyze the sentiment (Natural Language Processing)\n",
    "\"\"\")\n",
    "\n",
    "# Load models (cached)\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    \"\"\"Load models once and cache.\"\"\"\n",
    "    blip_processor = BlipProcessor.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-base\"\n",
    "    )\n",
    "    blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-base\"\n",
    "    )\n",
    "    sentiment_analyzer = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    )\n",
    "    return blip_processor, blip_model, sentiment_analyzer\n",
    "\n",
    "blip_processor, blip_model, sentiment_analyzer = load_models()\n",
    "\n",
    "# File uploader\n",
    "uploaded_file = st.file_uploader(\n",
    "    \"Choose an image...\",\n",
    "    type=[\"jpg\", \"jpeg\", \"png\"]\n",
    ")\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Display image\n",
    "    image = Image.open(uploaded_file).convert('RGB')\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"üì∑ Your Image\")\n",
    "        st.image(image, use_column_width=True)\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"üîç AI Analysis\")\n",
    "        \n",
    "        with st.spinner(\"Analyzing...\"):\n",
    "            # Generate caption\n",
    "            inputs = blip_processor(image, return_tensors=\"pt\")\n",
    "            outputs = blip_model.generate(**inputs, max_length=50)\n",
    "            caption = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            sentiment_result = sentiment_analyzer(caption)[0]\n",
    "            sentiment = sentiment_result['label'].lower()\n",
    "            confidence = sentiment_result['score']\n",
    "            \n",
    "            # Display results\n",
    "            st.markdown(\"### üìù Generated Caption\")\n",
    "            st.info(caption)\n",
    "            \n",
    "            st.markdown(\"### üòä Sentiment Analysis\")\n",
    "            \n",
    "            if sentiment == 'positive':\n",
    "                st.success(f\"‚úÖ Positive ({confidence:.1%} confidence)\")\n",
    "            else:\n",
    "                st.error(f\"‚ö†Ô∏è Negative ({confidence:.1%} confidence)\")\n",
    "            \n",
    "            # Progress bar for confidence\n",
    "            st.progress(confidence)\n",
    "\n",
    "# Sidebar\n",
    "with st.sidebar:\n",
    "    st.header(\"‚ÑπÔ∏è About\")\n",
    "    st.markdown(\"\"\"\n",
    "    This app demonstrates **multi-modal AI**:\n",
    "    \n",
    "    **Computer Vision:**\n",
    "    - BLIP model for image captioning\n",
    "    \n",
    "    **Natural Language Processing:**\n",
    "    - DistilBERT for sentiment analysis\n",
    "    \n",
    "    **Tech Stack:**\n",
    "    - PyTorch\n",
    "    - Transformers\n",
    "    - Streamlit\n",
    "    \"\"\")\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"Made with ‚ù§Ô∏è using AI\")\n",
    "'''\n",
    "\n",
    "print(\"üìÑ STREAMLIT APP CODE\")\n",
    "print(\"=\" * 70)\n",
    "print(streamlit_code)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nüíæ To deploy:\")\n",
    "print(\"\\n1. Save this code to: app/streamlit_app.py\")\n",
    "print(\"2. Run: streamlit run app/streamlit_app.py\")\n",
    "print(\"3. Deploy to Streamlit Cloud (free!)\")\n",
    "print(\"\\nüåê Streamlit Cloud: https://streamlit.io/cloud\")\n",
    "print(\"\\n‚úÖ Your app will be live with a public URL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentation",
   "metadata": {},
   "source": [
    "## üìö Documentation Best Practices\n",
    "\n",
    "**Great code needs great docs!**\n",
    "\n",
    "### üéØ Essential Documentation\n",
    "\n",
    "**1. README.md** (Most Important!)\n",
    "\n",
    "See Day 1 for complete README template. Key sections:\n",
    "- Project overview with demo GIF\n",
    "- Installation instructions\n",
    "- Usage examples\n",
    "- Results and metrics\n",
    "- Architecture diagram\n",
    "- Contributing guidelines\n",
    "- License\n",
    "\n",
    "**2. Model Card** (docs/model_card.md)\n",
    "\n",
    "```markdown\n",
    "# Model Card: Image Caption Sentiment Analyzer\n",
    "\n",
    "## Model Details\n",
    "- **Developed by:** Your Name\n",
    "- **Model date:** January 2025\n",
    "- **Model type:** Multi-modal (CV + NLP)\n",
    "- **License:** MIT\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "### Component 1: Image Captioning\n",
    "- Base model: BLIP (Salesforce)\n",
    "- Task: Image-to-text generation\n",
    "- Parameters: 224M\n",
    "\n",
    "### Component 2: Sentiment Analysis\n",
    "- Base model: DistilBERT\n",
    "- Task: Binary classification (positive/negative)\n",
    "- Parameters: 67M\n",
    "\n",
    "## Intended Use\n",
    "- **Primary use:** Analyze sentiment conveyed by images\n",
    "- **Out-of-scope:** Medical diagnosis, illegal content detection\n",
    "\n",
    "## Training Data\n",
    "- BLIP: Pre-trained on COCO, Visual Genome\n",
    "- DistilBERT: Fine-tuned on SST-2 (movie reviews)\n",
    "\n",
    "## Performance\n",
    "- Caption quality: Human evaluation score 4.2/5\n",
    "- Sentiment accuracy: 94% on test set\n",
    "- Latency: ~2 seconds per image\n",
    "\n",
    "## Limitations\n",
    "- May struggle with abstract/artistic images\n",
    "- Sentiment model trained on English only\n",
    "- Requires good image quality\n",
    "\n",
    "## Bias Considerations\n",
    "- Training data may contain biases\n",
    "- Not tested on all demographic groups\n",
    "- Should not be used for high-stakes decisions\n",
    "\n",
    "## Contact\n",
    "your.email@example.com\n",
    "```\n",
    "\n",
    "**3. API Documentation** (docs/api_reference.md)\n",
    "\n",
    "```markdown\n",
    "# API Reference\n",
    "\n",
    "## ImageCaptionSentimentAnalyzer\n",
    "\n",
    "Main class for multi-modal analysis.\n",
    "\n",
    "### Methods\n",
    "\n",
    "#### `analyze_image(image_source, verbose=True)`\n",
    "\n",
    "Analyze an image and return caption + sentiment.\n",
    "\n",
    "**Parameters:**\n",
    "- `image_source` (str): URL or file path to image\n",
    "- `verbose` (bool, optional): Print progress. Default: True\n",
    "\n",
    "**Returns:**\n",
    "- dict with keys:\n",
    "  - `caption` (str): Generated image caption\n",
    "  - `sentiment` (str): \"positive\" or \"negative\"\n",
    "  - `confidence` (float): Model confidence (0-1)\n",
    "  - `image_source` (str): Original image source\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "analyzer = ImageCaptionSentimentAnalyzer()\n",
    "result = analyzer.analyze_image(\"path/to/image.jpg\")\n",
    "print(f\"Caption: {result['caption']}\")\n",
    "print(f\"Sentiment: {result['sentiment']}\")\n",
    "```\n",
    "\n",
    "**Raises:**\n",
    "- `ValueError`: If image cannot be loaded\n",
    "```\n",
    "\n",
    "**4. CHANGELOG.md**\n",
    "\n",
    "```markdown\n",
    "# Changelog\n",
    "\n",
    "All notable changes to this project will be documented here.\n",
    "\n",
    "## [1.0.0] - 2025-01-15\n",
    "\n",
    "### Added\n",
    "- Initial release\n",
    "- BLIP image captioning\n",
    "- DistilBERT sentiment analysis\n",
    "- Streamlit web interface\n",
    "- Comprehensive test suite\n",
    "\n",
    "### Changed\n",
    "- N/A (initial release)\n",
    "\n",
    "### Fixed\n",
    "- N/A (initial release)\n",
    "```\n",
    "\n",
    "**5. CONTRIBUTING.md**\n",
    "\n",
    "```markdown\n",
    "# Contributing Guidelines\n",
    "\n",
    "Thank you for considering contributing!\n",
    "\n",
    "## How to Contribute\n",
    "\n",
    "1. Fork the repository\n",
    "2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n",
    "3. Commit your changes (`git commit -m 'Add amazing feature'`)\n",
    "4. Push to the branch (`git push origin feature/amazing-feature`)\n",
    "5. Open a Pull Request\n",
    "\n",
    "## Code Style\n",
    "\n",
    "- Follow PEP 8\n",
    "- Use Black for formatting\n",
    "- Add docstrings to all functions\n",
    "- Write tests for new features\n",
    "\n",
    "## Testing\n",
    "\n",
    "```bash\n",
    "pytest tests/ --cov=src\n",
    "```\n",
    "\n",
    "## Questions?\n",
    "\n",
    "Open an issue or email your.email@example.com\n",
    "```\n",
    "\n",
    "### üé® Documentation Tools\n",
    "\n",
    "**Sphinx (Auto-generate docs):**\n",
    "```bash\n",
    "pip install sphinx\n",
    "sphinx-quickstart docs/\n",
    "sphinx-build -b html docs/ docs/_build\n",
    "```\n",
    "\n",
    "**MkDocs (Beautiful docs):**\n",
    "```bash\n",
    "pip install mkdocs mkdocs-material\n",
    "mkdocs new .\n",
    "mkdocs serve  # Preview locally\n",
    "mkdocs gh-deploy  # Deploy to GitHub Pages\n",
    "```\n",
    "\n",
    "### üí° Documentation Tips\n",
    "\n",
    "**DO:**\n",
    "- ‚úÖ Write docs as you code (not after)\n",
    "- ‚úÖ Include examples for every function\n",
    "- ‚úÖ Add screenshots/GIFs\n",
    "- ‚úÖ Keep it updated\n",
    "- ‚úÖ Explain WHY, not just WHAT\n",
    "\n",
    "**DON'T:**\n",
    "- ‚ùå Assume readers know context\n",
    "- ‚ùå Use jargon without explanation\n",
    "- ‚ùå Let docs get outdated\n",
    "- ‚ùå Skip error documentation\n",
    "- ‚ùå Forget to proofread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercises\n",
    "\n",
    "**Build YOUR project!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### Exercise 1: Extend the Multi-Modal System\n",
    "\n",
    "**Task:** Add a new feature to the ImageCaptionSentimentAnalyzer\n",
    "\n",
    "**Options (choose one):**\n",
    "\n",
    "1. **Emotion Detection**: Add detailed emotion analysis (joy, anger, sadness, etc.)\n",
    "   - Use a multi-class emotion classifier\n",
    "   - Update output to include emotion breakdown\n",
    "\n",
    "2. **Object Detection**: Identify and count objects in image\n",
    "   - Use DETR or YOLO model\n",
    "   - List detected objects with confidence scores\n",
    "\n",
    "3. **Multi-Language Support**: Translate captions to other languages\n",
    "   - Use MarianMT translation models\n",
    "   - Support 3+ languages\n",
    "\n",
    "4. **Batch Processing**: Process multiple images at once\n",
    "   - Accept list of images\n",
    "   - Return aggregated statistics\n",
    "\n",
    "**Requirements:**\n",
    "- Add new method to the class\n",
    "- Write docstring\n",
    "- Test with example\n",
    "- Update README with new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Example: Adding emotion detection\n",
    "\n",
    "class ExtendedImageAnalyzer(ImageCaptionSentimentAnalyzer):\n",
    "    \"\"\"\n",
    "    Extended version with additional features.\n",
    "    \n",
    "    Add your new feature here!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load additional models if needed\n",
    "        pass\n",
    "    \n",
    "    def your_new_feature(self, image):\n",
    "        \"\"\"\n",
    "        TODO: Implement your new feature\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "        \n",
    "        Returns:\n",
    "            dict: Results from your feature\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "print(\"TODO: Extend the multi-modal system with your feature!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### Exercise 2: Create Your Project Structure\n",
    "\n",
    "**Task:** Set up the complete directory structure for YOUR capstone project\n",
    "\n",
    "**Steps:**\n",
    "1. Create all necessary directories\n",
    "2. Add __init__.py files\n",
    "3. Create placeholder files for each module\n",
    "4. Write initial README\n",
    "5. Set up requirements.txt\n",
    "6. Create .gitignore\n",
    "7. Make initial commit\n",
    "\n",
    "**Use the template from earlier in this lesson!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéâ Key Takeaways\n",
    "\n",
    "**Congratulations! You've learned to build production-ready AI projects!**\n",
    "\n",
    "### 1Ô∏è‚É£ **ML Project Workflow**\n",
    "   - ‚úÖ Master the 7-stage pipeline: Problem ‚Üí Data ‚Üí Model ‚Üí Deploy\n",
    "   - ‚úÖ Start simple, iterate based on results\n",
    "   - ‚úÖ Test on fresh data frequently\n",
    "   - ‚úÖ Deploy early to find integration issues\n",
    "   - **Remember:** Shipping beats perfecting\n",
    "\n",
    "### 2Ô∏è‚É£ **Code Organization**\n",
    "   - ‚úÖ Follow industry-standard project structure\n",
    "   - ‚úÖ Use proper naming conventions (PEP 8)\n",
    "   - ‚úÖ Write comprehensive docstrings\n",
    "   - ‚úÖ Implement error handling\n",
    "   - ‚úÖ Use configuration files\n",
    "   - **Remember:** Clean code = professional developer\n",
    "\n",
    "### 3Ô∏è‚É£ **Multi-Modal AI**\n",
    "   - ‚úÖ Combine CV + NLP for cutting-edge projects\n",
    "   - ‚úÖ Use pre-trained models (BLIP, BERT, etc.)\n",
    "   - ‚úÖ Create modular, extensible systems\n",
    "   - ‚úÖ Showcase 2024-2025 skills\n",
    "   - **Remember:** Multi-modal = impressive portfolio\n",
    "\n",
    "### 4Ô∏è‚É£ **Documentation**\n",
    "   - ‚úÖ README is your project's first impression\n",
    "   - ‚úÖ Include model cards for AI projects\n",
    "   - ‚úÖ Document API with examples\n",
    "   - ‚úÖ Keep CHANGELOG up to date\n",
    "   - **Remember:** Great docs = professional work\n",
    "\n",
    "### 5Ô∏è‚É£ **Deployment**\n",
    "   - ‚úÖ Use Streamlit for quick demos\n",
    "   - ‚úÖ FastAPI for production APIs\n",
    "   - ‚úÖ Deploy to cloud (free tiers!)\n",
    "   - ‚úÖ Add monitoring and logging\n",
    "   - **Remember:** Deployed projects get jobs\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Action Items for Tomorrow (Day 3)\n",
    "\n",
    "**Complete these before Day 3:**\n",
    "\n",
    "1. **Implement Core Features**\n",
    "   - Build your MVP\n",
    "   - Test thoroughly\n",
    "   - Fix bugs\n",
    "\n",
    "2. **Clean Up Code**\n",
    "   - Run Black formatter\n",
    "   - Add docstrings\n",
    "   - Remove commented code\n",
    "   - Organize imports\n",
    "\n",
    "3. **Create Demo**\n",
    "   - Build Streamlit/Gradio interface\n",
    "   - Test with various inputs\n",
    "   - Make it user-friendly\n",
    "\n",
    "4. **Write Documentation**\n",
    "   - Complete README\n",
    "   - Add usage examples\n",
    "   - Document limitations\n",
    "\n",
    "**Tomorrow (Day 3), we'll:**\n",
    "- Polish your portfolio presentation\n",
    "- Deploy your project\n",
    "- Prepare for AI job interviews\n",
    "- Plan your continued AI learning journey\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "**Code Quality:**\n",
    "- PEP 8 Style Guide: https://pep8.org\n",
    "- Google Python Style Guide: https://google.github.io/styleguide/pyguide.html\n",
    "- Black Formatter: https://black.readthedocs.io\n",
    "\n",
    "**Project Templates:**\n",
    "- Cookiecutter Data Science: https://drivendata.github.io/cookiecutter-data-science/\n",
    "- ML Project Template: https://github.com/khuyentran1401/data-science-template\n",
    "\n",
    "**Documentation:**\n",
    "- Sphinx: https://www.sphinx-doc.org\n",
    "- MkDocs: https://www.mkdocs.org\n",
    "- Model Cards: https://modelcards.withgoogle.com\n",
    "\n",
    "**Deployment:**\n",
    "- Streamlit: https://streamlit.io\n",
    "- Gradio: https://gradio.app\n",
    "- FastAPI: https://fastapi.tiangolo.com\n",
    "- HuggingFace Spaces: https://huggingface.co/spaces\n",
    "\n",
    "**Multi-Modal AI:**\n",
    "- BLIP: https://github.com/salesforce/BLIP\n",
    "- CLIP: https://github.com/openai/CLIP\n",
    "- LLaVA: https://llava-vl.github.io\n",
    "\n",
    "---\n",
    "\n",
    "**üí¨ Final Thought:**\n",
    "\n",
    "*\"The best AI projects aren't the most complex - they're the ones that solve real problems, are well-documented, and actually work. Focus on shipping complete, polished projects rather than perfect models. Recruiters can't interview your Jupyter notebooks - deploy your work!\"*\n",
    "\n",
    "**üöÄ Tomorrow: Portfolio polish, deployment, and career prep! Almost there!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
