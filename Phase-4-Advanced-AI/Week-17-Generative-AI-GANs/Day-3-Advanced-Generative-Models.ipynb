{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Day 3: Advanced Generative Models\n",
    "\n",
    "**ğŸ¯ Goal:** Master cutting-edge generative AI - StyleGAN, Diffusion Models, and Text-to-Image generation\n",
    "\n",
    "**â±ï¸ Time:** 90-120 minutes\n",
    "\n",
    "**ğŸŒŸ Why This Matters for AI:**\n",
    "- Diffusion models power Stable Diffusion, DALL-E 3, Midjourney, and Sora\n",
    "- Text-to-image is THE breakthrough AI application of 2022-2025\n",
    "- StyleGAN revolutionized face generation (This Person Does Not Exist)\n",
    "- Understanding these models = Understanding modern generative AI\n",
    "- Prompt engineering is a critical skill for AI practitioners\n",
    "- These models are used by millions daily (Adobe, OpenAI, Midjourney, Runway)\n",
    "- Foundation for the creative AI revolution happening NOW\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylegan-intro",
   "metadata": {},
   "source": [
    "## ğŸ¨ StyleGAN: Photorealistic Face Generation\n",
    "\n",
    "**StyleGAN = Style-Based Generator for High-Quality Images**\n",
    "\n",
    "### The Problem with Traditional GANs:\n",
    "\n",
    "**Issues:**\n",
    "- Entangled latent space (can't control specific features)\n",
    "- Example: Can't change \"hair color\" without changing \"face shape\"\n",
    "- Hard to control generation\n",
    "- Limited resolution\n",
    "\n",
    "### StyleGAN Innovation:\n",
    "\n",
    "**Key Idea: Separate \"style\" control at different scales**\n",
    "\n",
    "```\n",
    "Random Noise (z)\n",
    "    â†“\n",
    "Mapping Network (8 layers)\n",
    "    â†“\n",
    "Intermediate Latent (w) â† Disentangled!\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Synthesis Network      â”‚\n",
    "â”‚                         â”‚\n",
    "â”‚  4Ã—4   â† Style (coarse: pose, face shape)\n",
    "â”‚   â†“                     â”‚\n",
    "â”‚  8Ã—8   â† Style (middle: hair, eyes)\n",
    "â”‚   â†“                     â”‚\n",
    "â”‚  16Ã—16 â† Style (middle: facial features)\n",
    "â”‚   â†“                     â”‚\n",
    "â”‚  32Ã—32 â† Style (fine: skin texture)\n",
    "â”‚   â†“                     â”‚\n",
    "â”‚  1024Ã—1024 â† Final Image!\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ¯ StyleGAN Features:\n",
    "\n",
    "**1. Mapping Network:**\n",
    "- Transforms z â†’ w (disentangled latent space)\n",
    "- w space is more linear and controllable\n",
    "- Enables precise feature control\n",
    "\n",
    "**2. Adaptive Instance Normalization (AdaIN):**\n",
    "- Injects \"style\" at each resolution\n",
    "- Controls coarse to fine features\n",
    "- Coarse: Overall structure (pose, face shape)\n",
    "- Middle: Features (hair, eyes, mouth)\n",
    "- Fine: Details (skin texture, freckles)\n",
    "\n",
    "**3. Progressive Growing:**\n",
    "- Start training at low resolution (4Ã—4)\n",
    "- Gradually add layers â†’ increase resolution\n",
    "- More stable training\n",
    "- Enables 1024Ã—1024 generation!\n",
    "\n",
    "**4. Noise Injection:**\n",
    "- Random noise at each layer\n",
    "- Controls stochastic variation (hair placement, pores)\n",
    "- Adds realism without affecting identity\n",
    "\n",
    "### ğŸŒŸ Real-World Applications (2024-2025):\n",
    "\n",
    "**This Person Does Not Exist:**\n",
    "- StyleGAN2 generates photorealistic faces\n",
    "- 100% fake, indistinguishable from real\n",
    "- Millions of unique faces\n",
    "\n",
    "**Commercial Uses:**\n",
    "- ğŸ® **Gaming:** Generate character faces (EA, Ubisoft)\n",
    "- ğŸ¬ **Film:** De-aging actors, digital doubles\n",
    "- ğŸ‘— **Fashion:** Virtual models (Levi's, Tommy Hilfiger)\n",
    "- ğŸ“± **Social Media:** Filters, avatars (Snapchat, Instagram)\n",
    "- ğŸ¢ **Stock Photography:** AI-generated people for ads\n",
    "\n",
    "**StyleGAN Evolution:**\n",
    "- **StyleGAN (2018):** 1024Ã—1024 faces\n",
    "- **StyleGAN2 (2019):** Better quality, fewer artifacts\n",
    "- **StyleGAN3 (2021):** Alias-free generation, smooth animations\n",
    "- **StyleGAN-XL (2022):** ImageNet-scale, diverse images\n",
    "\n",
    "### ğŸ’¡ Why StyleGAN Matters:\n",
    "\n",
    "**Disentangled Control:**\n",
    "- Change specific features independently\n",
    "- Age face without changing gender\n",
    "- Add glasses without changing hair\n",
    "- Precision control for creative applications\n",
    "\n",
    "**High Quality:**\n",
    "- Photorealistic outputs\n",
    "- No blur (unlike VAEs)\n",
    "- Consistent quality\n",
    "\n",
    "**Influence:**\n",
    "- Inspired countless variations\n",
    "- Foundation for modern GANs\n",
    "- Techniques used in other domains (text, 3D, video)\n",
    "\n",
    "Let's explore StyleGAN concepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import Image, display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"Let's explore cutting-edge generative AI! ğŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylegan-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StyleGAN Concept Demo: Disentangled Latent Space\n",
    "\n",
    "def visualize_stylegan_concept():\n",
    "    \"\"\"\n",
    "    Visualize how StyleGAN controls different features at different scales\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    fig.suptitle('ğŸ¨ StyleGAN: Multi-Scale Style Control', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Simulate different style injections\n",
    "    resolutions = ['4Ã—4', '8Ã—8', '16Ã—16', '32Ã—32', '64Ã—64', '128Ã—128', '256Ã—256', '512Ã—512']\n",
    "    controls = [\n",
    "        'Pose\\nFace Shape',\n",
    "        'Overall\\nStructure', \n",
    "        'Hair Style\\nGender',\n",
    "        'Facial\\nFeatures',\n",
    "        'Eye Color\\nNose Shape',\n",
    "        'Skin Tone\\nSmile',\n",
    "        'Fine Details\\nWrinkles',\n",
    "        'Micro Texture\\nPores'\n",
    "    ]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, 8))\n",
    "    \n",
    "    for idx, (res, control, color) in enumerate(zip(resolutions, controls, colors)):\n",
    "        ax = axes[idx // 4, idx % 4]\n",
    "        \n",
    "        # Create circular representation\n",
    "        circle = plt.Circle((0.5, 0.5), 0.3, color=color, alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(0.5, 0.5, res, ha='center', va='center', \n",
    "               fontsize=14, fontweight='bold', color='white')\n",
    "        ax.text(0.5, 0.15, control, ha='center', va='center', \n",
    "               fontsize=9, style='italic')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Layer {idx+1}', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ¯ StyleGAN Multi-Scale Control:\")\n",
    "    print(\"\\nğŸ“Š Coarse Styles (4Ã—4 to 16Ã—16):\")\n",
    "    print(\"   - Overall pose and position\")\n",
    "    print(\"   - Face shape and general structure\")\n",
    "    print(\"   - Gender and age (rough)\")\n",
    "    \n",
    "    print(\"\\nğŸ¨ Middle Styles (32Ã—32 to 128Ã—128):\")\n",
    "    print(\"   - Hair style and color\")\n",
    "    print(\"   - Eye shape and color\")\n",
    "    print(\"   - Facial features (nose, mouth, ears)\")\n",
    "    print(\"   - Skin tone\")\n",
    "    \n",
    "    print(\"\\nâœ¨ Fine Styles (256Ã—256 to 1024Ã—1024):\")\n",
    "    print(\"   - Skin texture and pores\")\n",
    "    print(\"   - Freckles and moles\")\n",
    "    print(\"   - Fine wrinkles\")\n",
    "    print(\"   - Hair strands\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Key Innovation:\")\n",
    "    print(\"   You can mix styles from different sources!\")\n",
    "    print(\"   Example: Person A's pose + Person B's features + Person C's skin\")\n",
    "\n",
    "visualize_stylegan_concept()\n",
    "\n",
    "print(\"\\nğŸŒŸ This is why StyleGAN is revolutionary!\")\n",
    "print(\"   - Unprecedented control over generation\")\n",
    "print(\"   - Can edit specific features without affecting others\")\n",
    "print(\"   - Enables applications like face editing, aging, makeup transfer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diffusion-intro",
   "metadata": {},
   "source": [
    "## ğŸŒŠ Diffusion Models: The New King of Generation\n",
    "\n",
    "**Diffusion = Gradually denoise random noise into images**\n",
    "\n",
    "### The Breakthrough (2020-2024):\n",
    "\n",
    "**Why Diffusion Models Won:**\n",
    "- Better quality than GANs (in many cases)\n",
    "- More stable training than GANs\n",
    "- No mode collapse\n",
    "- Easier to condition (text-to-image!)\n",
    "- Scalable to high resolution\n",
    "\n",
    "**Modern Applications:**\n",
    "- ğŸ¨ **DALL-E 2 & 3:** OpenAI's text-to-image\n",
    "- ğŸ–¼ï¸ **Stable Diffusion:** Open-source image generation\n",
    "- ğŸ­ **Midjourney:** Artistic image generation\n",
    "- ğŸ¬ **Sora:** OpenAI's text-to-video (2024)\n",
    "- ğŸµ **Music generation:** Riffusion, MusicLM\n",
    "\n",
    "### How Diffusion Works:\n",
    "\n",
    "**Forward Process (Noising):**\n",
    "```\n",
    "Real Image\n",
    "    â†“ Add noise\n",
    "Slightly noisy\n",
    "    â†“ Add more noise\n",
    "Very noisy\n",
    "    â†“ Add more noise\n",
    "    ...\n",
    "    â†“ (1000 steps)\n",
    "Pure Gaussian Noise\n",
    "```\n",
    "\n",
    "**Reverse Process (Denoising - THE KEY!):**\n",
    "```\n",
    "Pure Gaussian Noise\n",
    "    â†“ Predict & remove noise\n",
    "Slightly less noisy\n",
    "    â†“ Predict & remove noise\n",
    "Image emerging\n",
    "    â†“ Predict & remove noise\n",
    "    ...\n",
    "    â†“ (1000 steps)\n",
    "Clear Image!\n",
    "```\n",
    "\n",
    "### ğŸ¯ The Diffusion Process:\n",
    "\n",
    "**Training:**\n",
    "1. Take real image\n",
    "2. Add random noise (timestep t)\n",
    "3. Train neural network to predict the noise\n",
    "4. Repeat for all timesteps (t=1 to T)\n",
    "\n",
    "**Generation:**\n",
    "1. Start with pure noise\n",
    "2. For each timestep (T to 1):\n",
    "   - Predict noise with neural network\n",
    "   - Remove predicted noise\n",
    "   - Get slightly denoised image\n",
    "3. After all steps: Clean image!\n",
    "\n",
    "### Mathematical Intuition:\n",
    "\n",
    "**Forward (add noise):**\n",
    "```\n",
    "x_t = âˆš(1-Î²_t) Â· x_{t-1} + âˆšÎ²_t Â· Îµ\n",
    "where Îµ ~ N(0, I) is Gaussian noise\n",
    "```\n",
    "\n",
    "**Reverse (denoise):**\n",
    "```\n",
    "x_{t-1} = 1/âˆšÎ±_t Â· (x_t - (1-Î±_t)/âˆš(1-á¾±_t) Â· Îµ_Î¸(x_t, t))\n",
    "where Îµ_Î¸ is the learned denoising network\n",
    "```\n",
    "\n",
    "**Don't worry about the math! Key insight:**\n",
    "- Learn to predict and remove noise\n",
    "- Iteratively denoise until you get clean image\n",
    "- Each step is small and stable\n",
    "\n",
    "### ğŸŒŸ Why Diffusion > GANs (for many tasks):\n",
    "\n",
    "| Aspect | GAN | Diffusion |\n",
    "|--------|-----|----------|\n",
    "| **Training Stability** | âŒ Unstable | âœ… Stable |\n",
    "| **Mode Collapse** | âŒ Common | âœ… Rare |\n",
    "| **Image Quality** | âœ… Excellent | âœ… Excellent |\n",
    "| **Diversity** | âš ï¸ Can be limited | âœ… High |\n",
    "| **Text Conditioning** | âš ï¸ Tricky | âœ… Natural |\n",
    "| **Training Time** | âœ… Faster | âŒ Slower |\n",
    "| **Generation Speed** | âœ… Fast (1 step) | âŒ Slow (1000 steps) |\n",
    "\n",
    "### Latent Diffusion (Stable Diffusion):\n",
    "\n",
    "**Problem:** Diffusion in pixel space is SLOW\n",
    "- 512Ã—512 image = 786,432 pixels\n",
    "- 1000 denoising steps\n",
    "- Very computationally expensive!\n",
    "\n",
    "**Solution: Latent Diffusion**\n",
    "```\n",
    "Image (512Ã—512)\n",
    "    â†“ VAE Encoder\n",
    "Latent (64Ã—64) â† 64x smaller!\n",
    "    â†“ Diffusion Process (1000 steps)\n",
    "Denoised Latent\n",
    "    â†“ VAE Decoder\n",
    "Final Image (512Ã—512)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- 64x faster (work in compressed space)\n",
    "- Same quality\n",
    "- Enables real-time generation\n",
    "- This is what Stable Diffusion uses!\n",
    "\n",
    "### ğŸ¯ Text Conditioning:\n",
    "\n",
    "**How to add text control:**\n",
    "```\n",
    "Text Prompt: \"A cat astronaut\"\n",
    "    â†“\n",
    "CLIP Text Encoder\n",
    "    â†“\n",
    "Text Embedding (77 tokens Ã— 768 dim)\n",
    "    â†“\n",
    "Cross-Attention with Image Features\n",
    "    â†“\n",
    "Denoising U-Net (conditioned on text)\n",
    "    â†“\n",
    "Generated Image\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "1. **CLIP:** Connects text and images (from OpenAI)\n",
    "2. **Cross-Attention:** Mechanism to inject text into diffusion\n",
    "3. **Classifier-Free Guidance:** Stronger text adherence\n",
    "\n",
    "Let's explore diffusion concepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diffusion-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion Process Visualization\n",
    "\n",
    "def visualize_diffusion_process():\n",
    "    \"\"\"\n",
    "    Simulate the forward and reverse diffusion process\n",
    "    \"\"\"\n",
    "    # Create a simple \"image\" (gradient)\n",
    "    img_size = 64\n",
    "    x = np.linspace(0, 1, img_size)\n",
    "    y = np.linspace(0, 1, img_size)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    clean_image = np.sin(4 * np.pi * X) * np.cos(4 * np.pi * Y)\n",
    "    \n",
    "    # Forward process: gradually add noise\n",
    "    timesteps = [0, 100, 250, 500, 750, 1000]\n",
    "    noisy_images = []\n",
    "    \n",
    "    for t in timesteps:\n",
    "        # Noise schedule (increasing noise over time)\n",
    "        noise_level = t / 1000.0\n",
    "        noise = np.random.randn(img_size, img_size)\n",
    "        noisy = clean_image * (1 - noise_level) + noise * noise_level\n",
    "        noisy_images.append(noisy)\n",
    "    \n",
    "    # Visualize forward process\n",
    "    fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "    fig.suptitle('ğŸŒŠ Diffusion Process: Forward (Add Noise) & Reverse (Denoise)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Forward process\n",
    "    for i, (t, img) in enumerate(zip(timesteps, noisy_images)):\n",
    "        axes[0, i].imshow(img, cmap='RdBu', vmin=-2, vmax=2)\n",
    "        axes[0, i].set_title(f'Forward\\nt={t}', fontsize=11, fontweight='bold')\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].text(-0.2, 0.5, 'Add Noise â†’', transform=axes[0, i].transAxes,\n",
    "                          fontsize=12, va='center', rotation=90, fontweight='bold')\n",
    "    \n",
    "    # Reverse process (reversed order)\n",
    "    for i, (t, img) in enumerate(zip(reversed(timesteps), reversed(noisy_images))):\n",
    "        axes[1, i].imshow(img, cmap='RdBu', vmin=-2, vmax=2)\n",
    "        axes[1, i].set_title(f'Reverse\\nt={t}', fontsize=11, fontweight='bold')\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].text(-0.2, 0.5, 'Remove Noise â†’', transform=axes[1, i].transAxes,\n",
    "                          fontsize=12, va='center', rotation=90, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“Š Understanding Diffusion:\")\n",
    "    print(\"\\nğŸ”µ Forward Process (Training):\")\n",
    "    print(\"   - Start with clean image\")\n",
    "    print(\"   - Gradually add Gaussian noise over 1000 steps\")\n",
    "    print(\"   - End with pure noise (no image information left)\")\n",
    "    print(\"   - Neural network learns to predict the noise at each step\")\n",
    "    \n",
    "    print(\"\\nğŸ”´ Reverse Process (Generation):\")\n",
    "    print(\"   - Start with pure random noise\")\n",
    "    print(\"   - Use trained network to predict noise at each step\")\n",
    "    print(\"   - Remove predicted noise iteratively\")\n",
    "    print(\"   - After 1000 steps: clean image emerges!\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Key Insight:\")\n",
    "    print(\"   - Each denoising step is SMALL and STABLE\")\n",
    "    print(\"   - Unlike GANs (one big jump from noise to image)\")\n",
    "    print(\"   - This gradual process is why diffusion is stable!\")\n",
    "\n",
    "visualize_diffusion_process()\n",
    "\n",
    "print(\"\\nğŸŒŸ Real Applications:\")\n",
    "print(\"   - DALL-E 2: Uses this exact process!\")\n",
    "print(\"   - Stable Diffusion: In latent space (faster)\")\n",
    "print(\"   - Midjourney: Proprietary diffusion variant\")\n",
    "print(\"   - Sora: Extends to video generation (diffusion in spacetime)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-to-image",
   "metadata": {},
   "source": [
    "## ğŸ¨ Text-to-Image Generation\n",
    "\n",
    "**Text-to-Image = Type words â†’ Get images**\n",
    "\n",
    "### The Revolution (2022-2025):\n",
    "\n",
    "**Timeline:**\n",
    "- **2021:** DALL-E 1 (proof of concept)\n",
    "- **2022:** DALL-E 2 (photorealistic)\n",
    "- **2022:** Midjourney (artistic focus)\n",
    "- **2022:** Stable Diffusion (open source!)\n",
    "- **2023:** DALL-E 3 (better prompt following)\n",
    "- **2024:** Midjourney v6, SDXL, Adobe Firefly\n",
    "- **2024:** Sora (text-to-video!)\n",
    "\n",
    "### How Text-to-Image Works:\n",
    "\n",
    "**Architecture (Stable Diffusion example):**\n",
    "\n",
    "```\n",
    "Text Prompt: \"A cozy cabin in snowy mountains at sunset\"\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  CLIP Text Encoder                     â”‚\n",
    "â”‚  - Tokenize text                       â”‚\n",
    "â”‚  - Convert to embeddings               â”‚\n",
    "â”‚  Output: 77 tokens Ã— 768 dimensions    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "Text Embeddings\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Latent Diffusion Process              â”‚\n",
    "â”‚                                        â”‚\n",
    "â”‚  Random Noise (64Ã—64Ã—4)                â”‚\n",
    "â”‚      â†“                                 â”‚\n",
    "â”‚  U-Net Denoiser (with cross-attention) â”‚\n",
    "â”‚      â†“                                 â”‚\n",
    "â”‚  Cross-Attention:                      â”‚\n",
    "â”‚    Query: Image features               â”‚\n",
    "â”‚    Key/Value: Text embeddings          â”‚\n",
    "â”‚      â†“                                 â”‚\n",
    "â”‚  Predict noise                         â”‚\n",
    "â”‚      â†“                                 â”‚\n",
    "â”‚  Remove noise                          â”‚\n",
    "â”‚      â†“                                 â”‚\n",
    "â”‚  Repeat 20-50 steps                    â”‚\n",
    "â”‚      â†“                                 â”‚\n",
    "â”‚  Clean Latent (64Ã—64Ã—4)                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "VAE Decoder\n",
    "    â†“\n",
    "Final Image (512Ã—512Ã—3)\n",
    "```\n",
    "\n",
    "### ğŸ¯ Key Components:\n",
    "\n",
    "**1. CLIP (Contrastive Language-Image Pre-training):**\n",
    "- Trained on 400M image-text pairs\n",
    "- Learns joint embedding space for text and images\n",
    "- Text \"cat\" is close to cat images in embedding space\n",
    "- Enables text conditioning!\n",
    "\n",
    "**2. Cross-Attention Mechanism:**\n",
    "- Connects text embeddings to image features\n",
    "- \"Where should 'cabin' appear in the image?\"\n",
    "- \"What color should 'sunset' be?\"\n",
    "- Each word influences relevant image regions\n",
    "\n",
    "**3. Classifier-Free Guidance (CFG):**\n",
    "- Controls how strongly to follow the prompt\n",
    "- CFG scale = 7: Balanced\n",
    "- CFG scale = 15: Very strict (sometimes worse quality)\n",
    "- CFG scale = 1: Ignores prompt (random)\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Îµ_guided = Îµ_uncond + CFG_scale * (Îµ_cond - Îµ_uncond)\n",
    "\n",
    "where:\n",
    "  Îµ_uncond = noise prediction without text\n",
    "  Îµ_cond = noise prediction with text\n",
    "  CFG_scale = guidance strength\n",
    "```\n",
    "\n",
    "### ğŸŒŸ Modern Systems (2024-2025):\n",
    "\n",
    "**DALL-E 3:**\n",
    "- Better prompt understanding\n",
    "- Improved text rendering in images\n",
    "- Integrated with ChatGPT\n",
    "- Can refine prompts automatically\n",
    "\n",
    "**Midjourney v6:**\n",
    "- Photorealistic and artistic styles\n",
    "- Community-driven aesthetic\n",
    "- Discord-based interface\n",
    "- Exceptional quality\n",
    "\n",
    "**Stable Diffusion XL (SDXL):**\n",
    "- 1024Ã—1024 native resolution\n",
    "- Better composition\n",
    "- Open source (can run locally!)\n",
    "- Active community (thousands of models)\n",
    "\n",
    "**Adobe Firefly:**\n",
    "- Commercially safe (trained on licensed content)\n",
    "- Integrated in Photoshop, Illustrator\n",
    "- Professional workflows\n",
    "\n",
    "### ğŸ’¡ Why This Matters:\n",
    "\n",
    "**Democratization of Creativity:**\n",
    "- No art skills needed\n",
    "- Describe what you want\n",
    "- AI creates it\n",
    "\n",
    "**Professional Impact:**\n",
    "- ğŸ¨ Graphic designers: Concept art, mockups\n",
    "- ğŸ“± App developers: UI designs, icons\n",
    "- ğŸ“š Authors: Book covers, illustrations\n",
    "- ğŸ® Game developers: Textures, concept art\n",
    "- ğŸ¬ Filmmakers: Storyboards, VFX concepts\n",
    "\n",
    "**Economic Impact:**\n",
    "- Billion-dollar industry (2024)\n",
    "- Millions of users daily\n",
    "- New job category: \"Prompt Engineers\"\n",
    "- Integration into Adobe, Canva, etc.\n",
    "\n",
    "Let's explore text-to-image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-to-image-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Image Architecture Visualization\n",
    "\n",
    "def visualize_text_to_image_pipeline():\n",
    "    \"\"\"\n",
    "    Visualize the text-to-image generation pipeline\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    fig.suptitle('ğŸ¨ Text-to-Image Pipeline (Stable Diffusion)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    stages = [\n",
    "        ('Text\\nPrompt', '\"A cat\\nastroanut\"', 'lightblue'),\n",
    "        ('CLIP\\nEncoder', 'Textâ†’\\nEmbeddings', 'lightgreen'),\n",
    "        ('Diffusion\\nU-Net', 'Denoise\\n(50 steps)', 'lightyellow'),\n",
    "        ('VAE\\nDecoder', 'Latentâ†’\\nPixels', 'lightcoral'),\n",
    "        ('Final\\nImage', 'High-res\\nOutput', 'lavender')\n",
    "    ]\n",
    "    \n",
    "    for i, (title, desc, color) in enumerate(stages):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Draw box\n",
    "        rect = plt.Rectangle((0.1, 0.3), 0.8, 0.4, \n",
    "                            facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add text\n",
    "        ax.text(0.5, 0.6, title, ha='center', va='center',\n",
    "               fontsize=12, fontweight='bold')\n",
    "        ax.text(0.5, 0.4, desc, ha='center', va='center',\n",
    "               fontsize=9, style='italic')\n",
    "        \n",
    "        # Add arrow\n",
    "        if i < 4:\n",
    "            ax.annotate('', xy=(1.05, 0.5), xytext=(0.95, 0.5),\n",
    "                       arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "        \n",
    "        ax.set_xlim(0, 1.1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ” Pipeline Breakdown:\")\n",
    "    print(\"\\n1ï¸âƒ£ Text Prompt:\")\n",
    "    print(\"   Input: Natural language description\")\n",
    "    print(\"   Example: 'A cat astronaut riding a rainbow unicorn'\")\n",
    "    \n",
    "    print(\"\\n2ï¸âƒ£ CLIP Text Encoder:\")\n",
    "    print(\"   - Tokenize text into words/subwords\")\n",
    "    print(\"   - Convert each token to 768-dim embedding\")\n",
    "    print(\"   - Output: 77 tokens Ã— 768 dimensions\")\n",
    "    print(\"   - These embeddings guide the generation\")\n",
    "    \n",
    "    print(\"\\n3ï¸âƒ£ Diffusion U-Net:\")\n",
    "    print(\"   - Start with random noise (64Ã—64Ã—4 latent)\")\n",
    "    print(\"   - For each denoising step:\")\n",
    "    print(\"     â€¢ Cross-attention with text embeddings\")\n",
    "    print(\"     â€¢ Predict noise\")\n",
    "    print(\"     â€¢ Remove predicted noise\")\n",
    "    print(\"   - Repeat 20-50 times\")\n",
    "    print(\"   - Output: Clean latent representation\")\n",
    "    \n",
    "    print(\"\\n4ï¸âƒ£ VAE Decoder:\")\n",
    "    print(\"   - Upscale latent (64Ã—64Ã—4) to pixels (512Ã—512Ã—3)\")\n",
    "    print(\"   - 8x spatial upsampling\")\n",
    "    print(\"   - Adds fine details\")\n",
    "    \n",
    "    print(\"\\n5ï¸âƒ£ Final Image:\")\n",
    "    print(\"   - High-resolution (512Ã—512 or 1024Ã—1024)\")\n",
    "    print(\"   - Matches the text description\")\n",
    "    print(\"   - Ready to use!\")\n",
    "\n",
    "visualize_text_to_image_pipeline()\n",
    "\n",
    "print(\"\\nâš™ï¸ Typical Settings:\")\n",
    "print(\"   - Steps: 20-50 (more = better quality, slower)\")\n",
    "print(\"   - CFG Scale: 7-9 (prompt adherence)\")\n",
    "print(\"   - Sampler: DPM++, Euler, DDIM\")\n",
    "print(\"   - Resolution: 512Ã—512 to 1024Ã—1024\")\n",
    "print(\"   - Generation time: 2-10 seconds (GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-engineering",
   "metadata": {},
   "source": [
    "## ğŸ¯ Prompt Engineering for Generative AI\n",
    "\n",
    "**Prompt Engineering = Art + Science of writing effective prompts**\n",
    "\n",
    "### Why Prompt Engineering Matters:\n",
    "\n",
    "**Same model, different results:**\n",
    "- âŒ Bad prompt â†’ mediocre output\n",
    "- âœ… Good prompt â†’ amazing output\n",
    "\n",
    "**Prompt engineering is now a JOB:**\n",
    "- Companies hire \"Prompt Engineers\"\n",
    "- Salary: $200k-$400k+ (2024)\n",
    "- Critical skill for AI practitioners\n",
    "\n",
    "### ğŸ¨ Anatomy of a Good Prompt:\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "[Subject] + [Description] + [Style] + [Quality] + [Technical]\n",
    "```\n",
    "\n",
    "**Example Breakdown:**\n",
    "```\n",
    "âŒ Bad: \"cat\"\n",
    "âœ… Good: \"A majestic Persian cat with blue eyes, sitting on a velvet cushion, \n",
    "         oil painting style, Renaissance lighting, highly detailed, 8k, \n",
    "         trending on ArtStation\"\n",
    "\n",
    "Components:\n",
    "  Subject: Persian cat with blue eyes\n",
    "  Action: sitting on velvet cushion\n",
    "  Style: oil painting, Renaissance lighting\n",
    "  Quality: highly detailed, 8k\n",
    "  Context: trending on ArtStation\n",
    "```\n",
    "\n",
    "### ğŸ“‹ Prompt Templates:\n",
    "\n",
    "**1. Photorealistic:**\n",
    "```\n",
    "\"[Subject], professional photography, natural lighting, \n",
    " sharp focus, 8k resolution, DSLR, award winning\"\n",
    "\n",
    "Example:\n",
    "\"Mountain landscape at sunset, professional photography, \n",
    " natural lighting, sharp focus, 8k resolution, DSLR, award winning\"\n",
    "```\n",
    "\n",
    "**2. Artistic/Painting:**\n",
    "```\n",
    "\"[Subject], [art style] by [artist], [medium], \n",
    " [lighting], highly detailed, masterpiece\"\n",
    "\n",
    "Example:\n",
    "\"Fantasy castle, digital art by Greg Rutkowski, \n",
    " dramatic lighting, highly detailed, ArtStation trending\"\n",
    "```\n",
    "\n",
    "**3. Concept Art:**\n",
    "```\n",
    "\"[Subject], concept art, [mood], [color palette], \n",
    " detailed, professional, trending on ArtStation\"\n",
    "\n",
    "Example:\n",
    "\"Cyberpunk city street, concept art, neon lighting, \n",
    " purple and blue tones, detailed, trending on ArtStation\"\n",
    "```\n",
    "\n",
    "**4. Character Design:**\n",
    "```\n",
    "\"Character design of [description], [style], \n",
    " full body, white background, reference sheet\"\n",
    "\n",
    "Example:\n",
    "\"Character design of a space explorer, anime style, \n",
    " full body, white background, turnaround reference sheet\"\n",
    "```\n",
    "\n",
    "### ğŸ¯ Power Words & Modifiers:\n",
    "\n",
    "**Quality Boosters:**\n",
    "- \"highly detailed\", \"8k\", \"4k\", \"ultra HD\"\n",
    "- \"photorealistic\", \"hyperrealistic\"\n",
    "- \"masterpiece\", \"award winning\"\n",
    "- \"professional\", \"studio quality\"\n",
    "- \"trending on ArtStation\", \"featured on Behance\"\n",
    "\n",
    "**Lighting:**\n",
    "- \"golden hour\", \"dramatic lighting\", \"studio lighting\"\n",
    "- \"volumetric lighting\", \"rim lighting\", \"ambient occlusion\"\n",
    "- \"soft lighting\", \"hard shadows\"\n",
    "- \"neon lighting\", \"bioluminescent\"\n",
    "\n",
    "**Camera/Technical:**\n",
    "- \"bokeh\", \"depth of field\", \"f/1.4\"\n",
    "- \"wide angle\", \"telephoto\", \"macro\"\n",
    "- \"cinematic\", \"film grain\"\n",
    "- \"sharp focus\", \"tack sharp\"\n",
    "\n",
    "**Artistic Styles:**\n",
    "- \"oil painting\", \"watercolor\", \"digital art\"\n",
    "- \"anime\", \"manga\", \"comic book\"\n",
    "- \"Art Nouveau\", \"Art Deco\", \"Baroque\"\n",
    "- \"cyberpunk\", \"steampunk\", \"vaporwave\"\n",
    "\n",
    "**Artists (style transfer):**\n",
    "- \"by Greg Rutkowski\" (fantasy art)\n",
    "- \"by Alphonse Mucha\" (Art Nouveau)\n",
    "- \"by Studio Ghibli\" (anime)\n",
    "- \"by Artgerm\" (character design)\n",
    "- \"by Simon StÃ¥lenhag\" (sci-fi)\n",
    "\n",
    "### âš ï¸ Negative Prompts:\n",
    "\n",
    "**What NOT to include:**\n",
    "```\n",
    "Negative: \"ugly, blurry, low quality, distorted, \n",
    "           duplicate, watermark, signature, text, \n",
    "           deformed, bad anatomy, extra limbs\"\n",
    "```\n",
    "\n",
    "**Why negative prompts?**\n",
    "- Guide away from common artifacts\n",
    "- Improve quality significantly\n",
    "- Essential for good results\n",
    "\n",
    "### ğŸ› ï¸ Advanced Techniques:\n",
    "\n",
    "**1. Weighted Prompts:**\n",
    "```\n",
    "\"(cat:1.5), (astronaut:1.2), space, stars\"\n",
    "â†’ Emphasizes \"cat\" more than \"astronaut\"\n",
    "```\n",
    "\n",
    "**2. Prompt Blending:**\n",
    "```\n",
    "\"cat AND astronaut\" \n",
    "â†’ Combines both concepts\n",
    "```\n",
    "\n",
    "**3. Step-by-Step Prompts:**\n",
    "```\n",
    "\"[cat:astronaut:0.5]\" \n",
    "â†’ Start as cat, morph to astronaut halfway\n",
    "```\n",
    "\n",
    "**4. LoRA/Embeddings:**\n",
    "```\n",
    "\"<lora:studio_ghibli:0.8> forest scene\"\n",
    "â†’ Apply Studio Ghibli style at 80% strength\n",
    "```\n",
    "\n",
    "### ğŸ’¡ Best Practices:\n",
    "\n",
    "**DO:**\n",
    "- âœ… Be specific and detailed\n",
    "- âœ… Use quality modifiers\n",
    "- âœ… Reference artistic styles\n",
    "- âœ… Include lighting and mood\n",
    "- âœ… Use negative prompts\n",
    "- âœ… Iterate and refine\n",
    "\n",
    "**DON'T:**\n",
    "- âŒ Be too vague (\"something cool\")\n",
    "- âŒ Use contradictory terms\n",
    "- âŒ Overload with too many concepts\n",
    "- âŒ Forget negative prompts\n",
    "- âŒ Expect perfection on first try\n",
    "\n",
    "### ğŸŒŸ Prompt Engineering Workflow:\n",
    "\n",
    "```\n",
    "1. Start Simple\n",
    "   \"A cat\"\n",
    "      â†“\n",
    "2. Add Details\n",
    "   \"A fluffy Persian cat with blue eyes\"\n",
    "      â†“\n",
    "3. Add Context\n",
    "   \"A fluffy Persian cat with blue eyes, sitting on red cushion\"\n",
    "      â†“\n",
    "4. Add Style\n",
    "   \"...oil painting style, Renaissance lighting\"\n",
    "      â†“\n",
    "5. Add Quality\n",
    "   \"...highly detailed, 8k, masterpiece\"\n",
    "      â†“\n",
    "6. Add Negatives\n",
    "   Negative: \"blurry, low quality, distorted\"\n",
    "      â†“\n",
    "7. Generate & Iterate!\n",
    "```\n",
    "\n",
    "Let's practice prompt engineering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Engineering Examples\n",
    "\n",
    "def compare_prompts():\n",
    "    \"\"\"\n",
    "    Compare bad vs good prompts\n",
    "    \"\"\"\n",
    "    examples = [\n",
    "        {\n",
    "            'bad': 'dog',\n",
    "            'good': 'A golden retriever puppy playing in autumn leaves, professional photography, golden hour lighting, bokeh background, sharp focus, 8k',\n",
    "            'negative': 'blurry, low quality, watermark',\n",
    "            'category': 'Photography'\n",
    "        },\n",
    "        {\n",
    "            'bad': 'fantasy castle',\n",
    "            'good': 'Majestic fantasy castle on mountain peak, dramatic sunset, matte painting by Greg Rutkowski, volumetric clouds, epic scale, highly detailed, trending on ArtStation',\n",
    "            'negative': 'ugly, distorted, low quality, frame',\n",
    "            'category': 'Digital Art'\n",
    "        },\n",
    "        {\n",
    "            'bad': 'robot',\n",
    "            'good': 'Futuristic humanoid robot, cyberpunk aesthetic, neon accents, metallic surfaces, concept art, by Simon StÃ¥lenhag, cinematic lighting, 4k, highly detailed',\n",
    "            'negative': 'blurry, duplicate, bad anatomy, cartoon',\n",
    "            'category': 'Concept Art'\n",
    "        },\n",
    "        {\n",
    "            'bad': 'landscape',\n",
    "            'good': 'Serene Japanese garden with cherry blossoms, traditional architecture, misty morning, soft pastel colors, anime style by Studio Ghibli, peaceful atmosphere, 8k',\n",
    "            'negative': 'ugly, modern buildings, cars, people',\n",
    "            'category': 'Anime Style'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ¯ Prompt Engineering Examples\\n\" + \"=\"*80)\n",
    "    \n",
    "    for i, ex in enumerate(examples, 1):\n",
    "        print(f\"\\n{i}. {ex['category']}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"âŒ Bad Prompt:\")\n",
    "        print(f\"   '{ex['bad']}'\")\n",
    "        print(f\"\\nâœ… Good Prompt:\")\n",
    "        print(f\"   '{ex['good']}'\")\n",
    "        print(f\"\\nğŸš« Negative Prompt:\")\n",
    "        print(f\"   '{ex['negative']}'\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ What Makes a Good Prompt:\")\n",
    "    print(\"\\nğŸ“ Components:\")\n",
    "    print(\"   1. Subject: What you want to generate\")\n",
    "    print(\"   2. Details: Specific characteristics\")\n",
    "    print(\"   3. Style: Artistic direction (photography, painting, etc.)\")\n",
    "    print(\"   4. Artist: Reference known artists for style\")\n",
    "    print(\"   5. Lighting: Mood and atmosphere\")\n",
    "    print(\"   6. Quality: Technical specs (8k, detailed, etc.)\")\n",
    "    print(\"   7. Context: Platforms (ArtStation), time period, etc.\")\n",
    "    \n",
    "    print(\"\\nğŸ¨ Iterative Process:\")\n",
    "    print(\"   1. Start with basic subject\")\n",
    "    print(\"   2. Add one component at a time\")\n",
    "    print(\"   3. Generate and evaluate\")\n",
    "    print(\"   4. Refine based on results\")\n",
    "    print(\"   5. Use negative prompts to fix issues\")\n",
    "\n",
    "compare_prompts()\n",
    "\n",
    "print(\"\\nğŸŒŸ Pro Tips:\")\n",
    "print(\"   - Study prompts from successful generations\")\n",
    "print(\"   - Build a 'prompt library' of what works\")\n",
    "print(\"   - Use prompt databases (Lexica.art, PromptHero)\")\n",
    "print(\"   - Experiment with different combinations\")\n",
    "print(\"   - Learn from the community (Reddit r/StableDiffusion)\")\n",
    "print(\"\\nğŸ’¼ This skill is VALUABLE in 2024-2025!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-ai-example",
   "metadata": {},
   "source": [
    "## ğŸŒŸ Real AI Example: Using Pre-trained Models\n",
    "\n",
    "**Task:** Access and use state-of-the-art generative models\n",
    "\n",
    "### Available Platforms (2024-2025):\n",
    "\n",
    "**1. OpenAI DALL-E 3:**\n",
    "- Access: ChatGPT Plus, API\n",
    "- Best for: High-quality, prompt following\n",
    "- Cost: $20/month (ChatGPT) or pay-per-image (API)\n",
    "- Example:\n",
    "```python\n",
    "# Via ChatGPT or OpenAI API\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='your-key')\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"A cat astronaut\",\n",
    "    size=\"1024x1024\"\n",
    ")\n",
    "```\n",
    "\n",
    "**2. Stable Diffusion (Open Source):**\n",
    "- Access: Free! (local or cloud)\n",
    "- Platforms: HuggingFace, Automatic1111, ComfyUI\n",
    "- Best for: Full control, customization\n",
    "- Can run on your own GPU!\n",
    "\n",
    "**3. Midjourney:**\n",
    "- Access: Discord bot\n",
    "- Best for: Artistic, creative styles\n",
    "- Cost: $10-60/month\n",
    "- Very popular among artists\n",
    "\n",
    "**4. Adobe Firefly:**\n",
    "- Access: Adobe Creative Cloud\n",
    "- Best for: Professional workflows\n",
    "- Commercially safe (licensed training data)\n",
    "- Integrated in Photoshop, Illustrator\n",
    "\n",
    "### ğŸ› ï¸ Tools & Interfaces:\n",
    "\n",
    "**HuggingFace Diffusers:**\n",
    "```python\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "prompt = \"A cat astronaut, digital art, detailed\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"cat_astronaut.png\")\n",
    "```\n",
    "\n",
    "**Automatic1111 WebUI:**\n",
    "- Most popular SD interface\n",
    "- Web-based GUI\n",
    "- Extensions: ControlNet, LoRA, etc.\n",
    "- Free and open source\n",
    "\n",
    "**ComfyUI:**\n",
    "- Node-based interface\n",
    "- More control than A1111\n",
    "- Steeper learning curve\n",
    "- Powerful workflows\n",
    "\n",
    "### ğŸ¯ Common Workflows:\n",
    "\n",
    "**Text-to-Image:**\n",
    "```\n",
    "Text Prompt â†’ Model â†’ Image\n",
    "```\n",
    "\n",
    "**Image-to-Image:**\n",
    "```\n",
    "Image + Text â†’ Model â†’ Modified Image\n",
    "(Example: Sketch â†’ Photorealistic)\n",
    "```\n",
    "\n",
    "**Inpainting:**\n",
    "```\n",
    "Image + Mask + Text â†’ Model â†’ Edited Image\n",
    "(Example: Replace background)\n",
    "```\n",
    "\n",
    "**Outpainting:**\n",
    "```\n",
    "Image + Text â†’ Model â†’ Extended Image\n",
    "(Example: Expand beyond borders)\n",
    "```\n",
    "\n",
    "**ControlNet:**\n",
    "```\n",
    "Control Map + Text â†’ Model â†’ Guided Image\n",
    "(Example: Pose â†’ Character in that pose)\n",
    "```\n",
    "\n",
    "### ğŸŒŸ Advanced Features:\n",
    "\n",
    "**LoRA (Low-Rank Adaptation):**\n",
    "- Small model tweaks (10-200MB)\n",
    "- Add styles, characters, concepts\n",
    "- Thousands available on Civitai\n",
    "- Easy to use and share\n",
    "\n",
    "**Textual Inversion:**\n",
    "- Teach new concepts\n",
    "- \"Your face\" or specific object\n",
    "- Small embeddings (10-100KB)\n",
    "\n",
    "**DreamBooth:**\n",
    "- Full model fine-tuning\n",
    "- More powerful than LoRA\n",
    "- Requires more compute\n",
    "- Best for specific subjects\n",
    "\n",
    "Let's see how to use these models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "using-pretrained",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guide to Using Pre-trained Models\n",
    "\n",
    "def pretrained_models_guide():\n",
    "    \"\"\"\n",
    "    Comprehensive guide to using pre-trained generative models\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¨ Using Pre-trained Generative Models (2024-2025)\\n\" + \"=\"*80)\n",
    "    \n",
    "    print(\"\\nğŸ“± Option 1: Web Interfaces (Easiest)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nğŸŒ DALL-E 3 (via ChatGPT):\")\n",
    "    print(\"   Cost: $20/month (ChatGPT Plus)\")\n",
    "    print(\"   Steps:\")\n",
    "    print(\"   1. Subscribe to ChatGPT Plus\")\n",
    "    print(\"   2. Ask: 'Generate an image of [your prompt]'\")\n",
    "    print(\"   3. Download the result\")\n",
    "    print(\"   Pros: Easiest, best prompt understanding\")\n",
    "    print(\"   Cons: Limited control, monthly cost\")\n",
    "    \n",
    "    print(\"\\nğŸ­ Midjourney:\")\n",
    "    print(\"   Cost: $10/month (basic)\")\n",
    "    print(\"   Steps:\")\n",
    "    print(\"   1. Join Midjourney Discord server\")\n",
    "    print(\"   2. Use /imagine command: /imagine prompt: your prompt here\")\n",
    "    print(\"   3. Upscale or create variations\")\n",
    "    print(\"   Pros: Artistic, community, high quality\")\n",
    "    print(\"   Cons: Discord-only, learning curve\")\n",
    "    \n",
    "    print(\"\\nğŸ’» Option 2: Local Installation (Most Control)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nğŸš€ Stable Diffusion (Automatic1111):\")\n",
    "    print(\"   Cost: FREE (need GPU)\")\n",
    "    print(\"   Requirements:\")\n",
    "    print(\"   - NVIDIA GPU with 6GB+ VRAM (RTX 3060 or better)\")\n",
    "    print(\"   - 20GB+ disk space\")\n",
    "    print(\"   - Python 3.10+\")\n",
    "    \n",
    "    print(\"\\n   Installation:\")\n",
    "    print(\"   ```bash\")\n",
    "    print(\"   # Clone repository\")\n",
    "    print(\"   git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git\")\n",
    "    print(\"   cd stable-diffusion-webui\")\n",
    "    print(\"   \")\n",
    "    print(\"   # Run (auto-installs dependencies)\")\n",
    "    print(\"   ./webui.sh  # Linux/Mac\")\n",
    "    print(\"   webui-user.bat  # Windows\")\n",
    "    print(\"   \")\n",
    "    print(\"   # Open browser to http://localhost:7860\")\n",
    "    print(\"   ```\")\n",
    "    \n",
    "    print(\"\\nğŸ Option 3: Python Code (Programmatic)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nğŸ“¦ Using HuggingFace Diffusers:\")\n",
    "    print(\"   ```python\")\n",
    "    print(\"   # Install\")\n",
    "    print(\"   pip install diffusers transformers accelerate\")\n",
    "    print(\"   \")\n",
    "    print(\"   # Code\")\n",
    "    print(\"   from diffusers import StableDiffusionPipeline\")\n",
    "    print(\"   import torch\")\n",
    "    print(\"   \")\n",
    "    print(\"   # Load model (downloads ~5GB first time)\")\n",
    "    print(\"   pipe = StableDiffusionPipeline.from_pretrained(\")\n",
    "    print(\"       'stabilityai/stable-diffusion-2-1',\")\n",
    "    print(\"       torch_dtype=torch.float16\")\n",
    "    print(\"   ).to('cuda')\")\n",
    "    print(\"   \")\n",
    "    print(\"   # Generate\")\n",
    "    print(\"   prompt = 'A cat astronaut, digital art'\")\n",
    "    print(\"   negative = 'blurry, low quality'\")\n",
    "    print(\"   \")\n",
    "    print(\"   image = pipe(\")\n",
    "    print(\"       prompt=prompt,\")\n",
    "    print(\"       negative_prompt=negative,\")\n",
    "    print(\"       num_inference_steps=50,\")\n",
    "    print(\"       guidance_scale=7.5\")\n",
    "    print(\"   ).images[0]\")\n",
    "    print(\"   \")\n",
    "    print(\"   image.save('output.png')\")\n",
    "    print(\"   ```\")\n",
    "    \n",
    "    print(\"\\nâ˜ï¸ Option 4: Cloud Services (No GPU Needed)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nğŸ® RunPod / Vast.ai:\")\n",
    "    print(\"   - Rent GPU by the hour (~$0.20-0.50/hour)\")\n",
    "    print(\"   - Pre-configured templates\")\n",
    "    print(\"   - Access via browser\")\n",
    "    \n",
    "    print(\"\\nğŸ¤— HuggingFace Spaces:\")\n",
    "    print(\"   - Free tier available\")\n",
    "    print(\"   - Run models in browser\")\n",
    "    print(\"   - Many community models\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š Comparison Table:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Platform':<20} {'Cost':<15} {'Quality':<10} {'Control':<10} {'Speed':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'DALL-E 3':<20} {'$20/month':<15} {'â­â­â­â­â­':<10} {'â­â­':<10} {'â­â­â­â­':<10}\")\n",
    "    print(f\"{'Midjourney':<20} {'$10-60/mo':<15} {'â­â­â­â­â­':<10} {'â­â­â­':<10} {'â­â­â­â­':<10}\")\n",
    "    print(f\"{'Stable Diffusion':<20} {'FREE*':<15} {'â­â­â­â­':<10} {'â­â­â­â­â­':<10} {'â­â­â­':<10}\")\n",
    "    print(f\"{'Cloud GPU':<20} {'~$0.30/hr':<15} {'â­â­â­â­':<10} {'â­â­â­â­':<10} {'â­â­â­â­':<10}\")\n",
    "    print(\"\\n* Requires your own GPU\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Recommendations:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nğŸ‘¤ Beginners:\")\n",
    "    print(\"   â†’ Start with DALL-E 3 (via ChatGPT)\")\n",
    "    print(\"   â†’ Easy to use, great results\")\n",
    "    \n",
    "    print(\"\\nğŸ¨ Artists/Creatives:\")\n",
    "    print(\"   â†’ Midjourney for artistic style\")\n",
    "    print(\"   â†’ Huge community and inspiration\")\n",
    "    \n",
    "    print(\"\\nğŸ’» Developers/Enthusiasts:\")\n",
    "    print(\"   â†’ Stable Diffusion (local or cloud)\")\n",
    "    print(\"   â†’ Full control, customization, free\")\n",
    "    \n",
    "    print(\"\\nğŸ¢ Professionals:\")\n",
    "    print(\"   â†’ Adobe Firefly (commercial safe)\")\n",
    "    print(\"   â†’ Integrated in professional tools\")\n",
    "\n",
    "pretrained_models_guide()\n",
    "\n",
    "print(\"\\nğŸš€ Next Steps:\")\n",
    "print(\"   1. Choose a platform based on your needs\")\n",
    "print(\"   2. Practice prompt engineering\")\n",
    "print(\"   3. Join communities (r/StableDiffusion, Discord servers)\")\n",
    "print(\"   4. Experiment and iterate!\")\n",
    "print(\"\\nğŸ’¼ This is a VALUABLE skill in 2024-2025!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## ğŸ¯ Interactive Exercises\n",
    "\n",
    "Test your understanding of advanced generative models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### Exercise 1: Prompt Engineering Challenge\n",
    "\n",
    "**Task:** Improve these basic prompts\n",
    "\n",
    "**Given these basic prompts, rewrite them to be highly effective:**\n",
    "\n",
    "1. \"a house\"\n",
    "2. \"sunset\"\n",
    "3. \"person\"\n",
    "4. \"car\"\n",
    "5. \"food\"\n",
    "\n",
    "**For each:**\n",
    "- Add specific details\n",
    "- Choose an artistic style\n",
    "- Add quality modifiers\n",
    "- Include lighting/mood\n",
    "- Write negative prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR IMPROVED PROMPTS HERE\n",
    "\n",
    "improved_prompts = {\n",
    "    \"house\": {\n",
    "        \"positive\": \"# Your improved prompt\",\n",
    "        \"negative\": \"# Your negative prompt\"\n",
    "    },\n",
    "    # Add others...\n",
    "}\n",
    "\n",
    "# Print your prompts\n",
    "for subject, prompts in improved_prompts.items():\n",
    "    print(f\"\\n{subject.upper()}:\")\n",
    "    print(f\"Positive: {prompts['positive']}\")\n",
    "    print(f\"Negative: {prompts['negative']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ“– Click here for example solutions</summary>\n",
    "\n",
    "```python\n",
    "improved_prompts = {\n",
    "    \"house\": {\n",
    "        \"positive\": \"Cozy wooden cabin in snowy pine forest, smoke from chimney, warm glowing windows, golden hour lighting, winter atmosphere, professional photography, 8k, highly detailed, trending on ArtStation\",\n",
    "        \"negative\": \"modern, urban, blurry, low quality, summer, watermark\"\n",
    "    },\n",
    "    \"sunset\": {\n",
    "        \"positive\": \"Dramatic ocean sunset with vibrant orange and purple clouds, silhouette of sailboat, long exposure water, professional landscape photography, HDR, 8k resolution, award winning\",\n",
    "        \"negative\": \"noon, overcast, blurry, people, buildings, low quality\"\n",
    "    },\n",
    "    \"person\": {\n",
    "        \"positive\": \"Portrait of elegant woman in 1920s Art Deco style, pearl necklace, vintage fashion, by Alphonse Mucha, soft lighting, detailed facial features, oil painting, masterpiece\",\n",
    "        \"negative\": \"modern clothes, smartphone, distorted face, bad anatomy, blurry\"\n",
    "    },\n",
    "    \"car\": {\n",
    "        \"positive\": \"Sleek futuristic sports car, cyberpunk city background, neon reflections on wet pavement, night scene, cinematic lighting, concept art by Simon StÃ¥lenhag, 8k, highly detailed\",\n",
    "        \"negative\": \"old car, daytime, rusty, low quality, blurry\"\n",
    "    },\n",
    "    \"food\": {\n",
    "        \"positive\": \"Gourmet chocolate lava cake with vanilla ice cream, fresh berries, mint garnish, elegant plating, professional food photography, soft studio lighting, shallow depth of field, 4k\",\n",
    "        \"negative\": \"messy, unappetizing, low quality, dark, blurry\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Improvements:**\n",
    "- Specific subjects instead of generic\n",
    "- Added context and atmosphere\n",
    "- Referenced styles/artists\n",
    "- Included lighting and mood\n",
    "- Quality modifiers (8k, detailed, etc.)\n",
    "- Comprehensive negative prompts\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### Exercise 2: Understanding Diffusion vs GANs\n",
    "\n",
    "**Question:** For each scenario, recommend Diffusion or GAN and explain why:\n",
    "\n",
    "1. Text-to-image generation from user descriptions\n",
    "2. Real-time face generation for video game (need speed)\n",
    "3. High-quality photorealistic faces for stock photography\n",
    "4. Image editing with text guidance (inpainting)\n",
    "5. Training with limited data (100 images)\n",
    "6. Generating diverse variations of product designs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ“– Click here for solution</summary>\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "**1. Text-to-image â†’ Diffusion**\n",
    "- Why: Diffusion models excel at text conditioning\n",
    "- Cross-attention naturally integrates text\n",
    "- Better prompt following than GANs\n",
    "- Examples: DALL-E, Stable Diffusion, Midjourney\n",
    "\n",
    "**2. Real-time face generation â†’ GAN**\n",
    "- Why: GANs generate in single forward pass (fast!)\n",
    "- Diffusion requires 20-50 steps (slower)\n",
    "- StyleGAN can generate faces in milliseconds\n",
    "- Ideal for real-time applications\n",
    "\n",
    "**3. Photorealistic faces â†’ Either (slight edge to GAN)**\n",
    "- StyleGAN2/3: Proven track record (This Person Does Not Exist)\n",
    "- Diffusion: Also capable, more stable training\n",
    "- Decision factors: Speed needs, editing requirements\n",
    "- 2024 trend: Diffusion gaining ground\n",
    "\n",
    "**4. Image editing with text â†’ Diffusion**\n",
    "- Why: Inpainting naturally fits diffusion process\n",
    "- Text conditioning is easier\n",
    "- Examples: Stable Diffusion inpainting, DALL-E editing\n",
    "- GANs can do it but less elegant\n",
    "\n",
    "**5. Limited data (100 images) â†’ GAN**\n",
    "- Why: GANs can work with smaller datasets\n",
    "- Diffusion typically needs more data\n",
    "- Consider: StyleGAN with transfer learning\n",
    "- Or: DreamBooth (diffusion fine-tuning, but needs more data)\n",
    "\n",
    "**6. Diverse product designs â†’ Diffusion**\n",
    "- Why: Less mode collapse than GANs\n",
    "- Better diversity naturally\n",
    "- Can easily condition on variations\n",
    "- GANs might generate similar designs\n",
    "\n",
    "**General Rule (2024-2025):**\n",
    "- **Text-to-image** â†’ Diffusion (clear winner)\n",
    "- **Speed-critical** â†’ GAN (single pass vs multi-step)\n",
    "- **Faces** â†’ StyleGAN (proven) or Diffusion (gaining)\n",
    "- **Diversity** â†’ Diffusion (less mode collapse)\n",
    "- **Training stability** â†’ Diffusion (easier to train)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3",
   "metadata": {},
   "source": [
    "### Exercise 3: Plan a Generative AI Project\n",
    "\n",
    "**Task:** Design a complete generative AI project\n",
    "\n",
    "**Choose ONE application:**\n",
    "1. AI-powered t-shirt design tool\n",
    "2. Personalized children's book illustrator\n",
    "3. Interior design visualization from descriptions\n",
    "4. Logo generator for startups\n",
    "5. Social media content creator\n",
    "\n",
    "**For your chosen project, specify:**\n",
    "- Which model/platform to use (DALL-E, SD, Midjourney, custom)\n",
    "- How users will input their requirements\n",
    "- Prompt template structure\n",
    "- Quality control measures\n",
    "- Potential challenges and solutions\n",
    "- Estimated costs and timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PROJECT PLAN HERE\n",
    "\n",
    "project_plan = {\n",
    "    \"application\": \"# Your chosen application\",\n",
    "    \"model\": \"# Which model/platform\",\n",
    "    \"user_input\": \"# How users provide requirements\",\n",
    "    \"prompt_template\": \"# Your prompt structure\",\n",
    "    \"quality_control\": \"# How to ensure quality\",\n",
    "    \"challenges\": \"# Potential issues\",\n",
    "    \"solutions\": \"# How to address them\",\n",
    "    \"costs\": \"# Estimated costs\",\n",
    "    \"timeline\": \"# Development timeline\"\n",
    "}\n",
    "\n",
    "# Print your plan\n",
    "for key, value in project_plan.items():\n",
    "    print(f\"\\n{key.upper()}:\")\n",
    "    print(f\"  {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "**You just learned:**\n",
    "\n",
    "### 1. **StyleGAN**\n",
    "   - âœ… Style-based generation with disentangled control\n",
    "   - âœ… Multi-scale style injection (coarse to fine)\n",
    "   - âœ… Photorealistic face generation\n",
    "   - âœ… Foundation for This Person Does Not Exist\n",
    "   - **Innovation:** Separate control of different features\n",
    "\n",
    "### 2. **Diffusion Models**\n",
    "   - âœ… Iterative denoising process (1000 steps)\n",
    "   - âœ… More stable training than GANs\n",
    "   - âœ… No mode collapse\n",
    "   - âœ… Natural text conditioning\n",
    "   - **Powers:** DALL-E, Stable Diffusion, Midjourney, Sora\n",
    "\n",
    "### 3. **Text-to-Image**\n",
    "   - âœ… CLIP for text-image alignment\n",
    "   - âœ… Cross-attention mechanism\n",
    "   - âœ… Classifier-free guidance\n",
    "   - âœ… Latent diffusion for efficiency\n",
    "   - **Revolution:** Type words â†’ Get images\n",
    "\n",
    "### 4. **Prompt Engineering**\n",
    "   - âœ… Structure: Subject + Details + Style + Quality\n",
    "   - âœ… Power words and modifiers\n",
    "   - âœ… Negative prompts for quality\n",
    "   - âœ… Iterative refinement process\n",
    "   - **Skill:** Now a $200k+ job!\n",
    "\n",
    "### 5. **Using Pre-trained Models**\n",
    "   - âœ… DALL-E 3: Best prompt following\n",
    "   - âœ… Midjourney: Artistic quality\n",
    "   - âœ… Stable Diffusion: Open source, customizable\n",
    "   - âœ… Adobe Firefly: Commercial safety\n",
    "   - **Access:** Multiple platforms for different needs\n",
    "\n",
    "### ğŸŒŸ The Generative AI Revolution (2022-2025):\n",
    "\n",
    "**Impact:**\n",
    "- ğŸ“ˆ **Market:** Billion-dollar industry\n",
    "- ğŸ‘¥ **Users:** 100M+ monthly active users\n",
    "- ğŸ’¼ **Jobs:** New career: Prompt Engineer\n",
    "- ğŸ¨ **Creators:** Artists, designers democratized\n",
    "- ğŸ¢ **Business:** Adobe, OpenAI, Stability AI, Midjourney\n",
    "\n",
    "**Timeline:**\n",
    "```\n",
    "2014: GANs invented\n",
    "2018: StyleGAN (photorealistic faces)\n",
    "2020: DALL-E 1 (proof of concept)\n",
    "2021: CLIP (text-image connection)\n",
    "2022: DALL-E 2, Midjourney, Stable Diffusion\n",
    "      â†’ TEXT-TO-IMAGE REVOLUTION!\n",
    "2023: DALL-E 3, SDXL, Midjourney v6\n",
    "2024: Sora (text-to-video), advanced models\n",
    "2025: What's next? ğŸš€\n",
    "```\n",
    "\n",
    "### ğŸ“Š Model Comparison (2024-2025):\n",
    "\n",
    "| Model | Quality | Speed | Cost | Control | Best For |\n",
    "|-------|---------|-------|------|---------|----------|\n",
    "| **DALL-E 3** | â­â­â­â­â­ | â­â­â­â­ | $$$ | â­â­ | Prompt following |\n",
    "| **Midjourney** | â­â­â­â­â­ | â­â­â­â­ | $$ | â­â­â­ | Artistic style |\n",
    "| **Stable Diffusion** | â­â­â­â­ | â­â­â­ | FREE* | â­â­â­â­â­ | Customization |\n",
    "| **StyleGAN** | â­â­â­â­â­ | â­â­â­â­â­ | FREE | â­â­â­â­ | Faces |\n",
    "\n",
    "### ğŸ’¡ Future Trends:\n",
    "\n",
    "**Coming Soon:**\n",
    "- ğŸ¬ **Text-to-video:** Sora and competitors\n",
    "- ğŸµ **Music generation:** AI composers\n",
    "- ğŸ® **Game assets:** Automated content creation\n",
    "- ğŸ—ï¸ **3D generation:** Text to 3D models\n",
    "- ğŸ­ **Real-time:** Live generative art\n",
    "- ğŸ§  **Personalization:** AI trained on your style\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations!** You now understand:\n",
    "- The cutting edge of generative AI (2024-2025)\n",
    "- How DALL-E, Midjourney, and Stable Diffusion work\n",
    "- Prompt engineering (a $200k+ skill!)\n",
    "- How to use pre-trained models\n",
    "- The future of creative AI\n",
    "\n",
    "**You're ready to create with AI!** ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## ğŸš€ Next Steps\n",
    "\n",
    "**Immediate Actions:**\n",
    "1. **Try a platform:** Sign up for DALL-E 3, Midjourney, or install Stable Diffusion\n",
    "2. **Practice prompts:** Generate 50+ images with different prompts\n",
    "3. **Join communities:**\n",
    "   - r/StableDiffusion\n",
    "   - r/midjourney\n",
    "   - Discord servers (Midjourney, Stable Diffusion)\n",
    "4. **Study successful prompts:** Lexica.art, PromptHero, Civitai\n",
    "5. **Build a project:** Create something real!\n",
    "\n",
    "**Learning Resources:**\n",
    "- ğŸ“š \"High-Resolution Image Synthesis with Latent Diffusion Models\" (Stable Diffusion paper)\n",
    "- ğŸ¥ Two Minute Papers (YouTube - latest AI breakthroughs)\n",
    "- ğŸ“– Stable Diffusion Guide (stability.ai documentation)\n",
    "- ğŸŒ HuggingFace Course on Diffusion Models\n",
    "- ğŸ’¬ Community tutorials and guides\n",
    "\n",
    "**Projects to Build:**\n",
    "1. Personal art generator (your style)\n",
    "2. Product mockup tool\n",
    "3. Social media content creator\n",
    "4. Book cover generator\n",
    "5. Interior design visualizer\n",
    "6. Character design tool\n",
    "7. Logo generator\n",
    "\n",
    "**Career Paths:**\n",
    "- ğŸ¨ **Prompt Engineer:** $200k-$400k/year\n",
    "- ğŸ’» **ML Engineer (Generative AI):** Build models\n",
    "- ğŸ­ **Creative Technologist:** AI + Art\n",
    "- ğŸ“Š **AI Product Manager:** Generative AI products\n",
    "- ğŸ¢ **Consultant:** Help companies adopt AI\n",
    "\n",
    "**Stay Updated:**\n",
    "- Follow: @StabilityAI, @OpenAI, @midjourney on Twitter\n",
    "- Subscribe: HuggingFace newsletter\n",
    "- Read: Papers With Code (generative models section)\n",
    "- Watch: AI conferences (NeurIPS, CVPR, ICCV)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸŒŸ What's Next in AI?**\n",
    "\n",
    "**2024-2025 Frontiers:**\n",
    "- ğŸ¬ **Text-to-video:** Sora, Runway, Pika\n",
    "- ğŸµ **Music AI:** Suno, Udio, MusicLM\n",
    "- ğŸ® **Game generation:** Procedural content\n",
    "- ğŸ—ï¸ **3D generation:** DreamFusion, Point-E\n",
    "- ğŸ¤– **Multimodal AI:** Understanding + Generation\n",
    "- ğŸ§  **Personalized AI:** Models trained on you\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: You're living through the creative AI revolution. The skills you learned today are shaping the future of content creation!* ğŸŒŸ\n",
    "\n",
    "**ğŸ¯ You now know how the most advanced AI systems create!**\n",
    "\n",
    "**Congratulations on completing Week 17: Generative AI & GANs! ğŸ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
