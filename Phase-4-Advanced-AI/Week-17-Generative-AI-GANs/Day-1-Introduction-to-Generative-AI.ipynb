{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üìò Day 1: Introduction to Generative AI\n",
    "\n",
    "**üéØ Goal:** Master the fundamentals of Generative AI and understand how machines create new content\n",
    "\n",
    "**‚è±Ô∏è Time:** 90-120 minutes\n",
    "\n",
    "**üåü Why This Matters for AI:**\n",
    "- Generative AI is behind DALL-E, Midjourney, Stable Diffusion, Sora, and ChatGPT\n",
    "- Powers the AI revolution of 2023-2025 (text-to-image, text-to-video)\n",
    "- Used for data augmentation, creative content, drug discovery, and design\n",
    "- Foundation for understanding modern AI systems that create (not just classify)\n",
    "- Critical skill for AI practitioners in 2024-2025\n",
    "- Autoencoders and VAEs are building blocks for more advanced models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generative-vs-discriminative",
   "metadata": {},
   "source": [
    "## ü§î What is Generative AI?\n",
    "\n",
    "**Generative AI = Models that CREATE new data**\n",
    "\n",
    "### Discriminative vs Generative Models\n",
    "\n",
    "**Discriminative Models (What you've learned so far):**\n",
    "- **Task:** Classify or predict labels\n",
    "- **Question:** \"Is this a cat or dog?\"\n",
    "- **Output:** Category/Label (Cat = 0.9, Dog = 0.1)\n",
    "- **Examples:** Image classification, spam detection, sentiment analysis\n",
    "- **Models:** CNN, RNN, Random Forest, SVM\n",
    "\n",
    "**Generative Models (This week!):**\n",
    "- **Task:** Create new data samples\n",
    "- **Question:** \"Generate a picture of a cat\"\n",
    "- **Output:** New image, text, music, video\n",
    "- **Examples:** DALL-E, Midjourney, ChatGPT, Sora\n",
    "- **Models:** GANs, VAEs, Diffusion Models, Transformers\n",
    "\n",
    "### üéØ Key Difference:\n",
    "\n",
    "| Aspect | Discriminative | Generative |\n",
    "|--------|---------------|------------|\n",
    "| **Goal** | Classify existing data | Create new data |\n",
    "| **Learns** | Decision boundary | Data distribution |\n",
    "| **Output** | Labels/Predictions | New samples |\n",
    "| **Example** | \"This is a cat\" | \"Here's a new cat image\" |\n",
    "| **Math** | P(y\\|x) - probability of label given data | P(x) - probability of data itself |\n",
    "\n",
    "### üåü Real-World Examples (2024-2025):\n",
    "\n",
    "**Discriminative AI:**\n",
    "- üîç Google image search: \"Is this a cat?\"\n",
    "- üìß Gmail spam filter: \"Is this spam?\"\n",
    "- ü©∫ Medical diagnosis: \"Is this cancer?\"\n",
    "\n",
    "**Generative AI:**\n",
    "- üé® **DALL-E 3:** \"Create an image of a cyberpunk cat\"\n",
    "- üé¨ **Sora:** \"Generate a video of waves crashing\"\n",
    "- ‚úçÔ∏è **ChatGPT:** \"Write a poem about AI\"\n",
    "- üéµ **Suno:** \"Compose a jazz melody\"\n",
    "- üß¨ **AlphaFold:** \"Predict protein structures\"\n",
    "\n",
    "Let's build generative models from scratch! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Make plots beautiful\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"Let's create new data with Generative AI! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "autoencoders-intro",
   "metadata": {},
   "source": [
    "## üîÑ Autoencoders Explained\n",
    "\n",
    "**Autoencoder = Compress then Reconstruct**\n",
    "\n",
    "### The Concept:\n",
    "\n",
    "**Human Analogy:**\n",
    "Imagine describing a photo to someone:\n",
    "1. **Encoder:** You compress the image into words: \"A red car on a beach at sunset\"\n",
    "2. **Latent Space:** The compressed description (just 7 words instead of millions of pixels)\n",
    "3. **Decoder:** The listener reconstructs the image in their mind\n",
    "\n",
    "**In AI:**\n",
    "```\n",
    "Input Image (784 pixels)\n",
    "     ‚Üì\n",
    "  ENCODER (compresses)\n",
    "     ‚Üì\n",
    "Latent Space (32 numbers) ‚Üê Compressed representation!\n",
    "     ‚Üì\n",
    "  DECODER (reconstructs)\n",
    "     ‚Üì\n",
    "Output Image (784 pixels)\n",
    "```\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "**Encoder (Compression):**\n",
    "- Input: Original image (28√ó28 = 784 pixels)\n",
    "- Layers: Gradually reduce dimensions\n",
    "- Output: Latent vector (e.g., 32 numbers)\n",
    "- **Learns:** Important features that capture the essence\n",
    "\n",
    "**Decoder (Reconstruction):**\n",
    "- Input: Latent vector (32 numbers)\n",
    "- Layers: Gradually increase dimensions\n",
    "- Output: Reconstructed image (28√ó28 = 784 pixels)\n",
    "- **Learns:** How to recreate the image from compressed form\n",
    "\n",
    "### üéØ Why Autoencoders?\n",
    "\n",
    "**Applications:**\n",
    "1. **Dimensionality Reduction:** Compress data (like PCA but better)\n",
    "2. **Denoising:** Remove noise from images/audio\n",
    "3. **Anomaly Detection:** Find unusual patterns\n",
    "4. **Feature Learning:** Extract meaningful representations\n",
    "5. **Generation:** Sample from latent space to create new data!\n",
    "\n",
    "### üåü Real-World Uses (2024-2025):\n",
    "\n",
    "- **Image Compression:** JPEG uses autoencoder-like principles\n",
    "- **Recommendation Systems:** Netflix, Spotify (collaborative filtering)\n",
    "- **Medical Imaging:** Denoise MRI scans, detect anomalies\n",
    "- **Fraud Detection:** Bank transactions (anomaly detection)\n",
    "- **Foundation for GANs and VAEs:** Building blocks for DALL-E, Stable Diffusion\n",
    "\n",
    "Let's build one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-autoencoder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Autoencoder for MNIST Digits\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, latent_dim=32):\n",
    "        \"\"\"\n",
    "        Simple Autoencoder\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input size (28*28 = 784 for MNIST)\n",
    "            latent_dim: Compressed representation size\n",
    "        \"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # ENCODER: 784 ‚Üí 128 ‚Üí 64 ‚Üí 32\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # DECODER: 32 ‚Üí 64 ‚Üí 128 ‚Üí 784\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  # Output between 0 and 1 (pixel values)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: Encode then Decode\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        latent = self.encoder(x)\n",
    "        \n",
    "        # Decode\n",
    "        reconstructed = self.decoder(latent)\n",
    "        \n",
    "        return reconstructed, latent\n",
    "\n",
    "# Create model\n",
    "autoencoder = Autoencoder(input_dim=784, latent_dim=32).to(device)\n",
    "\n",
    "print(\"‚úÖ Autoencoder Created!\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(autoencoder)\n",
    "print(f\"\\nüí° Compression Ratio: 784 ‚Üí 32 (24.5x compression!)\")\n",
    "print(f\"   Like compressing a 784KB file to 32KB!\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in autoencoder.parameters())\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-mnist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Download and load training data\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"‚úÖ MNIST Dataset Loaded!\")\n",
    "print(f\"\\nTraining samples: {len(train_dataset):,}\")\n",
    "print(f\"Test samples: {len(test_dataset):,}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Visualize some examples\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.suptitle('üìä Sample MNIST Digits', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(16):\n",
    "    ax = axes[i // 8, i % 8]\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {label}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° These are the digits our autoencoder will learn to compress and reconstruct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-autoencoder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Autoencoder\n",
    "\n",
    "def train_autoencoder(model, train_loader, epochs=5):\n",
    "    \"\"\"\n",
    "    Train autoencoder to reconstruct images\n",
    "    \"\"\"\n",
    "    # Loss function: How different is reconstruction from original?\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, _) in enumerate(train_loader):\n",
    "            # Flatten images: (batch, 1, 28, 28) ‚Üí (batch, 784)\n",
    "            images = images.view(-1, 784).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed, _ = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(reconstructed, images)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Average loss for epoch\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "print(\"üöÄ Training Autoencoder...\")\n",
    "print(\"Goal: Learn to compress and reconstruct MNIST digits\\n\")\n",
    "\n",
    "losses = train_autoencoder(autoencoder, train_loader, epochs=5)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, marker='o', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Reconstruction Loss', fontsize=12)\n",
    "plt.title('üìâ Autoencoder Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\")\n",
    "print(\"üí° Lower loss = Better reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Reconstruction Results\n",
    "\n",
    "def visualize_reconstruction(model, test_loader, n_samples=10):\n",
    "    \"\"\"\n",
    "    Show original vs reconstructed images\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images[:n_samples]\n",
    "    labels = labels[:n_samples]\n",
    "    \n",
    "    # Flatten and reconstruct\n",
    "    images_flat = images.view(-1, 784).to(device)\n",
    "    with torch.no_grad():\n",
    "        reconstructed, latent = model(images_flat)\n",
    "    \n",
    "    # Reshape for visualization\n",
    "    images = images.cpu().numpy()\n",
    "    reconstructed = reconstructed.view(-1, 1, 28, 28).cpu().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, n_samples, figsize=(20, 4))\n",
    "    fig.suptitle('üé® Autoencoder: Original vs Reconstructed', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Original\n",
    "        axes[0, i].imshow(images[i].squeeze(), cmap='gray')\n",
    "        axes[0, i].set_title(f'Original\\n(Label: {labels[i]})', fontsize=10)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Reconstructed\n",
    "        axes[1, i].imshow(reconstructed[i].squeeze(), cmap='gray')\n",
    "        axes[1, i].set_title('Reconstructed', fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print latent space info\n",
    "    print(f\"\\nüí° Latent Space Analysis:\")\n",
    "    print(f\"   Original: {28*28} pixels\")\n",
    "    print(f\"   Compressed: {latent.shape[1]} numbers\")\n",
    "    print(f\"   Compression: {(28*28/latent.shape[1]):.1f}x\")\n",
    "    print(f\"\\n   Latent vector for first image: {latent[0][:8].cpu().numpy()}...\")\n",
    "\n",
    "visualize_reconstruction(autoencoder, test_loader, n_samples=10)\n",
    "\n",
    "print(\"\\nüéØ Key Observations:\")\n",
    "print(\"  - Reconstructions look very similar to originals!\")\n",
    "print(\"  - We compressed 784 numbers ‚Üí 32 numbers ‚Üí 784 numbers\")\n",
    "print(\"  - The 32 numbers capture the 'essence' of the digit\")\n",
    "print(\"  - This is the foundation for generative models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latent-space",
   "metadata": {},
   "source": [
    "## üåå Understanding Latent Space\n",
    "\n",
    "**Latent Space = The compressed representation space**\n",
    "\n",
    "### What is Latent Space?\n",
    "\n",
    "**Concept:**\n",
    "- The \"hidden\" representation learned by the encoder\n",
    "- Low-dimensional space that captures key features\n",
    "- Like a \"coordinate system\" for all possible images\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Image of \"5\" ‚Üí Encoder ‚Üí [0.2, -0.5, 0.8, ...] ‚Üê Latent vector (32 numbers)\n",
    "                              ‚Üì\n",
    "                    These 32 numbers encode:\n",
    "                    - Curvature of the digit\n",
    "                    - Thickness of lines\n",
    "                    - Orientation\n",
    "                    - Style\n",
    "```\n",
    "\n",
    "### üéØ Why Latent Space Matters:\n",
    "\n",
    "**1. Dimensionality Reduction:**\n",
    "- 784 pixels ‚Üí 32 numbers (much easier to work with!)\n",
    "- Removes redundancy (neighboring pixels are correlated)\n",
    "\n",
    "**2. Feature Learning:**\n",
    "- Automatically discovers important features\n",
    "- No manual feature engineering needed!\n",
    "\n",
    "**3. Generation (The Key!):**\n",
    "- Sample random points in latent space\n",
    "- Decode them ‚Üí New images!\n",
    "- This is how we \"generate\" new data\n",
    "\n",
    "**4. Interpolation:**\n",
    "- Smoothly transition between images\n",
    "- Morph a \"3\" into a \"5\"\n",
    "- Used in DeepFakes, style transfer\n",
    "\n",
    "### üåü In Modern AI (2024-2025):\n",
    "\n",
    "**DALL-E / Stable Diffusion:**\n",
    "- Latent space encodes \"concepts\" (not just pixels)\n",
    "- \"Cat\" + \"Astronaut\" = points in latent space\n",
    "- Decoder generates \"astronaut cat\" image\n",
    "\n",
    "**ChatGPT:**\n",
    "- Word embeddings are latent representations\n",
    "- \"King\" - \"Man\" + \"Woman\" = \"Queen\" (vector arithmetic in latent space!)\n",
    "\n",
    "Let's explore latent space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-latent-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Latent Space (2D projection)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_latent_space(model, test_loader, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Visualize latent space using t-SNE\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    latent_vectors = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Collect latent vectors\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 784).to(device)\n",
    "            _, latent = model(images)\n",
    "            latent_vectors.append(latent.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "            \n",
    "            if len(latent_vectors) * batch_size >= n_samples:\n",
    "                break\n",
    "    \n",
    "    # Concatenate\n",
    "    latent_vectors = np.concatenate(latent_vectors)[:n_samples]\n",
    "    labels_list = np.concatenate(labels_list)[:n_samples]\n",
    "    \n",
    "    # Reduce to 2D using t-SNE\n",
    "    print(\"üîÑ Computing t-SNE (may take a moment...)\")\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    latent_2d = tsne.fit_transform(latent_vectors)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], \n",
    "                         c=labels_list, cmap='tab10', \n",
    "                         alpha=0.6, s=20)\n",
    "    plt.colorbar(scatter, label='Digit')\n",
    "    plt.xlabel('Latent Dimension 1', fontsize=12)\n",
    "    plt.ylabel('Latent Dimension 2', fontsize=12)\n",
    "    plt.title('üåå Latent Space Visualization (32D ‚Üí 2D)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Observations:\")\n",
    "    print(\"  - Similar digits cluster together (same color)\")\n",
    "    print(\"  - The model learned meaningful representations!\")\n",
    "    print(\"  - Each cluster = one digit in latent space\")\n",
    "    print(\"  - Points between clusters = 'in-between' digits\")\n",
    "\n",
    "visualize_latent_space(autoencoder, test_loader, n_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vae-intro",
   "metadata": {},
   "source": [
    "## üé≤ Variational Autoencoders (VAEs)\n",
    "\n",
    "**VAE = Autoencoder + Probability + Generation**\n",
    "\n",
    "### Problem with Regular Autoencoders:\n",
    "\n",
    "**Issue: Latent space is NOT continuous**\n",
    "- Regular autoencoder: Latent vectors are scattered\n",
    "- Random sampling from latent space ‚Üí garbage output\n",
    "- Example: Sample random [0.3, -0.2, 0.7, ...] ‚Üí doesn't decode to a valid digit\n",
    "\n",
    "**Why?**\n",
    "- Only trained on REAL data points\n",
    "- Gaps in latent space have no meaning\n",
    "- Can't generate NEW samples reliably\n",
    "\n",
    "### Solution: Variational Autoencoder (VAE)\n",
    "\n",
    "**Key Idea: Force latent space to be continuous and smooth**\n",
    "\n",
    "**How?**\n",
    "1. **Encoder outputs distribution** (not just a point)\n",
    "   - Regular: Encoder ‚Üí single vector\n",
    "   - VAE: Encoder ‚Üí mean (Œº) and variance (œÉ¬≤)\n",
    "\n",
    "2. **Sample from distribution**\n",
    "   - z = Œº + œÉ * Œµ (where Œµ ~ N(0,1))\n",
    "   - Adds randomness during training\n",
    "\n",
    "3. **Regularize latent space**\n",
    "   - Force distributions to be similar to N(0,1)\n",
    "   - Uses KL-Divergence loss\n",
    "   - Result: Smooth, continuous latent space!\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "```\n",
    "Input Image\n",
    "    ‚Üì\n",
    "  ENCODER\n",
    "    ‚Üì\n",
    "  Œº (mean) and œÉ¬≤ (variance)\n",
    "    ‚Üì\n",
    "z = Œº + œÉ * Œµ  ‚Üê Sampling!\n",
    "    ‚Üì\n",
    "  DECODER\n",
    "    ‚Üì\n",
    "Reconstructed Image\n",
    "```\n",
    "\n",
    "### Loss Function:\n",
    "\n",
    "**Total Loss = Reconstruction Loss + KL Divergence**\n",
    "\n",
    "1. **Reconstruction Loss:** How well can we reconstruct the input?\n",
    "   - Same as regular autoencoder (MSE)\n",
    "\n",
    "2. **KL Divergence:** How different is our distribution from standard normal?\n",
    "   - Regularizes latent space\n",
    "   - Prevents overfitting to specific points\n",
    "\n",
    "### üéØ Why VAEs Matter:\n",
    "\n",
    "**Generation:**\n",
    "- Sample z ~ N(0,1)\n",
    "- Decode(z) ‚Üí NEW image!\n",
    "- Smooth latent space ‚Üí realistic outputs\n",
    "\n",
    "**Interpolation:**\n",
    "- Smooth transitions between images\n",
    "- Morph one face into another\n",
    "\n",
    "### üåü Modern Applications (2024-2025):\n",
    "\n",
    "- **Stable Diffusion:** Uses VAE for image compression before diffusion\n",
    "- **Music Generation:** Generate new melodies (MusicVAE)\n",
    "- **Drug Discovery:** Generate molecular structures\n",
    "- **Image Editing:** Smooth interpolation for transitions\n",
    "- **Anomaly Detection:** Identify outliers in latent space\n",
    "\n",
    "Let's implement a VAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vae-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Autoencoder Implementation\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, latent_dim=32):\n",
    "        \"\"\"\n",
    "        Variational Autoencoder\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # ENCODER\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Latent space parameters\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)      # Mean\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)  # Log variance\n",
    "        \n",
    "        # DECODER\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input to latent distribution parameters\n",
    "        \"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = Œº + œÉ * Œµ\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)  # Standard deviation\n",
    "        eps = torch.randn_like(std)    # Random noise from N(0,1)\n",
    "        z = mu + std * eps              # Sample from N(Œº, œÉ¬≤)\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent vector to reconstruction\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full forward pass\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        mu, logvar = self.encode(x)\n",
    "        \n",
    "        # Sample latent vector\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Decode\n",
    "        reconstructed = self.decode(z)\n",
    "        \n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "# Create VAE\n",
    "vae = VAE(input_dim=784, latent_dim=32).to(device)\n",
    "\n",
    "print(\"‚úÖ Variational Autoencoder Created!\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(vae)\n",
    "print(f\"\\nüí° Key Difference from Regular Autoencoder:\")\n",
    "print(\"   - Encoder outputs Œº (mean) and œÉ¬≤ (variance)\")\n",
    "print(\"   - Sampling step: z = Œº + œÉ * Œµ\")\n",
    "print(\"   - Enables generation from random sampling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vae-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Loss Function\n",
    "\n",
    "def vae_loss(reconstructed, original, mu, logvar):\n",
    "    \"\"\"\n",
    "    VAE loss = Reconstruction Loss + KL Divergence\n",
    "    \n",
    "    Args:\n",
    "        reconstructed: Decoder output\n",
    "        original: Original input\n",
    "        mu: Mean from encoder\n",
    "        logvar: Log variance from encoder\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (Binary Cross Entropy)\n",
    "    BCE = F.binary_cross_entropy(reconstructed, original, reduction='sum')\n",
    "    \n",
    "    # KL Divergence: KL(N(Œº,œÉ¬≤) || N(0,1))\n",
    "    # Formula: -0.5 * sum(1 + log(œÉ¬≤) - Œº¬≤ - œÉ¬≤)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "print(\"‚úÖ VAE Loss Function Defined\")\n",
    "print(\"\\nüìä Loss Components:\")\n",
    "print(\"  1. Reconstruction Loss (BCE):\")\n",
    "print(\"     - Measures how well we reconstruct the input\")\n",
    "print(\"     - Lower = better reconstruction\")\n",
    "print(\"\\n  2. KL Divergence (KLD):\")\n",
    "print(\"     - Measures distance from standard normal N(0,1)\")\n",
    "print(\"     - Regularizes latent space to be smooth\")\n",
    "print(\"     - Prevents overfitting\")\n",
    "print(\"\\n  Total Loss = BCE + KLD\")\n",
    "print(\"  (Balance between reconstruction and regularization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-vae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE\n",
    "\n",
    "def train_vae(model, train_loader, epochs=5):\n",
    "    \"\"\"\n",
    "    Train VAE\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    \n",
    "    losses = {'total': [], 'bce': [], 'kld': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_bce = 0\n",
    "        epoch_kld = 0\n",
    "        \n",
    "        for batch_idx, (images, _) in enumerate(train_loader):\n",
    "            images = images.view(-1, 784).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed, mu, logvar = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, bce, kld = vae_loss(reconstructed, images, mu, logvar)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_bce += bce.item()\n",
    "            epoch_kld += kld.item()\n",
    "        \n",
    "        # Average losses\n",
    "        avg_loss = epoch_loss / len(train_loader.dataset)\n",
    "        avg_bce = epoch_bce / len(train_loader.dataset)\n",
    "        avg_kld = epoch_kld / len(train_loader.dataset)\n",
    "        \n",
    "        losses['total'].append(avg_loss)\n",
    "        losses['bce'].append(avg_bce)\n",
    "        losses['kld'].append(avg_kld)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        print(f\"  Total: {avg_loss:.4f} | BCE: {avg_bce:.4f} | KLD: {avg_kld:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "print(\"üöÄ Training VAE...\")\n",
    "print(\"Goal: Learn smooth latent space for generation\\n\")\n",
    "\n",
    "vae_losses = train_vae(vae, train_loader, epochs=5)\n",
    "\n",
    "# Plot losses\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Total loss\n",
    "axes[0].plot(vae_losses['total'], marker='o', linewidth=2, color='purple')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('üìâ Total Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# BCE\n",
    "axes[1].plot(vae_losses['bce'], marker='s', linewidth=2, color='blue')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('üìâ Reconstruction Loss (BCE)', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# KLD\n",
    "axes[2].plot(vae_losses['kld'], marker='^', linewidth=2, color='red')\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('Loss', fontsize=12)\n",
    "axes[2].set_title('üìâ KL Divergence', fontsize=13, fontweight='bold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ VAE Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-ai-example",
   "metadata": {},
   "source": [
    "## üåü Real AI Example: Image Denoising and Reconstruction\n",
    "\n",
    "**Task:** Remove noise from corrupted images using VAE\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "**Medical Imaging (2024-2025):**\n",
    "- üè• MRI scans: Remove noise, enhance quality\n",
    "- ü©ª X-rays: Denoise for better diagnosis\n",
    "- üß† Brain scans: Clean up artifacts\n",
    "\n",
    "**Photography:**\n",
    "- üì∏ Low-light enhancement (Google Night Sight)\n",
    "- üåô Astrophotography: Remove sensor noise\n",
    "- üì± Smartphone cameras: Computational photography\n",
    "\n",
    "**Satellite Imagery:**\n",
    "- üõ∞Ô∏è Weather prediction: Clean up atmospheric interference\n",
    "- üåç Earth observation: Enhance resolution\n",
    "\n",
    "**Video Restoration:**\n",
    "- üé¨ Old film restoration (Disney uses this!)\n",
    "- üì∫ Upscaling SD to HD/4K\n",
    "\n",
    "Let's denoise images with our VAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "image-denoising",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Denoising with VAE\n",
    "\n",
    "def add_noise(images, noise_factor=0.5):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to images\n",
    "    \"\"\"\n",
    "    noisy = images + noise_factor * torch.randn_like(images)\n",
    "    noisy = torch.clamp(noisy, 0., 1.)  # Keep values in [0, 1]\n",
    "    return noisy\n",
    "\n",
    "def denoise_images(model, test_loader, n_samples=10):\n",
    "    \"\"\"\n",
    "    Demonstrate denoising with VAE\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get images\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images[:n_samples]\n",
    "    labels = labels[:n_samples]\n",
    "    \n",
    "    # Add noise\n",
    "    noisy_images = add_noise(images, noise_factor=0.5)\n",
    "    \n",
    "    # Denoise\n",
    "    noisy_flat = noisy_images.view(-1, 784).to(device)\n",
    "    with torch.no_grad():\n",
    "        denoised, _, _ = model(noisy_flat)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    original = images.cpu().numpy()\n",
    "    noisy = noisy_images.cpu().numpy()\n",
    "    denoised = denoised.view(-1, 1, 28, 28).cpu().numpy()\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(3, n_samples, figsize=(20, 6))\n",
    "    fig.suptitle('üé® VAE Image Denoising: Real AI Application', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Original\n",
    "        axes[0, i].imshow(original[i].squeeze(), cmap='gray')\n",
    "        axes[0, i].set_title(f'Original\\n(Label: {labels[i]})', fontsize=9)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Noisy\n",
    "        axes[1, i].imshow(noisy[i].squeeze(), cmap='gray')\n",
    "        axes[1, i].set_title('Noisy (50%)', fontsize=9)\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Denoised\n",
    "        axes[2, i].imshow(denoised[i].squeeze(), cmap='gray')\n",
    "        axes[2, i].set_title('Denoised (VAE)', fontsize=9)\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "denoise_images(vae, test_loader, n_samples=10)\n",
    "\n",
    "print(\"\\nüéØ Real-World Impact:\")\n",
    "print(\"\\nüì± Smartphone Photography:\")\n",
    "print(\"  - Google Pixel: Uses similar denoising for Night Sight\")\n",
    "print(\"  - iPhone: Computational photography with neural networks\")\n",
    "print(\"  - Result: Clear photos even in very low light\")\n",
    "print(\"\\nüè• Medical Imaging:\")\n",
    "print(\"  - MRI/CT scans: Reduce noise without losing detail\")\n",
    "print(\"  - Allows lower radiation doses (safer for patients!)\")\n",
    "print(\"  - Better diagnosis from clearer images\")\n",
    "print(\"\\nüé¨ Video Production:\")\n",
    "print(\"  - Film restoration: Clean up old footage\")\n",
    "print(\"  - Disney+: Enhanced classic movies using AI denoising\")\n",
    "print(\"  - YouTube: Real-time noise reduction for creators\")\n",
    "print(\"\\nüí° The same VAE principles power these real applications!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-new-images",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate NEW Images from VAE\n",
    "\n",
    "def generate_new_images(model, n_samples=16):\n",
    "    \"\"\"\n",
    "    Generate completely new images by sampling from N(0,1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Sample from standard normal distribution\n",
    "        z = torch.randn(n_samples, 32).to(device)\n",
    "        \n",
    "        # Decode to images\n",
    "        generated = model.decode(z)\n",
    "    \n",
    "    # Visualize\n",
    "    generated = generated.view(-1, 1, 28, 28).cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "    fig.suptitle('‚ú® Generated Images from Random Latent Vectors', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        ax = axes[i // 4, i % 4]\n",
    "        ax.imshow(generated[i].squeeze(), cmap='gray')\n",
    "        ax.set_title(f'Sample {i+1}', fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüé® What Just Happened?\")\n",
    "    print(\"  1. Sampled random vectors from N(0,1)\")\n",
    "    print(\"  2. Passed through decoder\")\n",
    "    print(\"  3. Generated COMPLETELY NEW digit-like images!\")\n",
    "    print(\"\\nüí° This is the CORE of generative AI:\")\n",
    "    print(\"  - Random noise ‚Üí Meaningful output\")\n",
    "    print(\"  - Same principle as DALL-E, Midjourney, Stable Diffusion!\")\n",
    "\n",
    "generate_new_images(vae, n_samples=16)\n",
    "\n",
    "print(\"\\nüåü From VAE to Modern AI (2024-2025):\")\n",
    "print(\"\\nüìä Evolution:\")\n",
    "print(\"  VAE (2013) ‚Üí GANs (2014) ‚Üí Diffusion Models (2020) ‚Üí DALL-E 3 (2024)\")\n",
    "print(\"\\nüé® Stable Diffusion:\")\n",
    "print(\"  - Uses VAE for image compression (512x512 ‚Üí 64x64 latent)\")\n",
    "print(\"  - Diffusion happens in latent space (faster!)\")\n",
    "print(\"  - VAE decoder: latent ‚Üí final image\")\n",
    "print(\"\\n‚úçÔ∏è ChatGPT/GPT-4:\")\n",
    "print(\"  - Text generation = sampling from latent space of language\")\n",
    "print(\"  - Transformer = advanced encoder-decoder\")\n",
    "print(\"  - Same generative principles!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercises\n",
    "\n",
    "Test your understanding of Generative AI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### Exercise 1: Discriminative vs Generative\n",
    "\n",
    "**Task:** Classify these AI systems as Discriminative or Generative\n",
    "\n",
    "1. Gmail spam filter\n",
    "2. DALL-E image generator\n",
    "3. Face recognition (iPhone Face ID)\n",
    "4. ChatGPT text generation\n",
    "5. Credit card fraud detection\n",
    "6. Midjourney art generator\n",
    "7. Netflix recommendation system\n",
    "8. Sora video generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWERS HERE\n",
    "# Example: {\"system\": \"type\"}\n",
    "\n",
    "answers = {\n",
    "    \"Gmail spam filter\": \"?\",\n",
    "    \"DALL-E\": \"?\",\n",
    "    \"Face ID\": \"?\",\n",
    "    \"ChatGPT\": \"?\",\n",
    "    \"Fraud detection\": \"?\",\n",
    "    \"Midjourney\": \"?\",\n",
    "    \"Netflix recommendations\": \"?\",\n",
    "    \"Sora\": \"?\",\n",
    "}\n",
    "\n",
    "# Uncomment to see solution\n",
    "# for system, answer in answers.items():\n",
    "#     print(f\"{system}: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "answers = {\n",
    "    \"Gmail spam filter\": \"Discriminative (classifies spam vs not spam)\",\n",
    "    \"DALL-E\": \"Generative (creates new images)\",\n",
    "    \"Face ID\": \"Discriminative (verifies identity)\",\n",
    "    \"ChatGPT\": \"Generative (generates text)\",\n",
    "    \"Fraud detection\": \"Discriminative (classifies fraud vs legitimate)\",\n",
    "    \"Midjourney\": \"Generative (creates art)\",\n",
    "    \"Netflix recommendations\": \"Discriminative (predicts what you'll like)\",\n",
    "    \"Sora\": \"Generative (creates videos)\",\n",
    "}\n",
    "```\n",
    "\n",
    "**Pattern:**\n",
    "- Classification/Detection ‚Üí Discriminative\n",
    "- Creation/Generation ‚Üí Generative\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### Exercise 2: Understanding Latent Space\n",
    "\n",
    "**Question:** Why is a smooth, continuous latent space important for VAEs?\n",
    "\n",
    "**Think about:**\n",
    "- What happens when you sample a random point?\n",
    "- Why do we need KL Divergence loss?\n",
    "- How does this enable generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for answer</summary>\n",
    "\n",
    "**Why Smooth Latent Space Matters:**\n",
    "\n",
    "1. **Generation Capability:**\n",
    "   - Smooth space: ANY random point decodes to valid output\n",
    "   - Gaps/holes: Random samples ‚Üí garbage\n",
    "   - Example: If latent space has gaps, sampling might give \"non-digit\" output\n",
    "\n",
    "2. **Interpolation:**\n",
    "   - Smooth transitions between points\n",
    "   - Morph digit \"3\" to \"5\" smoothly\n",
    "   - Used in face morphing, style transfer\n",
    "\n",
    "3. **KL Divergence Role:**\n",
    "   - Forces latent distributions to be similar to N(0,1)\n",
    "   - Prevents scattered, disconnected clusters\n",
    "   - Creates continuous manifold of valid samples\n",
    "\n",
    "4. **Analogy:**\n",
    "   - Regular autoencoder: Scattered islands in ocean\n",
    "   - VAE: Continuous landmass (can walk anywhere)\n",
    "\n",
    "**Real Impact:**\n",
    "- Stable Diffusion: Smooth latent space allows smooth image variations\n",
    "- Face generation: Realistic faces at ANY latent point\n",
    "- Text: GPT models learn smooth language manifold\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3",
   "metadata": {},
   "source": [
    "### Exercise 3: Modify the VAE\n",
    "\n",
    "**Task:** Experiment with different latent dimensions\n",
    "\n",
    "**Questions:**\n",
    "1. What happens with latent_dim = 2? (very compressed)\n",
    "2. What happens with latent_dim = 128? (less compressed)\n",
    "3. Which gives better reconstruction?\n",
    "4. Which is better for generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Try different latent dimensions\n",
    "\n",
    "# Example:\n",
    "# vae_small = VAE(input_dim=784, latent_dim=2).to(device)\n",
    "# train_vae(vae_small, train_loader, epochs=3)\n",
    "\n",
    "# Compare reconstruction quality\n",
    "# Visualize latent space (2D is easy to plot!)\n",
    "\n",
    "# Your experiments..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution-3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üìñ Click here for insights</summary>\n",
    "\n",
    "**Latent Dimension Trade-offs:**\n",
    "\n",
    "**latent_dim = 2:**\n",
    "- ‚úÖ Easy to visualize (2D plot)\n",
    "- ‚úÖ Very compressed\n",
    "- ‚ùå Poor reconstruction (too much information loss)\n",
    "- ‚ùå Limited generation diversity\n",
    "- **Use:** Visualization, understanding structure\n",
    "\n",
    "**latent_dim = 32 (our choice):**\n",
    "- ‚úÖ Good reconstruction quality\n",
    "- ‚úÖ Reasonable compression\n",
    "- ‚úÖ Good generation\n",
    "- **Use:** Balanced performance\n",
    "\n",
    "**latent_dim = 128:**\n",
    "- ‚úÖ Excellent reconstruction\n",
    "- ‚úÖ Captures fine details\n",
    "- ‚ùå Less compression\n",
    "- ‚ùå Harder to train (more parameters)\n",
    "- **Use:** When quality > compression\n",
    "\n",
    "**General Rule:**\n",
    "- Larger latent dim = Better reconstruction, less compression\n",
    "- Smaller latent dim = Worse reconstruction, more compression\n",
    "- Sweet spot depends on application!\n",
    "\n",
    "**Modern Models:**\n",
    "- Stable Diffusion: 4-channel latent (64√ó64)\n",
    "- DALL-E: Uses CLIP embeddings (512-dim)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**You just learned:**\n",
    "\n",
    "### 1. **Generative vs Discriminative AI**\n",
    "   - ‚úÖ Discriminative: Classify/predict (P(y|x))\n",
    "   - ‚úÖ Generative: Create new data (P(x))\n",
    "   - ‚úÖ Generative AI is behind DALL-E, ChatGPT, Sora\n",
    "   - **Key insight:** Generation requires modeling data distribution\n",
    "\n",
    "### 2. **Autoencoders**\n",
    "   - ‚úÖ Encoder: Compress input ‚Üí latent representation\n",
    "   - ‚úÖ Decoder: Reconstruct from latent\n",
    "   - ‚úÖ Applications: Denoising, compression, feature learning\n",
    "   - **Limitation:** Can't generate new samples reliably\n",
    "\n",
    "### 3. **Variational Autoencoders (VAEs)**\n",
    "   - ‚úÖ Probabilistic latent space (Œº, œÉ¬≤)\n",
    "   - ‚úÖ Reparameterization trick: z = Œº + œÉ * Œµ\n",
    "   - ‚úÖ KL Divergence: Regularizes latent space\n",
    "   - ‚úÖ Enables generation from random sampling!\n",
    "   - **Key innovation:** Smooth, continuous latent space\n",
    "\n",
    "### 4. **Real Applications (2024-2025)**\n",
    "   - üé® **Image Generation:** Stable Diffusion uses VAE\n",
    "   - üì∏ **Denoising:** Smartphone cameras, medical imaging\n",
    "   - üé¨ **Restoration:** Film enhancement, upscaling\n",
    "   - üß¨ **Drug Discovery:** Generate molecular structures\n",
    "   - **Impact:** Foundation for modern generative AI\n",
    "\n",
    "### üåü Connections to Modern AI:\n",
    "\n",
    "**How VAEs relate to 2024-2025 AI:**\n",
    "\n",
    "1. **Stable Diffusion:**\n",
    "   - VAE encoder: Image ‚Üí latent space\n",
    "   - Diffusion: Refine in latent space\n",
    "   - VAE decoder: Latent ‚Üí high-res image\n",
    "\n",
    "2. **DALL-E:**\n",
    "   - Uses VQ-VAE (Vector Quantized VAE)\n",
    "   - Compresses images to discrete tokens\n",
    "   - Transformer generates tokens\n",
    "   - Decoder: Tokens ‚Üí image\n",
    "\n",
    "3. **ChatGPT:**\n",
    "   - Latent space = embedding space\n",
    "   - Generation = sampling from language manifold\n",
    "   - Attention = sophisticated encoder-decoder\n",
    "\n",
    "### üìä Comparison:\n",
    "\n",
    "| Feature | Autoencoder | VAE |\n",
    "|---------|------------|-----|\n",
    "| Latent space | Deterministic | Probabilistic |\n",
    "| Generation | ‚ùå Poor | ‚úÖ Good |\n",
    "| Reconstruction | ‚úÖ Excellent | ‚úÖ Good |\n",
    "| Training | Simple | More complex |\n",
    "| Loss | MSE only | MSE + KL Div |\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now understand:\n",
    "- How generative AI works at a fundamental level\n",
    "- The building blocks of DALL-E, Stable Diffusion, and modern AI\n",
    "- How to compress and generate data with neural networks\n",
    "\n",
    "**Next:** We'll learn GANs - an even more powerful generative approach! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "**Practice Exercises:**\n",
    "1. Train VAE on different datasets (Fashion-MNIST, CIFAR-10)\n",
    "2. Experiment with different latent dimensions (2, 8, 64, 128)\n",
    "3. Implement conditional VAE (control what digit to generate)\n",
    "4. Try interpolation between two images\n",
    "5. Build a denoising autoencoder for audio\n",
    "\n",
    "**Coming Next:**\n",
    "- **Day 2:** GANs (Generative Adversarial Networks) - Generator vs Discriminator!\n",
    "- **Day 3:** Advanced Models - Diffusion, StyleGAN, DALL-E concepts\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Deep Dive Resources:**\n",
    "- \"Auto-Encoding Variational Bayes\" (Kingma & Welling, 2013)\n",
    "- \"Tutorial on Variational Autoencoders\" (Carl Doersch)\n",
    "- Stanford CS231n Lecture on Generative Models\n",
    "- Fast.ai: Deep Learning for Coders (Part 2)\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: VAEs are the foundation of modern generative AI. Understanding them unlocks understanding DALL-E, Stable Diffusion, and more!* üåü\n",
    "\n",
    "**üéØ You now know how machines learn to create!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
